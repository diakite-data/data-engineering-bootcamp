[
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir Polars, la bibliothÃ¨que DataFrame ultra-rapide qui rÃ©volutionne le traitement de donnÃ©es en Python. Tu apprendras pourquoi Polars surpasse Pandas, comment exploiter son moteur dâ€™exÃ©cution lazy, et comment construire des pipelines ETL performants !",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#prÃ©requis",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#prÃ©requis",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nConnaissances de base en Python\n\n\nâœ… Requis\nAvoir utilisÃ© Pandas (mÃªme basiquement)\n\n\nğŸ’¡ RecommandÃ©\nAvoir suivi les modules prÃ©cÃ©dents du bootcamp",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#objectifs-du-module",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#objectifs-du-module",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre pourquoi Polars est 5-100x plus rapide que Pandas\nMaÃ®triser lâ€™architecture columnar et le format Apache Arrow\nUtiliser les expressions Polars pour des transformations efficaces\nExploiter lâ€™exÃ©cution Lazy pour des pipelines optimisÃ©s\nMigrer du code Pandas vers Polars\nConstruire des pipelines ETL performants en production",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#polars-vs-pandas-pourquoi-changer",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#polars-vs-pandas-pourquoi-changer",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸš€ 1. Polars vs Pandas : Pourquoi changer ?",
    "text": "ğŸš€ 1. Polars vs Pandas : Pourquoi changer ?\nAvant de plonger dans Polars, comprenons pourquoi cette bibliothÃ¨que existe et ce quâ€™elle apporte.\n\n1.1 Les limitations de Pandas\nPandas est formidable pour lâ€™exploration de donnÃ©es, mais il a des limitations structurelles :\n\n\n\n\n\n\n\n\nLimitation\nExplication\nImpact\n\n\n\n\nSingle-threaded\nLe GIL Python bloque le parallÃ©lisme\nNâ€™utilise quâ€™1 CPU\n\n\nRow-based en mÃ©moire\nDonnÃ©es stockÃ©es par ligne\nCache CPU inefficace\n\n\nEager execution\nChaque opÃ©ration sâ€™exÃ©cute immÃ©diatement\nPas dâ€™optimisation globale\n\n\nCopies frÃ©quentes\nBeaucoup dâ€™opÃ©rations copient les donnÃ©es\nRAM x2 ou x3\n\n\nMÃ©moire gourmande\n~5-10x la taille du fichier\nLimite les gros datasets\n\n\n\n\n\n1.2 Les forces de Polars\n\n\n\nAspect\nPandas\nPolars\n\n\n\n\nBackend\nNumPy (C)\nRust ğŸ¦€\n\n\nThreading\nSingle (GIL)\nMulti-threaded\n\n\nMÃ©moire\nRow-based\nColumnar (Arrow)\n\n\nExecution\nEager only\nEager + Lazy\n\n\nVitesse\nBaseline\n5-100x plus rapide\n\n\nOut-of-core\nâŒ\nâœ… (streaming)\n\n\nOptimiseur\nâŒ\nâœ… Query planner\n\n\n\n\nğŸ’¡ En rÃ©sumÃ© : Polars est conÃ§u dÃ¨s le dÃ©part pour la performance et les gros volumes, lÃ  oÃ¹ Pandas a Ã©tÃ© conÃ§u pour lâ€™exploration interactive.\n\n\nâ„¹ï¸ Le savais-tu ?\nPolars a Ã©tÃ© crÃ©Ã© en 2020 par Ritchie Vink, un ingÃ©nieur nÃ©erlandais frustrÃ© par la lenteur de Pandas.\nLe nom â€œPolarsâ€ fait rÃ©fÃ©rence Ã  lâ€™ours polaire (ğŸ»â€â„ï¸) â€” un clin dâ€™Å“il Ã  Pandas (ğŸ¼) tout en Ã©tant plus rapide et adaptÃ© aux environnements â€œfroidsâ€ (haute performance).\nPolars est Ã©crit en Rust, un langage rÃ©putÃ© pour sa vitesse et sa sÃ©curitÃ© mÃ©moire.\nğŸ“– Site officiel Polars\n\n\n\n1.3 Benchmark concret\nComparons Pandas et Polars sur une opÃ©ration simple : lire un CSV et faire une agrÃ©gation.\n\n\nCode\n# CrÃ©er un fichier de test\nimport random\nimport csv\nimport os\n\nos.makedirs(\"data\", exist_ok=True)\n\n# GÃ©nÃ©rer 500K lignes\ncategories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\nn_rows = 500_000\n\nwith open(\"data/benchmark.csv\", \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"id\", \"category\", \"amount\", \"quantity\"])\n    for i in range(n_rows):\n        writer.writerow([\n            i,\n            random.choice(categories),\n            round(random.uniform(10, 1000), 2),\n            random.randint(1, 100)\n        ])\n\nprint(f\"âœ… Fichier crÃ©Ã© : data/benchmark.csv ({n_rows:,} lignes)\")\n\n\n\n\nCode\nimport pandas as pd\nimport time\n\n# Benchmark Pandas\nstart = time.time()\n\ndf_pandas = pd.read_csv(\"data/benchmark.csv\")\nresult_pandas = (\n    df_pandas\n    .groupby(\"category\")\n    .agg({\"amount\": \"sum\", \"quantity\": \"mean\"})\n    .reset_index()\n)\n\npandas_time = time.time() - start\nprint(f\"ğŸ¼ Pandas : {pandas_time:.3f} secondes\")\nprint(result_pandas)\n\n\n\n\nCode\nimport polars as pl\nimport time\n\n# Benchmark Polars (Eager)\nstart = time.time()\n\ndf_polars = pl.read_csv(\"data/benchmark.csv\")\nresult_polars = (\n    df_polars\n    .group_by(\"category\")\n    .agg(\n        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n    )\n)\n\npolars_time = time.time() - start\nprint(f\"ğŸ»â€â„ï¸ Polars : {polars_time:.3f} secondes\")\nprint(f\"âš¡ Polars est {pandas_time/polars_time:.1f}x plus rapide !\")\nprint(result_polars)\n\n\n\n\nCode\n# Benchmark Polars (Lazy) - encore plus rapide !\nstart = time.time()\n\nresult_lazy = (\n    pl.scan_csv(\"data/benchmark.csv\")  # Lazy !\n    .group_by(\"category\")\n    .agg(\n        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n    )\n    .collect()  # ExÃ©cution optimisÃ©e\n)\n\nlazy_time = time.time() - start\nprint(f\"ğŸš€ Polars Lazy : {lazy_time:.3f} secondes\")\nprint(f\"âš¡ Polars Lazy est {pandas_time/lazy_time:.1f}x plus rapide que Pandas !\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#comprendre-larchitecture-de-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#comprendre-larchitecture-de-polars",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ—ï¸ 2. Comprendre lâ€™architecture de Polars",
    "text": "ğŸ—ï¸ 2. Comprendre lâ€™architecture de Polars\nPour bien utiliser Polars, il faut comprendre pourquoi il est si rapide.\n\n2.1 Columnar vs Row-based\nROW-BASED (Pandas)                    COLUMNAR (Polars)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id  â”‚ name â”‚ age â”‚                  â”‚ id:   [1, 2, 3]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  1  â”‚ Ana  â”‚ 25  â”‚  â† Ligne 1       â”‚ name: [A, B, C]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  2  â”‚ Bob  â”‚ 30  â”‚  â† Ligne 2       â”‚ age:  [25, 30, 22]â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚  3  â”‚ Cat  â”‚ 22  â”‚  â† Ligne 3              â†‘\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                   Colonnes contiguÃ«s\n        â†‘                              en mÃ©moire\n  Lignes contiguÃ«s\n  en mÃ©moire\nPourquoi columnar est plus rapide ?\n\n\n\nAvantage\nExplication\n\n\n\n\nCache CPU\nDonnÃ©es contiguÃ«s = moins de cache misses\n\n\nSIMD\nOpÃ©rations vectorisÃ©es sur colonnes entiÃ¨res\n\n\nCompression\nColonnes homogÃ¨nes = meilleure compression\n\n\nSÃ©lection\nLire seulement les colonnes nÃ©cessaires\n\n\n\n\n\n2.2 Apache Arrow : le format sous-jacent\nPolars utilise Apache Arrow comme format mÃ©moire :\n\n\n\nAvantage\nDescription\n\n\n\n\nZero-copy\nPartage de donnÃ©es sans copie\n\n\nInteropÃ©rabilitÃ©\nCompatible Spark, DuckDB, PyArrow\n\n\nStandardisÃ©\nFormat ouvert et documentÃ©\n\n\n\n\n\n2.3 Eager vs Lazy execution\n\n\n\n\n\n\n\n\nMode\nDescription\nQuand lâ€™utiliser\n\n\n\n\nEager\nExÃ©cute immÃ©diatement chaque opÃ©ration\nExploration, debug, petits datasets\n\n\nLazy\nConstruit un plan, optimise, puis exÃ©cute\nProduction, gros fichiers, pipelines\n\n\n\n# Eager : rÃ©sultat immÃ©diat\ndf = pl.read_csv(\"data.csv\")        # Lit maintenant\ndf = df.filter(pl.col(\"x\") &gt; 5)     # Filtre maintenant\n\n# Lazy : plan d'exÃ©cution\nlf = pl.scan_csv(\"data.csv\")        # CrÃ©e un plan\nlf = lf.filter(pl.col(\"x\") &gt; 5)     # Ajoute au plan\ndf = lf.collect()                    # ExÃ©cute tout (optimisÃ©)\n\n\n2.4 Query Optimizer\nLe Query Optimizer de Polars applique automatiquement des optimisations :\n\n\n\nOptimisation\nDescription\n\n\n\n\nPredicate pushdown\nFiltres appliquÃ©s le plus tÃ´t possible\n\n\nProjection pruning\nColonnes inutiles jamais lues\n\n\nCommon subexpression\nCalculs redondants factorisÃ©s\n\n\nParallelization\nOpÃ©rations distribuÃ©es sur tous les CPUs\n\n\n\nPLAN ORIGINAL                    PLAN OPTIMISÃ‰\nâ•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nscan_csv(all cols)               scan_csv(only needed cols)\n      â”‚                                â”‚\n      â–¼                                â–¼\nwith_columns(...)                filter(amount &gt; 100)  â† Pushdown!\n      â”‚                                â”‚\n      â–¼                                â–¼\nfilter(amount &gt; 100)             with_columns(...)\n      â”‚                                â”‚\n      â–¼                                â–¼\n   result                           result",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#installation-configuration",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#installation-configuration",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ’» 3. Installation & Configuration",
    "text": "ğŸ’» 3. Installation & Configuration\n\nInstallation\n# Installation de base\npip install polars\n\n# Avec toutes les features (recommandÃ©)\npip install 'polars[all]'\n\n# Features spÃ©cifiques\npip install 'polars[pyarrow,pandas,numpy,fsspec]'\n\n\nVÃ©rification\n\n\nCode\nimport polars as pl\n\nprint(f\"âœ… Polars version : {pl.__version__}\")\n\n# Configuration de l'affichage\npl.Config.set_tbl_rows(10)           # Lignes affichÃ©es\npl.Config.set_tbl_cols(12)           # Colonnes affichÃ©es\npl.Config.set_fmt_str_lengths(50)    # Longueur des strings\n\n# Voir le nombre de threads utilisÃ©s\nprint(f\"ğŸ”§ Threads disponibles : {pl.thread_pool_size()}\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#charger-exporter-des-donnÃ©es",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#charger-exporter-des-donnÃ©es",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ“‚ 4. Charger & Exporter des donnÃ©es",
    "text": "ğŸ“‚ 4. Charger & Exporter des donnÃ©es\n\n4.1 Formats supportÃ©s\n\n\n\n\n\n\n\n\n\nFormat\nRead (Eager)\nScan (Lazy)\nWrite\n\n\n\n\nCSV\nread_csv()\nscan_csv()\nwrite_csv()\n\n\nParquet\nread_parquet()\nscan_parquet()\nwrite_parquet()\n\n\nJSON\nread_json()\nscan_ndjson()\nwrite_json()\n\n\nExcel\nread_excel()\nâŒ\nwrite_excel()\n\n\nDatabase\nread_database()\nâŒ\nâŒ\n\n\nIPC/Feather\nread_ipc()\nscan_ipc()\nwrite_ipc()\n\n\n\n\n\n4.2 Lecture Eager vs Lazy\n\n\nCode\nimport polars as pl\n\n# ============ EAGER (tout en mÃ©moire) ============\ndf = pl.read_csv(\"data/benchmark.csv\")\nprint(\"Eager - Type:\", type(df))\nprint(df.head(3))\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# ============ LAZY (plan d'exÃ©cution) ============\nlf = pl.scan_csv(\"data/benchmark.csv\")\nprint(\"Lazy - Type:\", type(lf))\nprint(lf)  # Affiche le plan, pas les donnÃ©es\n\n\n\n\nCode\n# CrÃ©er plusieurs fichiers pour l'exemple\nimport os\nos.makedirs(\"data/multi\", exist_ok=True)\n\nfor i in range(3):\n    pl.DataFrame({\n        \"id\": range(i*100, (i+1)*100),\n        \"value\": [i*10 + j for j in range(100)]\n    }).write_csv(f\"data/multi/file_{i}.csv\")\n\nprint(\"âœ… Fichiers crÃ©Ã©s\")\n\n# Lire plusieurs fichiers avec glob pattern\nlf = pl.scan_csv(\"data/multi/*.csv\")\nprint(f\"\\nNombre de lignes : {lf.collect().height}\")\n\n\n\n\nCode\n# Ã‰criture\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"Paris\", \"Lyon\", \"Marseille\"]\n})\n\n# CSV\ndf.write_csv(\"data/output.csv\")\n\n# Parquet (recommandÃ© pour la production)\ndf.write_parquet(\"data/output.parquet\")\n\n# JSON\ndf.write_json(\"data/output.json\")\n\nprint(\"âœ… Fichiers exportÃ©s\")\n\n# VÃ©rifier avec Parquet\ndf_parquet = pl.read_parquet(\"data/output.parquet\")\nprint(df_parquet)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#expressions-polars-le-cÅ“ur-du-moteur",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#expressions-polars-le-cÅ“ur-du-moteur",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ”¥ 5. Expressions Polars â€” Le cÅ“ur du moteur",
    "text": "ğŸ”¥ 5. Expressions Polars â€” Le cÅ“ur du moteur\n\nğŸ§  Les expressions sont ce qui rend Polars si puissant. Câ€™est un changement de paradigme par rapport Ã  Pandas.\n\n\n5.1 Philosophie : tout est expression\n# âŒ Pandas : opÃ©rations sur colonnes\ndf[\"new_col\"] = df[\"a\"] + df[\"b\"]\n\n# âœ… Polars : expressions\ndf.with_columns(\n    (pl.col(\"a\") + pl.col(\"b\")).alias(\"new_col\")\n)\n\n\n5.2 Expressions de base\n\n\n\nExpression\nDescription\nExemple\n\n\n\n\npl.col(\"x\")\nSÃ©lectionner une colonne\npl.col(\"amount\")\n\n\npl.col(\"x\", \"y\")\nPlusieurs colonnes\npl.col(\"a\", \"b\", \"c\")\n\n\npl.all()\nToutes les colonnes\ndf.select(pl.all())\n\n\npl.exclude(\"x\")\nToutes sauf x\npl.exclude(\"id\")\n\n\npl.lit(42)\nValeur littÃ©rale\npl.lit(\"constant\")\n\n\npl.col(\"*\")\nToutes (autre syntaxe)\npl.col(\"*\")\n\n\n\n\n\nCode\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"age\": [25, 30, 35, 28],\n    \"salary\": [50000, 60000, 75000, 55000],\n    \"department\": [\"IT\", \"HR\", \"IT\", \"Finance\"]\n})\n\nprint(\"DataFrame original :\")\nprint(df)\n\n# SÃ©lectionner des colonnes avec expressions\nprint(\"\\nSÃ©lection avec expressions :\")\nprint(\n    df.select(\n        pl.col(\"name\"),\n        pl.col(\"salary\") / 12,  # Salaire mensuel\n    )\n)\n\n\n\n\nCode\n# Expressions conditionnelles : when/then/otherwise\nprint(\"Expressions conditionnelles :\")\nprint(\n    df.with_columns(\n        pl.when(pl.col(\"age\") &gt;= 30)\n          .then(pl.lit(\"Senior\"))\n          .otherwise(pl.lit(\"Junior\"))\n          .alias(\"level\"),\n        \n        pl.when(pl.col(\"salary\") &gt; 60000)\n          .then(pl.lit(\"High\"))\n          .when(pl.col(\"salary\") &gt; 50000)\n          .then(pl.lit(\"Medium\"))\n          .otherwise(pl.lit(\"Low\"))\n          .alias(\"salary_band\")\n    )\n)\n\n\n\n\nCode\n# ChaÃ®nage d'expressions\nprint(\"ChaÃ®nage d'expressions :\")\nprint(\n    df.with_columns(\n        # String operations\n        pl.col(\"name\").str.to_uppercase().alias(\"NAME_UPPER\"),\n        pl.col(\"name\").str.len_chars().alias(\"name_length\"),\n        \n        # Math operations\n        (pl.col(\"salary\") * 1.1).round(2).alias(\"salary_raised\"),\n    )\n)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#manipulations-de-donnÃ©es-essentielles",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#manipulations-de-donnÃ©es-essentielles",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ› ï¸ 6. Manipulations de donnÃ©es essentielles",
    "text": "ğŸ› ï¸ 6. Manipulations de donnÃ©es essentielles\n\n6.1 SÃ©lection de colonnes\n\n\nCode\ndf = pl.read_csv(\"data/benchmark.csv\")\n\n# SÃ©lection simple\nprint(\"SÃ©lection simple :\")\nprint(df.select(\"category\", \"amount\").head(3))\n\n# SÃ©lection avec transformation\nprint(\"\\nSÃ©lection avec transformation :\")\nprint(\n    df.select(\n        pl.col(\"category\"),\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    ).head(3)\n)\n\n# SÃ©lection par type\nprint(\"\\nColonnes numÃ©riques uniquement :\")\nprint(df.select(pl.col(pl.Float64, pl.Int64)).head(3))\n\n\n\n\n6.2 Filtrage\n\n\nCode\n# Filtre simple\nprint(\"Filtre simple (amount &gt; 500) :\")\nprint(df.filter(pl.col(\"amount\") &gt; 500).head(3))\n\n# Filtres multiples (AND)\nprint(\"\\nFiltres multiples (AND) :\")\nprint(\n    df.filter(\n        (pl.col(\"amount\") &gt; 500) & \n        (pl.col(\"category\") == \"Electronics\")\n    ).head(3)\n)\n\n# Filtres multiples (OR)\nprint(\"\\nFiltres multiples (OR) :\")\nprint(\n    df.filter(\n        (pl.col(\"category\") == \"Electronics\") | \n        (pl.col(\"category\") == \"Books\")\n    ).head(3)\n)\n\n# Filtre avec is_in\nprint(\"\\nFiltre avec is_in :\")\nprint(\n    df.filter(\n        pl.col(\"category\").is_in([\"Electronics\", \"Books\"])\n    ).head(3)\n)\n\n\n\n\n6.3 Ajout / modification de colonnes\n\n\nCode\nprint(\"Ajout de colonnes :\")\nresult = df.with_columns(\n    # Calcul\n    (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n    \n    # Valeur constante\n    pl.lit(\"USD\").alias(\"currency\"),\n    \n    # Transformation de colonne existante\n    pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\"),\n    \n    # Conditionnel\n    pl.when(pl.col(\"amount\") &gt; 500)\n      .then(pl.lit(\"High\"))\n      .otherwise(pl.lit(\"Low\"))\n      .alias(\"amount_level\")\n)\n\nprint(result.head(5))\n\n\n\n\n6.4 GroupBy & Aggregations\n\n\nCode\nprint(\"GroupBy simple :\")\nprint(\n    df.group_by(\"category\").agg(\n        pl.col(\"amount\").sum().alias(\"total_amount\"),\n        pl.col(\"amount\").mean().alias(\"avg_amount\"),\n        pl.col(\"amount\").max().alias(\"max_amount\"),\n        pl.len().alias(\"count\")\n    ).sort(\"total_amount\", descending=True)\n)\n\n\n\n\nCode\n# Aggregations avancÃ©es\nprint(\"Aggregations avancÃ©es :\")\nprint(\n    df.group_by(\"category\").agg(\n        # Statistiques\n        pl.col(\"amount\").mean().alias(\"avg\"),\n        pl.col(\"amount\").std().alias(\"std\"),\n        pl.col(\"amount\").quantile(0.5).alias(\"median\"),\n        \n        # Comptages conditionnels\n        (pl.col(\"amount\") &gt; 500).sum().alias(\"high_amount_count\"),\n        \n        # Premier/Dernier\n        pl.col(\"amount\").first().alias(\"first_amount\"),\n    )\n)\n\n\n\n\n6.5 Tri, renommage, suppression\n\n\nCode\n# Tri\nprint(\"Tri dÃ©croissant :\")\nprint(df.sort(\"amount\", descending=True).head(3))\n\n# Tri multiple\nprint(\"\\nTri multiple :\")\nprint(df.sort([\"category\", \"amount\"], descending=[True, False]).head(5))\n\n# Renommer\nprint(\"\\nRenommer :\")\nprint(df.rename({\"amount\": \"montant\", \"quantity\": \"quantite\"}).head(2))\n\n# Supprimer des colonnes\nprint(\"\\nSupprimer colonnes :\")\nprint(df.drop(\"id\").head(2))\n\n\n\n\n6.6 Joins\n\n\nCode\n# CrÃ©er des DataFrames pour les joins\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4],\n    \"customer_id\": [101, 102, 101, 103],\n    \"amount\": [100, 200, 150, 300]\n})\n\ncustomers = pl.DataFrame({\n    \"customer_id\": [101, 102, 104],\n    \"name\": [\"Alice\", \"Bob\", \"Diana\"]\n})\n\nprint(\"Orders:\", orders)\nprint(\"\\nCustomers:\", customers)\n\n# Inner join\nprint(\"\\nInner Join :\")\nprint(orders.join(customers, on=\"customer_id\", how=\"inner\"))\n\n# Left join\nprint(\"\\nLeft Join :\")\nprint(orders.join(customers, on=\"customer_id\", how=\"left\"))\n\n\n\n\n6.7 Dates et timestamps\n\n\nCode\nfrom datetime import datetime, date\n\ndf_dates = pl.DataFrame({\n    \"event\": [\"A\", \"B\", \"C\", \"D\"],\n    \"timestamp\": [\n        datetime(2024, 1, 15, 10, 30),\n        datetime(2024, 3, 20, 14, 45),\n        datetime(2024, 6, 5, 9, 0),\n        datetime(2024, 12, 25, 18, 30)\n    ]\n})\n\nprint(\"DataFrame avec dates :\")\nprint(df_dates)\n\nprint(\"\\nExtractions de dates :\")\nprint(\n    df_dates.with_columns(\n        pl.col(\"timestamp\").dt.year().alias(\"year\"),\n        pl.col(\"timestamp\").dt.month().alias(\"month\"),\n        pl.col(\"timestamp\").dt.day().alias(\"day\"),\n        pl.col(\"timestamp\").dt.hour().alias(\"hour\"),\n        pl.col(\"timestamp\").dt.weekday().alias(\"weekday\"),\n        pl.col(\"timestamp\").dt.strftime(\"%Y-%m-%d\").alias(\"date_str\"),\n    )\n)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#lazy-execution-le-game-changer",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#lazy-execution-le-game-changer",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸš€ 7. Lazy Execution â€” Le Game Changer",
    "text": "ğŸš€ 7. Lazy Execution â€” Le Game Changer\n\nğŸ¯ Câ€™est ce qui rend Polars adaptÃ© Ã  la production et aux gros volumes.\n\n\n7.1 CrÃ©er un LazyFrame\n\n\nCode\n# Depuis un fichier (recommandÃ©)\nlf = pl.scan_csv(\"data/benchmark.csv\")\nprint(\"Type:\", type(lf))\nprint(\"\\nLazyFrame (pas encore exÃ©cutÃ©) :\")\nprint(lf)\n\n\n\n\nCode\n# Depuis un DataFrame existant\ndf = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nlf = df.lazy()\nprint(\"Converti en LazyFrame:\", type(lf))\n\n\n\n\n7.2 Construire le pipeline\n\n\nCode\n# Pipeline complet en Lazy\npipeline = (\n    pl.scan_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n        pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\")\n    )\n    .group_by(\"CATEGORY\")\n    .agg(\n        pl.col(\"total\").sum().alias(\"total_revenue\"),\n        pl.len().alias(\"transaction_count\")\n    )\n    .sort(\"total_revenue\", descending=True)\n)\n\nprint(\"Pipeline dÃ©fini (pas encore exÃ©cutÃ©) :\")\nprint(pipeline)\nprint(\"\\nâš ï¸ Rien n'a Ã©tÃ© lu ou calculÃ© !\")\n\n\n\n\n7.3 ExÃ©cuter avec .collect()\n\n\nCode\nimport time\n\nstart = time.time()\nresult = pipeline.collect()  # MAINTENANT Ã§a s'exÃ©cute\nprint(f\"â±ï¸ Temps d'exÃ©cution : {time.time() - start:.3f}s\")\nprint(\"\\nRÃ©sultat :\")\nprint(result)\n\n\n\n\n7.4 Voir le plan dâ€™exÃ©cution\n\n\nCode\n# Plan logique (ce que tu as Ã©crit)\nprint(\"=== PLAN LOGIQUE ===\")\nprint(pipeline.explain())\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Plan optimisÃ© (ce que Polars exÃ©cute rÃ©ellement)\nprint(\"=== PLAN OPTIMISÃ‰ ===\")\nprint(pipeline.explain(optimized=True))\n\n\n\n\n7.5 Streaming pour fichiers massifs\nLe mode streaming permet de traiter des fichiers plus grands que la RAM.\n\n\nCode\n# Streaming : traite par chunks\nresult_streaming = (\n    pl.scan_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"category\") == \"Electronics\")\n    .group_by(\"category\")\n    .agg(pl.col(\"amount\").sum())\n    .collect(streaming=True)  # Mode streaming\n)\n\nprint(\"RÃ©sultat avec streaming :\")\nprint(result_streaming)\n\n\n\n\nğŸ–¼ï¸ SchÃ©ma : Pipeline Lazy\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  scan_csv() â”‚â”€â”€â”€â”€â–¶â”‚  filter()   â”‚â”€â”€â”€â”€â–¶â”‚with_columns()â”‚â”€â”€â”€â–¶â”‚  group_by() â”‚\nâ”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                                   â–¼\n                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                            â”‚  collect()  â”‚\n                                                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                    â”‚    Query Optimizer          â”‚\n                                                    â”‚  â€¢ Predicate pushdown       â”‚\n                                                    â”‚  â€¢ Column pruning           â”‚\n                                                    â”‚  â€¢ Parallel execution       â”‚\n                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                                   â–¼\n                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                            â”‚  DataFrame  â”‚\n                                                            â”‚  (rÃ©sultat) â”‚\n                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#migration-pandas-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#migration-pandas-polars",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ”„ 8. Migration Pandas â†’ Polars",
    "text": "ğŸ”„ 8. Migration Pandas â†’ Polars\n\n8.1 Tableau de correspondance\n\n\n\n\n\n\n\n\nOpÃ©ration\nPandas\nPolars\n\n\n\n\nLire CSV\npd.read_csv()\npl.read_csv() / pl.scan_csv()\n\n\nLire Parquet\npd.read_parquet()\npl.read_parquet() / pl.scan_parquet()\n\n\nSÃ©lection colonne\ndf[\"col\"]\ndf.select(\"col\")\n\n\nPlusieurs colonnes\ndf[[\"a\", \"b\"]]\ndf.select(\"a\", \"b\")\n\n\nFiltre\ndf[df[\"x\"] &gt; 5]\ndf.filter(pl.col(\"x\") &gt; 5)\n\n\nNouvelle colonne\ndf[\"new\"] = df[\"a\"] + 1\ndf.with_columns((pl.col(\"a\") + 1).alias(\"new\"))\n\n\nGroupBy\ndf.groupby(\"x\").agg({\"y\": \"sum\"})\ndf.group_by(\"x\").agg(pl.col(\"y\").sum())\n\n\nTri\ndf.sort_values(\"x\")\ndf.sort(\"x\")\n\n\nRenommer\ndf.rename(columns={\"a\": \"b\"})\ndf.rename({\"a\": \"b\"})\n\n\nDrop\ndf.drop(columns=[\"x\"])\ndf.drop(\"x\")\n\n\nReset index\ndf.reset_index()\nN/A (pas dâ€™index)\n\n\nApply\ndf.apply(func)\ndf.map_rows(func) âš ï¸ Ã©viter\n\n\n\n\n\n8.2 InteropÃ©rabilitÃ©\n\n\nCode\nimport pandas as pd\nimport polars as pl\n\n# CrÃ©er un DataFrame Pandas\npandas_df = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 30]\n})\n\n# Pandas â†’ Polars\npolars_df = pl.from_pandas(pandas_df)\nprint(\"Pandas â†’ Polars :\")\nprint(polars_df)\n\n# Polars â†’ Pandas\nback_to_pandas = polars_df.to_pandas()\nprint(\"\\nPolars â†’ Pandas :\")\nprint(back_to_pandas)\n\n\n\n\n8.3 DiffÃ©rences clÃ©s Ã  retenir\n\n\n\n\n\n\n\n\nAspect\nPandas\nPolars\n\n\n\n\nIndex\nâœ… Index par dÃ©faut\nâŒ Pas dâ€™index\n\n\nModification in-place\nâœ… inplace=True\nâŒ Toujours immutable\n\n\nTypage\nFlexible\nStrict\n\n\nNaN vs null\nNaN (float)\nnull (natif)\n\n\nChaÃ®nage\nLimitÃ©\nNaturel et optimisÃ©\n\n\n\n\n\nCode\n# Exemple de migration complÃ¨te\n\n# ============ VERSION PANDAS ============\n# df = pd.read_csv(\"data.csv\")\n# df = df[df[\"amount\"] &gt; 100]\n# df[\"total\"] = df[\"amount\"] * df[\"quantity\"]\n# result = df.groupby(\"category\").agg({\"total\": \"sum\"}).reset_index()\n\n# ============ VERSION POLARS (Eager) ============\nresult_eager = (\n    pl.read_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    )\n    .group_by(\"category\")\n    .agg(pl.col(\"total\").sum())\n)\n\n# ============ VERSION POLARS (Lazy - recommandÃ©) ============\nresult_lazy = (\n    pl.scan_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    )\n    .group_by(\"category\")\n    .agg(pl.col(\"total\").sum())\n    .collect()\n)\n\nprint(\"RÃ©sultat :\")\nprint(result_lazy)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "âš ï¸ 9. Bonnes pratiques & Erreurs frÃ©quentes",
    "text": "âš ï¸ 9. Bonnes pratiques & Erreurs frÃ©quentes\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nProblÃ¨me\nSolution\n\n\n\n\n.apply() sur chaque ligne\nExtrÃªmement lent\nUtiliser expressions natives\n\n\ndf[\"col\"] style Pandas\nNe fonctionne pas\ndf.select(\"col\") ou pl.col()\n\n\nread_csv() sur 100 fichiers\nLent, beaucoup de RAM\nscan_csv(\"*.csv\") + glob\n\n\nOublier .collect()\nPas dâ€™exÃ©cution\nToujours .collect() Ã  la fin\n\n\nMÃ©langer eager/lazy\nErreurs de type\nRester cohÃ©rent dans le pipeline\n\n\nPas dâ€™alias sur les expressions\nNoms de colonnes illisibles\nToujours .alias(\"nom\")\n\n\n\n\n\nCode\n# âŒ MAUVAIS : apply() ligne par ligne\n# df.map_rows(lambda row: row[0] * 2)  # TRÃˆS LENT\n\n# âœ… BON : expression native\ndf = pl.DataFrame({\"x\": [1, 2, 3]})\nresult = df.with_columns((pl.col(\"x\") * 2).alias(\"x_doubled\"))\nprint(\"âœ… Expression native :\")\nprint(result)\n\n\n\n\nCode\n# âŒ MAUVAIS : read_csv sur plusieurs fichiers sÃ©parÃ©ment\n# dfs = [pl.read_csv(f) for f in files]  # Pas optimisÃ©\n\n# âœ… BON : scan_csv avec glob\nlf = pl.scan_csv(\"data/multi/*.csv\")\nprint(\"âœ… Scan avec glob :\")\nprint(lf.collect())\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nUtiliser Lazy en production\nOptimisation automatique\n\n\nPrÃ©fÃ©rer Parquet\n10x plus rapide que CSV, compression\n\n\nChaÃ®ner les expressions\nPlus lisible, plus optimisÃ©\n\n\nÃ‰viter .apply()\nUtiliser expressions natives\n\n\nProfiler avec .explain()\nComprendre lâ€™exÃ©cution\n\n\nToujours .alias()\nNoms de colonnes explicites\n\n\nscan_* pour gros fichiers\nLazy = optimisations\n\n\nStreaming pour &gt; RAM\ncollect(streaming=True)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#quiz-de-fin-de-module",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quel est le principal avantage de lâ€™architecture columnar de Polars ?\n\nPlus facile Ã  lire pour les humains\n\nOpÃ©rations vectorisÃ©es plus rapides et meilleure utilisation du cache CPU\n\nCompatible avec Excel\n\nUtilise moins de colonnes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le stockage columnar permet des opÃ©rations vectorisÃ©es (SIMD) et une meilleure utilisation du cache CPU car les donnÃ©es dâ€™une colonne sont contiguÃ«s en mÃ©moire.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre pl.read_csv() et pl.scan_csv() ?\n\nread_csv est plus rapide\n\nscan_csv crÃ©e un LazyFrame et permet lâ€™optimisation\n\nscan_csv ne supporte pas les gros fichiers\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” scan_csv crÃ©e un LazyFrame (plan dâ€™exÃ©cution) qui sera optimisÃ© avant exÃ©cution, tandis que read_csv charge immÃ©diatement tout en mÃ©moire.\n\n\n\n\nâ“ Q3. Comment ajouter une nouvelle colonne en Polars ?\n\ndf[\"new\"] = df[\"old\"] * 2\n\ndf.with_columns((pl.col(\"old\") * 2).alias(\"new\"))\n\ndf.add_column(\"new\", df[\"old\"] * 2)\n\ndf.insert(\"new\", df[\"old\"] * 2)\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En Polars, on utilise with_columns() avec des expressions. La syntaxe df[\"col\"] style Pandas ne fonctionne pas.\n\n\n\n\nâ“ Q4. Que fait le Query Optimizer avec â€œpredicate pushdownâ€ ?\n\nSupprime les colonnes inutiles\n\nApplique les filtres le plus tÃ´t possible dans le pipeline\n\nParallÃ©lise les calculs\n\nCompresse les donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le predicate pushdown dÃ©place les filtres le plus tÃ´t possible, rÃ©duisant ainsi la quantitÃ© de donnÃ©es Ã  traiter dans les Ã©tapes suivantes.\n\n\n\n\nâ“ Q5. Quand utiliser .collect() ?\n\nAprÃ¨s chaque opÃ©ration\n\nÃ€ la fin du pipeline Lazy pour dÃ©clencher lâ€™exÃ©cution\n\nPour convertir en Pandas\n\nPour Ã©crire un fichier\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .collect() dÃ©clenche lâ€™exÃ©cution dâ€™un LazyFrame et retourne un DataFrame. Sans .collect(), rien nâ€™est calculÃ©.\n\n\n\n\nâ“ Q6. Pourquoi Ã©viter .apply() en Polars ?\n\nCe nâ€™est pas supportÃ©\n\nCâ€™est lent car Ã§a passe par Python pour chaque ligne\n\nÃ‡a modifie les donnÃ©es en place\n\nÃ‡a consomme trop de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .apply() (ou map_rows) passe par Python pour chaque ligne, perdant tous les avantages du moteur Rust vectorisÃ©. PrÃ©fÃ©rer les expressions natives.\n\n\n\n\nâ“ Q7. Quel format de fichier est recommandÃ© en production avec Polars ?\n\nCSV\n\nJSON\n\nParquet\n\nExcel\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Parquet est columnar (comme Polars), compressÃ©, et supporte les types. Il est 10x+ plus rapide que CSV.\n\n\n\n\nâ“ Q8. Comment voir le plan dâ€™exÃ©cution optimisÃ© dâ€™un LazyFrame ?\n\nlf.show_plan()\n\nlf.explain(optimized=True)\n\nlf.describe()\n\nprint(lf)\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .explain(optimized=True) affiche le plan dâ€™exÃ©cution aprÃ¨s les optimisations du Query Optimizer.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#mini-projet-pipeline-etl-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#mini-projet-pipeline-etl-polars",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸš€ Mini-projet : Pipeline ETL Polars",
    "text": "ğŸš€ Mini-projet : Pipeline ETL Polars\n\nğŸ¯ Objectif\nConstruire un pipeline ETL complet en mode Lazy qui : - Lit plusieurs fichiers CSV - Nettoie et transforme les donnÃ©es - AgrÃ¨ge par catÃ©gorie et pÃ©riode - Exporte en Parquet\n\n\nğŸ—ï¸ Architecture\ndata/raw/*.csv\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   scan_csv()    â”‚  Lazy read (glob pattern)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    filter()     â”‚  Nettoyage (nulls, invalides)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ with_columns()  â”‚  Enrichissement\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   group_by()    â”‚  AgrÃ©gation\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    collect()    â”‚  ExÃ©cution optimisÃ©e\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\ndata/processed/output.parquet\n\n\nğŸ“ Structure projet\npolars-etl-project/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”‚   â”œâ”€â”€ transactions_01.csv\nâ”‚   â”‚   â”œâ”€â”€ transactions_02.csv\nâ”‚   â”‚   â””â”€â”€ transactions_03.csv\nâ”‚   â””â”€â”€ processed/\nâ”‚       â””â”€â”€ output.parquet\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ etl_pipeline.py\nâ””â”€â”€ requirements.txt\n\n\nCode\n# Setup : crÃ©er les donnÃ©es de test\nimport polars as pl\nimport random\nfrom datetime import datetime, timedelta\nimport os\n\nos.makedirs(\"data/raw\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\n\ncategories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\nbase_date = datetime(2024, 1, 1)\n\n# GÃ©nÃ©rer 3 fichiers CSV\nfor file_num in range(1, 4):\n    n_rows = 10000\n    data = {\n        \"transaction_id\": range(file_num * 10000, file_num * 10000 + n_rows),\n        \"timestamp\": [base_date + timedelta(days=random.randint(0, 365)) for _ in range(n_rows)],\n        \"category\": [random.choice(categories) for _ in range(n_rows)],\n        \"amount\": [round(random.uniform(-50, 1000), 2) for _ in range(n_rows)],  # Certains nÃ©gatifs !\n        \"quantity\": [random.randint(0, 100) for _ in range(n_rows)],  # Certains Ã  0 !\n        \"customer_id\": [random.randint(1000, 9999) for _ in range(n_rows)]\n    }\n    df = pl.DataFrame(data)\n    df.write_csv(f\"data/raw/transactions_{file_num:02d}.csv\")\n\nprint(\"âœ… DonnÃ©es de test crÃ©Ã©es (3 fichiers x 10,000 lignes)\")\n\n\n\n\nCode\nimport polars as pl\nimport time\n\nprint(\"ğŸš€ DÃ©marrage du pipeline ETL Polars...\\n\")\nstart = time.time()\n\n# ============ PIPELINE LAZY ============\nresult = (\n    # 1. EXTRACT : Lire tous les CSV avec glob pattern\n    pl.scan_csv(\"data/raw/*.csv\")\n    \n    # 2. CLEAN : Filtrer les donnÃ©es invalides\n    .filter(\n        (pl.col(\"amount\") &gt; 0) &           # Montants positifs\n        (pl.col(\"quantity\") &gt; 0) &         # QuantitÃ©s positives\n        (pl.col(\"customer_id\").is_not_null())  # Pas de null\n    )\n    \n    # 3. TRANSFORM : Enrichir les donnÃ©es\n    .with_columns(\n        # Calculer le total\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total_revenue\"),\n        \n        # Extraire annÃ©e et mois\n        pl.col(\"timestamp\").str.to_datetime().dt.year().alias(\"year\"),\n        pl.col(\"timestamp\").str.to_datetime().dt.month().alias(\"month\"),\n        \n        # CatÃ©goriser les montants\n        pl.when(pl.col(\"amount\") &gt; 500)\n          .then(pl.lit(\"High\"))\n          .when(pl.col(\"amount\") &gt; 100)\n          .then(pl.lit(\"Medium\"))\n          .otherwise(pl.lit(\"Low\"))\n          .alias(\"amount_tier\"),\n        \n        # Uppercase category\n        pl.col(\"category\").str.to_uppercase().alias(\"category_upper\")\n    )\n    \n    # 4. AGGREGATE : Par catÃ©gorie et mois\n    .group_by([\"year\", \"month\", \"category_upper\"])\n    .agg(\n        pl.col(\"total_revenue\").sum().alias(\"total_revenue\"),\n        pl.col(\"total_revenue\").mean().alias(\"avg_revenue\"),\n        pl.len().alias(\"transaction_count\"),\n        pl.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n        (pl.col(\"amount_tier\") == \"High\").sum().alias(\"high_value_count\")\n    )\n    \n    # 5. SORT\n    .sort([\"year\", \"month\", \"total_revenue\"], descending=[False, False, True])\n    \n    # 6. EXECUTE\n    .collect()\n)\n\nexecution_time = time.time() - start\nprint(f\"â±ï¸ Pipeline exÃ©cutÃ© en {execution_time:.3f} secondes\")\nprint(f\"ğŸ“Š RÃ©sultat : {result.height} lignes, {result.width} colonnes\\n\")\n\n# Afficher un aperÃ§u\nprint(\"AperÃ§u des rÃ©sultats :\")\nprint(result.head(10))\n\n# 7. EXPORT en Parquet\nresult.write_parquet(\"data/processed/monthly_summary.parquet\")\nprint(\"\\nâœ… RÃ©sultat exportÃ© : data/processed/monthly_summary.parquet\")\n\n\n\n\nCode\n# VÃ©rifier le fichier Parquet\nprint(\"ğŸ“– Lecture du fichier Parquet exportÃ© :\")\ndf_check = pl.read_parquet(\"data/processed/monthly_summary.parquet\")\nprint(df_check.describe())",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nPolars User Guide â€” Documentation complÃ¨te\nPolars API Reference â€” RÃ©fÃ©rence API\nPolars GitHub â€” Code source\n\n\n\nğŸ“– Tutoriels & Articles\n\nPolars vs Pandas Benchmark â€” Benchmarks officiels\nModern Polars â€” Guide approfondi\n\n\n\nğŸ”§ Outils complÃ©mentaires\n\nDuckDB â€” SQL analytique ultra-rapide (compatible Polars)\nPyArrow â€” Format Arrow sous-jacent",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#prochaine-Ã©tape",
    "title": "ğŸ»â€â„ï¸ Polars pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Polars, dÃ©couvrons dâ€™autres outils haute performance pour Python !\nğŸ‘‰ Module suivant : 18_high_performance_python.ipynb â€” Python Haute Performance\nTu vas apprendre : - Dask : parallÃ©lisation de Pandas/NumPy - Vaex : traitement out-of-core - multiprocessing : parallÃ©lisme CPU - concurrent.futures : ThreadPool et ProcessPool - async/await : I/O asynchrone\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Polars pour Data Engineers.\n\n\nCode\n# Nettoyage des fichiers temporaires (optionnel)\nimport shutil\nimport os\n\n# DÃ©commenter pour nettoyer\n# if os.path.exists(\"data\"):\n#     shutil.rmtree(\"data\")\n#     print(\"ğŸ§¹ Dossier data/ supprimÃ©\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ»â€â„ï¸ Polars pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  orchestrer des containers Ã  grande Ã©chelle avec Kubernetes. Tu dÃ©couvriras comment dÃ©ployer, gÃ©rer et monitorer des applications data dans un cluster K8s â€” des compÃ©tences essentielles pour industrialiser tes pipelines !",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#prÃ©requis",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#prÃ©requis",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 14_docker_for_data_engineers\n\n\nâœ… Requis\nMaÃ®triser les bases de Docker (images, containers, volumes)\n\n\nâœ… Requis\nConnaissances de base en YAML\n\n\nğŸ’¡ RecommandÃ©\nDocker Desktop ou Minikube installÃ©",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#objectifs-du-module",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#objectifs-du-module",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture et les concepts clÃ©s de Kubernetes\nInstaller et configurer un cluster Kubernetes local\nDÃ©ployer et gÃ©rer des applications avec kubectl\nUtiliser les Jobs et CronJobs pour des tÃ¢ches Data Engineering\nConfigurer le stockage persistant (PVC)\nDÃ©bugger des pods et diagnostiquer les erreurs courantes\nDÃ©ployer et orchestrer un pipeline ETL complet dans Kubernetes",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#cest-quoi-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#cest-quoi-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ§  Câ€™est quoi Kubernetes ?",
    "text": "ğŸ§  Câ€™est quoi Kubernetes ?\n\nâ˜¸ï¸ Kubernetes (K8s) est une plateforme open-source dâ€™orchestration de containers qui automatise le dÃ©ploiement, la mise Ã  lâ€™Ã©chelle et la gestion dâ€™applications containerisÃ©es.\n\nEn pratique, Kubernetes te permet de : - DÃ©ployer des containers sur plusieurs machines (un cluster) - Scaler automatiquement selon la charge - RedÃ©marrer les containers qui plantent - GÃ©rer la configuration et les secrets - Exposer tes applications au rÃ©seau\n\nğŸ”„ Kubernetes â‰  Docker (mais complÃ©mentaires)\n\n\n\n\n\n\n\n\nAspect\nDocker\nKubernetes\n\n\n\n\nRÃ´le\nCrÃ©er et exÃ©cuter des containers\nOrchestrer des containers\n\n\nÃ‰chelle\n1 machine\nCluster de machines\n\n\nFocus\nBuild & Run\nDeploy, Scale, Manage\n\n\nAnalogie\nLe musicien\nLe chef dâ€™orchestre\n\n\n\n\nğŸ’¡ Docker crÃ©e les containers, Kubernetes les orchestre Ã  grande Ã©chelle.\n\n\n\nğŸ¯ Analogies pour bien comprendre\n\n\n\n\n\n\n\nAnalogie\nExplication\n\n\n\n\nğŸ¼ Chef dâ€™orchestre\nK8s coordonne tous les containers (musiciens) pour quâ€™ils jouent en harmonie\n\n\nğŸ¢ Agence immobiliÃ¨re\nK8s place tes containers (locataires) dans les nodes (appartements) disponibles\n\n\nâœˆï¸ Pilote automatique\nTu dÃ©finis la destination (Ã©tat souhaitÃ©), K8s sâ€™occupe dâ€™y arriver et dâ€™y rester\n\n\nğŸ­ Usine automatisÃ©e\nTu donnes les plans, K8s fabrique, surveille et remplace les piÃ¨ces dÃ©fectueuses\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nLe nom Kubernetes vient du grec ÎºÏ…Î²ÎµÏÎ½Î®Ï„Î·Ï‚ (kuberná¸—tÄ“s) qui signifie â€œpiloteâ€ ou â€œgouverneurâ€ â€” celui qui tient la barre dâ€™un navire.\nK8s est nÃ© chez Google, inspirÃ© de leur systÃ¨me interne Borg qui gÃ¨re des milliards de containers depuis 2003. Google a ouvert le projet en 2014 et lâ€™a donnÃ© Ã  la Cloud Native Computing Foundation (CNCF).\nLe â€œ8â€ dans K8s reprÃ©sente les 8 lettres entre le K et le S de Kubernetes !\nğŸ“– Histoire de Kubernetes",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#pourquoi-kubernetes-pour-un-data-engineer",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#pourquoi-kubernetes-pour-un-data-engineer",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸš€ 1. Pourquoi Kubernetes pour un Data Engineer ?",
    "text": "ğŸš€ 1. Pourquoi Kubernetes pour un Data Engineer ?\nEn Data Engineering moderne, tu dois souvent :\n\nExÃ©cuter des jobs ETL de maniÃ¨re fiable et planifiÃ©e\nDÃ©ployer des bases de donnÃ©es et des brokers (Kafka)\nFaire tourner des jobs Spark distribuÃ©s\nOrchestrer avec Airflow ou Prefect\nScaler selon le volume de donnÃ©es\n\n\nâŒ Sans Kubernetes\n\n\n\nProblÃ¨me\nConsÃ©quence\n\n\n\n\nDÃ©ploiement manuel sur chaque serveur\nLent et source dâ€™erreurs\n\n\nPas de redÃ©marrage automatique\nJobs perdus en cas de crash\n\n\nScaling manuel\nSous/sur-utilisation des ressources\n\n\nConfiguration dispersÃ©e\nDifficile Ã  maintenir\n\n\n\n\n\nâœ… Avec Kubernetes\n\n\n\nAvantage\nExemple concret\n\n\n\n\nDÃ©ploiement dÃ©claratif\nkubectl apply -f etl-job.yaml\n\n\nAuto-healing\nPod crashÃ© = recrÃ©Ã© automatiquement\n\n\nCronJobs natifs\nETL planifiÃ© sans cron externe\n\n\nSecrets management\nCredentials gÃ©rÃ©s proprement\n\n\nPortabilitÃ©\nMÃªme config en dev, staging, prod\n\n\n\n\n\nğŸ–¼ï¸ Vue dâ€™ensemble : Cluster K8s\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   CLUSTER K8s                       â”‚\nâ”‚                                                     â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”‚   NODE 1    â”‚   â”‚   NODE 2    â”‚                â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                â”‚\nâ”‚   â”‚  â”‚ Pod A â”‚  â”‚   â”‚  â”‚ Pod C â”‚  â”‚                â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                â”‚\nâ”‚   â”‚  â”‚ Pod B â”‚  â”‚   â”‚  â”‚ Pod D â”‚  â”‚                â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#architecture-kubernetes-simplifiÃ©e",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#architecture-kubernetes-simplifiÃ©e",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ—ï¸ 2. Architecture Kubernetes (simplifiÃ©e)",
    "text": "ğŸ—ï¸ 2. Architecture Kubernetes (simplifiÃ©e)\nUn cluster Kubernetes est composÃ© de deux types de machines :\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nAnalogie\n\n\n\n\nControl Plane (Master)\nCerveau du cluster, prend les dÃ©cisions\nLe management\n\n\nWorker Nodes\nExÃ©cutent les containers\nLes ouvriers\n\n\n\n\nğŸ§  Control Plane (les composants essentiels)\n\n\n\nComposant\nRÃ´le\n\n\n\n\nAPI Server\nPoint dâ€™entrÃ©e unique (kubectl â†’ API)\n\n\nScheduler\nDÃ©cide sur quel node placer les pods\n\n\n\n\nğŸ’¡ Pour ce module â€œFundamentalsâ€, on se concentre sur ces 2 composants. Les autres (etcd, Controller Manager) seront vus en niveau avancÃ©.\n\n\n\nğŸ’ª Worker Nodes\n\n\n\nComposant\nRÃ´le\n\n\n\n\nKubelet\nAgent qui gÃ¨re les pods sur le node\n\n\nContainer Runtime\nDocker/containerd qui exÃ©cute les containers\n\n\n\n\n\nğŸ–¼ï¸ SchÃ©ma Architecture\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚           CONTROL PLANE                 â”‚\n                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n       kubectl â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚   API Server   â”‚  â”‚   Scheduler   â”‚ â”‚\n                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                    â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚                     â”‚                     â”‚\n              â–¼                     â–¼                     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     WORKER NODE 1   â”‚ â”‚     WORKER NODE 2   â”‚ â”‚     WORKER NODE 3   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚    Kubelet    â”‚  â”‚ â”‚  â”‚    Kubelet    â”‚  â”‚ â”‚  â”‚    Kubelet    â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚ Pod â”‚  â”‚ Pod â”‚   â”‚ â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚   â”‚ â”‚  â”‚ Pod â”‚           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚ â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚ â”‚  â””â”€â”€â”€â”€â”€â”˜           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#concepts-clÃ©s-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#concepts-clÃ©s-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ”‘ 3. Concepts clÃ©s Kubernetes",
    "text": "ğŸ”‘ 3. Concepts clÃ©s Kubernetes\nVoici les concepts fondamentaux Ã  maÃ®triser :\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nPod\nPlus petite unitÃ© dÃ©ployable (1+ containers)\nUn appartement\n\n\nDeployment\nGÃ¨re les replicas de pods, rolling updates\nUn syndic dâ€™immeuble\n\n\nService\nExpose les pods sur le rÃ©seau\nLâ€™adresse postale\n\n\nNamespace\nIsolation logique des ressources\nUn quartier de la ville\n\n\nConfigMap\nConfiguration externe (non sensible)\nFichier .env public\n\n\nSecret\nDonnÃ©es sensibles (base64)\nUn coffre-fort\n\n\nPersistentVolumeClaim\nDemande de stockage persistant\nLocation dâ€™un disque dur\n\n\nJob\nTÃ¢che one-shot (sâ€™exÃ©cute puis se termine)\nUne mission ponctuelle\n\n\nCronJob\nJob planifiÃ© (comme cron Linux)\nUn rÃ©veil programmÃ©\n\n\n\n\nğŸ“¦ Pod\nLe Pod est lâ€™unitÃ© de base dans Kubernetes : - Contient 1 ou plusieurs containers qui partagent le rÃ©seau et le stockage - A sa propre adresse IP dans le cluster - Est Ã©phÃ©mÃ¨re : peut Ãªtre dÃ©truit et recrÃ©Ã© Ã  tout moment\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mon-pod\nspec:\n  containers:\n  - name: mon-container\n    image: python:3.11-slim\n\n\nğŸš€ Deployment\nLe Deployment gÃ¨re un ensemble de pods identiques : - Garantit le nombre de replicas souhaitÃ© - GÃ¨re les rolling updates (mise Ã  jour sans downtime) - RecrÃ©e les pods qui crashent\n\n\nğŸŒ Service\nLe Service expose les pods sur le rÃ©seau :\n\n\n\n\n\n\n\n\nType\nDescription\nUsage\n\n\n\n\nClusterIP\nIP interne au cluster\nCommunication entre pods\n\n\nNodePort\nPort ouvert sur chaque node\nTests, dÃ©veloppement\n\n\nLoadBalancer\nLoad balancer cloud\nProduction (AWS, GCP, Azure)\n\n\n\n\n\nğŸ” Secret vs ConfigMap\n\n\n\nAspect\nConfigMap\nSecret\n\n\n\n\nUsage\nConfig non sensible\nPasswords, tokens, clÃ©s\n\n\nEncodage\nTexte clair\nBase64\n\n\nExemple\nURLs, feature flags\nDB_PASSWORD, API_KEY\n\n\n\nâš ï¸ Attention : Base64 nâ€™est PAS du chiffrement !\n\nBase64 est juste un encodage, pas une protection. Nâ€™importe qui avec accÃ¨s au cluster peut dÃ©coder les secrets. En production, utilise Sealed Secrets, HashiCorp Vault, ou les secrets managers cloud (AWS Secrets Manager, GCP Secret Manager).\n\n\n\nğŸ–¼ï¸ SchÃ©ma : Relations entre concepts\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       User â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Service    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                           â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Deployment  â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â–¼            â–¼            â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Pod 1 â”‚    â”‚ Pod 2 â”‚    â”‚ Pod 3 â”‚\n         â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n             â”‚            â”‚            â”‚\n             â–¼            â–¼            â–¼\n        Container    Container    Container",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#installation-de-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#installation-de-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ’» 4. Installation de Kubernetes",
    "text": "ğŸ’» 4. Installation de Kubernetes\nPour ce module, tu as besoin dâ€™un cluster Kubernetes local.\n\n\n\nOption\nDifficultÃ©\nRecommandÃ© pour\n\n\n\n\nDocker Desktop\nâ­ Facile\nMac/Windows, dÃ©butants\n\n\nMinikube\nâ­â­ Moyen\nTous OS, plus de contrÃ´le\n\n\nk3d/k3s\nâ­â­ Moyen\nLÃ©ger, CI/CD\n\n\n\n\nğŸ³ Option 1 : Docker Desktop (recommandÃ©)\n\nOuvrir Docker Desktop\nAller dans Settings â†’ Kubernetes\nCocher Enable Kubernetes\nCliquer Apply & Restart\nAttendre que le status passe au vert âœ…\n\n\n\nğŸ“¦ Option 2 : Minikube\n# Installation (macOS avec Homebrew)\nbrew install minikube\n\n# Installation (Linux)\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# DÃ©marrer le cluster\nminikube start\n\n# VÃ©rifier\nminikube status\n\n\nğŸ”§ Installer kubectl\nkubectl est lâ€™outil CLI pour interagir avec Kubernetes.\n# macOS\nbrew install kubectl\n\n# Linux\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install kubectl /usr/local/bin/kubectl\n\n# Windows (avec Docker Desktop, kubectl est inclus)\n\n\nâœ… VÃ©rification\n# Version de kubectl\nkubectl version --client\n\n# Liste des nodes du cluster\nkubectl get nodes\n\n# VÃ©rifier le contexte actif\nkubectl config get-contexts\n\n\nCode\n%%bash\n# VÃ©rifier l'installation de kubectl\necho \"=== Version kubectl ===\"\nkubectl version --client\n\necho \"\"\necho \"=== Nodes du cluster ===\"\nkubectl get nodes\n\necho \"\"\necho \"=== Contexte actif ===\"\nkubectl config current-context",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#commandes-kubectl-essentielles",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#commandes-kubectl-essentielles",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ› ï¸ 5. Commandes kubectl essentielles",
    "text": "ğŸ› ï¸ 5. Commandes kubectl essentielles\nVoici les commandes que tu utiliseras au quotidien :\n\nğŸ“‹ CRUD sur les ressources\n\n\n\nAction\nCommande\n\n\n\n\nCrÃ©er/Mettre Ã  jour\nkubectl apply -f manifest.yaml\n\n\nLire\nkubectl get pods\n\n\nDÃ©tails\nkubectl describe pod &lt;name&gt;\n\n\nSupprimer\nkubectl delete -f manifest.yaml\n\n\n\n\n\nğŸ” Lister les ressources\nkubectl get pods                    # Lister les pods\nkubectl get pods -o wide            # Avec plus de dÃ©tails (node, IP)\nkubectl get deployments             # Lister les deployments\nkubectl get services                # Lister les services\nkubectl get all                     # Tout lister\nkubectl get all -n &lt;namespace&gt;      # Dans un namespace spÃ©cifique\n\n\nğŸ› Debug\nkubectl logs &lt;pod&gt;                  # Voir les logs\nkubectl logs -f &lt;pod&gt;               # Suivre les logs en temps rÃ©el\nkubectl logs -p &lt;pod&gt;               # Logs du container prÃ©cÃ©dent (aprÃ¨s crash)\nkubectl describe pod &lt;pod&gt;          # DÃ©tails + Events\nkubectl exec -it &lt;pod&gt; -- bash      # Shell dans le pod\nkubectl get events                  # Tous les events du namespace\n\n\nğŸŒ RÃ©seau\nkubectl port-forward &lt;pod&gt; 8080:80           # Forward un port\nkubectl port-forward svc/&lt;service&gt; 8080:80   # Forward depuis un service\n\n\nğŸ“¦ Namespaces\nkubectl get namespaces              # Lister les namespaces\nkubectl create namespace dev        # CrÃ©er un namespace\nkubectl config set-context --current --namespace=dev  # Changer de namespace par dÃ©faut\n\n\nğŸ—‚ï¸ Cheat Sheet visuel\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    KUBECTL CHEAT SHEET                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  CREATE/UPDATE â”‚  kubectl apply -f manifest.yaml               â”‚\nâ”‚  READ          â”‚  kubectl get pods/deploy/svc/all              â”‚\nâ”‚  DESCRIBE      â”‚  kubectl describe &lt;resource&gt; &lt;name&gt;           â”‚\nâ”‚  DELETE        â”‚  kubectl delete -f manifest.yaml              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  LOGS          â”‚  kubectl logs -f &lt;pod&gt;                        â”‚\nâ”‚  SHELL         â”‚  kubectl exec -it &lt;pod&gt; -- bash               â”‚\nâ”‚  PORT-FORWARD  â”‚  kubectl port-forward &lt;pod&gt; 8080:80           â”‚\nâ”‚  EVENTS        â”‚  kubectl get events --sort-by='.lastTimestamp'â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n%%bash\n# Exemples de commandes kubectl\n\necho \"=== Namespaces ===\"\nkubectl get namespaces\n\necho \"\"\necho \"=== Pods dans tous les namespaces ===\"\nkubectl get pods --all-namespaces\n\necho \"\"\necho \"=== Services ===\"\nkubectl get services",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#premier-dÃ©ploiement-hello-world",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#premier-dÃ©ploiement-hello-world",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ¯ 6. Premier dÃ©ploiement : Hello World",
    "text": "ğŸ¯ 6. Premier dÃ©ploiement : Hello World\nDÃ©ployons une application simple pour comprendre le cycle de dÃ©ploiement.\n\nğŸ“ Manifest YAML : Deployment + Service\n# hello-world.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\n  labels:\n    app: hello-world\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello-world\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world-svc\nspec:\n  selector:\n    app: hello-world\n  ports:\n  - port: 80\n    targetPort: 80\n  type: NodePort\n\n\nğŸš€ DÃ©ployer\n# Appliquer le manifest\nkubectl apply -f hello-world.yaml\n\n# VÃ©rifier le dÃ©ploiement\nkubectl get deployments\nkubectl get pods\nkubectl get services\n\n\nğŸŒ AccÃ©der Ã  lâ€™application\nMÃ©thode 1 : Port-forward (recommandÃ©)\nkubectl port-forward svc/hello-world-svc 8080:80\n# Ouvrir http://localhost:8080\nMÃ©thode 2 : NodePort\n# Trouver le NodePort\nkubectl get svc hello-world-svc\n# AccÃ©der via http://localhost:&lt;NodePort&gt;\n\nğŸ’¡ Astuce entreprise : Le NodePort nâ€™est pas toujours autorisÃ© en entreprise (firewall). port-forward fonctionne partout !\n\n\n\nğŸ”„ Cycle de dÃ©ploiement\nkubectl apply     Deployment crÃ©Ã©     ReplicaSet crÃ©Ã©     Pods crÃ©Ã©s\n     â”‚                  â”‚                   â”‚                  â”‚\n     â–¼                  â–¼                   â–¼                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  YAML   â”‚ â”€â”€â”€â–¶  â”‚Deployment â”‚ â”€â”€â”€â–¶  â”‚ReplicaSet â”‚ â”€â”€â”€â–¶ â”‚  Pods   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                               â”‚\n                    Service expose les pods â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ§¹ Nettoyage\nkubectl delete -f hello-world.yaml",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#kubernetes-pour-data-engineering",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#kubernetes-pour-data-engineering",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ—„ï¸ 7. Kubernetes pour Data Engineering",
    "text": "ğŸ—„ï¸ 7. Kubernetes pour Data Engineering\nVoici les ressources Kubernetes particuliÃ¨rement utiles pour le Data Engineering.\n\n7.1 Job : tÃ¢che one-shot\nUn Job exÃ©cute un ou plusieurs pods jusquâ€™Ã  complÃ©tion : - ETL ponctuel - Migration de donnÃ©es - Backfill\n# etl-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: etl\n        image: python:3.11-slim\n        command: [\"python\", \"-c\", \"print('ETL terminÃ© !')\"]\n      restartPolicy: Never\n  backoffLimit: 3  # Nombre de retries en cas d'Ã©chec\n# Lancer le job\nkubectl apply -f etl-job.yaml\n\n# Voir le status\nkubectl get jobs\n\n# Voir les logs\nkubectl logs job/etl-job\n\n\n\n7.2 CronJob : tÃ¢che planifiÃ©e\nUn CronJob lance des Jobs selon un schedule (comme cron Linux) : - ETL quotidien - Nettoyage de donnÃ©es - Rapports automatisÃ©s\n# etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h du matin\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: etl\n            image: my-etl-image:1.0\n            env:\n            - name: DB_HOST\n              value: \"postgres\"\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\nSchedule cron :\n\n\n\nExpression\nSignification\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h00\n\n\n*/15 * * * *\nToutes les 15 minutes\n\n\n0 0 * * 0\nTous les dimanches Ã  minuit\n\n\n0 8 1 * *\nLe 1er de chaque mois Ã  8h\n\n\n\n# Lister les cronjobs\nkubectl get cronjobs\n\n# DÃ©clencher manuellement\nkubectl create job --from=cronjob/etl-daily test-etl\n\n\n\n7.3 DÃ©ployer PostgreSQL dans K8s\n# postgres-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: \"de_db\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n\nâš ï¸ Note production : Les bases de donnÃ©es dans K8s nÃ©cessitent toujours un stockage persistant (PVC) (A voir plus en bas). En production, on prÃ©fÃ¨re souvent des services managÃ©s (Cloud SQL, RDS, Azure Database) pour la haute disponibilitÃ© et les backups automatiques.\n\n\n\nğŸ–¼ï¸ SchÃ©ma Pipeline Data Engineering sur K8s\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   CronJob    â”‚â”€â”€â”€â”€â–¶â”‚     Job      â”‚â”€â”€â”€â”€â–¶â”‚     Pod      â”‚\nâ”‚  (0 2 * * *) â”‚     â”‚              â”‚     â”‚  (etl.py)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                  â”‚\n                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                          â”‚                       â”‚\n                          â–¼                       â–¼\n                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                   â”‚     PVC      â”‚       â”‚  PostgreSQL  â”‚\n                   â”‚ (input data) â”‚       â”‚   (output)   â”‚\n                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#stockage-volumes-pvc",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#stockage-volumes-pvc",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ’¾ 8. Stockage : Volumes & PVC",
    "text": "ğŸ’¾ 8. Stockage : Volumes & PVC\nLes donnÃ©es ne doivent jamais vivre uniquement dans un Pod (Ã©phÃ©mÃ¨re).\n\nğŸ”‘ Concepts clÃ©s\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nPersistentVolume (PV)\nRessource de stockage provisionnÃ©e par lâ€™admin\n\n\nPersistentVolumeClaim (PVC)\nDemande de stockage par lâ€™utilisateur\n\n\nStorageClass\nType de stockage (SSD, HDD, cloudâ€¦)\n\n\n\n\n\nğŸ–¼ï¸ SchÃ©ma : Comment Ã§a marche\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Pod   â”‚â”€â”€â”€â”€â–¶â”‚   PVC   â”‚â”€â”€â”€â”€â–¶â”‚     PV      â”‚â”€â”€â”€â”€â–¶â”‚  Stockage rÃ©el   â”‚\nâ”‚         â”‚     â”‚ (claim) â”‚     â”‚ (provision) â”‚     â”‚ (disk, NFS, EBS) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“ Exemple : PVC pour PostgreSQL\n# postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nAccess Modes :\n\n\n\nMode\nDescription\n\n\n\n\nReadWriteOnce\nLecture/Ã©criture par un seul node\n\n\nReadOnlyMany\nLecture seule par plusieurs nodes\n\n\nReadWriteMany\nLecture/Ã©criture par plusieurs nodes\n\n\n\n\n\nğŸ”— Utilisation dans un Deployment\nspec:\n  containers:\n  - name: postgres\n    volumeMounts:\n    - name: postgres-storage\n      mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: postgres-storage\n    persistentVolumeClaim:\n      claimName: postgres-pvc\n# VÃ©rifier les PVC\nkubectl get pvc\n\n# VÃ©rifier les PV\nkubectl get pv",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#configuration-configmaps-secrets",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#configuration-configmaps-secrets",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ” 9. Configuration : ConfigMaps & Secrets",
    "text": "ğŸ” 9. Configuration : ConfigMaps & Secrets\n\nğŸ“„ ConfigMap : configuration non sensible\n# config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\ndata:\n  DB_HOST: \"postgres\"\n  DB_PORT: \"5432\"\n  LOG_LEVEL: \"INFO\"\n\n\nğŸ”’ Secret : donnÃ©es sensibles\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secret\ntype: Opaque\ndata:\n  password: ZGVfcGFzc3dvcmQ=  # base64 de \"de_password\"\nCrÃ©er un secret en ligne de commande :\n# CrÃ©er un secret\nkubectl create secret generic postgres-secret \\\n  --from-literal=password=de_password\n\n# Encoder en base64\necho -n \"de_password\" | base64\n\n# DÃ©coder\necho \"ZGVfcGFzc3dvcmQ=\" | base64 -d\n\n\nğŸ”— Utilisation dans un Pod\nspec:\n  containers:\n  - name: etl\n    env:\n    # Depuis ConfigMap\n    - name: DB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: etl-config\n          key: DB_HOST\n    # Depuis Secret\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: postgres-secret\n          key: password",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#debug-monitoring",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#debug-monitoring",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ”§ 10. Debug & Monitoring",
    "text": "ğŸ”§ 10. Debug & Monitoring\n\nğŸ“œ Commandes de debug essentielles\n# Logs\nkubectl logs &lt;pod&gt;                  # Logs actuels\nkubectl logs -f &lt;pod&gt;               # Suivre en temps rÃ©el\nkubectl logs -p &lt;pod&gt;               # Logs du container prÃ©cÃ©dent (aprÃ¨s crash)\nkubectl logs &lt;pod&gt; -c &lt;container&gt;   # Logs d'un container spÃ©cifique\n\n# DÃ©tails et Events\nkubectl describe pod &lt;pod&gt;          # DÃ©tails complets + Events\nkubectl get events --sort-by='.lastTimestamp'  # Events triÃ©s par date\n\n# Shell dans le pod\nkubectl exec -it &lt;pod&gt; -- bash      # Ouvrir un shell\nkubectl exec -it &lt;pod&gt; -- sh        # Pour images Alpine\n\n# Ressources\nkubectl top pods                    # CPU/RAM des pods (si metrics-server)\nkubectl top nodes                   # CPU/RAM des nodes\n\n\nâŒ Erreurs classiques et solutions\n\n\n\n\n\n\n\n\nErreur\nCause probable\nSolution\n\n\n\n\nCrashLoopBackOff\nLâ€™app plante au dÃ©marrage\nkubectl logs &lt;pod&gt; pour voir lâ€™erreur\n\n\nImagePullBackOff\nImage introuvable ou accÃ¨s refusÃ©\nVÃ©rifier nom/tag, credentials registry\n\n\nPending\nPas de ressources disponibles\nkubectl describe pod â†’ voir Events\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter resources.limits.memory\n\n\nCreateContainerConfigError\nConfigMap/Secret manquant\nVÃ©rifier que les refs existent\n\n\nErrImageNeverPull\nImage locale introuvable\nimagePullPolicy: IfNotPresent\n\n\n\n\n\nğŸ” Processus de debug\n1. kubectl get pods              â†’ Voir le status\n           â”‚\n           â–¼\n2. kubectl describe pod &lt;pod&gt;    â†’ Voir les Events\n           â”‚\n           â–¼\n3. kubectl logs &lt;pod&gt;            â†’ Voir les logs de l'app\n           â”‚\n           â–¼\n4. kubectl exec -it &lt;pod&gt; -- sh  â†’ Investiguer dans le container",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "âš ï¸ 11. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "âš ï¸ 11. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes des dÃ©butants\n\n\n\nErreur\nConsÃ©quence\n\n\n\n\nPas de resources.requests/limits\nPods non schedulÃ©s ou OOMKilled\n\n\nSecrets dans ConfigMap\nFuite de credentials\n\n\nBase de donnÃ©es sans PVC\nPerte de donnÃ©es au restart\n\n\nTout dans namespace default\nDifficile Ã  gÃ©rer, pas dâ€™isolation\n\n\nImages en :latest\nComportement non reproductible\n\n\nPas de labels\nImpossible de filtrer/organiser\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nToujours dÃ©finir requests/limits\nScheduling prÃ©visible, protection contre OOM\n\n\nUn namespace par projet/env\nIsolation, quotas, RBAC\n\n\nTags dâ€™image versionnÃ©s\nimage:1.0.0 au lieu de :latest\n\n\nLabels systÃ©matiques\nOrganisation et filtrage\n\n\nHealthchecks (probes)\nK8s sait si lâ€™app est prÃªte/vivante\n\n\nPVC pour toute donnÃ©e importante\nPersistance garantie\n\n\n\n\n\nğŸ·ï¸ Exemple de labels recommandÃ©s\nmetadata:\n  name: etl-job\n  labels:\n    app: etl-pipeline\n    component: etl\n    env: dev\n    team: data-engineering\n    version: \"1.0.0\"\n\n\nğŸ“Š Exemple de resources requests/limits\nspec:\n  containers:\n  - name: etl\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n\n\n\nRessource\nUnitÃ©\nExemple\n\n\n\n\nCPU\nmillicores\n500m = 0.5 CPU\n\n\nMemory\nMi/Gi\n256Mi, 1Gi",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#quiz-de-fin-de-module",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre un Pod et un Deployment ?\n\nUn Pod contient plusieurs Deployments\n\nUn Deployment gÃ¨re des replicas de Pods et leur cycle de vie\n\nCâ€™est la mÃªme chose\n\nUn Pod est pour la production, un Deployment pour le dev\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Un Pod est lâ€™unitÃ© de base (Ã©phÃ©mÃ¨re), un Deployment gÃ¨re plusieurs replicas de Pods, les rolling updates, et recrÃ©e les Pods qui crashent.\n\n\n\n\nâ“ Q2. Pourquoi utiliser un PersistentVolumeClaim (PVC) ?\n\nPour augmenter la vitesse du CPU\n\nPour persister les donnÃ©es mÃªme si le Pod est supprimÃ©\n\nPour exposer un Pod sur internet\n\nPour chiffrer les secrets\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Les Pods sont Ã©phÃ©mÃ¨res. Sans PVC, les donnÃ©es sont perdues quand le Pod est supprimÃ© ou recrÃ©Ã©.\n\n\n\n\nâ“ Q3. Quelle commande permet de voir les logs dâ€™un Pod qui a crashÃ© ?\n\nkubectl logs &lt;pod&gt;\n\nkubectl logs -p &lt;pod&gt;\n\nkubectl describe pod &lt;pod&gt;\n\nkubectl get logs &lt;pod&gt;\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Lâ€™option -p (previous) affiche les logs du container prÃ©cÃ©dent, utile aprÃ¨s un crash.\n\n\n\n\nâ“ Q4. Lâ€™encodage Base64 des Secrets Kubernetes est-il sÃ©curisÃ© ?\n\nOui, câ€™est du chiffrement fort\n\nNon, Base64 est juste un encodage, pas du chiffrement\n\nOui, si on utilise une clÃ© de 256 bits\n\nÃ‡a dÃ©pend de la version de Kubernetes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Base64 est un simple encodage rÃ©versible. Nâ€™importe qui avec accÃ¨s au cluster peut dÃ©coder les secrets. En production, utilisez Sealed Secrets ou un secrets manager.\n\n\n\n\nâ“ Q5. Quelle ressource utiliser pour exÃ©cuter un ETL tous les jours Ã  2h du matin ?\n\nDeployment\n\nJob\n\nCronJob\n\nDaemonSet\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Un CronJob permet de planifier des Jobs selon une expression cron (0 2 * * *).\n\n\n\n\nâ“ Q6. Que signifie lâ€™erreur CrashLoopBackOff ?\n\nLâ€™image Docker nâ€™existe pas\n\nLe Pod manque de mÃ©moire\n\nLâ€™application dans le container plante au dÃ©marrage de maniÃ¨re rÃ©pÃ©tÃ©e\n\nLe rÃ©seau est inaccessible\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” K8s tente de redÃ©marrer le container mais il plante Ã  chaque fois. Utilisez kubectl logs &lt;pod&gt; pour voir lâ€™erreur.\n\n\n\n\nâ“ Q7. Quelle est la meilleure pratique pour les tags dâ€™images en production ?\n\nToujours utiliser :latest\n\nUtiliser des tags versionnÃ©s comme :1.0.0\n\nNe pas mettre de tag\n\nUtiliser la date comme tag\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Les tags versionnÃ©s garantissent la reproductibilitÃ©. :latest peut changer et causer des comportements inattendus.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#mini-projet-pipeline-etl-sur-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#mini-projet-pipeline-etl-sur-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸš€ Mini-projet : Pipeline ETL sur Kubernetes",
    "text": "ğŸš€ Mini-projet : Pipeline ETL sur Kubernetes\n\nğŸ¯ Objectif\nDÃ©ployer un pipeline ETL complet sur Kubernetes : - PostgreSQL avec stockage persistant - CronJob Python qui charge des donnÃ©es CSV dans PostgreSQL\n\n\nğŸ”§ Contexte\nTu dois crÃ©er une stack Data Engineering minimale mais rÃ©aliste, entiÃ¨rement orchestrÃ©e par Kubernetes.\n\n\nğŸ“ Structure du projet\nk8s-etl-project/\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ postgres-secret.yaml\nâ”‚   â”œâ”€â”€ postgres-pvc.yaml\nâ”‚   â”œâ”€â”€ postgres-deployment.yaml\nâ”‚   â”œâ”€â”€ postgres-service.yaml\nâ”‚   â”œâ”€â”€ etl-configmap.yaml\nâ”‚   â””â”€â”€ etl-cronjob.yaml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sales.csv\nâ””â”€â”€ README.md\n\n\nğŸ§  Ã‰tapes\n\nCrÃ©er le namespace data-pipeline\nDÃ©ployer PostgreSQL (Secret + PVC + Deployment + Service)\nBuild & push lâ€™image Docker de lâ€™ETL\nDÃ©ployer le CronJob ETL\nTester manuellement : kubectl create job --from=cronjob/etl-daily test-etl\nVÃ©rifier les donnÃ©es dans PostgreSQL\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. manifests/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: data-pipeline\n  labels:\n    team: data-engineering\n2. manifests/postgres-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secret\n  namespace: data-pipeline\ntype: Opaque\ndata:\n  password: ZGVfcGFzc3dvcmQ=  # de_password\n3. manifests/postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\n  namespace: data-pipeline\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n4. manifests/postgres-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: data-pipeline\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: \"de_db\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n5. manifests/postgres-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: data-pipeline\nspec:\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\n  type: ClusterIP\n6. manifests/etl-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\n  namespace: data-pipeline\ndata:\n  DB_HOST: \"postgres\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"de_db\"\n  DB_USER: \"de_user\"\n7. manifests/etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\n  namespace: data-pipeline\n  labels:\n    app: etl-pipeline\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: etl-job\n        spec:\n          containers:\n          - name: etl\n            image: my-etl-image:1.0  # Ã€ remplacer par ton image\n            envFrom:\n            - configMapRef:\n                name: etl-config\n            env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-secret\n                  key: password\n            resources:\n              requests:\n                memory: \"128Mi\"\n                cpu: \"100m\"\n              limits:\n                memory: \"256Mi\"\n                cpu: \"200m\"\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n8. etl/etl.py\nimport os\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef main():\n    # Config depuis variables d'environnement\n    db_host = os.environ['DB_HOST']\n    db_port = os.environ['DB_PORT']\n    db_name = os.environ['DB_NAME']\n    db_user = os.environ['DB_USER']\n    db_pass = os.environ['DB_PASSWORD']\n    \n    print(\"ğŸš€ DÃ©marrage ETL...\")\n    \n    # Simuler des donnÃ©es (en vrai: lire depuis PVC ou S3)\n    df = pd.DataFrame({\n        'date': ['2024-01-01', '2024-01-02'],\n        'product': ['Laptop', 'Mouse'],\n        'quantity': [5, 20],\n        'price': [999.99, 29.99]\n    })\n    df['total'] = df['quantity'] * df['price']\n    \n    # Charger dans PostgreSQL\n    engine = create_engine(\n        f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n    )\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© !\")\n\nif __name__ == \"__main__\":\n    main()\n9. Commandes de dÃ©ploiement\n# CrÃ©er le namespace\nkubectl apply -f manifests/namespace.yaml\n\n# DÃ©ployer PostgreSQL\nkubectl apply -f manifests/postgres-secret.yaml\nkubectl apply -f manifests/postgres-pvc.yaml\nkubectl apply -f manifests/postgres-deployment.yaml\nkubectl apply -f manifests/postgres-service.yaml\n\n# VÃ©rifier que PostgreSQL est prÃªt\nkubectl get pods -n data-pipeline\n\n# DÃ©ployer le CronJob\nkubectl apply -f manifests/etl-configmap.yaml\nkubectl apply -f manifests/etl-cronjob.yaml\n\n# Tester manuellement\nkubectl create job --from=cronjob/etl-daily test-etl -n data-pipeline\n\n# Voir les logs\nkubectl logs -f job/test-etl -n data-pipeline\n\n# VÃ©rifier dans PostgreSQL\nkubectl exec -it $(kubectl get pod -l app=postgres -n data-pipeline -o name) \\\n  -n data-pipeline -- psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#ressources-pour-aller-plus-loin",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nKubernetes Docs â€” Documentation officielle\nkubectl Cheat Sheet â€” Toutes les commandes\nKubernetes API Reference â€” RÃ©fÃ©rence API\n\n\n\nğŸ® Pratique\n\nKatacoda K8s â€” Tutoriels interactifs (gratuit)\nPlay with Kubernetes â€” Cluster K8s gratuit en ligne\nKiller.sh â€” Simulateur dâ€™examen CKA/CKAD\n\n\n\nğŸ“– Livres & Cours\n\nKubernetes Up & Running â€” Kelsey Hightower\nThe Kubernetes Book â€” Nigel Poulton\n\n\n\nğŸ”§ Outils utiles\n\nk9s â€” Terminal UI pour Kubernetes\nLens â€” IDE Kubernetes\nkubectx/kubens â€” Changer de contexte/namespace facilement",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#prochaine-Ã©tape",
    "title": "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les fondamentaux de Kubernetes, passons aux workloads Data Engineering avancÃ©s !\nğŸ‘‰ Module suivant : 16_k8s_for_data_workloads.ipynb â€” Kubernetes pour les workloads Data\nTu vas apprendre : - Spark on Kubernetes - Airflow on Kubernetes - Helm charts pour packager tes dÃ©ploiements - Horizontal Pod Autoscaler\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Kubernetes Fundamentals pour Data Engineers.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes Fundamentals pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html",
    "href": "notebooks/intermediate/19_pyspark_advanced.html",
    "title": "ğŸš€ PySpark Advanced",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas apprendre Ã  optimiser Spark comme un expert. Tu dÃ©couvriras lâ€™architecture interne, les techniques dâ€™optimisation, et comment diagnostiquer et rÃ©soudre les problÃ¨mes de performance.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#prÃ©requis",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#prÃ©requis",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 11 : PySpark for Data Engineering (bases Spark)\n\n\nâœ… Requis\nModule 18 : High Performance Python\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec des datasets &gt; 1 Go",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#objectifs-du-module",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#objectifs-du-module",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture interne de Spark (Catalyst, Tungsten)\nExÃ©cuter Spark en production avec spark-submit\nOptimiser les partitions, shuffles et joins\nDiagnostiquer un job lent avec Spark UI\nRÃ©duire le temps dâ€™exÃ©cution de 80-90%\n\n\nğŸ¯ Objectif concret\n\nTransformer un pipeline de 20 minutes en 2 minutes.\n\nCâ€™est ce qui distingue un Data Engineer junior dâ€™un senior sur Spark.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#rappels-spark-essentiels",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#rappels-spark-essentiels",
    "title": "ğŸš€ PySpark Advanced",
    "section": "âš¡ 1. Rappels Spark Essentiels",
    "text": "âš¡ 1. Rappels Spark Essentiels\n\nğŸ’¡ Si tu as suivi le module 11 (PySpark for Data Engineering), cette section est un rappel rapide.\nSinon, commence par ce module avant de continuer â€” les concepts de base sont indispensables.\n\n\n1.1 SparkSession\n\n\nCode\nfrom pyspark.sql import SparkSession\n\n# CrÃ©er une SparkSession (point d'entrÃ©e unique)\nspark = SparkSession.builder \\\n    .appName(\"PySpark Advanced\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\nprint(f\"Spark version: {spark.version}\")\nprint(f\"App name: {spark.sparkContext.appName}\")\n\n\n\n\n1.2 DataFrame vs RDD\n\n\n\nAspect\nRDD\nDataFrame\n\n\n\n\nAPI\nBas niveau\nHaut niveau\n\n\nOptimisation\nManuelle\nCatalyst (automatique)\n\n\nPerformance\nBaseline\n10-100x plus rapide\n\n\nUsage\nLegacy, cas spÃ©ciaux\nStandard\n\n\n\nğŸ‘‰ RÃ¨gle : Toujours utiliser DataFrame/Dataset, jamais RDD (sauf cas trÃ¨s spÃ©cifiques).\n\n\n1.3 Transformations vs Actions\n\n\n\n\n\n\n\n\nType\nExemples\nExÃ©cution\n\n\n\n\nTransformation\nfilter, select, join, groupBy\nLazy (diffÃ©rÃ©e)\n\n\nAction\ncount, collect, write, show\nImmÃ©diate\n\n\n\n\n\n1.4 Lazy Evaluation\ndf.filter(...)     # Rien ne s'exÃ©cute\n  .select(...)     # Rien ne s'exÃ©cute\n  .groupBy(...)    # Rien ne s'exÃ©cute\n  .count()         # MAINTENANT tout s'exÃ©cute !\nAvantage : Spark peut optimiser lâ€™ensemble du pipeline avant exÃ©cution.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#architecture-interne-ce-que-les-dÃ©butants-ne-savent-pas",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#architecture-interne-ce-que-les-dÃ©butants-ne-savent-pas",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ§  2. Architecture Interne â€” Ce que les dÃ©butants ne savent pas",
    "text": "ğŸ§  2. Architecture Interne â€” Ce que les dÃ©butants ne savent pas\n\nLa vraie maÃ®trise de Spark commence ici. Comprendre lâ€™architecture interne te permet de prÃ©dire et rÃ©soudre les problÃ¨mes de performance.\n\n\n2.1 Le cycle de vie dâ€™un Job Spark\nCode Python/SQL\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Logical Plan     â”‚  Arbre d'opÃ©rations (ce que tu veux faire)\nâ”‚    (non optimisÃ©)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Catalyst       â”‚  ğŸ§  Optimiseur de requÃªtes\nâ”‚      Optimizer      â”‚  - Predicate pushdown\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Projection pruning\n           â”‚             - Join reordering\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Physical Plan     â”‚  Comment exÃ©cuter (stratÃ©gie)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚        DAG          â”‚  Directed Acyclic Graph\nâ”‚   (Stages + Tasks)  â”‚  - Stages = unitÃ©s de travail\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Tasks = exÃ©cution par partition\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Tungsten       â”‚  âš¡ ExÃ©cution optimisÃ©e\nâ”‚       Engine        â”‚  - Code generation\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Off-heap memory\n\n\n2.2 Catalyst Optimizer â€” Lâ€™arme secrÃ¨te\n\n\n\n\n\n\n\n\nOptimisation\nDescription\nGain\n\n\n\n\nPredicate pushdown\nFiltre appliquÃ© Ã  la source (Parquet, DB)\nI/O rÃ©duit drastiquement\n\n\nProjection pruning\nColonnes inutiles non lues\nI/O rÃ©duit\n\n\nConstant folding\nCalculs constants prÃ©-calculÃ©s\nCPU rÃ©duit\n\n\nJoin reordering\nOrdre optimal des joins\nShuffle rÃ©duit\n\n\n\n\n\n2.3 Tungsten Engine\n\nOff-heap memory : stockage hors JVM â†’ Ã©vite le Garbage Collector\nWhole-stage code generation : gÃ©nÃ¨re du bytecode optimisÃ© Ã  la volÃ©e\nVectorized execution : traitement par batch (comme Polars !)\n\n\n\nCode\nfrom pyspark.sql.functions import col, sum as spark_sum\n\n# CrÃ©er des donnÃ©es de test\ndata = [(i, f\"cat_{i % 5}\", float(i * 10)) for i in range(1000)]\ndf = spark.createDataFrame(data, [\"id\", \"category\", \"amount\"])\n\n# Pipeline avec transformations\nresult = (\n    df\n    .filter(col(\"amount\") &gt; 100)\n    .select(\"category\", \"amount\")\n    .groupBy(\"category\")\n    .agg(spark_sum(\"amount\").alias(\"total\"))\n)\n\n# Voir le plan d'exÃ©cution\nprint(\"=\" * 50)\nprint(\"PLAN D'EXÃ‰CUTION (explain)\")\nprint(\"=\" * 50)\nresult.explain(\"formatted\")\n\n\n\n\nCode\n# Plan complet avec toutes les Ã©tapes\nprint(\"PLAN COMPLET (Parsed â†’ Analyzed â†’ Optimized â†’ Physical)\")\nprint(\"=\" * 60)\nresult.explain(True)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#spark-submit-exÃ©cuter-spark-comme-un-pro",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#spark-submit-exÃ©cuter-spark-comme-un-pro",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ–¥ï¸ 3. spark-submit â€” ExÃ©cuter Spark comme un pro",
    "text": "ğŸ–¥ï¸ 3. spark-submit â€” ExÃ©cuter Spark comme un pro\n\nğŸ”¥ CompÃ©tence indispensable en entreprise. TrÃ¨s peu de formations lâ€™enseignent correctement.\n\n\n3.1 Pourquoi spark-submit ?\n\n\n\n\n\n\n\n\nContexte\nOutil\nUsage\n\n\n\n\nExploration, dÃ©veloppement\nNotebooks (Jupyter, Databricks)\nDev, prototypage\n\n\nProduction, CI/CD, scheduling\nspark-submit\nDÃ©ploiement rÃ©el\n\n\n\n\nğŸ’¡ En entreprise, spark-submit est rarement lancÃ© manuellement. Il est appelÃ© par : - Airflow (orchestration) â†’ Module 25 - CI/CD pipelines (GitLab CI, GitHub Actions) - Schedulers (cron, Kubernetes CronJobs)\n\n\n\n3.2 Deploy Modes\nCLIENT MODE                          CLUSTER MODE\nâ•â•â•â•â•â•â•â•â•â•â•                          â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Client    â”‚                      â”‚   Client    â”‚\nâ”‚   Machine   â”‚                      â”‚   Machine   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚             â”‚\nâ”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici       â”‚  (submit)   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n       â”‚                                    â”‚\n       â–¼                                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Cluster   â”‚                      â”‚   Cluster   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚Exec 1 â”‚  â”‚                      â”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\nâ”‚  â”‚Exec 2 â”‚  â”‚                      â”‚  â”‚Exec 1 â”‚  â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\nâ”‚  â”‚Exec 3 â”‚  â”‚                      â”‚  â”‚Exec 2 â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nUsage: Debug, interactif            Usage: Production\n\n\n\nMode\nDriver tourne sur\nUsage\n\n\n\n\nclient\nMachine qui soumet\nDebug, logs visibles\n\n\ncluster\nWorker du cluster\nProduction\n\n\n\n\n\n3.3 Resource Managers (Masters)\n\n\n\nMaster\nCommande\nUsage\n\n\n\n\nLocal\n--master local[*]\nDev/test (tous les cores)\n\n\nLocal (N cores)\n--master local[4]\nDev/test (4 cores)\n\n\nStandalone\n--master spark://host:7077\nCluster Spark simple\n\n\nYARN\n--master yarn\nClusters Hadoop\n\n\nKubernetes\n--master k8s://https://...\nCloud native\n\n\n\n\n\n3.4 Syntaxe complÃ¨te spark-submit\nspark-submit \\\n  # === Resource Manager ===\n  --master local[4] \\\n  --deploy-mode client \\\n  \n  # === Ressources ===\n  --driver-memory 4g \\\n  --executor-memory 8g \\\n  --executor-cores 4 \\\n  --num-executors 10 \\\n  \n  # === Configuration Spark ===\n  --conf spark.sql.shuffle.partitions=200 \\\n  --conf spark.sql.adaptive.enabled=true \\\n  --conf spark.executor.memoryOverhead=2g \\\n  \n  # === DÃ©pendances ===\n  --packages io.delta:delta-spark_2.12:3.2.0 \\\n  --jars /path/to/postgres-42.7.jar \\\n  --py-files utils.zip \\\n  \n  # === Application ===\n  main.py \\\n  \n  # === Arguments application ===\n  --date 2024-01-01 \\\n  --env prod\n\n\n3.5 Structure projet production\nspark_project/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e\nâ”‚   â”œâ”€â”€ etl/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ extract.py\nâ”‚   â”‚   â”œâ”€â”€ transform.py\nâ”‚   â”‚   â””â”€â”€ load.py\nâ”‚   â””â”€â”€ utils/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ config.py\nâ”‚       â””â”€â”€ logger.py\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ dev.yaml\nâ”‚   â””â”€â”€ prod.yaml\nâ”œâ”€â”€ jars/\nâ”‚   â””â”€â”€ postgres-42.7.jar\nâ”œâ”€â”€ scripts/\nâ”‚   â”œâ”€â”€ run_local.sh\nâ”‚   â””â”€â”€ run_cluster.sh\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ setup.py                 # Pour crÃ©er .whl\nâ””â”€â”€ README.md\n\n\n3.6 Packaging pour production\n# âŒ MAUVAIS : liste de fichiers (difficile Ã  maintenir)\n--py-files utils.py,config.py,helpers.py\n\n# âœ… MIEUX : Package .zip\ncd src/ && zip -r ../app.zip . && cd ..\nspark-submit --py-files app.zip main.py\n\n# âœ… MEILLEUR : Wheel (.whl) - le plus propre\npip wheel . -w dist/\nspark-submit --py-files dist/myproject-1.0.0-py3-none-any.whl main.py\n\nğŸ’¡ En production, prÃ©fÃ¨re .zip ou .whl pour un dÃ©ploiement propre et versionnÃ©.\n\n\n\nCode\n# Pattern : arguments en ligne de commande (main.py)\n\n# === Exemple de main.py pour spark-submit ===\nexample_main = '''\n#!/usr/bin/env python3\n\"\"\"Point d'entrÃ©e du job Spark.\"\"\"\n\nimport argparse\nfrom pyspark.sql import SparkSession\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"ETL Pipeline\")\n    parser.add_argument(\"--date\", required=True, help=\"Date de traitement (YYYY-MM-DD)\")\n    parser.add_argument(\"--env\", default=\"dev\", choices=[\"dev\", \"prod\"])\n    parser.add_argument(\"--input\", required=True, help=\"Chemin input\")\n    parser.add_argument(\"--output\", required=True, help=\"Chemin output\")\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    \n    spark = SparkSession.builder \\\n        .appName(f\"ETL-{args.env}-{args.date}\") \\\n        .getOrCreate()\n    \n    # Charger les donnÃ©es\n    df = spark.read.parquet(f\"{args.input}/date={args.date}\")\n    \n    # Transformations...\n    result = df.filter(df.amount &gt; 0)\n    \n    # Ã‰crire\n    result.write.mode(\"overwrite\").parquet(f\"{args.output}/date={args.date}\")\n    \n    spark.stop()\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nprint(example_main)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#partitionnement-shuffle-la-source-de-lenteur",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#partitionnement-shuffle-la-source-de-lenteur",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ“¦ 4. Partitionnement & Shuffle â€” LA source de lenteur",
    "text": "ğŸ“¦ 4. Partitionnement & Shuffle â€” LA source de lenteur\n\nâš ï¸ 80% des problÃ¨mes de performance Spark viennent du shuffle et du partitionnement.\n\n\n4.1 Quâ€™est-ce quâ€™un Shuffle ?\nLe shuffle est la redistribution des donnÃ©es entre partitions. Il est nÃ©cessaire pour : - groupBy, reduceByKey - join (sauf broadcast) - distinct, repartition - orderBy (tri global)\nSHUFFLE = GOULOT D'Ã‰TRANGLEMENT\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nStage 1                              Stage 2\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚                          â”‚Partitionâ”‚\nâ”‚    1    â”‚â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¶â”‚   1'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\nâ”‚    2    â”‚       â”‚  SHUFFLE â”‚       â”‚   2'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ (rÃ©seau) â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\nâ”‚    3    â”‚                          â”‚   3'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nCoÃ»t du Shuffle :\n1. SÃ©rialisation des donnÃ©es\n2. Ã‰criture sur disque (shuffle write)\n3. Transfert rÃ©seau\n4. Lecture depuis disque (shuffle read)\n5. DÃ©sÃ©rialisation\n\n\n4.2 repartition vs coalesce\n\n\n\n\n\n\n\n\nMÃ©thode\nShuffle\nUsage\n\n\n\n\nrepartition(n)\nâœ… Toujours\nAugmenter partitions, rÃ©Ã©quilibrer\n\n\ncoalesce(n)\nâŒ Non (si rÃ©duction)\nRÃ©duire partitions avant write\n\n\n\n\n\nCode\nfrom pyspark.sql.functions import spark_partition_id\n\n# CrÃ©er un DataFrame\ndf = spark.range(0, 1000000)\nprint(f\"Partitions initiales : {df.rdd.getNumPartitions()}\")\n\n# repartition = SHUFFLE (redistribue les donnÃ©es)\ndf_repart = df.repartition(10)\nprint(f\"AprÃ¨s repartition(10) : {df_repart.rdd.getNumPartitions()}\")\n\n# coalesce = PAS DE SHUFFLE (combine les partitions existantes)\ndf_coal = df_repart.coalesce(3)\nprint(f\"AprÃ¨s coalesce(3) : {df_coal.rdd.getNumPartitions()}\")\n\n# Voir la distribution des donnÃ©es par partition\nprint(\"\\nDistribution aprÃ¨s repartition(10):\")\ndf_repart.groupBy(spark_partition_id().alias(\"partition\")).count().orderBy(\"partition\").show()\n\n\n\n\n4.3 Taille optimale des partitions\n\n\n\nTaille partition\nProblÃ¨me\n\n\n\n\nTrop petit (&lt; 10 MB)\nOverhead de scheduling, trop de tÃ¢ches\n\n\nOptimal (128-256 MB)\nâœ… Sweet spot\n\n\nTrop grand (&gt; 1 GB)\nOOM, mauvaise parallÃ©lisation\n\n\n\nFormule :\nnum_partitions = data_size_mb / target_partition_size_mb\n\nExemple : 50 GB de donnÃ©es\nnum_partitions = 50000 MB / 200 MB = 250 partitions\n\n\n4.4 Data Skew â€” Le tueur de performance\nÃ‰QUILIBRÃ‰ (bon)                     SKEWED (problÃ¨me !)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚            â”‚100Kâ”‚â”‚100Kâ”‚â”‚100Kâ”‚â”‚        10M        â”‚\nâ””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nTemps: â–ˆâ–ˆâ–ˆâ–ˆ                         Temps: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n       4 tÃ¢ches parallÃ¨les                 â†‘\n       = temps minimal                     Un seul executor bloquÃ© !\n                                           Les autres attendent...\nTechniques anti-skew : 1. Broadcast join : si une table est petite (&lt; 100 MB) 2. Salting : ajouter une clÃ© alÃ©atoire pour distribuer 3. AQE (Adaptive Query Execution) : Spark 3.0+ gÃ¨re automatiquement\n\n\nCode\n# Activer AQE (recommandÃ© Spark 3.0+)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# VÃ©rifier la configuration\nprint(\"AQE enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\nprint(\"Skew Join enabled:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))\n\n\n\n\n4.5 Caching & Persistence\nQuand utiliser le cache ? - DataFrame rÃ©utilisÃ© dans plusieurs actions - Calcul coÃ»teux (join, aggregation) rÃ©utilisÃ© - ItÃ©rations (ML training)\ncache() vs persist()\n# cache() = persist(StorageLevel.MEMORY_AND_DISK)\ndf.cache()\n\n# persist() = contrÃ´le fin du niveau de stockage\nfrom pyspark import StorageLevel\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n\n\nNiveau\nRAM\nDisque\nSÃ©rialisÃ©\nUsage\n\n\n\n\nMEMORY_ONLY\nâœ…\nâŒ\nâŒ\nPetit DF, RAM suffisante\n\n\nMEMORY_AND_DISK\nâœ…\nâœ…\nâŒ\nDÃ©faut (cache())\n\n\nMEMORY_ONLY_SER\nâœ…\nâŒ\nâœ…\nÃ‰conomie RAM\n\n\nDISK_ONLY\nâŒ\nâœ…\nâœ…\nTrÃ¨s gros DF\n\n\nOFF_HEAP\nâœ…\nâŒ\nâœ…\nÃ‰viter GC Java\n\n\n\n\n\nCode\nfrom pyspark import StorageLevel\nimport time\n\n# CrÃ©er un DataFrame avec calculs\ndf = spark.range(0, 5000000).withColumn(\"squared\", col(\"id\") ** 2)\n\n# Sans cache : chaque action recalcule tout\nstart = time.time()\ncount1 = df.filter(col(\"squared\") &gt; 1000000).count()\ncount2 = df.filter(col(\"squared\") &lt; 100).count()\nprint(f\"Sans cache : {time.time() - start:.2f}s\")\n\n# Avec cache : calcul une seule fois\ndf_cached = df.cache()\n\nstart = time.time()\ncount1 = df_cached.filter(col(\"squared\") &gt; 1000000).count()\ncount2 = df_cached.filter(col(\"squared\") &lt; 100).count()\nprint(f\"Avec cache : {time.time() - start:.2f}s\")\n\n# âš ï¸ IMPORTANT : libÃ©rer la mÃ©moire !\ndf_cached.unpersist()\nprint(\"\\nâœ… Cache libÃ©rÃ© avec unpersist()\")\n\n\nâš ï¸ Quand NE PAS cacher :\n\n\n\nSituation\nPourquoi\n\n\n\n\nDF utilisÃ© une seule fois\nGaspillage mÃ©moire\n\n\nDF trÃ¨s volumineux (&gt; RAM)\nDÃ©bordement disque lent\n\n\nAvant un shuffle\nInutile, donnÃ©es redistribuÃ©es\n\n\nPipeline simple et rapide\nOverhead du cache &gt; gain",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#joins-avancÃ©s-la-compÃ©tence-qui-change-tout",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#joins-avancÃ©s-la-compÃ©tence-qui-change-tout",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ”— 5. Joins AvancÃ©s â€” La compÃ©tence qui change tout",
    "text": "ğŸ”— 5. Joins AvancÃ©s â€” La compÃ©tence qui change tout\n\nUn excellent Data Engineer sait optimiser ses joins. Câ€™est souvent lÃ  que se gagne (ou se perd) le plus de temps.\n\n\n5.1 Types de Joins internes Spark\n\n\n\n\n\n\n\n\nType\nQuand Spark lâ€™utilise\nPerformance\n\n\n\n\nBroadcast Hash Join\nPetite table (&lt; seuil)\nâ­â­â­ Meilleur\n\n\nSort Merge Join\nGrandes tables\nâ­â­ Standard\n\n\nShuffle Hash Join\nTables moyennes\nâ­ Ã‰viter si possible\n\n\n\n\n\n5.2 Broadcast Join â€” Ton meilleur ami\nSORT MERGE JOIN (shuffle)           BROADCAST JOIN (pas de shuffle)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n   Big Table      Small Table          Big Table      Small Table\n   â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”\n   â”‚  P1  â”‚       â”‚  P1  â”‚             â”‚  P1  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n   â”‚  P2  â”‚       â”‚  P2  â”‚             â”‚  P2  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ COPY â”‚\n   â”‚  P3  â”‚       â”‚  P3  â”‚             â”‚  P3  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n   â””â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”¬â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜\n      â”‚   SHUFFLE    â”‚                    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               Broadcast to\n             â”‚                       all executors\n         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”                   (pas de shuffle !)\n         â”‚ JOIN  â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\nfrom pyspark.sql.functions import broadcast\n\n# Grande table (10M lignes simulÃ©es)\nbig_df = spark.range(0, 1000000).withColumn(\"category_id\", (col(\"id\") % 100).cast(\"int\"))\n\n# Petite table (100 lignes)\nsmall_df = spark.createDataFrame(\n    [(i, f\"Category {i}\") for i in range(100)],\n    [\"category_id\", \"category_name\"]\n)\n\n# âŒ SANS broadcast hint (Spark peut ou non broadcaster)\nresult_no_hint = big_df.join(small_df, \"category_id\")\nprint(\"Sans broadcast hint:\")\nresult_no_hint.explain()\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# âœ… AVEC broadcast hint (force le broadcast)\nresult_broadcast = big_df.join(broadcast(small_df), \"category_id\")\nprint(\"Avec broadcast():\")\nresult_broadcast.explain()\n\n\n\n\n5.3 Configurer le seuil de broadcast\n# DÃ©faut : 10 MB\n# Si une table &lt; 10 MB â†’ broadcast automatique\n\n# Augmenter pour broadcaster des tables plus grandes\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)  # 100 MB\n\n# DÃ©sactiver le broadcast automatique (forcer shuffle)\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\n\n5.4 Join Hints (Spark 3.0+)\n# DataFrame API\nbig_df.join(small_df.hint(\"broadcast\"), \"key\")\n\n# SQL\nspark.sql(\"\"\"\n    SELECT /*+ BROADCAST(small_table) */ *\n    FROM big_table\n    JOIN small_table ON big_table.key = small_table.key\n\"\"\")\n\n\n5.5 Anti-pattern : Join sur clÃ© skewed\n\n\nCode\n# Configuration pour gÃ©rer le skew automatiquement (AQE)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\nprint(\"âœ… AQE Skew Join activÃ©\")\nprint(\"   Spark va automatiquement dÃ©tecter et gÃ©rer les partitions skewed\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#lectureÃ©criture-optimisÃ©es",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#lectureÃ©criture-optimisÃ©es",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ“ 6. Lecture/Ã‰criture OptimisÃ©es",
    "text": "ğŸ“ 6. Lecture/Ã‰criture OptimisÃ©es\n\n6.1 Formats : Parquet toujours !\n\n\n\nFormat\nLecture\nÃ‰criture\nCompression\nPredicate Pushdown\n\n\n\n\nCSV\nğŸ¢ Lent\nğŸ¢ Lent\nâŒ\nâŒ\n\n\nJSON\nğŸ¢ Lent\nğŸ¢ Lent\nâŒ\nâŒ\n\n\nParquet\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ…\n\n\nORC\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ…\n\n\nDelta\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ… + ACID\n\n\n\n\nğŸ”­ Les formats modernes (Delta, Iceberg, Hudi) seront approfondis dans le module 21 (Lakehouse) : - Transactions ACID - Time Travel - Vacuum, Compaction - Z-Ordering\n\n\n\n6.2 Predicate Pushdown\n\n\nCode\nimport os\nimport shutil\n\n# CrÃ©er des donnÃ©es de test\ntest_data = spark.range(0, 100000).withColumn(\"category\", (col(\"id\") % 10).cast(\"string\"))\n\n# Sauvegarder en Parquet\noutput_path = \"/tmp/test_parquet\"\nif os.path.exists(output_path):\n    shutil.rmtree(output_path)\ntest_data.write.parquet(output_path)\n\n# Lecture avec filtre â†’ predicate pushdown\ndf_filtered = spark.read.parquet(output_path).filter(col(\"id\") &lt; 100)\n\nprint(\"Plan d'exÃ©cution avec Predicate Pushdown:\")\ndf_filtered.explain()\n# Observe \"PushedFilters\" dans le plan !\n\n\n\n\n6.3 Partitionnement sur disque\n\n\nCode\nfrom pyspark.sql.functions import year, month, dayofmonth, lit\nfrom datetime import datetime, timedelta\nimport random\n\n# CrÃ©er des donnÃ©es avec dates\ndates = [(datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90))).strftime(\"%Y-%m-%d\") \n         for _ in range(10000)]\ndata = [(i, dates[i % len(dates)], float(random.randint(10, 1000))) \n        for i in range(10000)]\n\ndf = spark.createDataFrame(data, [\"id\", \"date\", \"amount\"])\ndf = df.withColumn(\"date\", col(\"date\").cast(\"date\"))\ndf = df.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n\n# Ã‰criture partitionnÃ©e\noutput_partitioned = \"/tmp/partitioned_data\"\nif os.path.exists(output_partitioned):\n    shutil.rmtree(output_partitioned)\n\ndf.write.partitionBy(\"year\", \"month\").parquet(output_partitioned)\n\nprint(\"Structure crÃ©Ã©e:\")\nfor root, dirs, files in os.walk(output_partitioned):\n    level = root.replace(output_partitioned, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    if level &lt; 2:  # Limiter la profondeur\n        for d in sorted(dirs)[:3]:\n            print(f\"{indent}  {d}/\")\n\n\n\n\n6.4 SchÃ©mas explicites\nPourquoi dÃ©finir un schÃ©ma ? - Ã‰vite lâ€™infÃ©rence (coÃ»teuse sur gros fichiers) - Garantit les types attendus - DÃ©tecte les erreurs tÃ´t\n\n\nCode\nfrom pyspark.sql.types import StructType, StructField, LongType, StringType, DecimalType, DateType\n\n# DÃ©finir un schÃ©ma explicite\nschema = StructType([\n    StructField(\"id\", LongType(), nullable=False),\n    StructField(\"customer_name\", StringType(), nullable=True),\n    StructField(\"amount\", DecimalType(10, 2), nullable=True),  # PrÃ©cision pour montants !\n    StructField(\"transaction_date\", DateType(), nullable=True)\n])\n\nprint(\"SchÃ©ma dÃ©fini:\")\nprint(schema.simpleString())\n\n# Utilisation\n# df = spark.read.schema(schema).parquet(\"data/transactions/\")\n\nprint(\"\\nğŸ’¡ Conseil : Utiliser DecimalType pour les montants financiers\")\nprint(\"   Ã‰vite les erreurs d'arrondi des float/double\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#udfs-le-piÃ¨ge-de-performance",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#udfs-le-piÃ¨ge-de-performance",
    "title": "ğŸš€ PySpark Advanced",
    "section": "âš ï¸ 7. UDFs : Le piÃ¨ge de performance",
    "text": "âš ï¸ 7. UDFs : Le piÃ¨ge de performance\n\n7.1 Pourquoi les Python UDFs sont toxiques\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 â”‚   SÃ©rialisation    â”‚                 â”‚\nâ”‚       JVM       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚     Python      â”‚\nâ”‚     (Spark)     â”‚                    â”‚   (UDF lente)   â”‚\nâ”‚                 â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   DÃ©sÃ©rialisation  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â”‚\n                    TRÃˆS COÃ›TEUX !\n                    (par ligne)\n\n\n7.2 HiÃ©rarchie de performance\n\n\n\n\n\n\n\n\nType\nPerformance\nQuand lâ€™utiliser\n\n\n\n\nExpressions Spark natives\nâ­â­â­â­â­\nToujours si possible\n\n\nPandas UDF (vectorized)\nâ­â­â­\nSi besoin Python\n\n\nPython UDF\nâ­\nDernier recours\n\n\n\n\n\n7.3 Remplacer UDF par expressions natives\n\n\nCode\nfrom pyspark.sql.functions import udf, when, col\nfrom pyspark.sql.types import StringType\nimport time\n\n# CrÃ©er des donnÃ©es de test\ndf = spark.range(0, 500000).withColumn(\"amount\", (col(\"id\") % 2000).cast(\"double\"))\n\n# âŒ MAUVAIS : Python UDF\n@udf(StringType())\ndef categorize_udf(amount):\n    if amount &gt; 1000:\n        return \"high\"\n    elif amount &gt; 100:\n        return \"medium\"\n    return \"low\"\n\nstart = time.time()\nresult_udf = df.withColumn(\"category\", categorize_udf(col(\"amount\")))\nresult_udf.write.mode(\"overwrite\").format(\"noop\").save()  # Force l'exÃ©cution\nudf_time = time.time() - start\nprint(f\"âŒ Python UDF : {udf_time:.2f}s\")\n\n# âœ… BON : Expression Spark native\nstart = time.time()\nresult_native = df.withColumn(\"category\",\n    when(col(\"amount\") &gt; 1000, \"high\")\n    .when(col(\"amount\") &gt; 100, \"medium\")\n    .otherwise(\"low\")\n)\nresult_native.write.mode(\"overwrite\").format(\"noop\").save()\nnative_time = time.time() - start\nprint(f\"âœ… Expression native : {native_time:.2f}s\")\n\nprint(f\"\\nğŸ“Š Speedup : {udf_time/native_time:.1f}x plus rapide avec expression native !\")\n\n\n\n\n7.4 Pandas UDF (si Python nÃ©cessaire)\n\n\nCode\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\nimport numpy as np\n\n# Pandas UDF = vectorisÃ© (traite des Series, pas des scalaires)\n@pandas_udf(\"double\")\ndef log_transform(s: pd.Series) -&gt; pd.Series:\n    return np.log1p(s)\n\n# Test\ndf = spark.range(0, 100000).withColumn(\"value\", col(\"id\").cast(\"double\"))\n\nstart = time.time()\nresult = df.withColumn(\"log_value\", log_transform(col(\"value\")))\nresult.write.mode(\"overwrite\").format(\"noop\").save()\nprint(f\"Pandas UDF : {time.time() - start:.2f}s\")\n\nresult.show(5)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#configuration-tuning",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#configuration-tuning",
    "title": "ğŸš€ PySpark Advanced",
    "section": "âš™ï¸ 8. Configuration & Tuning",
    "text": "âš™ï¸ 8. Configuration & Tuning\n\n8.1 ParamÃ¨tres essentiels\n\n\n\n\n\n\n\n\nParamÃ¨tre\nDÃ©faut\nRecommandation\n\n\n\n\nspark.sql.shuffle.partitions\n200\nAdapter Ã  la taille des donnÃ©es\n\n\nspark.default.parallelism\nSelon cluster\n2-3x num_cores\n\n\nspark.sql.autoBroadcastJoinThreshold\n10MB\n50-100MB\n\n\nspark.executor.memory\n1g\n4-16g selon cluster\n\n\nspark.driver.memory\n1g\n2-8g\n\n\nspark.executor.memoryOverhead\n10%\n15-20% pour PySpark\n\n\n\n\n\n8.2 Dimensionnement : Executors vs Cores\nâŒ MAUVAIS : 2 executors Ã— 10 cores chacun\n   - GC Java doit gÃ©rer Ã©norme heap (~50 GB)\n   - Si 1 executor crash â†’ 50% de perte\n   - ParallÃ©lisme moins granulaire\n\nâœ… BON : 10 executors Ã— 4 cores chacun  \n   - GC plus efficace (heap ~10 GB)\n   - Meilleure isolation des erreurs\n   - ParallÃ©lisme plus granulaire\nRÃ¨gles de dimensionnement :\nexecutor_cores = 4-5 max (sweet spot pour GC)\nexecutor_memory = 4-16g (selon donnÃ©es)\nnum_executors = (total_cores / executor_cores) - 1\n\n# RÃ©server pour le Driver et l'OS\ndriver_memory = 2-8g\nmemoryOverhead = 15-20% pour PySpark (sÃ©rialisation Python)\nExemple concret (cluster 100 cores, 400 GB RAM) :\nspark-submit \\\n  --executor-cores 4 \\\n  --executor-memory 12g \\\n  --num-executors 20 \\\n  --driver-memory 4g \\\n  --conf spark.executor.memoryOverhead=2g \\\n  main.py\n\n\n8.3 Adaptive Query Execution (AQE)\nLâ€™AQE (Spark 3.0+) rend certaines optimisations manuelles obsolÃ¨tes :\n\n\n\nAvant AQE (manuel)\nAvec AQE (automatique)\n\n\n\n\ncoalesce(n) aprÃ¨s filter\nAuto-coalesce des partitions\n\n\nCalculer shuffle.partitions\nAuto-optimize partitions\n\n\nDÃ©tecter skew manuellement\nAuto-skew handling\n\n\nBroadcast threshold fixe\nRuntime broadcast decisions\n\n\n\n\n\nCode\n# Configuration AQE complÃ¨te (recommandÃ©e)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n\n# Avec AQE, tu peux laisser shuffle.partitions Ã©levÃ©\n# Spark optimisera automatiquement\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n\nprint(\"âœ… Configuration AQE optimale :\")\nprint(f\"   adaptive.enabled = {spark.conf.get('spark.sql.adaptive.enabled')}\")\nprint(f\"   coalescePartitions = {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\nprint(f\"   skewJoin = {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\nprint(f\"   shuffle.partitions = {spark.conf.get('spark.sql.shuffle.partitions')}\")\nprint(\"\\nğŸ’¡ Avec AQE, Spark ajuste automatiquement le nombre de partitions !\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#spark-ui-diagnostic",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#spark-ui-diagnostic",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ” 9. Spark UI & Diagnostic",
    "text": "ğŸ” 9. Spark UI & Diagnostic\n\nSavoir lire Spark UI = savoir debugger. Câ€™est la compÃ©tence qui fait la diffÃ©rence.\n\n\n9.1 AccÃ©der Ã  Spark UI\n\nLocal : http://localhost:4040\nCluster : via le Resource Manager (YARN, K8s dashboard)\nDatabricks : intÃ©grÃ© dans lâ€™interface\n\n\n\n9.2 Les onglets importants\n\n\n\nOnglet\nInformation\n\n\n\n\nJobs\nVue dâ€™ensemble des jobs, durÃ©e\n\n\nStages\nDÃ©tail des stages, shuffle read/write\n\n\nStorage\nDataFrames en cache\n\n\nEnvironment\nConfiguration Spark\n\n\nExecutors\nRessources, GC, mÃ©moire\n\n\nSQL\nPlans dâ€™exÃ©cution des requÃªtes\n\n\n\n\n\n9.3 MÃ©triques Ã  surveiller\n\n\n\nMÃ©trique\nSignification\nğŸš¨ ProblÃ¨me siâ€¦\n\n\n\n\nShuffle Read/Write\nDonnÃ©es Ã©changÃ©es\nTrÃ¨s Ã©levÃ© (&gt; 10 GB)\n\n\nSpill (Memory/Disk)\nDÃ©bordement mÃ©moire\n&gt; 0\n\n\nTask Duration\nTemps par tÃ¢che\nTrÃ¨s variable (skew !)\n\n\nGC Time\nGarbage Collection\n&gt; 10% du temps total\n\n\nInput/Output\nDonnÃ©es lues/Ã©crites\nBeaucoup plus que prÃ©vu\n\n\n\n\n\n9.4 Patterns de problÃ¨mes\n\n\n\n\n\n\n\n\nSymptÃ´me dans Spark UI\nDiagnostic\nSolution\n\n\n\n\n1 tÃ¢che 10x plus longue\nData skew\nSalting, broadcast, AQE\n\n\nShuffle &gt; 50 GB\nJoin non optimisÃ©\nBroadcast join\n\n\nSpill to disk\nMÃ©moire insuffisante\nPlus de RAM, moins de partitions\n\n\nGC Time &gt; 20%\nTrop dâ€™objets Java\nTungsten, plus de memoryOverhead\n\n\nBeaucoup de petites tÃ¢ches\nTrop de partitions\nCoalesce, AQE\n\n\n\n\n\nCode\n# URL de Spark UI pour cette session\nprint(f\"ğŸ” Spark UI disponible sur : {spark.sparkContext.uiWebUrl}\")\nprint(\"\\nğŸ“Š Pour voir les mÃ©triques d'un job, lance une action puis consulte l'UI\")\n\n# Exemple : dÃ©clencher un job pour voir dans l'UI\ndf = spark.range(0, 1000000)\nresult = df.groupBy((col(\"id\") % 100).alias(\"group\")).count()\nresult.collect()  # Action qui dÃ©clenche le job\n\nprint(\"\\nâœ… Job exÃ©cutÃ© - consulte Spark UI pour voir les stages et mÃ©triques\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#bonnes-pratiques-anti-patterns",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#bonnes-pratiques-anti-patterns",
    "title": "ğŸš€ PySpark Advanced",
    "section": "âœ… 10. Bonnes pratiques & Anti-patterns",
    "text": "âœ… 10. Bonnes pratiques & Anti-patterns\n\nâŒ Anti-patterns (Ã  Ã©viter absolument)\n\n\n\n\n\n\n\n\nAnti-pattern\nPourquoi câ€™est mal\nSolution\n\n\n\n\ncollect() sur 100M lignes\nOOM Driver garanti\nwrite() vers fichier\n\n\nCSV en production\nLent, pas de schema\nParquet/Delta\n\n\nUDF Python partout\n10-100x plus lent\nExpressions natives\n\n\nshuffle.partitions=200 toujours\nPas adaptÃ© aux donnÃ©es\nAjuster ou AQE\n\n\nPas de cache() sur DF rÃ©utilisÃ©\nRecalcul inutile\ncache() + unpersist()\n\n\nIgnorer Spark UI\nDebug Ã  lâ€™aveugle\nToujours vÃ©rifier\n\n\nJoin sans broadcast\nShuffle Ã©norme\nbroadcast() sur petites tables\n\n\nrepartition() avant write()\nShuffle inutile\ncoalesce() pour rÃ©duire\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nBÃ©nÃ©fice\n\n\n\n\nToujours Parquet\nI/O optimisÃ©, predicate pushdown\n\n\nBroadcast petites tables\nÃ‰vite shuffle\n\n\nAQE activÃ©\nOptimisation runtime\n\n\nPartitionner par date\nPartition pruning\n\n\nÃ‰viter UDFs\nPerformance native\n\n\nMonitorer Spark UI\nDebug efficace\n\n\nCache + unpersist\nÃ‰vite recalculs\n\n\nSchema explicite\nÃ‰vite infÃ©rence",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#mini-projet-optimisation-dun-pipeline",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#mini-projet-optimisation-dun-pipeline",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸš€ Mini-Projet : Optimisation dâ€™un pipeline",
    "text": "ğŸš€ Mini-Projet : Optimisation dâ€™un pipeline\n\nğŸ¯ Objectif\nRÃ©duire un pipeline de 20 minutes Ã  &lt; 3 minutes en appliquant les techniques apprises.\n\n\nScÃ©nario : E-commerce Analytics\n\n\n\nTable\nLignes\nFormat initial\n\n\n\n\nTransactions\n5M\nParquet\n\n\nProduits\n10K\nCSV\n\n\nClients\n500K\nParquet\n\n\n\n\n\nArchitecture cible\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Transactions  â”‚    â”‚    Produits    â”‚    â”‚    Clients     â”‚\nâ”‚    (5M rows)   â”‚    â”‚   (10K rows)   â”‚    â”‚   (500K rows)  â”‚\nâ”‚   [Parquet]    â”‚    â”‚ [CSVâ†’Parquet]  â”‚    â”‚   [Parquet]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                     â”‚ broadcast           â”‚\n        â”‚                     â–¼                     â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\n                              â–¼\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚      Join + Aggregation       â”‚\n                 â”‚   - Broadcast products        â”‚\n                 â”‚   - AQE enabled               â”‚\n                 â”‚   - Native expressions        â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                 â”‚\n                                 â–¼\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚      Partitioned Output       â”‚\n                 â”‚   partitionBy(\"year\",\"month\") â”‚\n                 â”‚         [Parquet]             â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\nimport os\nimport shutil\nfrom pyspark.sql.functions import *\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\n# Setup : crÃ©er les donnÃ©es de test\nprint(\"ğŸ“¦ CrÃ©ation des donnÃ©es de test...\")\n\n# Configuration optimale\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# Nettoyer les rÃ©pertoires\nfor path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \"/tmp/output\"]:\n    if os.path.exists(path):\n        shutil.rmtree(path)\n\n# 1. Transactions (5M lignes)\ntransactions = spark.range(0, 500000).select(\n    col(\"id\").alias(\"transaction_id\"),\n    (col(\"id\") % 10000).alias(\"product_id\"),\n    (col(\"id\") % 50000).alias(\"customer_id\"),\n    (rand() * 1000).alias(\"amount\"),\n    date_add(lit(\"2024-01-01\"), (rand() * 90).cast(\"int\")).alias(\"date\")\n)\ntransactions.write.parquet(\"/tmp/transactions\")\nprint(f\"âœ… Transactions : {transactions.count()} lignes\")\n\n# 2. Produits (10K lignes) - CSV intentionnellement\nproducts_data = [(i, f\"Product_{i}\", f\"Category_{i % 50}\", float(10 + i % 100)) \n                 for i in range(10000)]\nproducts = spark.createDataFrame(products_data, \n    [\"product_id\", \"product_name\", \"category\", \"base_price\"])\nproducts.write.mode(\"overwrite\").option(\"header\", True).csv(\"/tmp/products\")\nprint(f\"âœ… Produits : {products.count()} lignes (CSV)\")\n\n# 3. Clients (500K lignes)\ncustomers = spark.range(0, 50000).select(\n    col(\"id\").alias(\"customer_id\"),\n    concat(lit(\"Customer_\"), col(\"id\")).alias(\"customer_name\"),\n    (col(\"id\") % 5).alias(\"segment\")\n)\ncustomers.write.parquet(\"/tmp/customers\")\nprint(f\"âœ… Clients : {customers.count()} lignes\")\n\nprint(\"\\nğŸ“Š DonnÃ©es crÃ©Ã©es avec succÃ¨s !\")\n\n\n\n\nCode\n# âŒ VERSION NON OPTIMISÃ‰E (baseline)\nprint(\"=\"*50)\nprint(\"âŒ PIPELINE NON OPTIMISÃ‰\")\nprint(\"=\"*50)\n\n# DÃ©sactiver AQE pour le baseline\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n\nstart = time.time()\n\n# Lecture\ntransactions_df = spark.read.parquet(\"/tmp/transactions\")\nproducts_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/products\")  # âŒ InfÃ©rence\ncustomers_df = spark.read.parquet(\"/tmp/customers\")\n\n# UDF non optimisÃ©\n@udf(StringType())\ndef get_amount_category(amount):\n    if amount &gt; 500: return \"high\"\n    elif amount &gt; 100: return \"medium\"\n    return \"low\"\n\n# Joins sans broadcast\nresult = transactions_df \\\n    .join(products_df, \"product_id\") \\\n    .join(customers_df, \"customer_id\") \\\n    .withColumn(\"amount_category\", get_amount_category(col(\"amount\"))) \\\n    .groupBy(\"category\", \"segment\", \"amount_category\") \\\n    .agg(\n        count(\"*\").alias(\"num_transactions\"),\n        sum(\"amount\").alias(\"total_amount\")\n    )\n\n# Ã‰criture non partitionnÃ©e\nif os.path.exists(\"/tmp/output\"):\n    shutil.rmtree(\"/tmp/output\")\nresult.write.parquet(\"/tmp/output\")\n\nbaseline_time = time.time() - start\nprint(f\"\\nâ±ï¸ Temps baseline : {baseline_time:.2f}s\")\n\n\n\n\nCode\n# âœ… VERSION OPTIMISÃ‰E\nprint(\"=\"*50)\nprint(\"âœ… PIPELINE OPTIMISÃ‰\")\nprint(\"=\"*50)\n\n# Activer AQE\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\nstart = time.time()\n\n# 1. Lecture avec schema explicite pour CSV\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n\nproducts_schema = StructType([\n    StructField(\"product_id\", IntegerType()),\n    StructField(\"product_name\", StringType()),\n    StructField(\"category\", StringType()),\n    StructField(\"base_price\", DoubleType())\n])\n\ntransactions_df = spark.read.parquet(\"/tmp/transactions\")\nproducts_df = spark.read.option(\"header\", True).schema(products_schema).csv(\"/tmp/products\")  # âœ… Schema explicite\ncustomers_df = spark.read.parquet(\"/tmp/customers\")\n\n# 2. Broadcast les petites tables\nproducts_df = broadcast(products_df)\n\n# 3. Expression native au lieu de UDF\namount_category_expr = when(col(\"amount\") &gt; 500, \"high\") \\\n    .when(col(\"amount\") &gt; 100, \"medium\") \\\n    .otherwise(\"low\")\n\n# 4. Pipeline optimisÃ©\nresult_optimized = transactions_df \\\n    .join(products_df, \"product_id\") \\\n    .join(customers_df, \"customer_id\") \\\n    .withColumn(\"amount_category\", amount_category_expr) \\\n    .withColumn(\"year\", year(\"date\")) \\\n    .withColumn(\"month\", month(\"date\")) \\\n    .groupBy(\"category\", \"segment\", \"amount_category\", \"year\", \"month\") \\\n    .agg(\n        count(\"*\").alias(\"num_transactions\"),\n        sum(\"amount\").alias(\"total_amount\")\n    )\n\n# 5. Ã‰criture partitionnÃ©e\nif os.path.exists(\"/tmp/output_optimized\"):\n    shutil.rmtree(\"/tmp/output_optimized\")\nresult_optimized.write.partitionBy(\"year\", \"month\").parquet(\"/tmp/output_optimized\")\n\noptimized_time = time.time() - start\nprint(f\"\\nâ±ï¸ Temps optimisÃ© : {optimized_time:.2f}s\")\n\n\n\n\nCode\n# RÃ©sumÃ© des optimisations\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š RÃ‰SUMÃ‰ DES OPTIMISATIONS\")\nprint(\"=\"*60)\n\nspeedup = baseline_time / optimized_time if optimized_time &gt; 0 else 0\nreduction = (1 - optimized_time / baseline_time) * 100 if baseline_time &gt; 0 else 0\n\nprint(f\"\"\"\nâ±ï¸ Temps baseline    : {baseline_time:.2f}s\nâ±ï¸ Temps optimisÃ©    : {optimized_time:.2f}s\nğŸš€ Speedup           : {speedup:.1f}x\nğŸ“‰ RÃ©duction         : {reduction:.0f}%\n\nOptimisations appliquÃ©es :\n  âœ… AQE activÃ© (Adaptive Query Execution)\n  âœ… Schema explicite pour CSV (pas d'infÃ©rence)\n  âœ… Broadcast join pour products (10K lignes)\n  âœ… Expression native au lieu de UDF Python\n  âœ… Ã‰criture partitionnÃ©e par year/month\n\"\"\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#quiz-de-fin-de-module",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quel composant de Spark optimise automatiquement le plan de requÃªte ?\n\nTungsten\n\nCatalyst\n\nDriver\n\nExecutor\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Catalyst est lâ€™optimiseur de requÃªtes qui applique predicate pushdown, projection pruning, et join reordering.\n\n\n\n\nâ“ Q2. Quelle opÃ©ration cause un shuffle ?\n\nfilter()\n\nselect()\n\ngroupBy()\n\nwithColumn()\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” groupBy() nÃ©cessite un shuffle pour regrouper les donnÃ©es par clÃ©. Les autres sont des transformations narrow.\n\n\n\n\nâ“ Q3. Quelle mÃ©thode utiliser pour rÃ©duire le nombre de partitions SANS shuffle ?\n\nrepartition()\n\ncoalesce()\n\npartitionBy()\n\nbucketBy()\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” coalesce() combine les partitions existantes sans shuffle (si rÃ©duction). repartition() cause toujours un shuffle.\n\n\n\n\nâ“ Q4. Quelle est la taille optimale dâ€™une partition Spark ?\n\n1-10 MB\n\n128-256 MB\n\n1-2 GB\n\n10-50 GB\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” 128-256 MB est le sweet spot. Trop petit = overhead, trop grand = OOM et mauvaise parallÃ©lisation.\n\n\n\n\nâ“ Q5. Pour joindre une table de 10 GB avec une table de 50 MB, quelle stratÃ©gie utiliser ?\n\nSort Merge Join\n\nShuffle Hash Join\n\nBroadcast Join\n\nNested Loop Join\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Broadcast Join envoie la petite table (50 MB) Ã  tous les executors, Ã©vitant le shuffle de la grande table.\n\n\n\n\nâ“ Q6. Pourquoi les Python UDFs sont-ils lents ?\n\nPython est un langage interprÃ©tÃ©\n\nSÃ©rialisation JVM â†”ï¸ Python pour chaque ligne\n\nLe GIL de Python\n\nManque de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Chaque ligne nÃ©cessite une sÃ©rialisation JVMâ†’Python et dÃ©sÃ©rialisation Pythonâ†’JVM, ce qui est trÃ¨s coÃ»teux.\n\n\n\n\nâ“ Q7. Que signifie â€œSpill to diskâ€ dans Spark UI ?\n\nLes donnÃ©es sont Ã©crites en Parquet\n\nLa mÃ©moire est insuffisante, donnÃ©es Ã©crites sur disque\n\nLe cache est activÃ©\n\nLe shuffle est terminÃ©\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Spill indique que la mÃ©moire est insuffisante et les donnÃ©es dÃ©bordent sur le disque, ce qui ralentit lâ€™exÃ©cution.\n\n\n\n\nâ“ Q8. Quel deploy mode utiliser en production ?\n\nclient\n\ncluster\n\nlocal\n\nstandalone\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En mode cluster, le Driver tourne sur un worker du cluster, ce qui est plus robuste pour la production.\n\n\n\n\nâ“ Q9. Que fait lâ€™AQE (Adaptive Query Execution) ?\n\nCompile le code Python\n\nOptimise le plan dâ€™exÃ©cution au runtime\n\nCompresse les donnÃ©es\n\nGÃ¨re lâ€™authentification\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” AQE optimise dynamiquement le plan dâ€™exÃ©cution pendant lâ€™exÃ©cution (coalesce, skew handling, broadcast).\n\n\n\n\nâ“ Q10. Combien de cores par executor est recommandÃ© ?\n\n1 core\n\n4-5 cores\n\n10-15 cores\n\nTous les cores disponibles\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” 4-5 cores est le sweet spot. Plus de cores = heap plus grand = GC moins efficace.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#ressources-pour-aller-plus-loin",
    "title": "ğŸš€ PySpark Advanced",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark SQL Guide\nSpark Configuration\nTuning Spark\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Spark Performance Tuning\nUnderstanding Spark Shuffle\n\n\n\nğŸ”§ Outils\n\nSpark UI â€” Diagnostic local\nspark-submit â€” Guide officiel",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#prochaine-Ã©tape",
    "title": "ğŸš€ PySpark Advanced",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises lâ€™optimisation Spark, passons aux fonctionnalitÃ©s SQL avancÃ©es !\nğŸ‘‰ Module suivant : 20_spark_sql_deep_dive.ipynb â€” Spark SQL Deep Dive\nTu vas apprendre : - Window functions avancÃ©es - CTEs et subqueries - Optimisation SQL - Spark SQL vs DataFrame API\n\n\nâš ï¸ Note : Spark Streaming sera couvert dans les modules 23-24 aprÃ¨s Kafka.\n\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nArchitecture\nCatalyst, Tungsten, DAG\n\n\nspark-submit\nDeploy modes, packaging, structure projet\n\n\nPartitionnement\nShuffle, repartition vs coalesce, skew\n\n\nCaching\ncache() vs persist(), storage levels\n\n\nJoins\nBroadcast, Sort Merge, hints\n\n\nI/O\nParquet, partitionnement disque, schemas\n\n\nUDFs\nÃ‰viter Python UDF, expressions natives\n\n\nTuning\nAQE, executors/cores, configuration\n\n\nDiagnostic\nSpark UI, mÃ©triques\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module PySpark Advanced.\n\n\nCode\n# Nettoyage\nspark.stop()\nprint(\"âœ… SparkSession arrÃªtÃ©e\")\n\n# Nettoyage des fichiers temporaires (optionnel)\n# import shutil\n# for path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \n#              \"/tmp/output\", \"/tmp/output_optimized\", \"/tmp/test_parquet\", \"/tmp/partitioned_data\"]:\n#     if os.path.exists(path):\n#         shutil.rmtree(path)\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸš€ PySpark Advanced"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  containeriser tes applications, lancer des services data en quelques secondes, et packager tes pipelines ETL de maniÃ¨re reproductible et portable â€” des compÃ©tences indispensables pour un Data Engineer moderne !",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi les modules 01_intro_data_engineering et 02_bash_for_data_engineers\n\n\nâœ… Requis\nConnaissances de base en Python\n\n\nâœ… Requis\nAvoir accÃ¨s Ã  un terminal (Linux, Mac, ou Windows avec WSL)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nExpliquer ce quâ€™est Docker et pourquoi il est essentiel en Data Engineering\nInstaller Docker sur Windows, macOS ou Linux\nLancer des services data (PostgreSQL, Kafka, Sparkâ€¦) en une commande\nÃ‰crire un Dockerfile pour packager un script ETL\nUtiliser les volumes, rÃ©seaux et docker-compose\nDÃ©bugger des containers comme un pro\nAppliquer les bonnes pratiques professionnelles",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#cest-quoi-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#cest-quoi-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ§  Câ€™est quoi Docker ?",
    "text": "ğŸ§  Câ€™est quoi Docker ?\n\nğŸ³ Docker est un outil qui permet dâ€™exÃ©cuter des applications dans des environnements isolÃ©s, reproductibles et portables, appelÃ©s containers.\n\nAu lieu dâ€™installer une application (et toutes ses dÃ©pendances) directement sur ton systÃ¨me, tu la mets dans une â€œboÃ®teâ€ (le container) avec tout ce dont elle a besoin.\n\nğŸ”„ Docker â‰  Machine Virtuelle\n\n\n\n\n\n\n\n\nAspect\nMachine Virtuelle (VM)\nContainer Docker\n\n\n\n\nContenu\nOS complet + kernel + drivers + apps\nApp + libs + systÃ¨me minimal\n\n\nTaille\nPlusieurs Go\nQuelques Mo Ã  centaines de Mo\n\n\nDÃ©marrage\nMinutes\nSecondes/millisecondes\n\n\nPerformance\nOverhead important\nQuasi-natif\n\n\nIsolation\nComplÃ¨te (hyperviseur)\nAu niveau processus\n\n\n\n\n\nğŸ¯ Analogies pour bien comprendre\n\n\n\n\n\n\n\nAnalogie\nExplication\n\n\n\n\nğŸ§³ La valise prÃªte\nUn container = une valise dÃ©jÃ  remplie. Tu la prends, tu voyages, tu es opÃ©rationnel partout.\n\n\nğŸ± Le tupperware\nTu prÃ©pares un plat, tu le mets dans une boÃ®te hermÃ©tique. Chez toi ou ailleurs : câ€™est le mÃªme plat.\n\n\nğŸ“¦ Le zip complet\nCode + librairies + config dans un package. Mais avec la garantie que lâ€™exÃ©cution est identique partout.\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nDocker a Ã©tÃ© crÃ©Ã© en 2013 par Solomon Hykes chez dotCloud (devenu Docker, Inc.).\nLe nom vient des dockers (ouvriers portuaires) qui chargent et dÃ©chargent des containers sur les bateaux â€” exactement ce que fait Docker avec les applications !\nğŸ“– Histoire de Docker sur Wikipedia",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#pourquoi-docker-est-indispensable-pour-un-data-engineer",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#pourquoi-docker-est-indispensable-pour-un-data-engineer",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸš€ 1. Pourquoi Docker est indispensable pour un Data Engineer",
    "text": "ğŸš€ 1. Pourquoi Docker est indispensable pour un Data Engineer\nEn Data Engineering, tu dois souvent :\n\nManipuler des bases de donnÃ©es (PostgreSQL, MySQL, MongoDBâ€¦)\nLancer des brokers de messages (Kafka, RabbitMQâ€¦)\nDÃ©ployer des pipelines ETL\nFaire tourner des jobs Spark ou des APIs\n\n\nâŒ Sans Docker\n\n\n\n\n\n\n\nProblÃ¨me\nConsÃ©quence\n\n\n\n\nInstallation manuelle complexe\nHeures perdues en config\n\n\nConflits de versions (Java, Python, drivers)\nâ€œÃ‡a marchait hierâ€¦â€\n\n\nEnvironnements diffÃ©rents (local â‰  prod)\nBugs en production uniquement\n\n\nOnboarding difficile\nNouveaux = 2 jours pour installer\n\n\n\n\n\nâœ… Avec Docker\n\n\n\nAvantage\nExemple concret\n\n\n\n\nPostgreSQL en 1 commande\ndocker run postgres:16\n\n\nTest Kafka + Spark sur laptop\nStack complÃ¨te en local\n\n\nETL packagÃ© et portable\nTourne identique partout\n\n\nOnboarding en 10 minutes\ndocker-compose up et câ€™est parti\n\n\n\n\nğŸ’¡ En rÃ©sumÃ© : Docker est un outil central pour crÃ©er des environnements data reproductibles et industrialisables. Fini le ğŸ˜… â€œchez moi Ã§a marcheâ€ !",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#concepts-clÃ©s-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#concepts-clÃ©s-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ”‘ 2. Concepts clÃ©s Docker",
    "text": "ğŸ”‘ 2. Concepts clÃ©s Docker\nAvant dâ€™aller plus loin, maÃ®trise ces 6 notions fondamentales :\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nImage\nModÃ¨le figÃ© (blueprint) contenant OS + dÃ©pendances + code\nUne recette de cuisine\n\n\nContainer\nInstance en cours dâ€™exÃ©cution dâ€™une image\nUn plat prÃ©parÃ© Ã  partir de la recette\n\n\nRegistry\nMagasin dâ€™images (Docker Hub, ECR, GHCR)\nUn catalogue de recettes en ligne\n\n\nVolume\nStockage persistant en dehors du container\nUn disque dur externe branchÃ©\n\n\nNetwork\nRÃ©seau virtuel entre containers\nUn rÃ©seau local privÃ©\n\n\nBuild context\nFichiers envoyÃ©s Ã  Docker lors du build\nLe dossier de travail\n\n\n\n\nğŸ“¦ Image Docker\nUne image contient : - Un systÃ¨me de base (ex: python:3.11-slim) - Des bibliothÃ¨ques (pandas, pyarrow, pysparkâ€¦) - Ton code (scripts, fichiers de config)\nOn ne modifie pas une image â€” on en reconstruit une nouvelle Ã  partir dâ€™un Dockerfile.\n\n\nğŸƒ Container\nUn container est une instance vivante dâ€™une image : - Tu crÃ©es une image â†’ tu la lances â†’ tu obtiens un container - Tu peux dÃ©marrer, arrÃªter, supprimer un container sans toucher Ã  lâ€™image - Plusieurs containers peuvent tourner Ã  partir de la mÃªme image\n\n\nğŸŒ Registry\nUn registry stocke et partage les images : - Docker Hub (public/privÃ©) â€” le plus connu - GitHub Container Registry (GHCR) - AWS ECR, GCP Artifact Registry, Azure ACR\n\n\nğŸ’¾ Volume\nLes donnÃ©es ne doivent pas â€œmourirâ€ avec le container : - Sans volume : container supprimÃ© = donnÃ©es perdues - Avec volume : donnÃ©es persistantes, partageables\n\n\nğŸ”— Network\nPermet aux containers de communiquer entre eux : - Ex: container etl se connecte Ã  container postgres - Isolation du rÃ©seau de la machine hÃ´te\n\n\nğŸ“ Build context\nQuand tu fais docker build -t mon-image . : - Le . = tout ce que Docker envoie au daemon - Dossier de 10 Go = envoi de 10 Go ğŸ¤¯ - Dâ€™oÃ¹ lâ€™importance du .dockerignore !\n\n\nğŸ–¼ï¸ SchÃ©ma visuel : Architecture Docker\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      DOCKER HOST                            â”‚\nâ”‚               (Laptop / Server / Cloud)                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚                  DOCKER ENGINE                       â”‚   â”‚\nâ”‚  â”‚                                                      â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚\nâ”‚  â”‚   â”‚  IMAGE   â”‚  â”‚  IMAGE   â”‚  â”‚  IMAGE   â”‚         â”‚   â”‚\nâ”‚  â”‚   â”‚ postgres â”‚  â”‚  python  â”‚  â”‚  spark   â”‚         â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚   â”‚\nâ”‚  â”‚        â”‚             â”‚             â”‚                â”‚   â”‚\nâ”‚  â”‚        â–¼             â–¼             â–¼                â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚   â”‚\nâ”‚  â”‚   â”‚CONTAINER â”‚  â”‚CONTAINER â”‚  â”‚CONTAINER â”‚         â”‚   â”‚\nâ”‚  â”‚   â”‚ de-postgresâ”‚ â”‚  de-etl  â”‚  â”‚ de-spark â”‚         â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   â”‚\nâ”‚  â”‚        â”‚                                            â”‚   â”‚\nâ”‚  â”‚        â–¼                                            â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚   â”‚\nâ”‚  â”‚   â”‚  VOLUME  â”‚       â”‚   NETWORK    â”‚              â”‚   â”‚\nâ”‚  â”‚   â”‚ pg_data  â”‚       â”‚  de-network  â”‚              â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#installation-de-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#installation-de-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ’» 3. Installation de Docker",
    "text": "ğŸ’» 3. Installation de Docker\n\n\n\nSystÃ¨me\nComment installer\n\n\n\n\nğŸªŸ Windows\nDocker Desktop + WSL2\n\n\nğŸ macOS\nDocker Desktop (Intel ou Apple Silicon)\n\n\nğŸ§ Linux\nDocker Engine (apt/yum)\n\n\n\n\nğŸªŸ Windows (Docker Desktop + WSL2)\n1. Activer WSL2 :\n# Dans PowerShell en administrateur\nwsl --install\n2. TÃ©lÃ©charger Docker Desktop : - ğŸ”— https://www.docker.com/products/docker-desktop/\n3. Installer et redÃ©marrer\n4. Tester :\ndocker --version\ndocker run --rm hello-world\n\n\n\nğŸ macOS (Intel & Apple Silicon)\n1. TÃ©lÃ©charger Docker Desktop : - ğŸ”— https://www.docker.com/products/docker-desktop/ - Choisir la version Intel ou Apple Silicon (M1/M2/M3)\n2. Glisser lâ€™app dans Applications\n3. Lancer Docker Desktop (icÃ´ne ğŸ³ dans la barre)\n4. Tester :\ndocker --version\ndocker run --rm hello-world\n\n\n\nğŸ§ Linux (Ubuntu/Debian)\n# 1. Mettre Ã  jour et installer les prÃ©requis\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\n\n# 2. Ajouter la clÃ© GPG officielle\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\n# 3. Ajouter le dÃ©pÃ´t Docker\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# 4. Installer Docker\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n# 5. Tester\nsudo docker run --rm hello-world\n\n# 6. (Optionnel) Utiliser Docker sans sudo\nsudo usermod -aG docker $USER\n# Puis dÃ©connexion/reconnexion\n\n\nâœ… VÃ©rifier ton installation\n\n\nCode\n%%bash\n# VÃ©rifier la version de Docker\ndocker --version\n\n# VÃ©rifier que Docker fonctionne\ndocker run --rm hello-world",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#commandes-docker-essentielles",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#commandes-docker-essentielles",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ› ï¸ 4. Commandes Docker essentielles",
    "text": "ğŸ› ï¸ 4. Commandes Docker essentielles\nVoici ton cheat sheet de base :\n\nğŸƒ Lancer un container\ndocker run image                    # Lancer un container\ndocker run -d image                 # En arriÃ¨re-plan (detached)\ndocker run -it image bash           # Mode interactif\ndocker run --rm image               # Supprime le container Ã  la fin\ndocker run --name mon-container image  # Nommer le container\n\n\nğŸ“‹ Lister\ndocker ps                           # Containers en cours\ndocker ps -a                        # Tous les containers (y compris stoppÃ©s)\ndocker images                       # Lister les images locales\n\n\nâ¹ï¸ Stopper / Supprimer\ndocker stop CONTAINER_ID            # ArrÃªter un container\ndocker rm CONTAINER_ID              # Supprimer un container\ndocker rmi IMAGE_ID                 # Supprimer une image\n\n\nğŸ“¥ TÃ©lÃ©charger une image\ndocker pull postgres:16             # TÃ©lÃ©charger depuis Docker Hub\n\n\nğŸ“œ Logs\ndocker logs CONTAINER_ID            # Voir les logs\ndocker logs -f CONTAINER_ID         # Suivre les logs en temps rÃ©el\n\n\nğŸ”‘ Raccourcis utiles\n\n\n\nCommande\nDescription\n\n\n\n\ndocker ps -a\nVoir tous les containers\n\n\ndocker logs -f\nSuivre les logs en live\n\n\ndocker exec -it &lt;container&gt; bash\nEntrer dans un container\n\n\ndocker system prune\nNettoyer les ressources inutilisÃ©es\n\n\n\n\n\nCode\n%%bash\n# Lister les images disponibles localement\necho \"=== Images locales ===\"\ndocker images\n\necho \"\"\necho \"=== Containers en cours ===\"\ndocker ps\n\necho \"\"\necho \"=== Tous les containers ===\"\ndocker ps -a",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-pour-lancer-des-services-data",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-pour-lancer-des-services-data",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ—„ï¸ 5. Docker pour lancer des services Data",
    "text": "ğŸ—„ï¸ 5. Docker pour lancer des services Data\nDocker est extrÃªmement pratique pour tester rapidement des services utilisÃ©s en Data Engineering.\n\nğŸ˜ PostgreSQL (exemple dÃ©taillÃ©)\ndocker run -d \\\n  --name demo-postgres \\\n  -e POSTGRES_USER=de_user \\\n  -e POSTGRES_PASSWORD=de_pass \\\n  -e POSTGRES_DB=de_db \\\n  -p 5432:5432 \\\n  postgres:16\nConnexion : - Host: localhost - Port: 5432 - User: de_user - Password: de_pass - Database: de_db\nTu peux ensuite te connecter avec DBeaver, psql, Python (psycopg2), etc.\n\n\n\nğŸš€ Autres services (one-liners)\n\n\n\n\n\n\n\nService\nCommande\n\n\n\n\nRedis\ndocker run -d --name demo-redis -p 6379:6379 redis:latest\n\n\nMongoDB\ndocker run -d --name demo-mongo -p 27017:27017 mongo:latest\n\n\nKafka\ndocker run -d --name demo-kafka -p 9092:9092 bitnami/kafka:latest\n\n\nSpark\ndocker run -it --name demo-spark bitnami/spark:latest pyspark\n\n\nAirflow\ndocker pull apache/airflow:latest\n\n\nJupyter\ndocker run -p 8888:8888 jupyter/scipy-notebook\n\n\n\n\nğŸ’¡ Astuce : Pour Kafka et Airflow, prÃ©fÃ¨re docker-compose car ils nÃ©cessitent plusieurs services (Zookeeper, webserver, schedulerâ€¦), on le verra un peu plus en bas.\n\n\n\nCode\n%%bash\n# Lancer PostgreSQL (si Docker est installÃ©)\ndocker run -d \\\n  --name demo-postgres \\\n  -e POSTGRES_USER=de_user \\\n  -e POSTGRES_PASSWORD=de_pass \\\n  -e POSTGRES_DB=de_db \\\n  -p 5432:5432 \\\n  postgres:16\n\necho \"âœ… PostgreSQL lancÃ© sur localhost:5432\"\n\n# VÃ©rifier qu'il tourne\ndocker ps --filter name=demo-postgres\n\n\n\n\nCode\n%%bash\n# Nettoyage : stopper et supprimer le container\ndocker stop demo-postgres 2&gt;/dev/null\ndocker rm demo-postgres 2&gt;/dev/null\necho \"ğŸ§¹ Container demo-postgres supprimÃ©\"",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#dockerfile-crÃ©er-son-image",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#dockerfile-crÃ©er-son-image",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ“ 6. Dockerfile : crÃ©er son image",
    "text": "ğŸ“ 6. Dockerfile : crÃ©er son image\nLe Dockerfile est un fichier texte qui dÃ©crit comment construire une image.\n\nğŸ§± Instructions principales\n\n\n\nInstruction\nRÃ´le\nExemple\n\n\n\n\nFROM\nImage de base\nFROM python:3.11-slim\n\n\nWORKDIR\nRÃ©pertoire de travail\nWORKDIR /app\n\n\nCOPY\nCopier des fichiers\nCOPY etl.py .\n\n\nRUN\nExÃ©cuter une commande (build)\nRUN pip install pandas\n\n\nCMD\nCommande par dÃ©faut (run)\nCMD [\"python\", \"etl.py\"]\n\n\nENTRYPOINT\nPoint dâ€™entrÃ©e fixe\nENTRYPOINT [\"python\"]\n\n\nENV\nVariable dâ€™environnement\nENV PYTHONUNBUFFERED=1\n\n\nEXPOSE\nPort exposÃ© (documentation)\nEXPOSE 8000\n\n\n\n\n\nğŸ“ Structure de projet recommandÃ©e\netl_project/\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ .dockerignore\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ etl.py\nâ”‚   â””â”€â”€ utils.py\nâ””â”€â”€ data/               # âš ï¸ Ne pas inclure dans l'image !\n    â””â”€â”€ input.csv\n\nğŸ’¡ Important : Le Dockerfile doit Ãªtre Ã  la racine du service que tu veux packager.\n\n\n\nğŸ Exemple : Dockerfile pour un ETL Python\n# 1. Image de base lÃ©gÃ¨re\nFROM python:3.11-slim\n\n# 2. Variables d'environnement\nENV PYTHONUNBUFFERED=1\nENV PYTHONDONTWRITEBYTECODE=1\n\n# 3. RÃ©pertoire de travail\nWORKDIR /app\n\n# 4. Copier et installer les dÃ©pendances (cache Docker)\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# 5. Copier le code\nCOPY src/ ./src/\n\n# 6. Commande par dÃ©faut\nCMD [\"python\", \"src/etl.py\"]\n\n\nğŸ”¨ Construire lâ€™image\ncd etl_project\ndocker build -t etl-image:1.0 .\n\n\nğŸƒ Lancer le container\ndocker run --rm etl-image:1.0",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#le-fichier-.dockerignore",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#le-fichier-.dockerignore",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ§¹ 7. Le fichier .dockerignore",
    "text": "ğŸ§¹ 7. Le fichier .dockerignore\nLe .dockerignore empÃªche dâ€™envoyer des fichiers inutiles dans le build context.\n\nâŒ Sans .dockerignore\n\nImages Ã©normes (datasets inclus)\nBuilds lents (envoi de Go de donnÃ©es)\nFuites de secrets (.env, clÃ©s SSH)\n\n\n\nâœ… Exemple de .dockerignore (Data Engineer)\n# DonnÃ©es\ndata/\n*.csv\n*.parquet\n*.json\n\n# Python\n__pycache__/\n*.pyc\n*.pyo\nvenv/\n.venv/\n*.egg-info/\n\n# Secrets\n.env\n*.key\n*.pem\nsecrets/\n\n# Notebooks\n*.ipynb\n.ipynb_checkpoints/\n\n# Git\n.git/\n.gitignore\n\n# IDE\n.idea/\n.vscode/\n*.swp\n\n# Logs\nlogs/\n*.log\n\n# OS\n.DS_Store\nThumbs.db\n\n# Docker\nDockerfile\ndocker-compose*.yml\n\nğŸ’¡ RÃ¨gle dâ€™or : le .dockerignore est aussi important que le Dockerfile !",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#multi-stage-builds",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#multi-stage-builds",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ—ï¸ 8. Multi-stage builds",
    "text": "ğŸ—ï¸ 8. Multi-stage builds\nLes multi-stage builds permettent de crÃ©er des images plus lÃ©gÃ¨res en sÃ©parant :\n\nStage build : compilation, installation des dÃ©pendances lourdes\nStage runtime : uniquement ce qui est nÃ©cessaire pour exÃ©cuter\n\n\nğŸ¯ Pourquoi câ€™est utile ?\n\n\n\nSans multi-stage\nAvec multi-stage\n\n\n\n\nImage de 1.5 Go\nImage de 200 Mo\n\n\nOutils de build inclus\nSeulement le runtime\n\n\nSurface dâ€™attaque large\nSÃ©curitÃ© renforcÃ©e\n\n\n\n\n\nğŸ“ Exemple : ETL Python avec multi-stage\n# ============== STAGE 1 : BUILD ==============\nFROM python:3.11-slim AS builder\n\nWORKDIR /app\n\n# Installer les dÃ©pendances dans un dossier isolÃ©\nCOPY requirements.txt .\nRUN pip install --prefix=/install --no-cache-dir -r requirements.txt\n\n# ============== STAGE 2 : RUNTIME ==============\nFROM python:3.11-slim AS runtime\n\nWORKDIR /app\n\n# Copier seulement les dÃ©pendances installÃ©es\nCOPY --from=builder /install /usr/local\n\n# Copier le code\nCOPY src/ ./src/\n\n# Variables d'environnement\nENV PYTHONUNBUFFERED=1\n\nCMD [\"python\", \"src/etl.py\"]\nRÃ©sultat : image finale lÃ©gÃ¨re et sÃ©curisÃ©e ! ğŸ‰",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#volumes-networks",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#volumes-networks",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ’¾ 9. Volumes & Networks",
    "text": "ğŸ’¾ 9. Volumes & Networks\n\n9.1 Volumes : persister les donnÃ©es\nLes volumes permettent de stocker des donnÃ©es en dehors des containers.\nTypes de volumes :\n\n\n\n\n\n\n\n\nType\nSyntaxe\nUsage\n\n\n\n\nBind mount\n-v /host/path:/container/path\nDonnÃ©es locales (dev)\n\n\nVolume nommÃ©\n-v myvolume:/container/path\nDonnÃ©es persistantes (prod)\n\n\n\nExemple : monter un dossier local\ndocker run -d \\\n  --name etl-with-data \\\n  -v $(pwd)/data:/app/data \\\n  etl-image:1.0\n\n$(pwd)/data â†’ dossier sur ta machine\n/app/data â†’ dossier dans le container\nTu supprimes le container â†’ les donnÃ©es restent dans ./data\n\n\n\n\n9.2 Networks : faire communiquer les containers\nPar dÃ©faut, Docker crÃ©e un rÃ©seau bridge. Tu peux crÃ©er un rÃ©seau dÃ©diÃ© :\n# CrÃ©er un rÃ©seau\ndocker network create de-network\n\n# Lancer des containers sur ce rÃ©seau\ndocker run -d --name de-postgres --network de-network postgres:16\ndocker run -d --name de-etl --network de-network etl-image:1.0\nAvantage : dans le code Python, tu te connectes Ã  de-postgres (nom du container) au lieu de localhost !\n# Dans etl.py\nconn = psycopg2.connect(\n    host=\"de-postgres\",  # Nom du container !\n    database=\"de_db\",\n    user=\"de_user\",\n    password=\"de_pass\"\n)",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-compose",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-compose",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ¼ 10. Docker Compose",
    "text": "ğŸ¼ 10. Docker Compose\nQuand tu as plusieurs services (Postgres + ETL + APIâ€¦), tu ne veux pas tout lancer Ã  la main.\ndocker-compose.yml permet de dÃ©crire une stack complÃ¨te et de la lancer avec une seule commande.\n\nğŸ“ Structure projet avec docker-compose\nde-pipeline/\nâ”œâ”€â”€ docker-compose.yml      # Ã€ la racine !\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ””â”€â”€ data/\n    â””â”€â”€ input.csv\n\n\nğŸ“ Exemple : PostgreSQL + ETL\nversion: \"3.9\"\n\nservices:\n  postgres:\n    image: postgres:16\n    container_name: de-postgres\n    environment:\n      POSTGRES_USER: de_user\n      POSTGRES_PASSWORD: de_pass\n      POSTGRES_DB: de_db\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U de_user -d de_db\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  etl:\n    build: ./etl\n    container_name: de-etl\n    depends_on:\n      postgres:\n        condition: service_healthy\n    environment:\n      DB_HOST: postgres\n      DB_USER: de_user\n      DB_PASSWORD: de_pass\n      DB_NAME: de_db\n    volumes:\n      - ./data:/app/data\n\nvolumes:\n  pg_data:\n\n\nğŸš€ Commandes docker-compose\n# Lancer la stack\ndocker compose up -d\n\n# Voir les logs\ndocker compose logs -f\n\n# Stopper la stack\ndocker compose down\n\n# Stopper et supprimer les volumes\ndocker compose down -v\n\n# Reconstruire les images\ndocker compose up --build",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#debug-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#debug-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ”§ 11. Debug Docker",
    "text": "ğŸ”§ 11. Debug Docker\nEn Data Engineering, tu devras souvent dÃ©bugger un job qui tourne dans un container.\n\nğŸ“œ Voir les logs\ndocker logs de-etl                  # Voir les logs\ndocker logs -f de-etl               # Suivre en temps rÃ©el\ndocker logs --tail 100 de-etl       # Les 100 derniÃ¨res lignes\n\n\nğŸš Entrer dans un container\ndocker exec -it de-etl bash         # Ouvrir un shell bash\ndocker exec -it de-etl sh           # Pour images Alpine\nCas dâ€™usage : - Inspecter les fichiers (ls, cat) - Tester une connexion DB (psql, ping) - VÃ©rifier les variables dâ€™environnement (env) - Lancer un script manuellement (python etl.py)\n\n\nğŸ” Inspecter un container\ndocker inspect de-etl               # DÃ©tails complets (JSON)\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' de-etl  # IP du container\n\n\nğŸ› Cas Data Engineering typiques\n\n\n\n\n\n\n\nProblÃ¨me\nSolution\n\n\n\n\nETL qui plante sans message\ndocker logs de-etl\n\n\nConnexion DB refusÃ©e\ndocker exec -it de-etl bash puis ping postgres\n\n\nSpark ne voit pas Kafka\nVÃ©rifier les networks Docker\n\n\nFichier introuvable\ndocker exec -it de-etl ls /app/data\n\n\nVariable dâ€™env manquante\ndocker exec -it de-etl env",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "âš ï¸ 12. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "âš ï¸ 12. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nConsÃ©quence\nSolution\n\n\n\n\nPas de .dockerignore\nImages de plusieurs Go\nCrÃ©er un .dockerignore complet\n\n\nTout en latest\nComportement non reproductible\nTags versionnÃ©s (image:1.0.0)\n\n\nDockerfile mal placÃ©\nBuild context gigantesque\nDockerfile Ã  la racine du service\n\n\nSecrets dans lâ€™image\nFuite de credentials\nVariables dâ€™env ou secrets manager\n\n\nPas de nettoyage\nDisque saturÃ©\ndocker system prune rÃ©guliÃ¨rement\n\n\nDonnÃ©es dans lâ€™image\nImage Ã©norme, non portable\nUtiliser des volumes\n\n\nVersions non fixÃ©es\nBuilds cassÃ©s aprÃ¨s mise Ã  jour\nFixer les versions dans requirements.txt\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nImages slim\npython:3.11-slim = plus lÃ©ger et rapide\n\n\nMulti-stage builds\nImages finales lÃ©gÃ¨res\n\n\nTags versionnÃ©s\netl:1.0.0, etl:prod, etl:staging\n\n\n.dockerignore\nBuilds rapides et sÃ©curisÃ©s\n\n\nHealthchecks\nSavoir si un service est prÃªt\n\n\nVolumes pour les donnÃ©es\nPersistance et portabilitÃ©\n\n\nNettoyage rÃ©gulier\ndocker system prune\n\n\n\n\n\nğŸ§¹ Commandes de nettoyage\n# Supprimer les containers arrÃªtÃ©s\ndocker container prune\n\n# Supprimer les images non utilisÃ©es\ndocker image prune\n\n# Tout nettoyer (âš ï¸ attention en prod !)\ndocker system prune -a",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre une image et un container Docker ?\n\nUne image est un container en cours dâ€™exÃ©cution\n\nUn container est une instance en cours dâ€™exÃ©cution dâ€™une image\n\nCâ€™est la mÃªme chose\n\nUne image contient plusieurs containers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Une image est un modÃ¨le figÃ©, un container est son exÃ©cution vivante.\n\n\n\n\nâ“ Q2. Pourquoi le fichier .dockerignore est-il important ?\n\nPour ignorer les erreurs Docker\n\nPour rÃ©duire la taille du build context et sÃ©curiser les builds\n\nPour ignorer les logs\n\nCâ€™est optionnel et inutile\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Il Ã©vite dâ€™envoyer des fichiers inutiles (donnÃ©es, secrets) dans le build context.\n\n\n\n\nâ“ Q3. Quelle commande permet dâ€™entrer dans un container en cours dâ€™exÃ©cution ?\n\ndocker run -it container bash\n\ndocker exec -it container bash\n\ndocker enter container\n\ndocker ssh container\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” docker exec -it &lt;container&gt; bash ouvre un shell interactif.\n\n\n\n\nâ“ Q4. Ã€ quoi servent les multi-stage builds ?\n\nÃ€ lancer plusieurs containers en parallÃ¨le\n\nÃ€ crÃ©er des images plus lÃ©gÃ¨res en sÃ©parant build et runtime\n\nÃ€ versionner les images\n\nÃ€ gÃ©rer les rÃ©seaux Docker\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Multi-stage = image finale lÃ©gÃ¨re avec seulement le runtime nÃ©cessaire.\n\n\n\n\nâ“ Q5. OÃ¹ doit-on placer le fichier docker-compose.yml ?\n\nDans le dossier /etc/docker/\n\nÃ€ la racine du projet multi-services\n\nDans chaque sous-dossier de service\n\nNâ€™importe oÃ¹\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le docker-compose.yml est Ã  la racine du projet, avec les services dans des sous-dossiers.\n\n\n\n\nâ“ Q6. Quelle commande permet de voir les logs dâ€™un container en temps rÃ©el ?\n\ndocker logs container\n\ndocker logs -f container\n\ndocker watch container\n\ndocker tail container\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Lâ€™option -f (follow) suit les logs en temps rÃ©el.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#mini-projet-etl-dockerisÃ©-complet",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#mini-projet-etl-dockerisÃ©-complet",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸš€ Mini-projet : ETL DockerisÃ© complet",
    "text": "ğŸš€ Mini-projet : ETL DockerisÃ© complet\n\nğŸ¯ Objectif\nCrÃ©er un pipeline ETL DockerisÃ© qui lit un CSV, transforme les donnÃ©es, et les charge dans PostgreSQL.\n\n\nğŸ”§ Contexte\nTu dois packager un job ETL Python avec Docker et le faire communiquer avec une base PostgreSQL, le tout orchestrÃ© par docker-compose.\n\n\nğŸ§  Contraintes\n\nCrÃ©er la structure de projet suivante :\n\nde-mini-projet/\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ .dockerignore\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ””â”€â”€ data/\n    â””â”€â”€ sales.csv\n\nLâ€™ETL doit :\n\nLire data/sales.csv\nCalculer une colonne total = quantity * price\nInsÃ©rer les donnÃ©es dans PostgreSQL\n\nUtiliser un healthcheck pour attendre que Postgres soit prÃªt\nLes donnÃ©es doivent Ãªtre montÃ©es via un volume\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. data/sales.csv\ndate,product,quantity,price\n2024-01-01,Laptop,5,999.99\n2024-01-02,Mouse,20,29.99\n2024-01-03,Keyboard,15,79.99\n2024-01-04,Monitor,8,299.99\n2024-01-05,Laptop,3,999.99\n2. etl/requirements.txt\npandas==2.1.4\npsycopg2-binary==2.9.9\nsqlalchemy==2.0.25\n3. etl/.dockerignore\n__pycache__/\n*.pyc\n.env\n*.log\n4. etl/Dockerfile\nFROM python:3.11-slim\n\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY etl.py .\n\nCMD [\"python\", \"etl.py\"]\n5. etl/etl.py\nimport os\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef main():\n    # Configuration depuis variables d'environnement\n    db_host = os.environ.get('DB_HOST', 'localhost')\n    db_user = os.environ.get('DB_USER', 'de_user')\n    db_pass = os.environ.get('DB_PASSWORD', 'de_pass')\n    db_name = os.environ.get('DB_NAME', 'de_db')\n    \n    print(\"ğŸš€ DÃ©marrage de l'ETL...\")\n    \n    # Extract\n    print(\"ğŸ“¥ Lecture du fichier CSV...\")\n    df = pd.read_csv('/app/data/sales.csv')\n    print(f\"   {len(df)} lignes lues\")\n    \n    # Transform\n    print(\"ğŸ”„ Transformation des donnÃ©es...\")\n    df['total'] = df['quantity'] * df['price']\n    df['loaded_at'] = pd.Timestamp.now()\n    \n    # Load\n    print(\"ğŸ“¤ Chargement dans PostgreSQL...\")\n    engine = create_engine(f'postgresql://{db_user}:{db_pass}@{db_host}/{db_name}')\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© avec succÃ¨s !\")\n    print(df.head())\n\nif __name__ == \"__main__\":\n    main()\n6. docker-compose.yml\nversion: \"3.9\"\n\nservices:\n  postgres:\n    image: postgres:16\n    container_name: de-postgres\n    environment:\n      POSTGRES_USER: de_user\n      POSTGRES_PASSWORD: de_pass\n      POSTGRES_DB: de_db\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U de_user -d de_db\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  etl:\n    build: ./etl\n    container_name: de-etl\n    depends_on:\n      postgres:\n        condition: service_healthy\n    environment:\n      DB_HOST: postgres\n      DB_USER: de_user\n      DB_PASSWORD: de_pass\n      DB_NAME: de_db\n    volumes:\n      - ./data:/app/data\n\nvolumes:\n  pg_data:\n7. Lancer le projet :\ncd de-mini-projet\ndocker compose up --build\n8. VÃ©rifier les donnÃ©es dans Postgres :\ndocker exec -it de-postgres psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nDocker Docs â€” Documentation officielle\nDocker Hub â€” Registry dâ€™images publiques\nDockerfile reference â€” Toutes les instructions\n\n\n\nğŸ® Pratique\n\nPlay with Docker â€” Environnement Docker gratuit en ligne\nDocker 101 Tutorial â€” Tutoriel officiel interactif\n\n\n\nğŸ“¦ Images utiles pour Data Engineering\n\npostgres â€” Base de donnÃ©es relationnelle\napache/airflow â€” Orchestrateur\nbitnami/spark â€” Traitement distribuÃ©\nbitnami/kafka â€” Streaming\njupyter/scipy-notebook â€” Notebooks",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Docker, passons Ã  lâ€™orchestration de containers Ã  grande Ã©chelle !\nğŸ‘‰ Module suivant : 15_kubernetes_fundamentals.ipynb â€” Orchestrer des containers avec Kubernetes\nTu vas apprendre : - Pods, Deployments, Services - ConfigMaps, Secrets - Spark et Airflow sur Kubernetes\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Docker pour Data Engineers.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ³ Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  dÃ©ployer Apache Spark sur Kubernetes. Câ€™est lâ€™architecture moderne de rÃ©fÃ©rence pour exÃ©cuter des workloads Spark en production.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#prÃ©requis",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#prÃ©requis",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nModule\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 14\nDocker Fundamentals\n\n\nâœ… Requis\nModule 15\nKubernetes Fundamentals\n\n\nâœ… Requis\nModule 16\nKubernetes for Data Workloads\n\n\nâœ… Requis\nModule 19\nPySpark Advanced\n\n\nâœ… Requis\nModule 20\nSpark SQL Deep Dive\n\n\nğŸ’¡ RecommandÃ©\n-\nExpÃ©rience avec kubectl et Helm",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#objectifs-du-module",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#objectifs-du-module",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture Spark on Kubernetes\nConstruire une image Docker Spark optimisÃ©e\nConfigurer les ressources Kubernetes (RBAC, Secrets, PVC)\nLancer des jobs avec spark-submit sur K8s\nUtiliser Spark Operator pour la production\nConfigurer lâ€™autoscaling (Dynamic Allocation, KEDA)\nMettre en place le monitoring (Spark UI, Prometheus, Grafana)\nDebugger les erreurs courantes",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#introduction-pourquoi-spark-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#introduction-pourquoi-spark-sur-kubernetes",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“š 1. Introduction â€” Pourquoi Spark sur Kubernetes ?",
    "text": "ğŸ“š 1. Introduction â€” Pourquoi Spark sur Kubernetes ?\n\n1.1 Lâ€™Ã©volution de lâ€™infrastructure Spark\n2010s                              2020s+\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Hadoop/YARN   â”‚                â”‚   Kubernetes    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚  Spark  â”‚   â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚   â”‚  Spark  â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚   On-premise    â”‚                â”‚   Cloud-native  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Avantages de Kubernetes\n\n\n\nAvantage\nDescription\n\n\n\n\nCloud-native\nMÃªme infra que le reste de tes applications\n\n\nMulti-cloud\nAWS, GCP, Azure, on-premise\n\n\nIsolation\nNamespaces, RBAC, Network Policies\n\n\nAutoscaling\nHPA, VPA, KEDA, Cluster Autoscaler\n\n\nCI/CD\nImages Docker versionnÃ©es, GitOps\n\n\nCost optimization\nSpot instances, autoscaling down\n\n\nStandardisation\nPlus besoin dâ€™expertise Hadoop/YARN\n\n\n\n\n\n1.3 Support officiel\n\nSpark 2.3 : Support expÃ©rimental K8s\nSpark 3.0 : Support stable (GA)\nSpark 3.1+ : AmÃ©liorations (PVC, Pod templates)\nSpark 3.4+ : External shuffle service sur K8s\n\n\n\n1.4 ğŸ†š Comparaison YARN vs Kubernetes\n\n\n\n\n\n\n\n\nAspect\nYARN\nKubernetes\n\n\n\n\nÃ‰cosystÃ¨me\nHadoop-centric\nCloud-native, polyglot\n\n\nScheduling\nResourceManager\nkube-scheduler\n\n\nIsolation\nContainers/cgroups\nPods, Namespaces, Network Policies\n\n\nScaling\nManual ou scripts\nHPA, VPA, KEDA, Cluster Autoscaler\n\n\nPackaging\nJAR/ZIP sur HDFS\nImages Docker\n\n\nSecrets\nHadoop credentials\nK8s Secrets, Vault\n\n\nMonitoring\nYARN UI, Ganglia\nPrometheus, Grafana, native\n\n\nMulti-tenancy\nQueues\nNamespaces + RBAC\n\n\nData locality\nâœ… Excellent\nâš ï¸ LimitÃ© (shuffle coÃ»teux)\n\n\nCourbe dâ€™apprentissage\nHadoop stack\nK8s stack\n\n\nAdoption 2024+\nLegacy\nStandard moderne\n\n\n\nğŸ“Š Quand choisir quoi ?\n\nYARN si :                           Kubernetes si :\nâ”œâ”€â”€ Cluster Hadoop existant         â”œâ”€â”€ Infrastructure cloud-native\nâ”œâ”€â”€ Data locality critique          â”œâ”€â”€ Multi-cloud ou hybrid\nâ”œâ”€â”€ Ã‰quipe Hadoop experte           â”œâ”€â”€ CI/CD moderne (GitOps)\nâ””â”€â”€ Pas de migration prÃ©vue         â””â”€â”€ Autoscaling avancÃ© requis",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#architecture-spark-on-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#architecture-spark-on-kubernetes",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ—ï¸ 2. Architecture Spark on Kubernetes",
    "text": "ğŸ—ï¸ 2. Architecture Spark on Kubernetes\n\n2.1 Vue dâ€™ensemble\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        KUBERNETES CLUSTER                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                     Namespace: spark                      â”‚  â”‚\nâ”‚  â”‚                                                           â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚  â”‚   â”‚   Driver Pod    â”‚      â”‚     Executor Pods       â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”‚  Spark    â”‚  â”‚â—€â”€â”€â”€â”€â–¶â”‚  â”‚ Exec  â”‚ â”‚ Exec  â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”‚  Driver   â”‚  â”‚      â”‚  â”‚  #1   â”‚ â”‚  #2   â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - Spark UI     â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - Coordinateur â”‚      â”‚  â”‚ Exec  â”‚ â”‚ Exec  â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚  #3   â”‚ â”‚  #4   â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚           â”‚                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚\nâ”‚  â”‚           â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚  â”‚           â–¼                                               â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚  â”‚   â”‚  ConfigMaps     â”‚      â”‚       Secrets           â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - spark-config â”‚      â”‚  - cloud credentials    â”‚   â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚  â”‚                                                           â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚\nâ”‚  â”‚   â”‚              PersistentVolumeClaims                 â”‚â”‚  â”‚\nâ”‚  â”‚   â”‚  - checkpoints  - logs  - shuffle (optional)       â”‚â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 RÃ´les et responsabilitÃ©s\n\n\n\nComposant\nGÃ©rÃ© par\nResponsabilitÃ©\n\n\n\n\nPods scheduling\nKubernetes\nPlacement sur les nodes\n\n\nNetworking\nKubernetes\nCommunication Driver â†”ï¸ Executors\n\n\nSecrets\nKubernetes\nCredentials cloud/DB\n\n\nStorage\nKubernetes\nPVC pour checkpoints/logs\n\n\nDriver\nSpark\nCoordination du job\n\n\nExecutors\nSpark\nExÃ©cution des tasks\n\n\nShuffle\nSpark\nÃ‰change de donnÃ©es entre stages\n\n\n\n\n\n2.3 Client Mode vs Cluster Mode\nCLIENT MODE                              CLUSTER MODE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Local Machineâ”‚                        â”‚      Kubernetes Cluster      â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚  â”‚ Driver â”‚  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚ Driver â”‚â—€â”€â”€â”€â”€â”            â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚             â”‚  â”‚  Pod   â”‚     â”‚            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚            â”‚\n                          â”‚             â”‚        â–²        â”‚            â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚        â”‚            â”‚\nâ”‚      K8s Cluster        â”‚         â”‚   â”‚        â–¼        â–¼            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚Executorâ”‚ â”‚Executorâ”‚â—€â”€â”˜         â”‚   â”‚  â”‚Executorâ”‚ â”‚Executorâ”‚      â”‚\nâ”‚  â”‚  Pod   â”‚ â”‚  Pod   â”‚            â”‚   â”‚  â”‚  Pod   â”‚ â”‚  Pod   â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\nAspect\nClient Mode\nCluster Mode\n\n\n\n\nDriver\nMachine locale\nPod Kubernetes\n\n\nUsage\nDÃ©veloppement, debug\nProduction\n\n\nSpark UI\nlocalhost:4040\nVia Service/Ingress\n\n\nRÃ©seau\nDoit atteindre le cluster\nInterne au cluster\n\n\nRÃ©silience\nâŒ Si local crash â†’ job perdu\nâœ… Pod peut Ãªtre reschedulÃ©\n\n\nCI/CD\nâŒ Difficile\nâœ… Natif\n\n\n\n\nğŸ’¡ RÃ¨gle : Client mode pour le dev/debug, Cluster mode pour la production.\n\n\n\nğŸ‹ï¸ Exercice 1 : Identifier les composants\nRÃ©ponds aux questions suivantes :\n\nDans quel mode le Driver tourne-t-il en tant que Pod K8s ?\nQui gÃ¨re le scheduling des Executor Pods ?\nOÃ¹ sont stockÃ©s les credentials cloud ?\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\nCluster mode â€” Le Driver est un Pod K8s\nKubernetes (kube-scheduler) â€” Spark demande des Pods, K8s les place\nKubernetes Secrets â€” MontÃ©s dans les Pods Driver/Executor",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#construire-une-image-docker-spark",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#construire-une-image-docker-spark",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ³ 3. Construire une Image Docker Spark",
    "text": "ğŸ³ 3. Construire une Image Docker Spark\n\n3.1 Image de base\nPlusieurs options :\n\n\n\nImage\nAvantage\nInconvÃ©nient\n\n\n\n\napache/spark\nOfficielle\nBasique\n\n\nbitnami/spark\nBien maintenue, non-root\nTaille moyenne\n\n\ngcr.io/spark-operator/spark\nPour Spark Operator\nSpÃ©cifique\n\n\nCustom\nContrÃ´le total\nPlus de travail\n\n\n\n\n\nCode\n%%writefile /tmp/spark-docker/Dockerfile.basic\n# Image Spark de base\nFROM bitnami/spark:3.5\n\n# Passer en root pour installer des packages\nUSER root\n\n# Installer des dÃ©pendances Python\nRUN pip install --no-cache-dir \\\n    boto3 \\\n    pyarrow \\\n    pandas\n\n# Copier l'application\nCOPY app/ /app/\n\n# Revenir Ã  l'utilisateur non-root (sÃ©curitÃ©)\nUSER 1001\n\n# DÃ©finir le working directory\nWORKDIR /app\n\n\n\n\n3.2 Image production-grade avec JARs\n\n\nCode\n%%writefile /tmp/spark-docker/Dockerfile.production\n# ============================================\n# Spark Production Image\n# ============================================\nFROM bitnami/spark:3.5 AS base\n\nUSER root\n\n# ---- DÃ©pendances systÃ¨me ----\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# ---- DÃ©pendances Python ----\nCOPY requirements.txt /tmp/\nRUN pip install --no-cache-dir -r /tmp/requirements.txt\n\n# ---- JARs pour connecteurs cloud ----\n# AWS S3\nRUN curl -sL https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \\\n    -o /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar\nRUN curl -sL https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar \\\n    -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar\n\n# ---- Application ----\nCOPY app/ /app/\n\n# ---- SÃ©curitÃ© : non-root ----\nRUN chown -R 1001:1001 /app\nUSER 1001\n\nWORKDIR /app\n\n# ---- Healthcheck ----\nHEALTHCHECK --interval=30s --timeout=10s --retries=3 \\\n    CMD curl -f http://localhost:4040/api/v1/applications || exit 1\n\n\n\n\nCode\n%%writefile /tmp/spark-docker/requirements.txt\n# Python dependencies for Spark jobs\nboto3&gt;=1.28.0\npyarrow&gt;=14.0.0\npandas&gt;=2.0.0\nrequests&gt;=2.31.0\n\n\n\n\n3.3 Best practices Docker pour Spark\n\n\n\nPratique\nPourquoi\nComment\n\n\n\n\nNon-root\nSÃ©curitÃ©\nUSER 1001\n\n\nMulti-stage builds\nImage plus lÃ©gÃ¨re\nFROM ... AS builder\n\n\nLayer caching\nBuilds plus rapides\nDÃ©pendances avant code\n\n\nNo cache pip\nImage plus lÃ©gÃ¨re\n--no-cache-dir\n\n\nHealthcheck\nK8s liveness probe\nHEALTHCHECK\n\n\nVersioning\nReproductibilitÃ©\nTags explicites (pas latest)\n\n\n\n\n\nCode\n# Commandes pour construire et pousser l'image\ndocker_commands = \"\"\"\n# Construire l'image\ndocker build -t my-registry/spark-app:1.0.0 -f Dockerfile.production .\n\n# Tester localement\ndocker run --rm my-registry/spark-app:1.0.0 spark-submit --version\n\n# Pousser vers un registry\ndocker push my-registry/spark-app:1.0.0\n\n# Pour Minikube (utiliser le Docker daemon de Minikube)\neval $(minikube docker-env)\ndocker build -t spark-app:1.0.0 .\n\"\"\"\nprint(docker_commands)\n\n\n\n\nğŸ‹ï¸ Exercice 2 : Construire une image Spark\nObjectif : CrÃ©er un Dockerfile Spark avec : - Base bitnami/spark:3.5 - DÃ©pendance Python : numpy - Un script hello.py qui affiche â€œHello Spark on K8s!â€\n\n\nğŸ’¡ Solution\n\nFROM bitnami/spark:3.5\n\nUSER root\nRUN pip install --no-cache-dir numpy\n\nCOPY hello.py /app/hello.py\n\nUSER 1001\nWORKDIR /app\n# hello.py\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Hello\").getOrCreate()\nprint(\"Hello Spark on K8s!\")\nspark.stop()",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#configuration-kubernetes-pour-spark",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#configuration-kubernetes-pour-spark",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "âš™ï¸ 4. Configuration Kubernetes pour Spark",
    "text": "âš™ï¸ 4. Configuration Kubernetes pour Spark\n\n4.1 Namespace dÃ©diÃ©\n\n\nCode\n%%writefile /tmp/spark-k8s/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark\n  labels:\n    app: spark\n    environment: development\n\n\n\n\n4.2 ServiceAccount & RBAC\nSpark a besoin de permissions pour : - CrÃ©er des Pods (executors) - Lister/supprimer des Pods - AccÃ©der aux ConfigMaps et Secrets\n\n\nCode\n%%writefile /tmp/spark-k8s/rbac.yaml\n# ServiceAccount pour Spark\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark-sa\n  namespace: spark\n---\n# Role avec permissions minimales\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: spark-role\n  namespace: spark\nrules:\n  # GÃ©rer les pods (executors)\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"delete\"]\n  # Logs des pods\n  - apiGroups: [\"\"]\n    resources: [\"pods/log\"]\n    verbs: [\"get\", \"list\"]\n  # ConfigMaps pour Spark config\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"delete\"]\n  # Services pour Spark UI\n  - apiGroups: [\"\"]\n    resources: [\"services\"]\n    verbs: [\"create\", \"get\", \"delete\"]\n  # PersistentVolumeClaims\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumeclaims\"]\n    verbs: [\"create\", \"get\", \"list\", \"delete\"]\n---\n# Binding Role â†’ ServiceAccount\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: spark-role-binding\n  namespace: spark\nsubjects:\n  - kind: ServiceAccount\n    name: spark-sa\n    namespace: spark\nroleRef:\n  kind: Role\n  name: spark-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n\n\n4.3 Secrets (credentials cloud)\n\nâš ï¸ Note : En production, utilise un gestionnaire de secrets (Vault, AWS Secrets Manager, etc.)\n\n\n\nCode\n%%writefile /tmp/spark-k8s/secrets.yaml\n# Secret pour MinIO (ou S3)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-credentials\n  namespace: spark\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"minioadmin\"\n  AWS_SECRET_ACCESS_KEY: \"minioadmin\"\n  S3_ENDPOINT: \"http://minio.minio.svc.cluster.local:9000\"\n\n\n\n\nCode\n# Alternative : crÃ©er le secret via CLI\nsecret_command = \"\"\"\nkubectl create secret generic minio-credentials \\\n  --namespace=spark \\\n  --from-literal=AWS_ACCESS_KEY_ID=minioadmin \\\n  --from-literal=AWS_SECRET_ACCESS_KEY=minioadmin \\\n  --from-literal=S3_ENDPOINT=http://minio.minio.svc.cluster.local:9000\n\"\"\"\nprint(secret_command)\n\n\n\n\n4.4 PersistentVolumeClaims\n\n\nCode\n%%writefile /tmp/spark-k8s/pvc.yaml\n# PVC pour les logs Spark\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: spark-logs-pvc\n  namespace: spark\nspec:\n  accessModes:\n    - ReadWriteMany  # Plusieurs pods peuvent Ã©crire\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard  # Adapter selon ton cluster\n---\n# PVC pour les checkpoints (streaming)\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: spark-checkpoints-pvc\n  namespace: spark\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 20Gi\n  storageClassName: standard\n\n\n\n\n4.5 ResourceQuotas & LimitRanges (optionnel mais recommandÃ©)\n\n\nCode\n%%writefile /tmp/spark-k8s/quotas.yaml\n# Limiter les ressources du namespace Spark\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: spark-quota\n  namespace: spark\nspec:\n  hard:\n    requests.cpu: \"20\"        # Max 20 CPU demandÃ©s\n    requests.memory: \"40Gi\"   # Max 40 Gi mÃ©moire demandÃ©e\n    limits.cpu: \"40\"          # Max 40 CPU limites\n    limits.memory: \"80Gi\"     # Max 80 Gi mÃ©moire limites\n    pods: \"50\"                # Max 50 pods\n---\n# Defaults pour les pods sans spec\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: spark-limits\n  namespace: spark\nspec:\n  limits:\n    - type: Container\n      default:\n        cpu: \"1\"\n        memory: \"2Gi\"\n      defaultRequest:\n        cpu: \"500m\"\n        memory: \"1Gi\"\n      max:\n        cpu: \"8\"\n        memory: \"16Gi\"\n\n\n\n\nğŸ‹ï¸ Exercice 3 : CrÃ©er les manifests RBAC\nQuestion : Pourquoi le ServiceAccount Spark a-t-il besoin de la permission pods/log ?\n\n\nğŸ’¡ RÃ©ponse\n\nLe Driver Spark a besoin de lire les logs des Executor Pods pour : - Afficher les erreurs dans le Spark UI - Permettre le debugging via kubectl logs - Collecter les mÃ©triques dâ€™exÃ©cution",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-submit-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-submit-sur-kubernetes",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸš€ 5. spark-submit sur Kubernetes",
    "text": "ğŸš€ 5. spark-submit sur Kubernetes\n\n5.1 Syntaxe de base\n\n\nCode\nspark_submit_basic = \"\"\"\n# Spark-submit basique sur K8s (cluster mode)\nspark-submit \\\n  --master k8s://https://&lt;kubernetes-api-server&gt;:6443 \\\n  --deploy-mode cluster \\\n  --name spark-pi \\\n  --conf spark.kubernetes.container.image=spark-app:1.0.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  local:///app/pi.py\n\"\"\"\nprint(spark_submit_basic)\n\n\n\n\n5.2 Commande complÃ¨te production-grade\n\n\nCode\nspark_submit_full = \"\"\"\n# Spark-submit complet pour production\nspark-submit \\\n  # === Master & Mode ===\n  --master k8s://https://$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}') \\\n  --deploy-mode cluster \\\n  --name etl-job-$(date +%Y%m%d-%H%M%S) \\\n  \\\n  # === Image & Namespace ===\n  --conf spark.kubernetes.container.image=my-registry/spark-app:1.0.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  \\\n  # === Driver Resources ===\n  --conf spark.driver.cores=2 \\\n  --conf spark.driver.memory=4g \\\n  --conf spark.kubernetes.driver.request.cores=1 \\\n  --conf spark.kubernetes.driver.limit.cores=2 \\\n  \\\n  # === Executor Resources ===\n  --conf spark.executor.instances=4 \\\n  --conf spark.executor.cores=2 \\\n  --conf spark.executor.memory=4g \\\n  --conf spark.kubernetes.executor.request.cores=1 \\\n  --conf spark.kubernetes.executor.limit.cores=2 \\\n  \\\n  # === Secrets (environnement) ===\n  --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=minio-credentials:AWS_ACCESS_KEY_ID \\\n  --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=minio-credentials:AWS_SECRET_ACCESS_KEY \\\n  --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=minio-credentials:AWS_ACCESS_KEY_ID \\\n  --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=minio-credentials:AWS_SECRET_ACCESS_KEY \\\n  \\\n  # === S3/MinIO Config ===\n  --conf spark.hadoop.fs.s3a.endpoint=http://minio.minio.svc.cluster.local:9000 \\\n  --conf spark.hadoop.fs.s3a.path.style.access=true \\\n  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n  \\\n  # === Application ===\n  local:///app/etl_job.py \\\n  --input s3a://bronze/data \\\n  --output s3a://silver/data\n\"\"\"\nprint(spark_submit_full)\n\n\n\n\n5.3 Configurations essentielles\n\n\n\n\n\n\n\n\nConfiguration\nDescription\nExemple\n\n\n\n\nspark.kubernetes.container.image\nImage Docker Spark\nregistry/spark:1.0\n\n\nspark.kubernetes.namespace\nNamespace K8s\nspark\n\n\nspark.kubernetes.authenticate.driver.serviceAccountName\nServiceAccount\nspark-sa\n\n\nspark.executor.instances\nNombre dâ€™executors\n4\n\n\nspark.executor.cores\nCores par executor\n2\n\n\nspark.executor.memory\nMÃ©moire par executor\n4g\n\n\nspark.kubernetes.driver.secretKeyRef.*\nSecrets â†’ env vars\nVoir exemple\n\n\nspark.kubernetes.executor.deleteOnTermination\nCleanup\ntrue\n\n\n\n\n\n5.4 AccÃ©der au Spark UI\nEn cluster mode, le Spark UI est dans le Pod Driver.\n\n\nCode\nspark_ui_access = \"\"\"\n# 1. Trouver le pod Driver\nkubectl get pods -n spark -l spark-role=driver\n\n# 2. Port-forward vers le Spark UI\nkubectl port-forward -n spark &lt;driver-pod-name&gt; 4040:4040\n\n# 3. Ouvrir dans le navigateur\n# http://localhost:4040\n\n# Alternative : crÃ©er un Service\nkubectl expose pod &lt;driver-pod-name&gt; -n spark \\\n  --port=4040 --target-port=4040 \\\n  --name=spark-ui --type=NodePort\n\"\"\"\nprint(spark_ui_access)\n\n\n\n\nğŸ‹ï¸ Exercice 4 : Lancer un job Spark sur Minikube\nÃ‰tapes : 1. DÃ©marrer Minikube : minikube start --cpus=4 --memory=8g 2. CrÃ©er le namespace et RBAC 3. Lancer le job SparkPi intÃ©grÃ©\n\n\nğŸ’¡ Solution\n\n# 1. Setup\nminikube start --cpus=4 --memory=8g\nkubectl create namespace spark\nkubectl apply -f rbac.yaml\n\n# 2. Obtenir l'URL de l'API K8s\nK8S_API=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')\n\n# 3. Lancer SparkPi\nspark-submit \\\n  --master k8s://$K8S_API \\\n  --deploy-mode cluster \\\n  --name spark-pi \\\n  --conf spark.kubernetes.container.image=apache/spark:3.5.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  --conf spark.executor.instances=2 \\\n  local:///opt/spark/examples/src/main/python/pi.py 100\n\n# 4. VÃ©rifier\nkubectl get pods -n spark\nkubectl logs -n spark &lt;driver-pod&gt; | tail -20",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-operator-production-grade",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-operator-production-grade",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ›ï¸ 6. Spark Operator â€” Production-Grade",
    "text": "ğŸ›ï¸ 6. Spark Operator â€” Production-Grade\n\n6.1 Pourquoi utiliser Spark Operator ?\n\n\n\nspark-submit\nSpark Operator\n\n\n\n\nCommande CLI\nManifeste YAML dÃ©claratif\n\n\nPas de retry automatique\nRetry policy configurable\n\n\nPas de scheduling\nCronJob-like scheduling\n\n\nDifficile Ã  intÃ©grer CI/CD\nGitOps natif\n\n\nLogs dispersÃ©s\nLogs centralisÃ©s\n\n\n\n\n\n6.2 Installation avec Helm\n\n\nCode\noperator_install = \"\"\"\n# Ajouter le repo Helm\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\nhelm repo update\n\n# Installer l'opÃ©rateur\nhelm install spark-operator spark-operator/spark-operator \\\n  --namespace spark-operator \\\n  --create-namespace \\\n  --set webhook.enable=true \\\n  --set sparkJobNamespace=spark\n\n# VÃ©rifier\nkubectl get pods -n spark-operator\n\"\"\"\nprint(operator_install)\n\n\n\n\n6.3 SparkApplication CRD\n\n\nCode\n%%writefile /tmp/spark-k8s/spark-application.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: etl-job\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: my-registry/spark-app:1.0.0\n  imagePullPolicy: Always\n  mainApplicationFile: local:///app/etl_job.py\n  arguments:\n    - \"--input\"\n    - \"s3a://bronze/data\"\n    - \"--output\"\n    - \"s3a://silver/data\"\n  sparkVersion: \"3.5.0\"\n  \n  # Configuration Spark\n  sparkConf:\n    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio.minio.svc.cluster.local:9000\"\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\"\n    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n  \n  # Restart policy\n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 3\n    onFailureRetryInterval: 30\n    onSubmissionFailureRetries: 3\n  \n  # Driver config\n  driver:\n    cores: 1\n    coreLimit: \"1200m\"\n    memory: \"2g\"\n    serviceAccount: spark-sa\n    labels:\n      app: spark-etl\n      role: driver\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n  \n  # Executor config\n  executor:\n    cores: 2\n    coreLimit: \"2400m\"\n    instances: 4\n    memory: \"4g\"\n    labels:\n      app: spark-etl\n      role: executor\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n\n\n\n\n6.4 ScheduledSparkApplication (Cron jobs)\n\n\nCode\n%%writefile /tmp/spark-k8s/scheduled-spark.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: ScheduledSparkApplication\nmetadata:\n  name: daily-etl\n  namespace: spark\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h du matin\n  concurrencyPolicy: Forbid  # Ne pas lancer si le prÃ©cÃ©dent tourne encore\n  successfulRunHistoryLimit: 5\n  failedRunHistoryLimit: 3\n  \n  template:\n    type: Python\n    pythonVersion: \"3\"\n    mode: cluster\n    image: my-registry/spark-app:1.0.0\n    mainApplicationFile: local:///app/daily_etl.py\n    sparkVersion: \"3.5.0\"\n    \n    restartPolicy:\n      type: OnFailure\n      onFailureRetries: 2\n    \n    driver:\n      cores: 1\n      memory: \"2g\"\n      serviceAccount: spark-sa\n    \n    executor:\n      cores: 2\n      instances: 3\n      memory: \"4g\"\n\n\n\n\nCode\noperator_commands = \"\"\"\n# Appliquer une SparkApplication\nkubectl apply -f spark-application.yaml\n\n# Voir le statut\nkubectl get sparkapplication -n spark\nkubectl describe sparkapplication etl-job -n spark\n\n# Voir les logs du driver\nkubectl logs -n spark -l spark-role=driver -f\n\n# Supprimer\nkubectl delete sparkapplication etl-job -n spark\n\"\"\"\nprint(operator_commands)\n\n\n\n\nğŸ‹ï¸ Exercice 5 : DÃ©ployer avec SparkOperator\nObjectif : CrÃ©er une SparkApplication qui calcule Pi avec 1000 itÃ©rations.\n\n\nğŸ’¡ Solution\n\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: apache/spark:3.5.0\n  mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py\n  arguments: [\"1000\"]\n  sparkVersion: \"3.5.0\"\n  \n  restartPolicy:\n    type: Never\n  \n  driver:\n    cores: 1\n    memory: \"1g\"\n    serviceAccount: spark-sa\n  \n  executor:\n    cores: 1\n    instances: 2\n    memory: \"1g\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#autoscaling-resource-management",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#autoscaling-resource-management",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“ˆ 7. Autoscaling & Resource Management",
    "text": "ğŸ“ˆ 7. Autoscaling & Resource Management\n\n7.1 Dynamic Allocation (Spark natif)\nSpark peut ajuster dynamiquement le nombre dâ€™executors.\n\n\nCode\ndynamic_allocation_config = \"\"\"\n# Dans sparkConf ou spark-submit\nspark.dynamicAllocation.enabled=true\nspark.dynamicAllocation.minExecutors=1\nspark.dynamicAllocation.maxExecutors=10\nspark.dynamicAllocation.initialExecutors=2\nspark.dynamicAllocation.executorIdleTimeout=60s\nspark.dynamicAllocation.schedulerBacklogTimeout=1s\n\n# Shuffle tracking (requis pour K8s)\nspark.dynamicAllocation.shuffleTracking.enabled=true\nspark.dynamicAllocation.shuffleTracking.timeout=600s\n\"\"\"\nprint(dynamic_allocation_config)\n\n\n\n\n7.2 Options dâ€™autoscaling K8s\n\n\n\n\n\n\n\n\nType\nDescription\nUse case\n\n\n\n\nDynamic Allocation\nSpark gÃ¨re les executors\nâœ… RecommandÃ© pour batch\n\n\nHPA\nScale sur CPU/memory\nâš ï¸ Pas idÃ©al pour Spark\n\n\nVPA\nAjuste les ressources\nğŸ’¡ Pour dimensionnement initial\n\n\nKEDA\nScale sur Ã©vÃ©nements externes\nâœ… IdÃ©al pour streaming (Kafka lag)\n\n\nCluster Autoscaler\nAjoute/retire des nodes\nâœ… CombinÃ© avec Dynamic Allocation\n\n\n\n\n\nCode\n%%writefile /tmp/spark-k8s/keda-scaledobject.yaml\n# KEDA ScaledObject pour Spark Streaming basÃ© sur Kafka lag\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: spark-streaming-scaler\n  namespace: spark\nspec:\n  scaleTargetRef:\n    name: spark-streaming  # Deployment Ã  scaler\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  triggers:\n    - type: kafka\n      metadata:\n        bootstrapServers: kafka.kafka.svc.cluster.local:9092\n        consumerGroup: spark-streaming-group\n        topic: events\n        lagThreshold: \"100\"  # Scale si lag &gt; 100\n\n\n\n\n7.3 Node Affinity & Tolerations\n\n\nCode\n%%writefile /tmp/spark-k8s/spark-with-affinity.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: gpu-job\n  namespace: spark\nspec:\n  type: Python\n  mode: cluster\n  image: my-registry/spark-gpu:1.0.0\n  mainApplicationFile: local:///app/gpu_job.py\n  sparkVersion: \"3.5.0\"\n  \n  driver:\n    cores: 1\n    memory: \"2g\"\n    serviceAccount: spark-sa\n  \n  executor:\n    cores: 4\n    instances: 2\n    memory: \"8g\"\n    # Placer les executors sur des nodes avec GPU\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - gpu\n    # TolÃ©rer les taints GPU\n    tolerations:\n      - key: \"nvidia.com/gpu\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n\n\n\n\nğŸ‹ï¸ Exercice 6 : Configurer Dynamic Allocation\nObjectif : Modifier la SparkApplication pour avoir : - Min 2 executors - Max 8 executors - Initial 3 executors\n\n\nğŸ’¡ Solution\n\nsparkConf:\n  \"spark.dynamicAllocation.enabled\": \"true\"\n  \"spark.dynamicAllocation.minExecutors\": \"2\"\n  \"spark.dynamicAllocation.maxExecutors\": \"8\"\n  \"spark.dynamicAllocation.initialExecutors\": \"3\"\n  \"spark.dynamicAllocation.shuffleTracking.enabled\": \"true\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#monitoring-observability",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#monitoring-observability",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“Š 8. Monitoring & Observability",
    "text": "ğŸ“Š 8. Monitoring & Observability\n\n8.1 Spark UI\n\n\nCode\n%%writefile /tmp/spark-k8s/spark-ui-ingress.yaml\n# Service pour Spark UI\napiVersion: v1\nkind: Service\nmetadata:\n  name: spark-ui\n  namespace: spark\nspec:\n  selector:\n    spark-role: driver\n  ports:\n    - port: 4040\n      targetPort: 4040\n  type: ClusterIP\n---\n# Ingress pour accÃ¨s externe\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: spark-ui-ingress\n  namespace: spark\nspec:\n  rules:\n    - host: spark-ui.mycompany.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: spark-ui\n                port:\n                  number: 4040\n\n\n\n\n8.2 Spark History Server\n\n\nCode\n%%writefile /tmp/spark-k8s/history-server.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spark-history-server\n  namespace: spark\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spark-history-server\n  template:\n    metadata:\n      labels:\n        app: spark-history-server\n    spec:\n      containers:\n        - name: history-server\n          image: bitnami/spark:3.5\n          command:\n            - /opt/bitnami/spark/sbin/start-history-server.sh\n          env:\n            - name: SPARK_HISTORY_OPTS\n              value: \"-Dspark.history.fs.logDirectory=s3a://spark-logs/history\"\n          ports:\n            - containerPort: 18080\n          envFrom:\n            - secretRef:\n                name: minio-credentials\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: spark-history-server\n  namespace: spark\nspec:\n  selector:\n    app: spark-history-server\n  ports:\n    - port: 18080\n      targetPort: 18080\n  type: ClusterIP\n\n\n\n\n8.3 Prometheus + Grafana\nConfiguration Spark pour exposer les mÃ©triques :\n\n\nCode\nprometheus_spark_config = \"\"\"\n# Dans sparkConf\nspark.metrics.conf.*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet\nspark.metrics.conf.*.sink.prometheusServlet.path=/metrics/prometheus\nspark.metrics.conf.master.sink.prometheusServlet.path=/metrics/master/prometheus\nspark.metrics.conf.applications.sink.prometheusServlet.path=/metrics/applications/prometheus\n\n# Activer les mÃ©triques JVM\nspark.metrics.conf.*.source.jvm.class=org.apache.spark.metrics.source.JvmSource\n\"\"\"\nprint(prometheus_spark_config)\n\n\n\n\nCode\n%%writefile /tmp/spark-k8s/servicemonitor.yaml\n# ServiceMonitor pour Prometheus Operator\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: spark-monitor\n  namespace: spark\n  labels:\n    release: prometheus  # Match avec ton installation Prometheus\nspec:\n  selector:\n    matchLabels:\n      spark-role: driver\n  endpoints:\n    - port: spark-ui\n      path: /metrics/prometheus\n      interval: 15s\n\n\n\n\n8.4 Dashboards Grafana\nMÃ©triques clÃ©s Ã  surveiller :\n\n\n\nMÃ©trique\nDescription\nAlerte si\n\n\n\n\nspark_executor_count\nNombre dâ€™executors actifs\n&lt; min expected\n\n\nspark_executor_memory_used\nMÃ©moire utilisÃ©e\n&gt; 80%\n\n\nspark_job_duration_seconds\nDurÃ©e des jobs\n&gt; baseline Ã— 2\n\n\nspark_task_failures_total\nTasks Ã©chouÃ©es\n&gt; 0\n\n\nspark_shuffle_write_bytes\nShuffle Ã©crit\nAnomalies\n\n\n\n\n\nğŸ‹ï¸ Exercice 7 : Exposer Spark UI\nObjectif : AccÃ©der au Spark UI dâ€™un job en cours.\n# 1. Lister les pods driver\nkubectl get pods -n spark -l spark-role=driver\n\n# 2. Port-forward\nkubectl port-forward -n spark &lt;driver-pod&gt; 4040:4040\n\n# 3. Ouvrir http://localhost:4040",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#debugging-troubleshooting",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#debugging-troubleshooting",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ”§ 9. Debugging & Troubleshooting",
    "text": "ğŸ”§ 9. Debugging & Troubleshooting\n\n9.1 Erreurs courantes et solutions\n\n\n\n\n\n\n\n\nErreur\nCause\nSolution\n\n\n\n\nImagePullBackOff\nImage introuvable\nVÃ©rifier registry, credentials, tag\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter memory, rÃ©duire donnÃ©es\n\n\nForbidden\nRBAC manquant\nVÃ©rifier Role/RoleBinding\n\n\nPending (pods)\nRessources insuffisantes\nVÃ©rifier quotas, cluster capacity\n\n\nConnection refused\nDriver inaccessible\nVÃ©rifier network policies, services\n\n\nClassNotFoundException\nJAR manquant\nAjouter dans lâ€™image Docker\n\n\n\n\n\nCode\ndebug_commands = \"\"\"\n# === Debugging Spark on K8s ===\n\n# 1. Voir les pods et leur statut\nkubectl get pods -n spark -o wide\n\n# 2. DÃ©tails d'un pod (Ã©vÃ©nements)\nkubectl describe pod &lt;pod-name&gt; -n spark\n\n# 3. Logs du driver\nkubectl logs -n spark -l spark-role=driver\n\n# 4. Logs d'un executor spÃ©cifique\nkubectl logs -n spark &lt;executor-pod-name&gt;\n\n# 5. Logs en temps rÃ©el (follow)\nkubectl logs -n spark -l spark-role=driver -f\n\n# 6. Shell dans un pod (debug)\nkubectl exec -it &lt;pod-name&gt; -n spark -- /bin/bash\n\n# 7. VÃ©rifier les ressources du namespace\nkubectl describe resourcequota -n spark\n\n# 8. VÃ©rifier les events\nkubectl get events -n spark --sort-by='.lastTimestamp'\n\n# 9. SparkApplication status\nkubectl get sparkapplication -n spark\nkubectl describe sparkapplication &lt;name&gt; -n spark\n\"\"\"\nprint(debug_commands)\n\n\n\n\n9.2 Debugging OOMKilled\nğŸ” SymptÃ´me : Pod executor en Ã©tat OOMKilled\n\nCauses possibles :\nâ”œâ”€â”€ spark.executor.memory trop bas\nâ”œâ”€â”€ spark.executor.memoryOverhead mal configurÃ©\nâ”œâ”€â”€ Trop de donnÃ©es par partition\nâ”œâ”€â”€ Broadcast variables trop grandes\nâ””â”€â”€ Fuite mÃ©moire dans le code\n\nSolutions :\nâ”œâ”€â”€ Augmenter spark.executor.memory\nâ”œâ”€â”€ spark.executor.memoryOverhead = max(0.1 Ã— memory, 384m)\nâ”œâ”€â”€ Repartitionner : df.repartition(200)\nâ”œâ”€â”€ RÃ©duire spark.sql.shuffle.partitions\nâ””â”€â”€ Utiliser spark.memory.fraction = 0.6 (default)\n\n\n9.3 Debugging Shuffle failures\nLe shuffle est plus coÃ»teux sur K8s (pas de data locality).\n\n\nCode\nshuffle_optimizations = \"\"\"\n# Optimisations shuffle pour K8s\n\n# Augmenter les timeouts\nspark.network.timeout=600s\nspark.shuffle.io.maxRetries=10\nspark.shuffle.io.retryWait=30s\n\n# Compression\nspark.shuffle.compress=true\nspark.shuffle.spill.compress=true\n\n# Buffer sizes\nspark.shuffle.file.buffer=64k\nspark.reducer.maxSizeInFlight=96m\n\n# Shuffle tracking (pour Dynamic Allocation)\nspark.dynamicAllocation.shuffleTracking.enabled=true\n\"\"\"\nprint(shuffle_optimizations)\n\n\n\n\nğŸ‹ï¸ Exercice 8 : Diagnostiquer un job qui Ã©choue\nScÃ©nario : Un job Spark Ã©choue avec le message â€œPod OOMKilledâ€.\nQuestions : 1. Quelle commande pour voir les events du pod ? 2. Quelles configs Spark vÃ©rifier ?\n\n\nğŸ’¡ Solution\n\n\nCommandes :\n\nkubectl describe pod &lt;pod-name&gt; -n spark\nkubectl get events -n spark --field-selector involvedObject.name=&lt;pod-name&gt;\n\nConfigs Ã  vÃ©rifier :\n\nspark.executor.memory\nspark.executor.memoryOverhead\nspark.kubernetes.executor.limit.memory",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#optimisations-spark-on-k8s",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#optimisations-spark-on-k8s",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "âš¡ 10. Optimisations Spark on K8s",
    "text": "âš¡ 10. Optimisations Spark on K8s\n\n10.1 Data Locality\nâš ï¸ ProblÃ¨me : Pas de data locality sur K8s\n\nSur YARN avec HDFS :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Node 1               â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚ â”‚Executorâ”‚â—€â”‚ Data  â”‚ â”‚  â† Data local (fast)\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nSur K8s avec Object Storage :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Node 1               â”‚      â”‚ S3/MinIO    â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚â—€â”€â”€â”€â”€â”€â”‚ (remote)    â”‚  â† Network transfer\nâ”‚ â”‚Executorâ”‚           â”‚      â”‚             â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nSolutions : - Utiliser des formats colonnaires (Parquet) optimisÃ©s - Configurer S3A/GCS pour le parallÃ©lisme - External Shuffle Service (Spark 3.4+)\n\n\nCode\ns3a_config = \"\"\"\n# Optimisations S3A pour Spark on K8s\n\n# ParallÃ©lisme\nspark.hadoop.fs.s3a.connection.maximum=200\nspark.hadoop.fs.s3a.threads.max=64\nspark.hadoop.fs.s3a.threads.core=16\n\n# Fast upload\nspark.hadoop.fs.s3a.fast.upload=true\nspark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer\nspark.hadoop.fs.s3a.multipart.size=104857600  # 100MB\n\n# Committer (Ã©viter les renames S3)\nspark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\nspark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n\"\"\"\nprint(s3a_config)\n\n\n\n\n10.2 Pod Templates\nPour des configurations avancÃ©es des pods Spark.\n\n\nCode\n%%writefile /tmp/spark-k8s/executor-pod-template.yaml\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: spark-executor\n      volumeMounts:\n        - name: spark-local-dir\n          mountPath: /tmp/spark\n      resources:\n        requests:\n          ephemeral-storage: \"10Gi\"\n        limits:\n          ephemeral-storage: \"20Gi\"\n  volumes:\n    - name: spark-local-dir\n      emptyDir:\n        medium: Memory  # Utiliser RAM pour shuffle local\n        sizeLimit: \"4Gi\"\n\n\n\n\nCode\npod_template_usage = \"\"\"\n# Utiliser le pod template dans spark-submit\nspark-submit \\\n  --conf spark.kubernetes.executor.podTemplateFile=/path/to/executor-pod-template.yaml \\\n  ...\n\n# Ou dans SparkApplication\nspec:\n  executor:\n    podTemplateFile: /path/to/executor-pod-template.yaml\n\"\"\"\nprint(pod_template_usage)\n\n\n\n\n10.3 Spot/Preemptible Instances\nÃ‰conomiser jusquâ€™Ã  90% sur les coÃ»ts compute.\n\n\nCode\n%%writefile /tmp/spark-k8s/spark-with-spot.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spot-etl\n  namespace: spark\nspec:\n  type: Python\n  mode: cluster\n  image: my-registry/spark-app:1.0.0\n  mainApplicationFile: local:///app/etl.py\n  sparkVersion: \"3.5.0\"\n  \n  # Driver sur nodes stables (on-demand)\n  driver:\n    cores: 1\n    memory: \"2g\"\n    serviceAccount: spark-sa\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - on-demand\n  \n  # Executors sur spot instances\n  executor:\n    cores: 2\n    instances: 5\n    memory: \"4g\"\n    affinity:\n      nodeAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - spot\n    tolerations:\n      - key: \"kubernetes.azure.com/scalesetpriority\"\n        operator: \"Equal\"\n        value: \"spot\"\n        effect: \"NoSchedule\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#mini-projet-etl-pipeline-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#mini-projet-etl-pipeline-sur-kubernetes",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸš€ 11. Mini-Projet : ETL Pipeline sur Kubernetes",
    "text": "ğŸš€ 11. Mini-Projet : ETL Pipeline sur Kubernetes\n\nğŸ¯ Objectif\nDÃ©ployer un pipeline Spark complet sur K8s avec MinIO (Object Storage local).\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Kubernetes Cluster                          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚   MinIO     â”‚     â”‚      Spark       â”‚     â”‚   MinIO     â”‚  â”‚\nâ”‚  â”‚  (source)   â”‚â”€â”€â”€â”€â–¶â”‚    on K8s        â”‚â”€â”€â”€â”€â–¶â”‚  (output)   â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚                  â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚ bucket:     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚ bucket:     â”‚  â”‚\nâ”‚  â”‚ bronze/     â”‚     â”‚  â”‚  Driver    â”‚  â”‚     â”‚ silver/     â”‚  â”‚\nâ”‚  â”‚   data.csv  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚   data.parquetâ”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â”‚Execâ”‚ â”‚Execâ”‚   â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜   â”‚     â”‚             â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nStructure du projet\nspark-k8s-project/\nâ”œâ”€â”€ docker/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â””â”€â”€ requirements.txt\nâ”œâ”€â”€ app/\nâ”‚   â””â”€â”€ etl_job.py\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ rbac.yaml\nâ”‚   â”œâ”€â”€ minio-secret.yaml\nâ”‚   â””â”€â”€ spark-application.yaml\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sample.csv\nâ””â”€â”€ README.md\n\n\nÃ‰tape 1 : DÃ©ployer MinIO sur K8s\n\n\nCode\nminio_deploy = \"\"\"\n# DÃ©ployer MinIO avec Helm\nhelm repo add minio https://charts.min.io/\nhelm repo update\n\nhelm install minio minio/minio \\\n  --namespace minio \\\n  --create-namespace \\\n  --set rootUser=minioadmin \\\n  --set rootPassword=minioadmin \\\n  --set mode=standalone \\\n  --set resources.requests.memory=512Mi \\\n  --set persistence.size=10Gi\n\n# Port-forward pour accÃ©der Ã  la console\nkubectl port-forward -n minio svc/minio-console 9001:9001\n\n# CrÃ©er les buckets\nmc alias set myminio http://localhost:9000 minioadmin minioadmin\nmc mb myminio/bronze\nmc mb myminio/silver\n\n# Uploader des donnÃ©es de test\nmc cp data/sample.csv myminio/bronze/\n\"\"\"\nprint(minio_deploy)\n\n\n\n\nÃ‰tape 2 : Code de lâ€™application ETL\n\n\nCode\n%%writefile /tmp/spark-k8s-project/app/etl_job.py\n\"\"\"\nETL Job : Bronze â†’ Silver\nLit des CSV depuis MinIO, transforme, et Ã©crit en Parquet.\n\"\"\"\nimport argparse\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, upper, current_timestamp\n\n\ndef create_spark_session():\n    \"\"\"CrÃ©er une SparkSession configurÃ©e pour MinIO.\"\"\"\n    return SparkSession.builder \\\n        .appName(\"ETL Bronze to Silver\") \\\n        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n        .getOrCreate()\n\n\ndef extract(spark, input_path):\n    \"\"\"Lire les donnÃ©es brutes depuis le bucket Bronze.\"\"\"\n    print(f\"ğŸ“¥ Reading from {input_path}\")\n    df = spark.read \\\n        .option(\"header\", \"true\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .csv(input_path)\n    print(f\"   Loaded {df.count()} rows\")\n    return df\n\n\ndef transform(df):\n    \"\"\"Appliquer les transformations mÃ©tier.\"\"\"\n    print(\"ğŸ”„ Transforming data\")\n    transformed = df \\\n        .dropDuplicates() \\\n        .dropna() \\\n        .withColumn(\"processed_at\", current_timestamp())\n    \n    # Normalisation des colonnes string\n    for col_name, dtype in transformed.dtypes:\n        if dtype == \"string\":\n            transformed = transformed.withColumn(col_name, upper(col(col_name)))\n    \n    print(f\"   Transformed to {transformed.count()} rows\")\n    return transformed\n\n\ndef load(df, output_path):\n    \"\"\"Ã‰crire les donnÃ©es transformÃ©es dans le bucket Silver.\"\"\"\n    print(f\"ğŸ“¤ Writing to {output_path}\")\n    df.write \\\n        .mode(\"overwrite\") \\\n        .parquet(output_path)\n    print(\"   Done!\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ETL Job\")\n    parser.add_argument(\"--input\", required=True, help=\"Input path (s3a://...)\")\n    parser.add_argument(\"--output\", required=True, help=\"Output path (s3a://...)\")\n    args = parser.parse_args()\n    \n    spark = create_spark_session()\n    \n    try:\n        # ETL Pipeline\n        df = extract(spark, args.input)\n        transformed = transform(df)\n        load(transformed, args.output)\n        \n        print(\"âœ… ETL completed successfully!\")\n    except Exception as e:\n        print(f\"âŒ ETL failed: {e}\")\n        raise\n    finally:\n        spark.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\nÃ‰tape 3 : Dockerfile\n\n\nCode\n%%writefile /tmp/spark-k8s-project/docker/Dockerfile\nFROM bitnami/spark:3.5\n\nUSER root\n\n# DÃ©pendances Python\nRUN pip install --no-cache-dir pyarrow pandas\n\n# JARs pour S3/MinIO\nRUN curl -sL https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \\\n    -o /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar && \\\n    curl -sL https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar \\\n    -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar\n\n# Application\nCOPY app/ /app/\nRUN chown -R 1001:1001 /app\n\nUSER 1001\nWORKDIR /app\n\n\n\n\nÃ‰tape 4 : Manifests Kubernetes\n\n\nCode\n%%writefile /tmp/spark-k8s-project/manifests/spark-application.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: etl-bronze-silver\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: spark-etl:1.0.0  # Image locale (Minikube)\n  imagePullPolicy: Never  # Pour Minikube\n  mainApplicationFile: local:///app/etl_job.py\n  arguments:\n    - \"--input\"\n    - \"s3a://bronze/sample.csv\"\n    - \"--output\"\n    - \"s3a://silver/data\"\n  sparkVersion: \"3.5.0\"\n  \n  sparkConf:\n    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio.minio.svc.cluster.local:9000\"\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\"\n    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n  \n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 2\n  \n  driver:\n    cores: 1\n    memory: \"1g\"\n    serviceAccount: spark-sa\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n  \n  executor:\n    cores: 1\n    instances: 2\n    memory: \"1g\"\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n\n\n\n\nÃ‰tape 5 : DÃ©ploiement complet\n\n\nCode\ndeployment_script = \"\"\"\n#!/bin/bash\nset -e\n\necho \"ğŸš€ DÃ©ploiement du pipeline ETL Spark on K8s\"\n\n# 1. CrÃ©er le namespace et RBAC\necho \"ğŸ“ CrÃ©ation namespace et RBAC...\"\nkubectl apply -f manifests/namespace.yaml\nkubectl apply -f manifests/rbac.yaml\nkubectl apply -f manifests/minio-secret.yaml\n\n# 2. Build de l'image (Minikube)\necho \"ğŸ³ Build de l'image Docker...\"\neval $(minikube docker-env)\ndocker build -t spark-etl:1.0.0 -f docker/Dockerfile .\n\n# 3. VÃ©rifier que MinIO est prÃªt\necho \"â³ Attente de MinIO...\"\nkubectl wait --for=condition=ready pod -l app=minio -n minio --timeout=120s\n\n# 4. Lancer le job Spark\necho \"ğŸ”¥ Lancement du job Spark...\"\nkubectl apply -f manifests/spark-application.yaml\n\n# 5. Suivre le job\necho \"ğŸ“Š Suivi du job...\"\nkubectl get sparkapplication -n spark -w\n\"\"\"\nprint(deployment_script)\n\n\n\n\nÃ‰tape 6 : VÃ©rification\n\n\nCode\nverify_commands = \"\"\"\n# VÃ©rifier le statut du job\nkubectl get sparkapplication -n spark\n\n# Voir les logs du driver\nkubectl logs -n spark -l spark-role=driver\n\n# VÃ©rifier les outputs dans MinIO\nmc ls myminio/silver/data/\n\n# Lire un sample des donnÃ©es\nmc cat myminio/silver/data/part-00000.parquet | head\n\"\"\"\nprint(verify_commands)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#quiz-de-fin-de-module",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre Client mode et Cluster mode ?\n\nClient mode est plus rapide\n\nEn Cluster mode, le Driver tourne dans un Pod K8s\n\nClient mode ne supporte pas les executors\n\nCluster mode nÃ©cessite YARN\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En Cluster mode, le Driver est un Pod K8s. En Client mode, il reste sur la machine locale.\n\n\n\n\nâ“ Q2. Pourquoi utiliser Spark Operator plutÃ´t que spark-submit directement ?\n\nSpark Operator est plus rapide\n\nspark-submit ne fonctionne pas sur K8s\n\nSpark Operator permet des manifestes YAML dÃ©claratifs et des retries automatiques\n\nSpark Operator ne nÃ©cessite pas de ServiceAccount\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Spark Operator offre une approche dÃ©clarative (YAML), des retry policies, du scheduling, et une meilleure intÃ©gration CI/CD.\n\n\n\n\nâ“ Q3. Quel problÃ¨me rÃ©sout Dynamic Allocation ?\n\nLa sÃ©curitÃ© des pods\n\nLâ€™ajustement automatique du nombre dâ€™executors\n\nLe stockage des logs\n\nLa communication rÃ©seau\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Dynamic Allocation permet Ã  Spark dâ€™ajuster automatiquement le nombre dâ€™executors selon la charge.\n\n\n\n\nâ“ Q4. Quelle config est requise pour Dynamic Allocation sur K8s ?\n\nspark.dynamicAllocation.enabled=true uniquement\n\nspark.dynamicAllocation.shuffleTracking.enabled=true\n\nspark.kubernetes.allocation.batch.size=5\n\nAucune config spÃ©ciale\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Sur K8s (sans External Shuffle Service), shuffle tracking est requis pour que Dynamic Allocation fonctionne.\n\n\n\n\nâ“ Q5. Que signifie lâ€™erreur OOMKilled ?\n\nLe pod nâ€™a pas dâ€™image\n\nLe pod a dÃ©passÃ© sa limite mÃ©moire\n\nLe rÃ©seau est indisponible\n\nLe ServiceAccount est invalide\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” OOMKilled signifie que le container a dÃ©passÃ© sa limite mÃ©moire et a Ã©tÃ© tuÃ© par K8s.\n\n\n\n\nâ“ Q6. Quel est lâ€™avantage principal de K8s sur YARN pour Spark ?\n\nMeilleure data locality\n\nInfrastructure cloud-native et multi-cloud\n\nPlus rapide\n\nMoins de configuration\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” K8s offre une infrastructure cloud-native, standardisÃ©e, multi-cloud, avec autoscaling avancÃ©.\n\n\n\n\nâ“ Q7. Comment exposer le Spark UI dâ€™un job en cluster mode ?\n\nIl est automatiquement accessible sur localhost:4040\n\nVia port-forward, Service, ou Ingress\n\nVia YARN ResourceManager\n\nCe nâ€™est pas possible en cluster mode\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” On utilise kubectl port-forward, un Service K8s, ou un Ingress pour exposer le Spark UI.\n\n\n\n\nâ“ Q8. Pourquoi le shuffle est-il plus coÃ»teux sur K8s que sur YARN/HDFS ?\n\nK8s est plus lent\n\nPas de data locality â€” les donnÃ©es doivent transiter par le rÃ©seau\n\nSpark nâ€™est pas optimisÃ© pour K8s\n\nLes executors ont moins de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Sur YARN/HDFS, les donnÃ©es peuvent Ãªtre locales. Sur K8s avec Object Storage, tout passe par le rÃ©seau.\n\n\n\n\nâ“ Q9. Quel est le rÃ´le du ServiceAccount dans Spark on K8s ?\n\nStocker les credentials\n\nPermettre au Driver de crÃ©er des Executor Pods\n\nGÃ©rer le Spark UI\n\nConfigurer le rÃ©seau\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le ServiceAccount donne au Driver les permissions RBAC pour crÃ©er, lister et supprimer des Pods (executors).\n\n\n\n\nâ“ Q10. Comment Ã©conomiser sur les coÃ»ts compute avec Spark on K8s ?\n\nUtiliser moins dâ€™executors\n\nDÃ©sactiver le monitoring\n\nUtiliser des Spot/Preemptible instances pour les executors\n\nNe pas utiliser de PVC\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les Spot instances peuvent coÃ»ter jusquâ€™Ã  90% moins cher. Le Driver reste sur des nodes stables, les executors sur Spot.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#ressources-pour-aller-plus-loin",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark on Kubernetes\nSpark Operator GitHub\nSpark Operator User Guide\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Spark on K8s Best Practices\nGoogle Cloud - Running Spark on GKE\nAWS - EMR on EKS",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#prochaine-Ã©tape",
    "title": "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu sais dÃ©ployer Spark sur Kubernetes, passons au Cloud et Object Storage !\nğŸ‘‰ Module suivant : 22_cloud_object_storage.ipynb â€” Cloud & Object Storage\nTu vas apprendre : - Cloud Computing : IaaS, PaaS, SaaS - AWS, GCP, Azure : Services Data Engineering - Object Storage : S3, GCS, Azure Blob - MinIO : Pratiquer localement\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nArchitecture\nDriver/Executor Pods, Client vs Cluster mode\n\n\nDocker Image\nBuild, JARs, best practices\n\n\nK8s Config\nRBAC, Secrets, PVC\n\n\nspark-submit\nConfigurations essentielles\n\n\nSpark Operator\nSparkApplication, ScheduledSparkApplication\n\n\nAutoscaling\nDynamic Allocation, KEDA\n\n\nMonitoring\nSpark UI, Prometheus, Grafana\n\n\nDebugging\nErreurs courantes, commandes kubectl\n\n\nOptimisations\nS3A config, Pod templates, Spot instances\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Spark on Kubernetes.\n\n\nCode\n# Commandes de nettoyage\ncleanup_commands = \"\"\"\n# Supprimer les ressources Spark\nkubectl delete sparkapplication --all -n spark\nkubectl delete namespace spark\n\n# Supprimer Spark Operator\nhelm uninstall spark-operator -n spark-operator\nkubectl delete namespace spark-operator\n\n# Supprimer MinIO\nhelm uninstall minio -n minio\nkubectl delete namespace minio\n\n# ArrÃªter Minikube (optionnel)\nminikube stop\n\"\"\"\nprint(cleanup_commands)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Spark on Kubernetes â€” Production-Grade Deployment"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "",
    "text": "Ce module prÃ©sente PySpark, lâ€™API Python pour Apache Spark â€” le moteur de traitement distribuÃ© le plus utilisÃ© en Big Data.",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre MapReduce et ses limites\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nMaÃ®triser SQL (module 07)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre lâ€™architecture Spark (Driver, Executors, Cluster Manager)\nâœ… CrÃ©er et manipuler des DataFrames distribuÃ©s\nâœ… Ã‰crire des transformations et actions\nâœ… Utiliser Spark SQL\nâœ… Optimiser les performances (partitioning, caching, broadcast)\nâœ… Lire/Ã©crire des fichiers (CSV, JSON, Parquet)\nâœ… DÃ©couvrir le streaming temps rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ¯ PySpark dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ PySpark dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 que Spark a remplacÃ© MapReduce comme moteur de traitement Big Data. Voici pourquoi :\n\nRappel : MapReduce vs Spark\nMapReduce :  DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                  â†‘           â†‘              â†‘              â†‘\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               LENT ! (I/O disque)\n\nSpark :      DISQUE â†’ Transformations â†’ MÃ‰MOIRE â†’ ... â†’ MÃ‰MOIRE â†’ Action\n                                          â†‘                â†‘\n                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           RAPIDE ! (in-memory)\n\n\nRappel : Les 5V et Spark\n\n\n\nV\nComment Spark rÃ©pond\n\n\n\n\nVolume\nTraitement distribuÃ© sur cluster (To â†’ Po)\n\n\nVelocity\nSpark Streaming pour le temps rÃ©el\n\n\nVariety\nLit CSV, JSON, Parquet, JDBC, Avroâ€¦\n\n\nVeracity\nTransformations pour nettoyer les donnÃ©es\n\n\nValue\nSpark SQL, MLlib pour extraire de la valeur\n\n\n\n\n\nPosition dans lâ€™Ã©cosystÃ¨me\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Ã‰COSYSTÃˆME BIG DATA                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources           Traitement              Stockage            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚                                                                 â”‚\nâ”‚   Kafka    â”€â”                         â”Œâ”€â–º  Data Lake (S3)      â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  âš¡ SPARK âš¡  â”€â”€â”€â”€â”€â”€â”¼â”€â–º  Data Warehouse       â”‚\nâ”‚   JDBC     â”€â”¤     (PySpark)          â”œâ”€â–º  NoSQL (MongoDB)     â”‚\nâ”‚   APIs     â”€â”˜                         â””â”€â–º  Elasticsearch       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter toutes les cellules de code !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“¦ Installation et Setup",
    "text": "ğŸ“¦ Installation et Setup\nPySpark nÃ©cessite Java. VÃ©rifions dâ€™abord lâ€™installation.\n\n\nCode\n# Installation de PySpark\n!pip install pyspark pandas numpy pyarrow\n\n\n\n\nCode\n# VÃ©rifier Java\n!java -version\n\n\n\n\nCode\n# Imports de base\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nprint(\"âœ… Imports rÃ©ussis !\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Quâ€™est-ce que Spark ?",
    "text": "Quâ€™est-ce que Spark ?\nApache Spark est un moteur de traitement distribuÃ© ultra-rapide pour le Big Data.\n\nğŸ”‘ Concepts clÃ©s\n\nSparkSession : Point dâ€™entrÃ©e de toute application Spark\nDataFrame : Collection distribuÃ©e de donnÃ©es organisÃ©es en colonnes\nRDD : Resilient Distributed Dataset (bas niveau)\nTransformations : OpÃ©rations lazy (map, filter, select, etc.)\nActions : DÃ©clenchent lâ€™exÃ©cution (count, collect, show, etc.)\n\n\n\nğŸš€ Avantages de Spark\n\nVitesse : 100x plus rapide que MapReduce\nScalabilitÃ© : De quelques MB Ã  plusieurs PB\nSimplicitÃ© : API unifiÃ©e (Python, Scala, Java, R)\nVersatilitÃ© : Batch, Streaming, ML, Graph processing",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.1 CrÃ©er une SparkSession",
    "text": "1.1 CrÃ©er une SparkSession\n\n\nCode\n# CrÃ©er une SparkSession\nspark = SparkSession.builder \\\n    .appName(\"PySpark Data Engineering Tutorial\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n    .getOrCreate()\n\nprint(\"âœ… SparkSession crÃ©Ã©e\")\nprint(f\"Version Spark : {spark.version}\")\nprint(f\"Application : {spark.sparkContext.appName}\")\nprint(f\"Master : {spark.sparkContext.master}\")\n\n\n\n\nCode\n# Configuration du logging\nspark.sparkContext.setLogLevel(\"ERROR\")\nprint(\"âœ… Logging configurÃ© sur ERROR\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.2 Premiers DataFrames",
    "text": "1.2 Premiers DataFrames\n\n\nCode\n# MÃ©thode 1 : Depuis une liste Python\ndata = [\n    (1, \"Alice\", 25, \"Paris\", 45000),\n    (2, \"Bob\", 30, \"Lyon\", 55000),\n    (3, \"Charlie\", 35, \"Paris\", 60000),\n    (4, \"David\", 28, \"Marseille\", 50000),\n    (5, \"Eve\", 32, \"Lyon\", 58000)\n]\n\ncolumns = [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"]\n\ndf = spark.createDataFrame(data, columns)\n\nprint(\"ğŸ“Š Premier DataFrame crÃ©Ã© :\")\ndf.show()\n\n\n\n\nCode\n# MÃ©thode 2 : Depuis un Pandas DataFrame\npandas_df = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'D'],\n    'prix': [10.5, 20.0, 15.75, 30.0],\n    'quantite': [100, 50, 75, 25]\n})\n\nspark_df = spark.createDataFrame(pandas_df)\n\nprint(\"ğŸ“Š DataFrame depuis Pandas :\")\nspark_df.show()\n\n\n\n\nCode\n# MÃ©thode 3 : Avec un schÃ©ma explicite\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"nom\", StringType(), False),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"ville\", StringType(), True),\n    StructField(\"salaire\", IntegerType(), True)\n])\n\ndf_with_schema = spark.createDataFrame(data, schema)\n\nprint(\"ğŸ“Š DataFrame avec schÃ©ma explicite :\")\ndf_with_schema.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.3 Explorer un DataFrame",
    "text": "1.3 Explorer un DataFrame\n\n\nCode\n# Afficher le schÃ©ma\nprint(\"ğŸ“‹ SchÃ©ma du DataFrame :\")\ndf.printSchema()\n\n# Afficher les premiÃ¨res lignes\nprint(\"\\nğŸ” PremiÃ¨res lignes :\")\ndf.show(3)\n\n# Compter les lignes\nprint(f\"\\nğŸ“ Nombre de lignes : {df.count()}\")\n\n# Colonnes\nprint(f\"\\nğŸ“‹ Colonnes : {df.columns}\")\n\n# Types de donnÃ©es\nprint(\"\\nğŸ”¤ Types de donnÃ©es :\")\nprint(df.dtypes)\n\n\n\n\nCode\n# Statistiques descriptives\nprint(\"ğŸ“Š Statistiques descriptives :\")\ndf.describe().show()\n\n# Statistiques sur colonnes spÃ©cifiques\nprint(\"\\nğŸ“Š Statistiques sur 'age' et 'salaire' :\")\ndf.select('age', 'salaire').describe().show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.1 SÃ©lection de colonnes",
    "text": "2.1 SÃ©lection de colonnes\n\n\nCode\n# SÃ©lectionner des colonnes\nprint(\"ğŸ“Œ SÃ©lection de colonnes :\")\ndf.select(\"nom\", \"ville\").show()\n\n# Avec alias\nprint(\"\\nğŸ“Œ Avec alias :\")\ndf.select(\n    F.col(\"nom\").alias(\"employee_name\"),\n    F.col(\"salaire\").alias(\"salary\")\n).show()\n\n# SÃ©lectionner avec expressions\nprint(\"\\nğŸ“Œ Avec expressions :\")\ndf.select(\n    \"nom\",\n    (F.col(\"salaire\") * 12).alias(\"salaire_annuel\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.2 Filtrage",
    "text": "2.2 Filtrage\n\n\nCode\n# Filtrer les lignes\nprint(\"ğŸ” EmployÃ©s de Paris :\")\ndf.filter(F.col(\"ville\") == \"Paris\").show()\n\n# Filtres multiples avec AND\nprint(\"\\nğŸ” EmployÃ©s de Paris avec salaire &gt; 50000 :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") & \n    (F.col(\"salaire\") &gt; 50000)\n).show()\n\n# Filtres avec OR\nprint(\"\\nğŸ” EmployÃ©s de Paris OU Lyon :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") | \n    (F.col(\"ville\") == \"Lyon\")\n).show()\n\n# Filtrer avec IN\nprint(\"\\nğŸ” Villes avec IN :\")\ndf.filter(F.col(\"ville\").isin([\"Paris\", \"Lyon\"])).show()\n\n\n\n\nCode\n# Filtres avancÃ©s\nprint(\"ğŸ” Noms commenÃ§ant par 'A' :\")\ndf.filter(F.col(\"nom\").startswith(\"A\")).show()\n\nprint(\"\\nğŸ” Noms contenant 'li' :\")\ndf.filter(F.col(\"nom\").contains(\"li\")).show()\n\nprint(\"\\nğŸ” Age entre 25 et 30 :\")\ndf.filter(F.col(\"age\").between(25, 30)).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.3 Ajouter et modifier des colonnes",
    "text": "2.3 Ajouter et modifier des colonnes\n\n\nCode\n# Ajouter une nouvelle colonne\ndf_with_bonus = df.withColumn(\n    \"bonus\",\n    F.col(\"salaire\") * 0.1\n)\n\nprint(\"â• Ajout de la colonne 'bonus' :\")\ndf_with_bonus.show()\n\n# Modifier une colonne existante\ndf_modified = df.withColumn(\n    \"salaire\",\n    F.col(\"salaire\") * 1.05  # Augmentation de 5%\n)\n\nprint(\"\\nâœï¸ Salaire augmentÃ© de 5% :\")\ndf_modified.show()\n\n\n\n\nCode\n# Ajouter plusieurs colonnes\ndf_enriched = df \\\n    .withColumn(\"salaire_mensuel\", F.col(\"salaire\")) \\\n    .withColumn(\"salaire_annuel\", F.col(\"salaire\") * 12) \\\n    .withColumn(\"bonus\", F.col(\"salaire\") * 0.1) \\\n    .withColumn(\"total_annuel\", F.col(\"salaire_annuel\") + F.col(\"bonus\"))\n\nprint(\"ğŸ“Š DataFrame enrichi :\")\ndf_enriched.select(\"nom\", \"salaire_mensuel\", \"salaire_annuel\", \"bonus\", \"total_annuel\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.4 Renommer et supprimer des colonnes",
    "text": "2.4 Renommer et supprimer des colonnes\n\n\nCode\n# Renommer une colonne\ndf_renamed = df.withColumnRenamed(\"nom\", \"employee_name\")\nprint(\"âœï¸ Colonne renommÃ©e :\")\ndf_renamed.show(3)\n\n# Supprimer des colonnes\ndf_dropped = df.drop(\"age\", \"ville\")\nprint(\"\\nğŸ—‘ï¸ Colonnes supprimÃ©es :\")\ndf_dropped.show(3)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.5 Tri",
    "text": "2.5 Tri\n\n\nCode\n# Trier par salaire (ascendant)\nprint(\"ğŸ“Š Tri par salaire (croissant) :\")\ndf.orderBy(\"salaire\").show()\n\n# Trier par salaire (descendant)\nprint(\"\\nğŸ“Š Tri par salaire (dÃ©croissant) :\")\ndf.orderBy(F.col(\"salaire\").desc()).show()\n\n# Tri multiple\nprint(\"\\nğŸ“Š Tri par ville puis salaire :\")\ndf.orderBy(\"ville\", F.col(\"salaire\").desc()).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.1 AgrÃ©gations simples",
    "text": "3.1 AgrÃ©gations simples\n\n\nCode\n# Statistiques de base\nprint(\"ğŸ“Š Statistiques simples :\")\ndf.select(\n    F.count(\"*\").alias(\"total\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\"),\n    F.sum(\"salaire\").alias(\"salaire_total\")\n).show()\n\n\n\n\nCode\n# AgrÃ©gations multiples\nfrom pyspark.sql.functions import stddev, variance\n\nprint(\"ğŸ“Š Statistiques avancÃ©es :\")\ndf.agg(\n    F.count(\"*\").alias(\"count\"),\n    F.avg(\"age\").alias(\"age_moyen\"),\n    F.stddev(\"salaire\").alias(\"salaire_stddev\"),\n    F.variance(\"salaire\").alias(\"salaire_variance\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.2 GroupBy",
    "text": "3.2 GroupBy\n\n\nCode\n# Grouper par ville\nprint(\"ğŸ“Š Statistiques par ville :\")\ndf.groupBy(\"ville\").agg(\n    F.count(\"*\").alias(\"nb_employes\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\")\n).orderBy(\"ville\").show()\n\n\n\n\nCode\n# CrÃ©er un DataFrame plus complexe pour les exemples\ndata_ventes = [\n    (\"2024-01\", \"Paris\", \"Produit A\", 100, 1500),\n    (\"2024-01\", \"Paris\", \"Produit B\", 50, 2000),\n    (\"2024-01\", \"Lyon\", \"Produit A\", 75, 1200),\n    (\"2024-02\", \"Paris\", \"Produit A\", 120, 1800),\n    (\"2024-02\", \"Lyon\", \"Produit B\", 60, 2400),\n    (\"2024-02\", \"Marseille\", \"Produit A\", 90, 1350),\n]\n\ncolumns_ventes = [\"mois\", \"ville\", \"produit\", \"quantite\", \"montant\"]\ndf_ventes = spark.createDataFrame(data_ventes, columns_ventes)\n\nprint(\"ğŸ“Š DonnÃ©es de ventes :\")\ndf_ventes.show()\n\n\n\n\nCode\n# GroupBy multiple\nprint(\"ğŸ“Š Ventes par mois et ville :\")\ndf_ventes.groupBy(\"mois\", \"ville\").agg(\n    F.sum(\"quantite\").alias(\"total_quantite\"),\n    F.sum(\"montant\").alias(\"total_montant\"),\n    F.count(\"*\").alias(\"nb_transactions\")\n).orderBy(\"mois\", \"ville\").show()\n\n\n\n\nCode\n# AgrÃ©gations conditionnelles\nprint(\"ğŸ“Š AgrÃ©gations conditionnelles :\")\ndf_ventes.groupBy(\"ville\").agg(\n    F.sum(\"montant\").alias(\"total\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit A\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_a\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit B\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_b\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.3 Window Functions",
    "text": "3.3 Window Functions\n\n\nCode\n# Ranking dans chaque ville\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"ville\").orderBy(F.col(\"salaire\").desc())\n\ndf_ranked = df.withColumn(\n    \"rank\",\n    F.row_number().over(window_spec)\n)\n\nprint(\"ğŸ† Ranking des salaires par ville :\")\ndf_ranked.orderBy(\"ville\", \"rank\").show()\n\n\n\n\nCode\n# Calculs cumulatifs\nwindow_cumul = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\ndf_cumul = df.withColumn(\n    \"salaire_cumul\",\n    F.sum(\"salaire\").over(window_cumul)\n)\n\nprint(\"ğŸ“ˆ Salaire cumulÃ© par ville :\")\ndf_cumul.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_cumul\").orderBy(\"ville\", \"id\").show()\n\n\n\n\nCode\n# Calcul de moyennes mobiles\nwindow_rolling = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(-1, 1)\n\ndf_rolling = df.withColumn(\n    \"salaire_avg_3\",\n    F.avg(\"salaire\").over(window_rolling)\n)\n\nprint(\"ğŸ“Š Moyenne mobile sur 3 lignes :\")\ndf_rolling.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_avg_3\").orderBy(\"ville\", \"id\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "4.1 CrÃ©er des DataFrames pour les exemples",
    "text": "4.1 CrÃ©er des DataFrames pour les exemples\n\n\nCode\n# DataFrame employÃ©s\nemployes = spark.createDataFrame([\n    (1, \"Alice\", \"IT\"),\n    (2, \"Bob\", \"Finance\"),\n    (3, \"Charlie\", \"IT\"),\n    (4, \"David\", \"HR\")\n], [\"emp_id\", \"nom\", \"dept_id\"])\n\n# DataFrame dÃ©partements\ndepartements = spark.createDataFrame([\n    (\"IT\", \"Information Technology\", \"Paris\"),\n    (\"Finance\", \"Finance Department\", \"Lyon\"),\n    (\"HR\", \"Human Resources\", \"Marseille\"),\n    (\"Marketing\", \"Marketing Department\", \"Paris\")\n], [\"dept_id\", \"dept_name\", \"location\"])\n\nprint(\"ğŸ‘¥ EmployÃ©s :\")\nemployes.show()\n\nprint(\"\\nğŸ¢ DÃ©partements :\")\ndepartements.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "4.2 Types de jointures",
    "text": "4.2 Types de jointures\n\n\nCode\n# INNER JOIN (par dÃ©faut)\nprint(\"ğŸ”— INNER JOIN :\")\nemployes.join(departements, \"dept_id\", \"inner\").show()\n\n# LEFT JOIN\nprint(\"\\nğŸ”— LEFT JOIN :\")\nemployes.join(departements, \"dept_id\", \"left\").show()\n\n# RIGHT JOIN\nprint(\"\\nğŸ”— RIGHT JOIN :\")\nemployes.join(departements, \"dept_id\", \"right\").show()\n\n# FULL OUTER JOIN\nprint(\"\\nğŸ”— FULL OUTER JOIN :\")\nemployes.join(departements, \"dept_id\", \"outer\").show()\n\n\n\n\nCode\n# Jointure avec colonnes diffÃ©rentes\nemployes_alt = employes.withColumnRenamed(\"dept_id\", \"department\")\n\nprint(\"ğŸ”— Jointure avec colonnes diffÃ©rentes :\")\nemployes_alt.join(\n    departements,\n    employes_alt.department == departements.dept_id,\n    \"inner\"\n).select(\n    employes_alt[\"*\"],\n    departements.dept_name,\n    departements.location\n).show()\n\n\n\n\nCode\n# Jointures multiples\nsalaires = spark.createDataFrame([\n    (1, 45000),\n    (2, 55000),\n    (3, 50000),\n    (4, 48000)\n], [\"emp_id\", \"salaire\"])\n\nprint(\"ğŸ”— Jointures multiples :\")\nresult = employes \\\n    .join(departements, \"dept_id\", \"inner\") \\\n    .join(salaires, \"emp_id\", \"inner\")\n\nresult.select(\"nom\", \"dept_name\", \"location\", \"salaire\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.1 CSV",
    "text": "5.1 CSV\n\n\nCode\n# CrÃ©er des donnÃ©es de test\nimport os\nos.makedirs('data', exist_ok=True)\n\n# Ã‰crire en CSV\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\"âœ… CSV Ã©crit\")\n\n# Lire le CSV\ndf_from_csv = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\"\\nğŸ“‚ CSV lu :\")\ndf_from_csv.show(3)\ndf_from_csv.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.2 JSON",
    "text": "5.2 JSON\n\n\nCode\n# Ã‰crire en JSON\ndf.write \\\n    .mode(\"overwrite\") \\\n    .json(\"data/employes.json\")\n\nprint(\"âœ… JSON Ã©crit\")\n\n# Lire le JSON\ndf_from_json = spark.read.json(\"data/employes.json\")\n\nprint(\"\\nğŸ“‚ JSON lu :\")\ndf_from_json.show(3)\ndf_from_json.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.3 Parquet (Format recommandÃ©)",
    "text": "5.3 Parquet (Format recommandÃ©)\n\n\nCode\n# Ã‰crire en Parquet\ndf.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"data/employes.parquet\")\n\nprint(\"âœ… Parquet Ã©crit\")\n\n# Lire le Parquet\ndf_from_parquet = spark.read.parquet(\"data/employes.parquet\")\n\nprint(\"\\nğŸ“‚ Parquet lu :\")\ndf_from_parquet.show(3)\ndf_from_parquet.printSchema()\n\n\n\n\nCode\n# Parquet avec partitionnement\ndf.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"ville\") \\\n    .parquet(\"data/employes_partitioned.parquet\")\n\nprint(\"âœ… Parquet partitionnÃ© Ã©crit\")\n\n# Lire avec filtre de partition (trÃ¨s performant)\ndf_paris = spark.read \\\n    .parquet(\"data/employes_partitioned.parquet\") \\\n    .filter(F.col(\"ville\") == \"Paris\")\n\nprint(\"\\nğŸ“‚ Parquet partitionnÃ© lu (ville=Paris) :\")\ndf_paris.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.4 Options dâ€™Ã©criture",
    "text": "5.4 Options dâ€™Ã©criture\n\n\nCode\n# Mode d'Ã©criture\n# - \"overwrite\" : Ã‰crase les donnÃ©es existantes\n# - \"append\" : Ajoute aux donnÃ©es existantes\n# - \"ignore\" : Ne fait rien si le fichier existe\n# - \"error\" (default) : Erreur si le fichier existe\n\n# Compression\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"compression\", \"snappy\") \\\n    .parquet(\"data/employes_compressed.parquet\")\n\nprint(\"âœ… Parquet compressÃ© Ã©crit\")\n\n# ContrÃ´ler le nombre de fichiers\ndf.coalesce(1).write \\\n    .mode(\"overwrite\") \\\n    .csv(\"data/employes_single_file.csv\")\n\nprint(\"âœ… CSV en un seul fichier Ã©crit\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "6.1 CrÃ©er des vues temporaires",
    "text": "6.1 CrÃ©er des vues temporaires\n\n\nCode\n# CrÃ©er une vue temporaire\ndf.createOrReplaceTempView(\"employes\")\ndf_ventes.createOrReplaceTempView(\"ventes\")\n\nprint(\"âœ… Vues temporaires crÃ©Ã©es\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "6.2 RequÃªtes SQL",
    "text": "6.2 RequÃªtes SQL\n\n\nCode\n# RequÃªte SQL simple\nresult = spark.sql(\"\"\"\n    SELECT nom, ville, salaire\n    FROM employes\n    WHERE salaire &gt; 50000\n    ORDER BY salaire DESC\n\"\"\")\n\nprint(\"ğŸ“Š EmployÃ©s avec salaire &gt; 50000 :\")\nresult.show()\n\n\n\n\nCode\n# AgrÃ©gation avec SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        ville,\n        COUNT(*) as nb_employes,\n        AVG(salaire) as salaire_moyen,\n        MIN(salaire) as salaire_min,\n        MAX(salaire) as salaire_max\n    FROM employes\n    GROUP BY ville\n    ORDER BY salaire_moyen DESC\n\"\"\")\n\nprint(\"ğŸ“Š Statistiques par ville :\")\nresult.show()\n\n\n\n\nCode\n# Window functions en SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        nom,\n        ville,\n        salaire,\n        ROW_NUMBER() OVER (PARTITION BY ville ORDER BY salaire DESC) as rank_ville,\n        DENSE_RANK() OVER (ORDER BY salaire DESC) as rank_global\n    FROM employes\n    ORDER BY ville, rank_ville\n\"\"\")\n\nprint(\"ğŸ† Ranking avec SQL :\")\nresult.show()\n\n\n\n\nCode\n# CTE (Common Table Expression)\nresult = spark.sql(\"\"\"\n    WITH stats_ville AS (\n        SELECT \n            ville,\n            AVG(salaire) as salaire_moyen\n        FROM employes\n        GROUP BY ville\n    )\n    SELECT \n        e.nom,\n        e.ville,\n        e.salaire,\n        s.salaire_moyen,\n        ROUND(e.salaire - s.salaire_moyen, 2) as diff_moyenne\n    FROM employes e\n    JOIN stats_ville s ON e.ville = s.ville\n    ORDER BY e.ville, e.salaire DESC\n\"\"\")\n\nprint(\"ğŸ“Š Comparaison Ã  la moyenne :\")\nresult.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.1 Partitionnement",
    "text": "9.1 Partitionnement\n\n\nCode\n# VÃ©rifier le nombre de partitions\nprint(f\"Nombre de partitions : {df.rdd.getNumPartitions()}\")\n\n# Repartitionner (shuffle)\ndf_repartitioned = df.repartition(4)\nprint(f\"AprÃ¨s repartition : {df_repartitioned.rdd.getNumPartitions()}\")\n\n# Coalesce (pas de shuffle, moins coÃ»teux)\ndf_coalesced = df.coalesce(2)\nprint(f\"AprÃ¨s coalesce : {df_coalesced.rdd.getNumPartitions()}\")\n\n\n\n\nCode\n# Repartitionner par colonne (utile avant les groupBy)\ndf_repartitioned_by_ville = df.repartition(\"ville\")\n\n# Maintenant les groupBy sur 'ville' seront plus efficaces\ndf_repartitioned_by_ville.groupBy(\"ville\").count().show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.2 Caching",
    "text": "9.2 Caching\n\n\nCode\n# Cache un DataFrame en mÃ©moire\ndf_cached = df.cache()\n\n# PremiÃ¨re action : calcul complet\nprint(\"PremiÃ¨re action (calcul complet) :\")\ndf_cached.count()\n\n# DeuxiÃ¨me action : utilise le cache (beaucoup plus rapide)\nprint(\"\\nDeuxiÃ¨me action (utilise le cache) :\")\ndf_cached.show()\n\n# LibÃ©rer le cache\ndf_cached.unpersist()\nprint(\"\\nâœ… Cache libÃ©rÃ©\")\n\n\n\n\nCode\n# Persist avec diffÃ©rents niveaux de stockage\nfrom pyspark import StorageLevel\n\n# MEMORY_ONLY : En mÃ©moire uniquement\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n# MEMORY_AND_DISK : MÃ©moire + disque si nÃ©cessaire\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n\n# DISK_ONLY : Disque uniquement\ndf.persist(StorageLevel.DISK_ONLY)\n\nprint(\"âœ… DiffÃ©rents niveaux de persistance disponibles\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.3 Broadcast Joins",
    "text": "9.3 Broadcast Joins\n\n\nCode\n# Pour les petits DataFrames (&lt; 10MB), utilisez broadcast\nfrom pyspark.sql.functions import broadcast\n\n# departements est petit, on le broadcast\nresult = employes.join(\n    broadcast(departements),\n    \"dept_id\",\n    \"inner\"\n)\n\nprint(\"ğŸš€ Broadcast join :\")\nresult.show()\n\n# Ã‰vite le shuffle, beaucoup plus rapide !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.4 Ã‰viter les UDFs quand possible",
    "text": "9.4 Ã‰viter les UDFs quand possible\n\n\nCode\n# âŒ Avec UDF (lent)\ndef add_ten(x):\n    return x + 10\n\nadd_ten_udf = udf(add_ten, IntegerType())\ndf.withColumn(\"salaire_plus_10_udf\", add_ten_udf(F.col(\"salaire\")))\n\n# âœ… Avec fonction native (rapide)\ndf.withColumn(\"salaire_plus_10\", F.col(\"salaire\") + 10)\n\nprint(\"âœ… Les fonctions natives sont toujours plus rapides !\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.5 Explain Plans",
    "text": "9.5 Explain Plans\n\n\nCode\n# Voir le plan d'exÃ©cution\nprint(\"ğŸ“Š Plan d'exÃ©cution :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain()\n\n# Plan dÃ©taillÃ©\nprint(\"\\nğŸ“Š Plan d'exÃ©cution dÃ©taillÃ© :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain(extended=True)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.1 Extract",
    "text": "11.1 Extract\n\n\nCode\ndef extract_data(spark, path):\n    \"\"\"Extrait des donnÃ©es depuis plusieurs sources\"\"\"\n    print(f\"ğŸ“¥ Extraction depuis {path}\")\n    \n    # CrÃ©er des donnÃ©es de test\n    data = [\n        (1, \"2024-01-15\", \"Paris\", \"Produit A\", 100, 1500, \"online\"),\n        (2, \"2024-01-15\", \"Lyon\", \"Produit B\", 50, 2000, \"store\"),\n        (3, \"2024-01-16\", \"Paris\", \"Produit A\", 75, 1200, \"online\"),\n        (4, \"2024-01-16\", \"Marseille\", \"Produit C\", 120, 1800, \"online\"),\n        (5, \"2024-01-17\", \"Lyon\", \"Produit B\", 60, 2400, \"store\"),\n        (6, \"2024-01-17\", None, \"Produit A\", 90, None, \"online\"),  # DonnÃ©es sales\n    ]\n    \n    columns = [\"id\", \"date\", \"ville\", \"produit\", \"quantite\", \"montant\", \"canal\"]\n    df = spark.createDataFrame(data, columns)\n    \n    # Sauvegarder les donnÃ©es brutes\n    df.write.mode(\"overwrite\").parquet(f\"{path}/ventes_raw.parquet\")\n    \n    print(f\"âœ… {df.count()} lignes extraites\")\n    return df\n\n# Test\ndf_raw = extract_data(spark, \"spark_pipeline/raw\")\ndf_raw.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.2 Transform",
    "text": "11.2 Transform\n\n\nCode\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    print(\"ğŸ”„ Transformation des donnÃ©es\")\n    \n    # 1. Convertir la date\n    df = df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n    \n    # 2. GÃ©rer les valeurs manquantes\n    df = df.fillna({\n        \"ville\": \"Inconnu\",\n        \"montant\": 0\n    })\n    \n    # 3. Filtrer les donnÃ©es invalides\n    df = df.filter(\n        (F.col(\"quantite\") &gt; 0) & \n        (F.col(\"montant\") &gt;= 0)\n    )\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    df = df.withColumn(\n        \"prix_unitaire\",\n        F.when(F.col(\"quantite\") &gt; 0, F.col(\"montant\") / F.col(\"quantite\")).otherwise(0)\n    )\n    \n    df = df.withColumn(\n        \"annee\",\n        F.year(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"mois\",\n        F.month(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"jour_semaine\",\n        F.dayofweek(F.col(\"date\"))\n    )\n    \n    # 5. CatÃ©goriser\n    df = df.withColumn(\n        \"categorie_montant\",\n        F.when(F.col(\"montant\") &lt; 1500, \"Faible\")\n         .when(F.col(\"montant\") &lt; 2000, \"Moyen\")\n         .otherwise(\"Ã‰levÃ©\")\n    )\n    \n    # 6. Ajouter metadata\n    df = df.withColumn(\"processed_at\", F.current_timestamp())\n    \n    print(f\"âœ… {df.count()} lignes transformÃ©es\")\n    return df\n\n# Test\ndf_transformed = transform_data(df_raw)\ndf_transformed.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.3 Aggregate",
    "text": "11.3 Aggregate\n\n\nCode\ndef aggregate_data(df):\n    \"\"\"CrÃ©e des agrÃ©gations mÃ©tier\"\"\"\n    print(\"ğŸ“Š AgrÃ©gation des donnÃ©es\")\n    \n    # AgrÃ©gation par ville et produit\n    agg_ville_produit = df.groupBy(\"ville\", \"produit\").agg(\n        F.sum(\"quantite\").alias(\"total_quantite\"),\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.avg(\"prix_unitaire\").alias(\"prix_moyen\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"ville\", \"produit\")\n    \n    print(\"\\nğŸ“Š AgrÃ©gation par ville et produit :\")\n    agg_ville_produit.show()\n    \n    # AgrÃ©gation par canal\n    agg_canal = df.groupBy(\"canal\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\"),\n        F.avg(\"montant\").alias(\"montant_moyen\")\n    )\n    \n    print(\"\\nğŸ“Š AgrÃ©gation par canal :\")\n    agg_canal.show()\n    \n    # AgrÃ©gation temporelle\n    agg_temporelle = df.groupBy(\"annee\", \"mois\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"annee\", \"mois\")\n    \n    print(\"\\nğŸ“Š AgrÃ©gation temporelle :\")\n    agg_temporelle.show()\n    \n    return {\n        \"ville_produit\": agg_ville_produit,\n        \"canal\": agg_canal,\n        \"temporelle\": agg_temporelle\n    }\n\n# Test\naggregations = aggregate_data(df_transformed)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.4 Load",
    "text": "11.4 Load\n\n\nCode\ndef load_data(df, aggregations, output_path):\n    \"\"\"Charge les donnÃ©es dans le datalake\"\"\"\n    print(\"ğŸ’¾ Chargement des donnÃ©es\")\n    \n    # 1. DonnÃ©es transformÃ©es (partitionnÃ©es par date)\n    df.write \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"annee\", \"mois\") \\\n        .parquet(f\"{output_path}/ventes_transformed\")\n    print(\"âœ… DonnÃ©es transformÃ©es sauvegardÃ©es\")\n    \n    # 2. AgrÃ©gations\n    for name, agg_df in aggregations.items():\n        agg_df.write \\\n            .mode(\"overwrite\") \\\n            .parquet(f\"{output_path}/agg_{name}\")\n        print(f\"âœ… AgrÃ©gation '{name}' sauvegardÃ©e\")\n    \n    # 3. Export CSV pour l'analyse\n    df.coalesce(1).write \\\n        .mode(\"overwrite\") \\\n        .option(\"header\", \"true\") \\\n        .csv(f\"{output_path}/ventes_export.csv\")\n    print(\"âœ… Export CSV crÃ©Ã©\")\n\n# Test\nload_data(df_transformed, aggregations, \"spark_pipeline/output\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.5 Pipeline complet orchestrÃ©",
    "text": "11.5 Pipeline complet orchestrÃ©\n\n\nCode\ndef run_pipeline(spark):\n    \"\"\"ExÃ©cute le pipeline ETL complet\"\"\"\n    import time\n    \n    start_time = time.time()\n    print(\"=\"*60)\n    print(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE PYSPARK\")\n    print(\"=\"*60)\n    \n    try:\n        # EXTRACT\n        print(\"\\nğŸ“¥ PHASE 1: EXTRACTION\")\n        df_raw = extract_data(spark, \"spark_pipeline/raw\")\n        \n        # TRANSFORM\n        print(\"\\nğŸ”„ PHASE 2: TRANSFORMATION\")\n        df_transformed = transform_data(df_raw)\n        \n        # Cache pour les performances\n        df_transformed.cache()\n        \n        # AGGREGATE\n        print(\"\\nğŸ“Š PHASE 3: AGRÃ‰GATION\")\n        aggregations = aggregate_data(df_transformed)\n        \n        # LOAD\n        print(\"\\nğŸ’¾ PHASE 4: CHARGEMENT\")\n        load_data(df_transformed, aggregations, \"spark_pipeline/output\")\n        \n        # STATISTICS\n        duration = time.time() - start_time\n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸ“Š STATISTIQUES DU PIPELINE\")\n        print(\"=\"*60)\n        print(f\"DurÃ©e totale: {duration:.2f}s\")\n        print(f\"Lignes traitÃ©es: {df_transformed.count()}\")\n        print(f\"Partitions: {df_transformed.rdd.getNumPartitions()}\")\n        print(\"=\"*60)\n        print(\"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        print(\"=\"*60)\n        \n        # LibÃ©rer le cache\n        df_transformed.unpersist()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"\\nâŒ ERREUR: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline(spark)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Ce que vous avez appris âœ…",
    "text": "Ce que vous avez appris âœ…\n\nFondamentaux Spark : Architecture, concepts, SparkSession\nDataFrames : CrÃ©ation, transformations, actions\nTransformations : Select, filter, withColumn, orderBy\nAgrÃ©gations : GroupBy, agrÃ©gations complexes, window functions\nJointures : Inner, left, right, outer, broadcast\nI/O : CSV, JSON, Parquet avec partitionnement\nSpark SQL : RequÃªtes SQL, CTEs, window functions\nOptimisation : Partitionnement, caching, broadcast joins\nUDFs : Fonctions personnalisÃ©es\nStreaming : Traitement temps rÃ©el (introduction)\nPipeline ETL : Architecture complÃ¨te production-ready",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "DiffÃ©rences clÃ©s Pandas vs PySpark ğŸ”„",
    "text": "DiffÃ©rences clÃ©s Pandas vs PySpark ğŸ”„\n\n\n\n\n\n\n\n\nAspect\nPandas\nPySpark\n\n\n\n\nExÃ©cution\nEager (immÃ©diate)\nLazy (diffÃ©rÃ©e)\n\n\nDonnÃ©es\nEn mÃ©moire (single machine)\nDistribuÃ©es (cluster)\n\n\nScalabilitÃ©\nLimitÃ© Ã  la RAM\nQuasi illimitÃ©\n\n\nAPI\ndf[df['col'] &gt; 5]\ndf.filter(F.col('col') &gt; 5)\n\n\nPerformances\nRapide pour petites donnÃ©es\nRapide pour Big Data",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Quand utiliser PySpark ? ğŸ¤”",
    "text": "Quand utiliser PySpark ? ğŸ¤”\nâœ… Utilisez PySpark si : - DonnÃ©es &gt; 10 GB - Besoin de parallÃ©lisation - Traitement distribuÃ© nÃ©cessaire - Streaming en temps rÃ©el\nâŒ Utilisez Pandas si : - DonnÃ©es &lt; 10 GB - Prototypage rapide - Analyses exploratoires - Machine learning local",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Prochaines Ã©tapes ğŸš€",
    "text": "Prochaines Ã©tapes ğŸš€\n\nPratiquer : CrÃ©er des pipelines avec vos propres donnÃ©es\nApprofondir :\n\nMLlib (Machine Learning)\nGraphX (Graph processing)\nDelta Lake (ACID transactions)\n\nProduction :\n\nDatabricks\nAWS EMR\nAzure Synapse\nGoogle Dataproc\n\nOrchestration :\n\nApache Airflow\nPrefect\nDagster",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Ressources ğŸ“š",
    "text": "Ressources ğŸ“š\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy\n\n\nFÃ©licitations ! ğŸ‰ Vous maÃ®trisez maintenant les fondamentaux de PySpark !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#votre-score",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#votre-score",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert PySpark ! PrÃªt pour la production\n8-9/10 : ğŸŒŸ Excellent ! Pratiquez les concepts avancÃ©s\n6-7/10 : ğŸ’ª Bon niveau ! Revoyez les optimisations\n&lt; 6/10 : ğŸ“š Relisez le notebook et pratiquez les exemples",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy â€” Cours gratuits\n\n\nğŸ­ Plateformes Cloud\n\n\n\nPlateforme\nService Spark\n\n\n\n\nDatabricks\nDatabricks Lakehouse\n\n\nAWS\nEMR (Elastic MapReduce)\n\n\nAzure\nSynapse Analytics, HDInsight\n\n\nGCP\nDataproc",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant le traitement Big Data avec PySpark !\nPour continuer ton parcours Data Engineering,\nğŸ‘‰ Module suivant : 12_orchestration_pipelines.ipynb â€” Orchestration de pipelines\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module PySpark et le parcours sur les bases de donnÃ©es et le Big Data !\n\n\nCode\n# Fermer la SparkSession\nspark.stop()\nprint(\"âœ… SparkSession fermÃ©e\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "",
    "text": "Ce notebook donne les bases de Python nÃ©cessaires pour la suite du parcours Data Engineering From Zero to Hero.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Version Python recommandÃ©e",
    "text": "âš ï¸ Version Python recommandÃ©e\n\n\n\n\n\n\n\n\nVersion\nStatut\nRecommandation\n\n\n\n\nPython 3.12\nâœ… DerniÃ¨re stable\nRecommandÃ©e pour nouveaux projets\n\n\nPython 3.11\nâœ… Stable\nExcellent choix, trÃ¨s performant\n\n\nPython 3.10\nâœ… SupportÃ©e\nMinimum pour les type hints modernes\n\n\nPython 3.9\nâš ï¸ Maintenance\nÃ‰viter pour nouveaux projets\n\n\nPython 3.8 et avant\nâŒ ObsolÃ¨te\nNe pas utiliser\n\n\n\n\nğŸ’¡ Pour ce cours, utilise Python 3.11 ou 3.12.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 03_git_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal (Bash)\n\n\nğŸŸ¡ Optionnel\nNotions de programmation dans un autre langage",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Installer et configurer ton environnement Python\nâœ… CrÃ©er et gÃ©rer des environnements virtuels\nâœ… Manipuler les types de base (nombres, chaÃ®nes, listes, dictionnaires)\nâœ… Utiliser les conditions et boucles\nâœ… Ã‰crire des fonctions et des classes simples\nâœ… GÃ©rer les erreurs avec try / except\nâœ… Lire et Ã©crire des fichiers (texte, JSON)\nâœ… Utiliser le module logging pour tracer un script",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ Pourquoi Python pour le Data Engineering ?",
    "text": "ğŸ Pourquoi Python pour le Data Engineering ?\n\n\n\n\n\n\n\nRaison\nDÃ©tail\n\n\n\n\nğŸ“š Ã‰cosystÃ¨me riche\nPandas, PySpark, Airflow, dbt, FastAPIâ€¦\n\n\nğŸ”§ Polyvalent\nScripts, APIs, pipelines, ML, automation\n\n\nğŸ¤ Standard de lâ€™industrie\nUtilisÃ© par Netflix, Spotify, Airbnbâ€¦\n\n\nğŸ“– Facile Ã  apprendre\nSyntaxe claire et lisible\n\n\nğŸ”— IntÃ©grations\nConnecteurs pour toutes les bases de donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0. Installation & Environnement ğŸ› ï¸",
    "text": "0. Installation & Environnement ğŸ› ï¸\nCette section explique comment installer Python, VS Code, Jupyter et vÃ©rifier que tout fonctionne. Les commandes sont donnÃ©es pour Windows, Linux et macOS.\n\n0.1 Installer Python\n\nSous Windows\n\nAller sur le site officiel : https://www.python.org/downloads/\nTÃ©lÃ©charger la derniÃ¨re version stable de Python 3.x.\nLors de lâ€™installation :\n\nCocher â€œAdd Python to PATHâ€ en bas de la premiÃ¨re fenÃªtre ;\npuis cliquer sur Install Now.\n\n\n\n\nSous Linux (Ubuntu / Debian)\nsudo apt update\nsudo apt install -y python3 python3-pip\n\n\nSous macOS\n\nOption 1 : paquet officiel :\n\nTÃ©lÃ©charger un .pkg depuis https://www.python.org/downloads/mac-osx/\nInstaller comme une application classique.\n\nOption 2 (si Homebrew est installÃ©) :\n\nbrew install python\n\n\n\n0.2 VÃ©rifier lâ€™installation de Python\nOuvrir un terminal (ou PowerShell sous Windows) puis taper :\npython --version   # ou parfois: python3 --version\nTu dois voir une version du type : Python 3.12.x.\nâš ï¸ Erreurs frÃ©quentes : - python nâ€™est pas reconnu â†’ Python nâ€™est pas dans le PATH ; - sur Linux/macOS, il faut parfois utiliser python3 au lieu de python.\n\n\n0.3 Installer Visual Studio Code (VS Code)\n\nTÃ©lÃ©charger VS Code : https://code.visualstudio.com/\nInstaller la version adaptÃ©e Ã  ton systÃ¨me (Windows, Linux, macOS).\nLancer VS Code.\n\nVS Code servira Ã  : - Ã©diter des scripts .py ; - ouvrir des notebooks Jupyter (.ipynb) ; - organiser un projet de data engineering complet.\n\n\n0.4 Extensions VS Code : Python & Jupyter\nDans VS Code, aller dans lâ€™onglet Extensions (icÃ´ne de blocs Ã  gauche), puis :\n\nRechercher â€œPythonâ€ (Ã©diteur : Microsoft) et lâ€™installer ;\nRechercher â€œJupyterâ€ (Ã©diteur : Microsoft) et lâ€™installer.\n\nEnsuite, ouvrir un fichier .py ou .ipynb : VS Code proposera de sÃ©lectionner un interprÃ©teur Python (en bas Ã  droite). Choisir ton installation Python 3.x.\n\n\n0.5 Jupyter Notebook â€” Installation et utilisation\nJupyter Notebook est un environnement interactif qui permet dâ€™Ã©crire du code, de lâ€™exÃ©cuter, et de voir les rÃ©sultats immÃ©diatement. Câ€™est lâ€™outil idÃ©al pour apprendre, explorer des donnÃ©es et documenter son travail.\n\nğŸ’¡ Ce cours est lui-mÃªme un Notebook Jupyter (fichier .ipynb) !\n\n\n\nğŸ“¦ Installation\n# Installer Jupyter\npip install notebook\n\n# Ou avec Anaconda (dÃ©jÃ  inclus)\n# Rien Ã  faire, c'est installÃ© par dÃ©faut\n\n\n\nğŸš€ Lancer Jupyter Notebook\n# Dans ton terminal, place-toi dans ton dossier de travail\ncd /chemin/vers/mon/projet\n\n# Lancer Jupyter\njupyter notebook\nCe qui se passe :\n\nUn serveur local dÃ©marre\nTon navigateur sâ€™ouvre automatiquement sur http://localhost:8888\nTu vois la liste des fichiers de ton dossier\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Jupyter                                    [Quit] [Logout]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Files    Running    Clusters                                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  ğŸ“ data/                                                       â”‚\nâ”‚  ğŸ“„ 01_intro.ipynb                                              â”‚\nâ”‚  ğŸ“„ 02_basics.ipynb                                             â”‚\nâ”‚  ğŸ“„ script.py                                                   â”‚\nâ”‚                                                                 â”‚\nâ”‚  [New â–¼]  â† Cliquer ici pour crÃ©er un nouveau notebook          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“ CrÃ©er un nouveau Notebook\n\nCliquer sur New (en haut Ã  droite)\nSÃ©lectionner Python 3 (ou Python 3 (ipykernel))\nUn nouveau notebook sâ€™ouvre : Untitled.ipynb\nCliquer sur â€œUntitledâ€ pour le renommer (ex: mon_premier_notebook.ipynb)\n\n\n\n\nğŸ–¥ï¸ Lâ€™interface Jupyter\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  mon_notebook.ipynb                              [Trusted]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  File  Edit  View  Insert  Cell  Kernel  Help                   â”‚\nâ”‚  [ğŸ’¾] [+] [âœ‚ï¸] [ğŸ“‹] [â–¶ï¸ Run] [â¹ï¸] [ğŸ”„]    | Code â–¼ |             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  In [1]: â–ˆ                              â† Cellule de code       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nÃ‰lÃ©ment\nDescription\n\n\n\n\nCellule\nBloc oÃ¹ tu Ã©cris du code ou du texte\n\n\nIn [1]:\nNumÃ©ro dâ€™exÃ©cution de la cellule\n\n\nâ–¶ï¸ Run\nExÃ©cute la cellule sÃ©lectionnÃ©e\n\n\nCode â–¼\nType de cellule (Code ou Markdown)\n\n\n\n\n\n\nâŒ¨ï¸ Raccourcis clavier essentiels\n\n\n\nRaccourci\nAction\n\n\n\n\nShift + Enter\nâ–¶ï¸ ExÃ©cuter la cellule et passer Ã  la suivante\n\n\nCtrl + Enter\nExÃ©cuter la cellule (rester dessus)\n\n\nEsc\nPasser en mode commande (cellule bleue)\n\n\nEnter\nPasser en mode Ã©dition (cellule verte)\n\n\nA (mode commande)\nInsÃ©rer une cellule au-dessus\n\n\nB (mode commande)\nInsÃ©rer une cellule en-dessous\n\n\nDD (mode commande)\nSupprimer la cellule\n\n\nM (mode commande)\nConvertir en cellule Markdown\n\n\nY (mode commande)\nConvertir en cellule Code\n\n\nCtrl + S\nSauvegarder le notebook\n\n\n\n\nğŸ’¡ Astuce : Apprends Shift + Enter en premier â€” câ€™est le raccourci que tu utiliseras le plus !\n\n\n\n\nğŸ§ª Premier test dans Jupyter\n\nDans une cellule, tape :\n\nprint(\"Hello Data Engineer !\")\n\nAppuie sur Shift + Enter\nTu dois voir :\n\nIn [1]: print(\"Hello Data Engineer !\")\n\nHello Data Engineer !\n\n\n\nğŸ“„ Types de cellules\n\n\n\nType\nUsage\nExemple\n\n\n\n\nCode\nExÃ©cuter du Python\nprint(\"Hello\")\n\n\nMarkdown\nDocumenter, titres, explications\n# Mon titre\n\n\n\n# Titre principal\n## Sous-titre\n\nDu texte en **gras** et en *italique*.\n\n- Liste Ã  puces\n- Autre Ã©lÃ©ment\n\n\n\nğŸ›‘ ArrÃªter Jupyter\n\nSauvegarder ton notebook (Ctrl + S)\nFermer lâ€™onglet du navigateur\nDans le terminal oÃ¹ Jupyter tourne : appuyer sur Ctrl + C deux fois\n\n^C\nShutdown this notebook server (y/[n])? y\n\n\n\nğŸ’¡ Alternative : Jupyter dans VS Code\nTu peux aussi ouvrir des notebooks directement dans VS Code (avec lâ€™extension Jupyter installÃ©e) :\n\nOuvrir VS Code\nFile &gt; Open File â†’ sÃ©lectionner un fichier .ipynb\nOu crÃ©er un nouveau fichier avec lâ€™extension .ipynb\n\nCâ€™est souvent plus pratique car tu as tout dans le mÃªme Ã©diteur !\n\n\n\n0.6 Premier test Python\nPython peut sâ€™utiliser de 3 faÃ§ons diffÃ©rentes :\n\n\n\nMode\nUsage\nCommande\n\n\n\n\nInteractif\nTester rapidement du code\npython ou python3\n\n\nScript\nExÃ©cuter un fichier .py\npython mon_script.py\n\n\nNotebook\nExploration, visualisation\nJupyter / VS Code\n\n\n\n\n\nğŸ–¥ï¸ Mode interactif (REPL)\nREPL = Read-Eval-Print Loop (Lire-Ã‰valuer-Afficher en boucle)\n1. Lancer lâ€™interprÃ©teur Python :\n# Dans ton terminal (PowerShell, CMD, Bash...)\npython\n# ou sur Linux/macOS\npython3\n2. Tu verras apparaÃ®tre le prompt &gt;&gt;&gt; :\nPython 3.12.0 (main, Oct  2 2024, 12:00:00)\n[GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n3. Taper du code Python directement :\n&gt;&gt;&gt; print(\"Hello Data Engineer\")\nHello Data Engineer\n\n&gt;&gt;&gt; 2 + 3\n5\n\n&gt;&gt;&gt; nom = \"Alice\"\n&gt;&gt;&gt; print(f\"Bonjour {nom}\")\nBonjour Alice\n4. Quitter lâ€™interprÃ©teur :\n&gt;&gt;&gt; exit()\nOu utiliser le raccourci clavier : - Windows : Ctrl + Z puis EntrÃ©e - Linux/macOS : Ctrl + D\n\n\n\nğŸ’¡ Quand utiliser le mode interactif ?\n\n\n\nâœ… Bon usage\nâŒ Mauvais usage\n\n\n\n\nTester une syntaxe rapidement\nÃ‰crire un programme complet\n\n\nVÃ©rifier le rÃ©sultat dâ€™une expression\nCode quâ€™on veut sauvegarder\n\n\nExplorer une librairie (help(fonction))\nPipeline de production\n\n\nCalculatrice avancÃ©e\nTravail collaboratif\n\n\n\n\nğŸ’¡ Pour ce cours, tu utiliseras surtout les Notebooks Jupyter (exploration) et les scripts .py (production). Le mode interactif est utile pour des tests rapides.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0.7 Environnements virtuels ğŸ”’",
    "text": "0.7 Environnements virtuels ğŸ”’\nUn environnement virtuel isole les dÃ©pendances de chaque projet. Câ€™est indispensable en Data Engineering pour Ã©viter les conflits de versions.\n\nPourquoi utiliser un environnement virtuel ?\n\n\n\n\n\n\n\nâŒ Sans environnement virtuel\nâœ… Avec environnement virtuel\n\n\n\n\nTous les projets partagent les mÃªmes packages\nChaque projet a ses propres packages\n\n\nConflits de versions\nIsolation complÃ¨te\n\n\nDifficile Ã  reproduire\nReproductible avec requirements.txt\n\n\n\n\n\nOption 1 : venv (intÃ©grÃ© Ã  Python)\n# CrÃ©er un environnement virtuel\npython -m venv mon_env\n\n# Activer l'environnement\n# Windows\nmon_env\\Scripts\\activate\n\n# Linux / macOS\nsource mon_env/bin/activate\n\n# Tu verras (mon_env) au dÃ©but de ta ligne de commande\n\n# DÃ©sactiver l'environnement\ndeactivate\n\n\nOption 2 : conda (Anaconda/Miniconda)\n# CrÃ©er un environnement\nconda create -n mon_projet python=3.11\n\n# Activer\nconda activate mon_projet\n\n# DÃ©sactiver\nconda deactivate\n\n# Lister les environnements\nconda env list\n\n\nğŸ’¡ Recommandation pour Data Engineers\n\n\n\nOutil\nQuand lâ€™utiliser\n\n\n\n\nvenv\nProjets Python purs, lÃ©gers, CI/CD\n\n\nconda\nData Science, dÃ©pendances complexes (NumPy, Spark)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0.8 Gestion des packages avec pip ğŸ“¦",
    "text": "0.8 Gestion des packages avec pip ğŸ“¦\npip est le gestionnaire de packages Python. Tu lâ€™utiliseras pour installer les librairies Data Engineering.\n\nCommandes essentielles\n# Installer un package\npip install pandas\n\n# Installer une version spÃ©cifique\npip install pandas==2.0.0\n\n# Installer plusieurs packages\npip install pandas numpy requests\n\n# Mettre Ã  jour un package\npip install --upgrade pandas\n\n# DÃ©sinstaller\npip uninstall pandas\n\n# Lister les packages installÃ©s\npip list\n\n# Voir les infos d'un package\npip show pandas\n\n\nLe fichier requirements.txt\nCe fichier liste toutes les dÃ©pendances dâ€™un projet. Indispensable pour la reproductibilitÃ©.\n# GÃ©nÃ©rer le fichier Ã  partir de l'environnement actuel\npip freeze &gt; requirements.txt\n\n# Installer toutes les dÃ©pendances d'un projet\npip install -r requirements.txt\n\n\nExemple de requirements.txt pour Data Engineering\n# Data Processing\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\npyarrow&gt;=12.0.0\n\n# APIs\nrequests&gt;=2.28.0\nfastapi&gt;=0.100.0\n\n# Database\nsqlalchemy&gt;=2.0.0\npsycopg2-binary&gt;=2.9.0\n\n# Testing\npytest&gt;=7.0.0\n\nğŸ’¡ Bonne pratique : Toujours travailler dans un environnement virtuel avant dâ€™installer des packages !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "1. Variables et Types ğŸ”¤",
    "text": "1. Variables et Types ğŸ”¤\nUne variable est un nom qui rÃ©fÃ©rence une valeur en mÃ©moire.\nPython possÃ¨de plusieurs types intÃ©grÃ©s (builtins). Voici ceux Ã  maÃ®triser absolument en Data Engineering :\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nType\nExemple\nUsage Data Engineering\n\n\n\n\nNumÃ©rique\nint\n3, 42, -5\nComptage, index, tailles, IDs\n\n\n\nfloat\n3.14, 0.99\nPrix, mesures, statistiques\n\n\nTexte\nstr\n\"Abidjan\"\nParsing CSV/JSON, nettoyage, logs\n\n\nBoolÃ©en\nbool\nTrue, False\nConditions, filtres, validation\n\n\nSÃ©quences ordonnÃ©es\nlist\n[1,2,3]\nLignes CSV, sÃ©ries numÃ©riques\n\n\n\ntuple\n(200, \"OK\")\nValeurs fixes, clÃ©s composites\n\n\nMapping\ndict\n{\"id\":1,\"ville\":\"Paris\"}\nJSON, API, MongoDB\n\n\nEnsemble (unique)\nset\n{\"python\",\"data\"}\nDÃ©duplication (emails, tags)\n\n\nBinaire\nbytes\nb\"abc\"\nFichiers binaires, images, rÃ©seau\n\n\n\n\n\nExemples de variables\nage = 30              # int\npi = 3.14             # float\nnom = \"Alice\"         # str\nest_data_engineer = True  # bool\n\nprint(age, type(age))\nprint(pi, type(pi))\nprint(nom, type(nom))\nprint(est_data_engineer, type(est_data_engineer))\n\n\nConversion de types\nIl est frÃ©quent de convertir des chaÃ®nes en nombres, par exemple aprÃ¨s lecture dâ€™un fichier.\nage_str = \"25\"\nage_int = int(age_str)\nâš ï¸ Erreurs frÃ©quentes : - Essayer de convertir une chaÃ®ne non numÃ©rique : int(\"abc\") â†’ ValueError ; - Additionner directement un str et un int : - \"25\" + 3 âŒ - int(\"25\") + 3 âœ”ï¸",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” 2. Conditions et Boucles",
    "text": "ğŸ” 2. Conditions et Boucles\nLes conditions permettent de prendre des dÃ©cisions dans le code.\nLes boucles permettent de rÃ©pÃ©ter des actions automatiquement.\n\nğŸ’¡ En Data Engineering, ces structures sont essentielles pour : - Filtrer des donnÃ©es (conditions) - Traiter plusieurs fichiers (boucles) - Valider des rÃ¨gles mÃ©tier (conditions) - Parcourir des bases de donnÃ©es (boucles)\n\n\n\nğŸ¯ 2.1 Conditions (if / elif / else)\nLes conditions testent des critÃ¨res et exÃ©cutent du code selon le rÃ©sultat.\n\nSyntaxe de base\nage = 20\n\nif age &lt; 18:\n    print(\"Mineur\")\nelif age == 18:\n    print(\"Tout juste majeur\")\nelse:\n    print(\"Majeur\")\nFonctionnement :\n\nif : PremiÃ¨re condition testÃ©e\nelif : Condition alternative (si if est fausse)\nelse : Cas par dÃ©faut (si toutes les conditions sont fausses)\n\n\n\nOpÃ©rateurs de comparaison\n\n\n\nOpÃ©rateur\nSignification\nExemple\n\n\n\n\n==\nÃ‰gal Ã \nage == 18\n\n\n!=\nDiffÃ©rent de\nage != 18\n\n\n&lt;\nInfÃ©rieur Ã \nage &lt; 18\n\n\n&lt;=\nInfÃ©rieur ou Ã©gal\nage &lt;= 18\n\n\n&gt;\nSupÃ©rieur Ã \nage &gt; 18\n\n\n&gt;=\nSupÃ©rieur ou Ã©gal\nage &gt;= 18\n\n\n\n\n\nConditions multiples\nage = 25\npays = \"France\"\n\n# OpÃ©rateur AND (et)\nif age &gt;= 18 and pays == \"France\":\n    print(\"Peut voter en France\")\n\n# OpÃ©rateur OR (ou)\nif age &lt; 18 or age &gt; 65:\n    print(\"Tarif rÃ©duit\")\n\n# OpÃ©rateur NOT (nÃ©gation)\nif not (age &lt; 18):\n    print(\"Majeur\")\n\n\n\n\nğŸ”„ 2.2 Boucles (for et while)\nLes boucles permettent dâ€™exÃ©cuter du code de maniÃ¨re rÃ©pÃ©tÃ©e.\n\nBoucle for : Parcourir une sÃ©quence\n# Parcourir une liste\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\n\nfor nom in noms:\n    print(f\"Bonjour {nom}\")\n\n# RÃ©sultat :\n# Bonjour Alice\n# Bonjour Bob\n# Bonjour Charlie\n\n\nBoucle avec range()\n# range(n) gÃ©nÃ¨re les nombres de 0 Ã  n-1\nfor i in range(5):\n    print(f\"ItÃ©ration {i}\")\n\n# RÃ©sultat : 0, 1, 2, 3, 4\n\n# range(dÃ©but, fin, pas)\nfor i in range(0, 10, 2):\n    print(i)  # 0, 2, 4, 6, 8\n\n\nBoucle while : RÃ©pÃ©ter tant quâ€™une condition est vraie\ncompteur = 0\n\nwhile compteur &lt; 3:\n    print(f\"Compteur = {compteur}\")\n    compteur += 1  # âš ï¸ IMPORTANT : incrÃ©mentation obligatoire\n\n# RÃ©sultat :\n# Compteur = 0\n# Compteur = 1\n# Compteur = 2\n\n\nContrÃ´le de flux : break et continue\n# break : Sortir immÃ©diatement de la boucle\nfor i in range(10):\n    if i == 5:\n        break  # ArrÃªte la boucle Ã  5\n    print(i)  # Affiche 0, 1, 2, 3, 4\n\n# continue : Passer Ã  l'itÃ©ration suivante\nfor i in range(5):\n    if i == 2:\n        continue  # Saute l'itÃ©ration quand i=2\n    print(i)  # Affiche 0, 1, 3, 4\n\n\nğŸ“Œ Cas dâ€™usage Data Engineering\n# Exemple 1 : Traitement de fichiers multiples\nfichiers = [\"users_2024_01.csv\", \"users_2024_02.csv\", \"users_2024_03.csv\"]\n\nfor fichier in fichiers:\n    print(f\"Traitement de {fichier}...\")\n    # Ici : logique de lecture/transformation\n    # df = pd.read_csv(fichier)\n    # process(df)\n\n# Exemple 2 : Nettoyage de donnÃ©es\nposts = [\n    {\"text\": \"Hello\", \"likes\": 10},\n    {\"text\": \"\", \"likes\": 5},       # âš ï¸ Texte vide\n    {\"text\": \"Python\", \"likes\": 20}\n]\n\nposts_valides = []\n\nfor post in posts:\n    # Skip les posts vides\n    if not post[\"text\"].strip():\n        continue\n    \n    # Nettoyer et garder\n    post[\"text\"] = post[\"text\"].strip().lower()\n    posts_valides.append(post)\n\nprint(posts_valides)\n# [{'text': 'hello', 'likes': 10}, {'text': 'python', 'likes': 20}]\n\n# Exemple 3 : Retry logic (tentatives multiples)\nmax_tentatives = 3\ntentative = 0\nsucces = False\n\nwhile tentative &lt; max_tentatives and not succes:\n    print(f\"Tentative {tentative + 1}...\")\n    \n    # Simulation d'une connexion\n    # succes = tenter_connexion()\n    \n    tentative += 1\n    \n    if not succes and tentative &lt; max_tentatives:\n        print(\"Ã‰chec, nouvelle tentative...\")\n\n\n\n\nğŸ” 2.3 Boucles avancÃ©es : enumerate() et zip()\n\nenumerate() : Obtenir lâ€™index ET la valeur\nfruits = [\"pomme\", \"banane\", \"orange\"]\n\n# Sans enumerate (moins pratique)\nfor i in range(len(fruits)):\n    print(f\"{i}: {fruits[i]}\")\n\n# Avec enumerate (recommandÃ©)\nfor index, fruit in enumerate(fruits):\n    print(f\"{index}: {fruit}\")\n\n# Avec enumerate dÃ©marrant Ã  1\nfor num, fruit in enumerate(fruits, start=1):\n    print(f\"Fruit #{num}: {fruit}\")\n\n\nzip() : Parcourir plusieurs listes simultanÃ©ment\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 35]\nvilles = [\"Paris\", \"Lyon\", \"Marseille\"]\n\nfor nom, age, ville in zip(noms, ages, villes):\n    print(f\"{nom} a {age} ans et habite Ã  {ville}\")\n\n# RÃ©sultat :\n# Alice a 25 ans et habite Ã  Paris\n# Bob a 30 ans et habite Ã  Lyon\n# Charlie a 35 ans et habite Ã  Marseille\n\n\n\n\nâš ï¸ Erreurs frÃ©quentes et bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ Erreur\nâœ… Correction\nğŸ’¡ Explication\n\n\n\n\nOublier : aprÃ¨s if/for/while\nif age &gt; 18:\nSyntaxe obligatoire\n\n\nMauvaise indentation\nUtiliser 4 espaces\nPython est sensible Ã  lâ€™indentation\n\n\nBoucle infinie while\nToujours incrÃ©menter\ncompteur += 1\n\n\nModifier liste pendant for\nCrÃ©er nouvelle liste\nÃ‰vite comportements imprÃ©visibles\n\n\n= au lieu de ==\nif age == 18:\n= assigne, == compare\n\n\n\nExemples dâ€™erreurs :\n# âŒ Erreur 1 : Oublier le :\nif age &gt; 18\n    print(\"Majeur\")  # SyntaxError\n\n# âœ… Correction\nif age &gt; 18:\n    print(\"Majeur\")\n\n# âŒ Erreur 2 : Boucle infinie\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    # Oubli d'incrÃ©menter â†’ boucle infinie !\n\n# âœ… Correction\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    compteur += 1\n\n# âŒ Erreur 3 : Modifier liste pendant boucle\nnombres = [1, 2, 3, 4, 5]\nfor n in nombres:\n    if n % 2 == 0:\n        nombres.remove(n)  # âš ï¸ Comportement imprÃ©visible\n\n# âœ… Correction : List comprehension\nnombres = [1, 2, 3, 4, 5]\nnombres_impairs = [n for n in nombres if n % 2 != 0]\n\n\n\nğŸ“š RÃ©capitulatif\n\n\n\nStructure\nUsage\nExemple\n\n\n\n\nif/elif/else\nPrendre des dÃ©cisions\nValidation, filtrage\n\n\nfor\nParcourir sÃ©quences\nTraiter fichiers, lignes\n\n\nwhile\nRÃ©pÃ©ter tant queâ€¦\nRetry logic, polling\n\n\nbreak\nSortir de boucle\nArrÃªt prÃ©maturÃ©\n\n\ncontinue\nSauter itÃ©ration\nSkip donnÃ©es invalides\n\n\nenumerate()\nIndex + valeur\nNumÃ©rotation\n\n\nzip()\nCombiner listes\nJoindre donnÃ©es parallÃ¨les\n\n\n\n\n\n\nğŸ¯ Points clÃ©s Ã  retenir\n\nConditions : Utilisent == pour comparer (pas =)\nIndentation : 4 espaces obligatoires aprÃ¨s :\nwhile : Toujours prÃ©voir une sortie de boucle\nfor : PrÃ©fÃ©rer enumerate() si besoin de lâ€™index\nList comprehension : Alternative Ã©lÃ©gante aux boucles simples",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "3. Structures de donnÃ©es ğŸ§±",
    "text": "3. Structures de donnÃ©es ğŸ§±\nPython propose plusieurs structures trÃ¨s utilisÃ©es en data engineering.\n\n\n\nStructure\nOrdonnÃ©\nModifiable\nDuplicats\nAccÃ¨s principal\n\n\n\n\nlist\nâœ”ï¸\nâœ”ï¸\nâœ”ï¸\nindex (0, 1, 2â€¦)\n\n\ndict\nâœ”ï¸ (3.7+)\nâœ”ï¸\nclÃ©s uniques\nclÃ© (\"nom\")\n\n\ntuple\nâœ”ï¸\nâŒ\nâœ”ï¸\nindex\n\n\nset\nâŒ\nâœ”ï¸\nâŒ\nappartenance (in)\n\n\n\n\n3.1 Listes (list)\nUne liste est une sÃ©quence ordonnÃ©e et modifiable.\n# CrÃ©ation d'une liste\nnombres = [10, 20, 30, 40]\n\n# AccÃ¨s par index\nprint(nombres[0])   # 10\nprint(nombres[2])   # 30\n\n# Ajout en fin de liste\nnombres.append(50)\nprint(\"AprÃ¨s append :\", nombres)\n\n# Insertion Ã  une position prÃ©cise\nnombres.insert(1, 15)\nprint(\"AprÃ¨s insert :\", nombres)\n\n# Modification d'un Ã©lÃ©ment\nnombres[0] = 5\nprint(\"AprÃ¨s modification :\", nombres)\n\n# Suppression par valeur\nnombres.remove(30)\nprint(\"AprÃ¨s remove :\", nombres)\n\n# Suppression par index\ndel nombres[0]\nprint(\"AprÃ¨s del :\", nombres)\n\nCrÃ©ation dynamique de listes\nOn crÃ©e trÃ¨s souvent des listes Ã  partir dâ€™autres listes, avec une boucle ou une list comprehension.\nnombres = [1, 2, 3, 4, 5, 6]\n\n# Version avec boucle\npairs = []\nfor n in nombres:\n    if n % 2 == 0:\n        pairs.append(n)\n\nprint(\"Pairs (boucle) :\", pairs)\n\n# Version list comprehension\npairs2 = [n for n in nombres if n % 2 == 0]\nprint(\"Pairs (list comprehension) :\", pairs2)\nâš ï¸ Erreurs frÃ©quentes avec les listes : - nombres[10] alors que la liste a moins dâ€™Ã©lÃ©ments â†’ IndexError ; - nombres.remove(999) alors que 999 nâ€™est pas dans la liste â†’ ValueError.\n\n\n\n3.2 Dictionnaires (dict)\nUn dictionnaire stocke des paires clÃ© â†’ valeur. Câ€™est lâ€™Ã©quivalent naturel des objets JSON, trÃ¨s utilisÃ© pour les APIs et NoSQL.\nutilisateur = {\n    \"id\": 1,\n    \"nom\": \"Alice\",\n    \"ville\": \"Abidjan\"\n}\n\n# AccÃ¨s Ã  une valeur par clÃ©\nprint(utilisateur[\"nom\"])  # Alice\n\n# Ajout / modification\nutilisateur[\"age\"] = 30\nutilisateur[\"ville\"] = \"BouakÃ©\"\n\n# Suppression\ndel utilisateur[\"id\"]\n\nprint(utilisateur)\n\nAccÃ¨s sÃ©curisÃ© avec .get()\nUtiliser dict.get() permet dâ€™Ã©viter un KeyError si la clÃ© nâ€™existe pas.\nprint(utilisateur.get(\"email\"))           # None\nprint(utilisateur.get(\"email\", \"Inconnu\"))  # Inconnu\n\n\nCrÃ©ation dynamique : comptage dâ€™occurrences\nnoms = [\"bob\", \"alice\", \"bob\", \"charlie\", \"alice\"]\ncompte = {}\n\nfor n in noms:\n    compte[n] = compte.get(n, 0) + 1\n\nprint(compte)  # {'bob': 2, 'alice': 2, 'charlie': 1}\nâš ï¸ Erreurs frÃ©quentes avec les dictionnaires : - utilisateur[\"email\"] alors que la clÃ© nâ€™existe pas â†’ KeyError ; - supposer quâ€™un dictionnaire est indexÃ© comme une liste (utilisateur[0]).\n\n\n\n3.3 Tuples (tuple)\nUn tuple est comme une liste non modifiable (immutable). On lâ€™utilise pour reprÃ©senter des collections fixes de valeurs : coordonnÃ©es, dates, etc.\ncoord = (5.0, 10.0)\nprint(coord[0])  # 5.0\nprint(coord[1])  # 10.0\n\n# coord[0] = 20.0  # âŒ TypeError : un tuple n'est pas modifiable\nEn data engineering, les tuples sont utiles pour : - retourner plusieurs valeurs depuis une fonction ; - reprÃ©senter des clÃ©s composites (ex : (annÃ©e, mois)).\n\n\n3.4 Ensembles (set)\nUn ensemble (set) contient des valeurs uniques, sans ordre garanti. TrÃ¨s utile pour dÃ©dupliquer une liste.\ntags = {\"python\", \"data\", \"python\"}\nprint(tags)  # 'python' n'apparaÃ®t qu'une seule fois\n\n# Ajout\ntags.add(\"engineer\")\n\n# Suppression (erreur si absent)\ntags.remove(\"data\")\n\n# Suppression sans erreur si absent\ntags.discard(\"ai\")\n\nprint(tags)\nğŸ’¡ Exemple de dÃ©duplication :\nemails = [\"a@test.com\", \"b@test.com\", \"a@test.com\"]\nemails_uniques = list(set(emails))\nprint(emails_uniques)\nâš ï¸ Erreurs frÃ©quentes : - Compter sur lâ€™ordre dâ€™un set (lâ€™ordre nâ€™est pas garanti) ; - Utiliser remove() pour un Ã©lÃ©ment possiblement absent â†’ prÃ©fÃ©rer discard().\n\n\nğŸ§± 3.5 Mini-exercice â€” Structures de donnÃ©es\nğŸ“Œ DonnÃ©es\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\nğŸ¯ Objectifs 1. CrÃ©er la liste de tous les utilisateurs (list) 2. CrÃ©er la liste unique des utilisateurs (set) 3. CrÃ©er un dictionnaire qui compte le nombre de requÃªtes par utilisateur (dict)\n# Ã€ toi de jouer ğŸ˜Š\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []            # TODO\nutilisateurs_uniques = set() # TODO\ncompte_par_user = {}         # TODO\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []\nutilisateurs_uniques = set()\ncompte_par_user = {}\n\nfor log in logs:\n    user = log[\"user\"]\n    utilisateurs.append(user)\n    utilisateurs_uniques.add(user)\n    compte_par_user[user] = compte_par_user.get(user, 0) + 1\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸš€ Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es",
    "text": "ğŸš€ Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es\nLes comprehensions sont une syntaxe Python Ã©lÃ©gante et performante pour crÃ©er des listes, dictionnaires ou sets en une seule ligne. Câ€™est fondamental en Data Engineering pour transformer des donnÃ©es efficacement.\n\n\nğŸ“‹ List Comprehension\nSyntaxe : [expression for item in iterable if condition]\n# âŒ MÃ©thode classique (verbose)\nnombres = [1, 2, 3, 4, 5]\ncarres = []\nfor n in nombres:\n    carres.append(n ** 2)\nprint(carres)  # [1, 4, 9, 16, 25]\n\n# âœ… List comprehension (recommandÃ©)\ncarres = [n ** 2 for n in nombres]\nprint(carres)  # [1, 4, 9, 16, 25]\n\nAvec condition (filtre)\n# Garder seulement les nombres pairs\nnombres = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npairs = [n for n in nombres if n % 2 == 0]\nprint(pairs)  # [2, 4, 6, 8, 10]\n\n# Exemple Data Engineering : filtrer les emails valides\nemails = [\"alice@test.com\", \"invalid\", \"bob@company.org\", \"\"]\nemails_valides = [e for e in emails if \"@\" in e and e]\nprint(emails_valides)  # ['alice@test.com', 'bob@company.org']\n\n\nCas dâ€™usage Data Engineering\n# Extraire les noms d'une liste de dictionnaires\nusers = [\n    {\"nom\": \"Alice\", \"actif\": True},\n    {\"nom\": \"Bob\", \"actif\": False},\n    {\"nom\": \"Charlie\", \"actif\": True}\n]\n\n# Noms des utilisateurs actifs en majuscules\nnoms_actifs = [u[\"nom\"].upper() for u in users if u[\"actif\"]]\nprint(noms_actifs)  # ['ALICE', 'CHARLIE']\n\n# Nettoyer une liste de fichiers\nfichiers = [\"data.csv\", \"readme.txt\", \"users.csv\", \"config.yaml\"]\ncsv_files = [f for f in fichiers if f.endswith(\".csv\")]\nprint(csv_files)  # ['data.csv', 'users.csv']\n\n\n\n\nğŸ“– Dict Comprehension\nSyntaxe : {key: value for item in iterable if condition}\n# CrÃ©er un dictionnaire Ã  partir de deux listes\nnoms = [\"alice\", \"bob\", \"charlie\"]\nages = [25, 30, 35]\n\nusers_dict = {nom: age for nom, age in zip(noms, ages)}\nprint(users_dict)  # {'alice': 25, 'bob': 30, 'charlie': 35}\n\n# Inverser un dictionnaire\noriginal = {\"a\": 1, \"b\": 2, \"c\": 3}\ninverse = {v: k for k, v in original.items()}\nprint(inverse)  # {1: 'a', 2: 'b', 3: 'c'}\n\nCas dâ€™usage Data Engineering\n# Transformer des donnÃ©es JSON\nraw_data = [\n    {\"id\": 1, \"value\": \"100\"},\n    {\"id\": 2, \"value\": \"200\"},\n    {\"id\": 3, \"value\": \"300\"}\n]\n\n# CrÃ©er un mapping id -&gt; valeur (convertie en int)\nid_to_value = {d[\"id\"]: int(d[\"value\"]) for d in raw_data}\nprint(id_to_value)  # {1: 100, 2: 200, 3: 300}\n\n\n\n\nğŸ¯ Set Comprehension\nSyntaxe : {expression for item in iterable if condition}\n# Valeurs uniques\nnombres = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\nuniques = {n for n in nombres}\nprint(uniques)  # {1, 2, 3, 4}\n\n# Domaines uniques des emails\nemails = [\"alice@gmail.com\", \"bob@yahoo.com\", \"charlie@gmail.com\"]\ndomaines = {e.split(\"@\")[1] for e in emails}\nprint(domaines)  # {'gmail.com', 'yahoo.com'}\n\n\n\nâš ï¸ Bonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nComprehensions simples et lisibles\nComprehensions imbriquÃ©es complexes\n\n\nUne seule transformation\nPlusieurs opÃ©rations chaÃ®nÃ©es\n\n\nUtiliser pour crÃ©er des collections\nUtiliser pour des effets de bord\n\n\n\n\n\n\nğŸ§ª Mini-exercice\nTransformer cette liste de transactions :\ntransactions = [\n    {\"id\": 1, \"montant\": 100, \"devise\": \"EUR\"},\n    {\"id\": 2, \"montant\": 50, \"devise\": \"USD\"},\n    {\"id\": 3, \"montant\": 200, \"devise\": \"EUR\"},\n    {\"id\": 4, \"montant\": 75, \"devise\": \"USD\"}\n]\n\nCrÃ©er une liste des montants en EUR uniquement\nCrÃ©er un dict {id: montant} pour les transactions &gt; 60\n\n\n\nğŸ’¡ Solution\n\n# 1. Montants EUR\nmontants_eur = [t[\"montant\"] for t in transactions if t[\"devise\"] == \"EUR\"]\nprint(montants_eur)  # [100, 200]\n\n# 2. Dict id -&gt; montant (&gt; 60)\nid_montant = {t[\"id\"]: t[\"montant\"] for t in transactions if t[\"montant\"] &gt; 60}\nprint(id_montant)  # {1: 100, 3: 200, 4: 75}",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "4. Fonctions",
    "text": "4. Fonctions\nUne fonction permet de :\n\nrÃ©utiliser du code ;\nencapsuler de la logique mÃ©tier (nettoyage, validationâ€¦) ;\nsÃ©curiser la qualitÃ© de donnÃ©es (type, structureâ€¦).\n\nSyntaxe gÃ©nÃ©rale :\ndef nom(param1, param2=valeur_par_defaut) -&gt; type_retour:\n    # traitement\n    return resultat\nExemple :\ndef somme(a: int, b: int) -&gt; int:\n    \"\"\"Retourne la somme de deux entiers.\"\"\"\n    return a + b\n\nresultat = somme(3, 5)\nprint(\"RÃ©sultat :\", resultat)\n\nâœ”ï¸ 4.1 Fonction pure (sans effet externe)\ndef normaliser_nom(nom: str) -&gt; str:\n    \"\"\"Nettoie un nom : supprime espaces, met en minuscule et capitalise.\"\"\"\n    return nom.strip().lower().capitalize()\n\nprint(normaliser_nom(\"  aLiCe  \"))  # Alice\n\n\nâš™ï¸ 4.2 ParamÃ¨tres + valeurs par dÃ©faut\ndef calculer_total(prix: float, quantite: int = 1, taxe: float = 0.18) -&gt; float:\n    \"\"\"Retourne le prix total avec taxe.\"\"\"\n    return prix * quantite * (1 + taxe)\n\nprint(calculer_total(2000))  \nprint(calculer_total(2000, quantite=3, taxe=0.09))\n\n\nğŸ“Š 4.3 Retourner plusieurs valeurs (tuple)\ndef stats_notes(notes: list[int]) -&gt; tuple[float, float]:\n    \"\"\"Retourne moyenne et maximum d'une liste de notes.\"\"\"\n    moyenne = sum(notes) / len(notes)\n    maxi = max(notes)\n    return moyenne, maxi\n\nm, mx = stats_notes([14, 9, 18])\nprint(\"Moyenne:\", m, \"Max:\", mx)\n\n\nğŸ”— 4.4 Fonctions qui manipulent un dictionnaire\ndef extraire_champ(data: dict, champ: str, default=None):\n    \"\"\"RÃ©cupÃ¨re un champ d'un dict, Ã©vite KeyError.\"\"\"\n    return data.get(champ, default)\n\nuser = {\"nom\": \"Sara\", \"ville\": \"Paris\"}\nprint(extraire_champ(user, \"nom\"))         # Sara\nprint(extraire_champ(user, \"age\", \"N/A\"))  # N/A\nâš ï¸ Erreurs frÃ©quentes : - Oublier les parenthÃ¨ses lors de lâ€™appel : somme au lieu de somme(3, 5) ; - Oublier return â†’ la fonction retourne None ; - Ne pas respecter le nombre dâ€™arguments attendus.\nâŒ Mauvaise pratique :\ndef ajouter(element, liste=[]):  # liste partagÃ©e !\n    liste.append(element)\n    return liste\nâœ”ï¸ Correct :\ndef ajouter(element, liste=None):\n    if liste is None: liste = []\n    liste.append(element)\n    return liste\n\n\nğŸ§ª Mini-exercice â€” Fonctions sur des posts\n\nğŸ“Œ Dataset simulÃ©\nposts = [\n  {\"user\": \"alice\", \"text\": \"  Hello World  \"},\n  {\"user\": \"bob\", \"text\": \"Data Engineer ici\"},\n  {\"user\": \"alice\", \"text\": \"Python est top \"}\n]\n\n\nğŸ¯ Instructions\nCrÃ©er 3 fonctions :\n\nnettoyer_texte(text: str) -&gt; str\nNettoie le texte (supprime espaces, convertit en minuscules)\nlongueur_post(post: dict) -&gt; int\nRetourne la longueur du texte nettoyÃ© dâ€™un post\nstats_posts(posts: list[dict]) -&gt; tuple[float, int, int]\nRetour attendu : (moyenne, maximum, minimum) des longueurs de texte\n\n# Ã€ toi de jouer ğŸ˜Š\n\n# TODO\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\ndef longueur_post(post: dict) -&gt; int:\n    return len(nettoyer_texte(post[\"text\"]))\n\ndef stats_posts(posts: list[dict]) -&gt; tuple[float, int, int]:\n    longueurs = [longueur_post(p) for p in posts]\n    return (sum(longueurs)/len(longueurs), max(longueurs), min(longueurs))\n\nprint(stats_posts(posts))\nRÃ©sultat attendu : (15.333333333333334, 19, 11)\n\n\n\n\nğŸ”§ 4.5 *args et **kwargs â€” Fonctions flexibles\nCes syntaxes permettent de crÃ©er des fonctions qui acceptent un nombre variable dâ€™arguments. TrÃ¨s utilisÃ© pour crÃ©er des wrappers, des dÃ©corateurs, ou des fonctions gÃ©nÃ©riques.\n\n\n*args â€” Arguments positionnels variables\ndef somme(*args):\n    \"\"\"Accepte n'importe quel nombre d'arguments.\"\"\"\n    print(f\"Arguments reÃ§us : {args}\")  # C'est un tuple\n    return sum(args)\n\nprint(somme(1, 2))           # 3\nprint(somme(1, 2, 3, 4, 5))  # 15\nprint(somme())               # 0\n\n\n\n**kwargs â€” Arguments nommÃ©s variables\ndef afficher_info(**kwargs):\n    \"\"\"Accepte n'importe quel argument nommÃ©.\"\"\"\n    print(f\"Arguments reÃ§us : {kwargs}\")  # C'est un dict\n    for cle, valeur in kwargs.items():\n        print(f\"  {cle} = {valeur}\")\n\nafficher_info(nom=\"Alice\", age=30, ville=\"Paris\")\n# Arguments reÃ§us : {'nom': 'Alice', 'age': 30, 'ville': 'Paris'}\n#   nom = Alice\n#   age = 30\n#   ville = Paris\n\n\n\nCombiner les deux\ndef fonction_flexible(obligatoire, *args, option=\"defaut\", **kwargs):\n    \"\"\"Ordre : obligatoire, *args, avec dÃ©faut, **kwargs\"\"\"\n    print(f\"Obligatoire : {obligatoire}\")\n    print(f\"Args : {args}\")\n    print(f\"Option : {option}\")\n    print(f\"Kwargs : {kwargs}\")\n\nfonction_flexible(\"premier\", 1, 2, 3, option=\"custom\", extra=\"valeur\")\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\ndef log_etl(etape: str, *messages, niveau: str = \"INFO\", **metadata):\n    \"\"\"Logger flexible pour pipeline ETL.\"\"\"\n    print(f\"[{niveau}] {etape}\")\n    for msg in messages:\n        print(f\"  - {msg}\")\n    if metadata:\n        print(f\"  Metadata: {metadata}\")\n\n# Utilisation\nlog_etl(\n    \"EXTRACT\",\n    \"Connexion Ã©tablie\",\n    \"1000 lignes lues\",\n    niveau=\"INFO\",\n    source=\"postgres\",\n    table=\"users\"\n)\n# Wrapper pour ajouter du logging Ã  n'importe quelle fonction\ndef avec_logging(func):\n    def wrapper(*args, **kwargs):\n        print(f\"Appel de {func.__name__} avec args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"RÃ©sultat : {result}\")\n        return result\n    return wrapper",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤",
    "text": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\nu = Utilisateur(1, \"Alice\", \"Abidjan\")\nprint(u.presentation())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.2 Ajouter une mÃ©thode utile ğŸ™ï¸",
    "text": "5.2 Ajouter une mÃ©thode utile ğŸ™ï¸\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\n\n    def changer_ville(self, nouvelle_ville: str) -&gt; None:\n        self.ville = nouvelle_ville",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s ğŸ”’",
    "text": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s ğŸ”’\nclass Compte:\n    def __init__(self, solde: float):\n        self._solde = solde        # usage interne\n        self.__secret = \"XYZ123\"   # privÃ© via name mangling",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.4 ReprÃ©sentation textuelle (str) ğŸ“",
    "text": "5.4 ReprÃ©sentation textuelle (str) ğŸ“\nclass Utilisateur:\n    def __init__(self, identifiant, nom, ville):\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def __str__(self) -&gt; str:\n        return f\"[{self.identifiant}] {self.nom} ({self.ville})\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.5 Composition dâ€™objets (objet dans un objet) ğŸ§±",
    "text": "5.5 Composition dâ€™objets (objet dans un objet) ğŸ§±\nclass Adresse:\n    def __init__(self, rue: str, ville: str, pays: str) -&gt; None:\n        self.rue = rue\n        self.ville = ville\n        self.pays = pays\n\n    def __str__(self) -&gt; str:\n        return f\"{self.rue}, {self.ville} ({self.pays})\"\n\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, adresse: Adresse) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.adresse = adresse\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.adresse}.\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.6 Pourquoi utiliser des classes en Data Engineering ? âš™ï¸",
    "text": "5.6 Pourquoi utiliser des classes en Data Engineering ? âš™ï¸\nclass Transaction:\n    def __init__(self, id, montant, devise, timestamp):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n        self.timestamp = timestamp\n\nclass Transaction:\n    def __init__(self, id, montant, devise):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n\n    def montant_fcfa(self):\n        taux = {\"EUR\": 655, \"USD\": 600}\n        return self.montant * taux.get(self.devise, 1)\n\nclass ExtracteurCSV:\n    def __init__(self, chemin):\n        self.chemin = chemin\n\n    def extract(self):\n        with open(self.chemin) as f:\n            return f.readlines()\n\nevent = EventHubRecord(payload)\nevent.clean()\nevent.validate()\nevent.to_parquet()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.7 Erreurs frÃ©quentes âš ï¸",
    "text": "5.7 Erreurs frÃ©quentes âš ï¸\n\nOublier self dans les mÃ©thodes.\nAccÃ©der Ã  un attribut avant de lâ€™avoir crÃ©Ã©.\nUtiliser une valeur mutable dans __init__ (list, dict).\nConfondre composition et hÃ©ritage.\nFaire une classe Â« God Object Â» avec trop de responsabilitÃ©s.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“¦ Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es",
    "text": "ğŸ“¦ Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es\nLes dataclasses (Python 3.7+) simplifient la crÃ©ation de classes qui servent principalement Ã  stocker des donnÃ©es. Câ€™est le standard moderne en Data Engineering pour modÃ©liser des structures de donnÃ©es.\n\n\nğŸ¤” ProblÃ¨me avec les classes classiques\n# âŒ Classe classique â€” verbose et rÃ©pÃ©titif\nclass User:\n    def __init__(self, id: int, nom: str, email: str, actif: bool = True):\n        self.id = id\n        self.nom = nom\n        self.email = email\n        self.actif = actif\n    \n    def __repr__(self):\n        return f\"User(id={self.id}, nom='{self.nom}', email='{self.email}', actif={self.actif})\"\n    \n    def __eq__(self, other):\n        return self.id == other.id and self.nom == other.nom\n\n\n\nâœ… Solution avec dataclass\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n    actif: bool = True  # Valeur par dÃ©faut\n\n# Utilisation\nuser = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user)  # User(id=1, nom='Alice', email='alice@test.com', actif=True)\n\n# Comparaison automatique\nuser2 = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user == user2)  # True\n\n\n\nğŸ¯ Avantages des dataclasses\n\n\n\nAvantage\nDescription\n\n\n\n\n__init__ auto-gÃ©nÃ©rÃ©\nPlus besoin dâ€™Ã©crire le constructeur\n\n\n__repr__ auto-gÃ©nÃ©rÃ©\nAffichage lisible pour le debug\n\n\n__eq__ auto-gÃ©nÃ©rÃ©\nComparaison par valeur\n\n\nType hints intÃ©grÃ©s\nDocumentation et validation IDE\n\n\nValeurs par dÃ©faut\nSyntaxe simple\n\n\n\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass Transaction:\n    id: int\n    montant: float\n    devise: str\n    timestamp: str\n    metadata: dict = field(default_factory=dict)  # Pour les types mutables\n\n# CrÃ©er une transaction\ntx = Transaction(\n    id=1001,\n    montant=150.50,\n    devise=\"EUR\",\n    timestamp=\"2024-01-15T10:30:00Z\"\n)\nprint(tx)\n\n# AccÃ©der aux attributs\nprint(f\"Montant: {tx.montant} {tx.devise}\")\n\nDataclass immuable (frozen)\n@dataclass(frozen=True)  # Immuable comme un tuple\nclass Config:\n    host: str\n    port: int\n    database: str\n\nconfig = Config(host=\"localhost\", port=5432, database=\"warehouse\")\n# config.port = 3306  # âŒ Erreur : FrozenInstanceError\n\n\n\n\nğŸ”„ Convertir en dict/tuple\nfrom dataclasses import asdict, astuple\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n\nuser = User(1, \"Alice\", \"alice@test.com\")\n\n# Conversion en dict (utile pour JSON)\nuser_dict = asdict(user)\nprint(user_dict)  # {'id': 1, 'nom': 'Alice', 'email': 'alice@test.com'}\n\n# Conversion en tuple\nuser_tuple = astuple(user)\nprint(user_tuple)  # (1, 'Alice', 'alice@test.com')\n\n\n\nâš ï¸ Attention : valeurs par dÃ©faut mutables\n# âŒ ERREUR : liste partagÃ©e entre instances\n@dataclass\nclass BadExample:\n    items: list = []  # ValueError!\n\n# âœ… CORRECT : utiliser field(default_factory=...)\nfrom dataclasses import field\n\n@dataclass\nclass GoodExample:\n    items: list = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n\n\n\nğŸ’¡ Quand utiliser dataclass vs classe classique ?\n\n\n\nSituation\nRecommandation\n\n\n\n\nStocker des donnÃ©es (models, records)\nâœ… @dataclass\n\n\nLogique mÃ©tier complexe\nClasse classique\n\n\nConfiguration, paramÃ¨tres\nâœ… @dataclass(frozen=True)\n\n\nValidation avancÃ©e\nPydantic (module suivant)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "6. Gestion dâ€™erreurs ğŸ›‘ â€” (Indispensable en Data Engineering)",
    "text": "6. Gestion dâ€™erreurs ğŸ›‘ â€” (Indispensable en Data Engineering)\nEn Data Engineering, les erreurs sont inÃ©vitables :\n\nfichiers manquants\n\nAPI indisponible\n\nJSON mal formÃ©\n\ndivision par zÃ©ro\n\nconnexion BD Ã©chouÃ©e\n\ntypage incorrect\n\nğŸ‘‰ La bonne pratique consiste Ã  capturer, expliquer, puis continuer proprement.\nCâ€™est le rÃ´le de try / except.\n\n\nğŸ¯ Exemple simple â€” division sÃ©curisÃ©e\ndef division(a: float, b: float) -&gt; float | None:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        print(\"âŒ Erreur : division par zÃ©ro.\")\n        return None\nprint(division(10, 2))  # OK\nprint(division(10, 0))  # Erreur gÃ©rÃ©e\n\n\n\nğŸ§° Exemple plus rÃ©aliste â€” lecture de fichier\ndef lire_csv(chemin: str) -&gt; list | None:\n    try:\n        with open(chemin, \"r\") as f:\n            return f.readlines()\n    except FileNotFoundError:\n        print(f\"âŒ Fichier introuvable : {chemin}\")\n        return None\n\n\n\nğŸ’¡ else et finally\nOn peut amÃ©liorer la lisibilitÃ© avec else et finally :\ntry:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print(\"Erreur\")\nelse:\n    print(\"Aucune erreur, rÃ©sultat =\", result)\nfinally:\n    print(\"Bloc exÃ©cutÃ© dans tous les cas\")\n\n\n\nğŸ—ï¸ Exemple Data Engineering : appel API sÃ©curisÃ©\nimport requests\n\ndef fetch_json(url: str) -&gt; dict | None:\n    try:\n        response = requests.get(url, timeout=3)\n        response.raise_for_status()   # GÃ©nÃ¨re une erreur HTTP si code â‰  200\n        return response.json()\n    except requests.exceptions.HTTPError as e:\n        print(\"âŒ Erreur HTTP :\", e)\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : serveur trop lent\")\n    except ValueError:\n        print(\"âŒ JSON mal formÃ©\")\n    except Exception as e:\n        print(\"âš ï¸ Erreur inconnue :\", e)\n    return None\n\n\n\nâš ï¸ Erreurs frÃ©quentes Ã  Ã©viter\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nâœ… Bonne pratique\n\n\n\n\nexcept: (attrape tout)\nSpÃ©cifier lâ€™erreur : except ValueError:\n\n\nCacher lâ€™erreur sans message\nFournir un contexte ğŸ—ƒï¸\n\n\nRetourner nâ€™importe quoi\nRetour cohÃ©rent (None ou valeur par dÃ©faut)\n\n\nMettre trop de logique dans try\nLimiter au strict nÃ©cessaire\n\n\nIgnorer les erreurs silencieusement\nLogguer ou notifier\n\n\n\n\n\n\nğŸš€ Conseil pro (trÃ¨s utile en Data Engineering)\nUtiliser raise pour propager lâ€™erreur si elle doit Ãªtre traitÃ©e ailleurs :\ndef parse_json(data: str) -&gt; dict:\n    try:\n        return json.loads(data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"JSON invalide : {e}\")\n\n\n\nğŸ“Œ Ã€ retenir\n\nToujours attraper le type exact dâ€™erreur.\n\nToujours expliquer lâ€™erreur (message clair).\n\nToujours garder un comportement cohÃ©rent (retour None ou valeur dÃ©faut).\n\nLes erreurs silencieuses sont pires que les erreurs visibles.\n\nLes pipelines cassent souvent â€” gÃ©rer les erreurs = Ãªtre un vrai Data Engineer.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "7. Modules et Imports ğŸ“¦ â€” Structurer son code comme un Data Engineer",
    "text": "7. Modules et Imports ğŸ“¦ â€” Structurer son code comme un Data Engineer\nEn Python :\n\nUn module = un fichier .py\nUn package = un dossier contenant plusieurs modules + un fichier __init__.py\n\nğŸ‘‰ Cela permet dâ€™organiser un projet data en blocs logiques :\ningestion, nettoyage, transformation, validation, etc.\n\n\nğŸ—‚ï¸ Exemple de structure de projet (propre & professionnelle)\nproject/\nâ”œâ”€â”€ utils/               â† Package (outils rÃ©utilisables)\nâ”‚   â”œâ”€â”€ __init__.py      â† Indique que `utils` est un package\nâ”‚   â””â”€â”€ math_utils.py     â† Module\nâ”œâ”€â”€ processors/           â† Package pour le traitement\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ text_cleaner.py   â† Module\nâ””â”€â”€ main.py               â† Point dâ€™entrÃ©e du projet\n\n\n\nğŸ“Œ Contenu du module : utils/math_utils.py\ndef somme(a, b):\n    return a + b\n\n\n\nğŸ“Œ Contenu dâ€™un autre module : processors/text_cleaner.py\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\n\n\nğŸ“Œ Importer dans main.py\nfrom utils.math_utils import somme\nfrom processors.text_cleaner import nettoyer_texte\n\nprint(somme(2, 3))\nprint(nettoyer_texte(\"  Hello WORLD  \"))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” ğŸ’¡ Import absolu vs import relatif",
    "text": "ğŸ” ğŸ’¡ Import absolu vs import relatif\n\nâœ”ï¸ Import absolu (recommandÃ©)\nfrom utils.math_utils import somme\nâ¡ï¸ Le plus lisible, idÃ©al pour projets pro / Data Engineering.\n\n\nâœ”ï¸ Import relatif (utile dans les packages)\nfrom .math_utils import somme\nâ¡ï¸ Courant dans les gros projets et dans les packages pip.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§  Pourquoi câ€™est important en Data Engineering ?",
    "text": "ğŸ§  Pourquoi câ€™est important en Data Engineering ?\n\nCrÃ©er des modules = rendre ton code rÃ©utilisable (API, pipelines, notebooks)\nStructurer ton projet = Ã©viter le â€œscript spaghettiâ€\nFaciliter les tests unitaires\nFaciliter la maintenance dâ€™un pipeline data\nRÃ©duire les erreurs de duplication de logique",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & solutions",
    "text": "âš ï¸ Erreurs frÃ©quentes & solutions\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nModuleNotFoundError\nexÃ©cuter Python depuis le mauvais dossier\nToujours lancer Python depuis la racine du projet\n\n\nImport relatif impossible\nabsence du fichier __init__.py\nAjouter __init__.py dans le dossier\n\n\nModules dupliquÃ©s\nfichiers ayant le mÃªme nom dans deux dossiers\nRenommer ou structurer les packages\n\n\nimport *\nimports imprÃ©cis\nToujours importer explicitement\n\n\n\n\n\nâœ”ï¸ Astuce pro : vÃ©rifier ton PYTHONPATH\nimport sys\nprint(sys.path)\nCela indique oÃ¹ Python cherche les modules.\n\n\n\nâ­ Rappel essentiel\n\nImporter = RÃ©utiliser.\nStructurer = Devenir un vrai Data Engineer.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8. Manipulation de fichiers ğŸ“ â€” CompÃ©tence essentielle du Data Engineer",
    "text": "8. Manipulation de fichiers ğŸ“ â€” CompÃ©tence essentielle du Data Engineer\nDans un pipeline Data, on manipule en permanence des fichiers :\n\nfichiers texte (logs, outputs)\nfichiers CSV (exports mÃ©tier, ingestion)\nfichiers JSON (APIs, NoSQL, Ã©vÃ©nements Kafka)\ndossiers qui contiennent les donnÃ©es\n\nPython fournit des outils natifs trÃ¨s puissants pour cela :\n\npathlib.Path â†’ gÃ©rer les chemins et dossiers\n\nopen() â†’ lire/Ã©crire des fichiers texte\n\njson â†’ sÃ©rialiser/dÃ©sÃ©rialiser des donnÃ©es JSON\n\n\n\nğŸ—‚ï¸ Initialisation du dossier data/\nfrom pathlib import Path\nimport json\n\n# CrÃ©ation du dossier de travail\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.1 ğŸ“„ Manipulation dâ€™un fichier texte (logs, outputsâ€¦)",
    "text": "8.1 ğŸ“„ Manipulation dâ€™un fichier texte (logs, outputsâ€¦)\ntexte_path = DATA_DIR / \"exemple.txt\"\n\n# Ã‰criture\nwith open(texte_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Bonjour Data Engineer\\n\")\n\n# Lecture\nwith open(texte_path, \"r\", encoding=\"utf-8\") as f:\n    contenu = f.read()\n\nprint(\"Contenu du fichier :\", contenu)\nğŸ” Bonnes pratiques :\n\nToujours utiliser encoding=\"utf-8\"\n\nToujours utiliser with ... (fermeture automatique du fichier)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.2 ğŸ§© Manipulation dâ€™un fichier JSON",
    "text": "8.2 ğŸ§© Manipulation dâ€™un fichier JSON\nFormat le plus utilisÃ© dans : - APIs REST - MongoDB - Events Kafka / Kinesis - Configurations de job\njson_path = DATA_DIR / \"utilisateur.json\"\nutilisateur = {\"nom\": \"Alice\", \"age\": 30}\n\n# Ã‰criture JSON\nwith open(json_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(utilisateur, f, ensure_ascii=False, indent=2)\n\n# Lecture JSON\nwith open(json_path, \"r\", encoding=\"utf-8\") as f:\n    utilisateur_charge = json.load(f)\n\nprint(\"Utilisateur chargÃ© :\", utilisateur_charge)\nğŸ’¡ ensure_ascii=False permet dâ€™Ã©crire proprement les accents.\nğŸ’¡ indent=2 rend le JSON lisible (log, debug, auditsâ€¦).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.3 ğŸ“Š (Optionnel) Manipulation dâ€™un CSV avec Pandas",
    "text": "8.3 ğŸ“Š (Optionnel) Manipulation dâ€™un CSV avec Pandas\nSouvent utilisÃ© en ingestion de donnÃ©es.\nimport pandas as pd\n\ncsv_path = DATA_DIR / \"exemple.csv\"\n\ndf = pd.DataFrame({\n    \"nom\": [\"Alice\", \"Bob\"],\n    \"age\": [30, 25]\n})\n\ndf.to_csv(csv_path, index=False)\n\ndf_loaded = pd.read_csv(csv_path)\nprint(df_loaded)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nFileNotFoundError\nMauvais chemin\nToujours utiliser Path() et vÃ©rifier path.exists()\n\n\nAccents cassÃ©s (Ã©, Ã â€¦)\nMauvais encoding\nToujours encoding=\"utf-8\"\n\n\nFichier non fermÃ©\nopen() sans contexte\nToujours utiliser with open(...)\n\n\nJSON mal formÃ©\nÃ©crit Ã  la main\nToujours utiliser json.dump / json.load\n\n\nChemins relatifs non fiables\nmauvais dossier dâ€™exÃ©cution\nUtiliser Path(__file__).resolve().parent dans un projet rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Points clÃ©s Ã  retenir",
    "text": "ğŸ“ Points clÃ©s Ã  retenir\n\npathlib.Path simplifie la gestion des chemins.\n\nwith open(...) est obligatoire pour Ã©viter les fuites de fichier.\n\nJSON = format standard du Data Engineering (MongoDB, API, logsâ€¦).\n\nToujours contrÃ´ler lâ€™encoding lors de la lecture/Ã©criture.\n\nLe dossier data/ centralise vos fichiers dans un projet propre.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "9. Packages et pip ğŸ“¦ â€” GÃ©rer les dÃ©pendances comme un Data Engineer",
    "text": "9. Packages et pip ğŸ“¦ â€” GÃ©rer les dÃ©pendances comme un Data Engineer\nPython devient puissant grÃ¢ce Ã  ses packages externes :\nPandas, Requests, SQLAlchemy, PyMongo, Polars, FastAPI, etc.\nPour installer ces packages, on utilise pip, le gestionnaire officiel de Python.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”§ Installer un package avec pip",
    "text": "ğŸ”§ Installer un package avec pip\nExemple : installer requests pour faire des appels HTTP (API REST).\npython -m pip install requests\nğŸ‘‰ Pourquoi python -m pip et pas juste pip install ?\nParce que cela garantit quâ€™on utilise le pip liÃ© Ã  la bonne version de Python, surtout si plusieurs versions sont installÃ©es.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“Œ Utilisation dans un script",
    "text": "ğŸ“Œ Utilisation dans un script\nimport requests\n\nresponse = requests.get(\"https://api.github.com\")\nprint(response.status_code)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Installer plusieurs packages â€” requirements.txt",
    "text": "ğŸ“ Installer plusieurs packages â€” requirements.txt\nDans un vrai projet Data Engineering, on liste les dÃ©pendances dans un fichier :\nrequests\npandas\nsqlalchemy\npymongo\nPuis on installe tout dâ€™un coup :\npython -m pip install -r requirements.txt",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â™»ï¸ Mettre Ã  jour un package",
    "text": "â™»ï¸ Mettre Ã  jour un package\npython -m pip install --upgrade requests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§¹ DÃ©sinstaller un package",
    "text": "ğŸ§¹ DÃ©sinstaller un package\npython -m pip uninstall requests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“¦ OÃ¹ sont installÃ©s les packages ?",
    "text": "ğŸ“¦ OÃ¹ sont installÃ©s les packages ?\npython -m pip show requests\nDonne : version, emplacement, dÃ©pendances, etc.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques",
    "text": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\npip: command not found\nPython mal installÃ©\nUtiliser python -m pip\n\n\nInstaller dans le mauvais Python\nPlusieurs versions installÃ©es\nToujours python -m pip install\n\n\nVersion incompatible\nConflit de dÃ©pendances\nSpÃ©cifier une version : requests==2.31.0\n\n\nInstaller globalement\nRisque de casser le systÃ¨me\nUtiliser un venv (python -m venv .venv)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce Pro : figer les versions pour la production",
    "text": "â­ Astuce Pro : figer les versions pour la production\npython -m pip freeze &gt; requirements.txt\nâ¡ï¸ Cela capture exactement les versions utilisÃ©es dans ton environnement.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”„ GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire",
    "text": "ğŸ”„ GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire\nLes gÃ©nÃ©rateurs produisent des valeurs une par une, sans tout charger en mÃ©moire. Câ€™est essentiel en Data Engineering pour traiter des fichiers volumineux ou des flux de donnÃ©es.\n\n\nğŸ¤” ProblÃ¨me : charger tout en mÃ©moire\n# âŒ Charge TOUT le fichier en mÃ©moire\ndef lire_fichier_complet(chemin):\n    with open(chemin) as f:\n        return f.readlines()  # Liste de TOUTES les lignes\n\n# Si le fichier fait 10 Go... ğŸ’¥ MemoryError\nlignes = lire_fichier_complet(\"huge_file.csv\")\n\n\n\nâœ… Solution : gÃ©nÃ©rateur avec yield\n# âœ… Lit UNE ligne Ã  la fois\ndef lire_fichier_ligne_par_ligne(chemin):\n    with open(chemin) as f:\n        for ligne in f:\n            yield ligne.strip()  # Retourne et \"pause\"\n\n# Utilisation : ne charge qu'une ligne Ã  la fois\nfor ligne in lire_fichier_ligne_par_ligne(\"huge_file.csv\"):\n    process(ligne)  # Traite ligne par ligne\n\n\n\nğŸ¯ DiffÃ©rence return vs yield\n\n\n\n\n\n\n\nreturn\nyield\n\n\n\n\nRetourne une valeur et termine\nRetourne une valeur et pause\n\n\nFonction normale\nGÃ©nÃ©rateur\n\n\nTout en mÃ©moire\nUne valeur Ã  la fois\n\n\n\n# Fonction normale\ndef carres_liste(n):\n    result = []\n    for i in range(n):\n        result.append(i ** 2)\n    return result  # Retourne toute la liste\n\n# GÃ©nÃ©rateur\ndef carres_generateur(n):\n    for i in range(n):\n        yield i ** 2  # Retourne un par un\n\n# Test mÃ©moire\nimport sys\nliste = carres_liste(1000000)\ngen = carres_generateur(1000000)\n\nprint(sys.getsizeof(liste))  # ~8 Mo\nprint(sys.getsizeof(gen))    # ~200 octets !\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\n\nLire un CSV volumineux\ndef lire_csv_en_chunks(chemin, chunk_size=1000):\n    \"\"\"Lit un CSV par lots de chunk_size lignes.\"\"\"\n    with open(chemin) as f:\n        header = next(f).strip().split(\",\")\n        chunk = []\n        \n        for ligne in f:\n            valeurs = ligne.strip().split(\",\")\n            row = dict(zip(header, valeurs))\n            chunk.append(row)\n            \n            if len(chunk) &gt;= chunk_size:\n                yield chunk\n                chunk = []\n        \n        if chunk:  # Dernier lot\n            yield chunk\n\n# Utilisation\nfor batch in lire_csv_en_chunks(\"users.csv\", chunk_size=500):\n    print(f\"Traitement de {len(batch)} lignes...\")\n    # insert_to_database(batch)\n\n\nPipeline de transformation\ndef extraire(source):\n    for item in source:\n        yield item\n\ndef transformer(items):\n    for item in items:\n        item[\"nom\"] = item[\"nom\"].upper()\n        yield item\n\ndef filtrer(items, condition):\n    for item in items:\n        if condition(item):\n            yield item\n\n# Pipeline chaÃ®nÃ© â€” Ã©valuation paresseuse (lazy)\nsource = [{\"nom\": \"alice\"}, {\"nom\": \"bob\"}, {\"nom\": \"charlie\"}]\npipeline = filtrer(\n    transformer(extraire(source)),\n    lambda x: len(x[\"nom\"]) &gt; 3\n)\n\nfor item in pipeline:\n    print(item)  # {'nom': 'ALICE'}, {'nom': 'CHARLIE'}\n\n\n\n\nğŸš€ Generator Expressions (syntaxe courte)\n# List comprehension â€” crÃ©e une liste en mÃ©moire\ncarres_liste = [x**2 for x in range(1000000)]\n\n# Generator expression â€” crÃ©e un gÃ©nÃ©rateur\ncarres_gen = (x**2 for x in range(1000000))  # ParenthÃ¨ses !\n\n# Utile pour les agrÃ©gations\ntotal = sum(x**2 for x in range(1000000))  # Efficient",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ–¥ï¸ CrÃ©er et exÃ©cuter un script Python (.py)",
    "text": "ğŸ–¥ï¸ CrÃ©er et exÃ©cuter un script Python (.py)\nJusquâ€™ici, tu as travaillÃ© dans un Notebook Jupyter. Mais en Data Engineering, on Ã©crit surtout des scripts .py qui sâ€™exÃ©cutent depuis le terminal ou sont orchestrÃ©s par Airflow, cron, etc.\n\n\nğŸ““ Notebook vs Script â€” Quand utiliser quoi ?\n\n\n\nCritÃ¨re\nğŸ““ Notebook (.ipynb)\nğŸ“„ Script (.py)\n\n\n\n\nExploration\nâœ… IdÃ©al\nâŒ Pas adaptÃ©\n\n\nVisualisation\nâœ… Graphiques inline\nâš ï¸ Doit sauvegarder\n\n\nProduction\nâŒ Difficile Ã  orchestrer\nâœ… Standard\n\n\nTests unitaires\nâŒ CompliquÃ©\nâœ… Facile avec pytest\n\n\nGit / Code review\nâš ï¸ Diffs illisibles\nâœ… Diffs propres\n\n\nCI/CD\nâŒ Pas adaptÃ©\nâœ… Standard\n\n\nAirflow / Orchestration\nâš ï¸ Possible mais pas idÃ©al\nâœ… Natif\n\n\n\n\nğŸ’¡ RÃ¨gle : Notebook pour explorer, Script pour produire.\n\n\n\n\nğŸ“ CrÃ©er ton premier script\n\nOuvre VS Code\nCrÃ©e un nouveau fichier : File &gt; New File\nSauvegarde-le avec lâ€™extension .py : mon_script.py\n\n\nExemple : hello.py\n# hello.py\nprint(\"Hello, Data Engineer!\")\n\n\n\n\nâ–¶ï¸ ExÃ©cuter depuis le terminal\n# Se placer dans le dossier du script\ncd /chemin/vers/mon/projet\n\n# ExÃ©cuter le script\npython hello.py\n# ou\npython3 hello.py\nRÃ©sultat :\nHello, Data Engineer!\n\n\n\nğŸ¯ Structure standard : if __name__ == \"__main__\"\nCâ€™est LA structure que tu verras dans tous les scripts Python professionnels.\n# etl_simple.py\n\ndef extract():\n    \"\"\"Extrait les donnÃ©es.\"\"\"\n    print(\"ğŸ“¥ Extraction...\")\n    return [{\"id\": 1, \"nom\": \"Alice\"}, {\"id\": 2, \"nom\": \"Bob\"}]\n\ndef transform(data):\n    \"\"\"Transforme les donnÃ©es.\"\"\"\n    print(\"ğŸ”„ Transformation...\")\n    return [{**d, \"nom\": d[\"nom\"].upper()} for d in data]\n\ndef load(data):\n    \"\"\"Charge les donnÃ©es.\"\"\"\n    print(\"ğŸ’¾ Chargement...\")\n    for row in data:\n        print(f\"  InsÃ©rÃ© : {row}\")\n\ndef main():\n    \"\"\"Point d'entrÃ©e principal.\"\"\"\n    print(\"ğŸš€ DÃ©marrage du pipeline ETL\")\n    \n    data = extract()\n    data = transform(data)\n    load(data)\n    \n    print(\"âœ… Pipeline terminÃ© !\")\n\n# Ce bloc s'exÃ©cute SEULEMENT si le script est lancÃ© directement\nif __name__ == \"__main__\":\n    main()\n\nPourquoi if __name__ == \"__main__\" ?\n\n\n\nSituation\n__name__ vaut\nLe bloc sâ€™exÃ©cute ?\n\n\n\n\npython etl_simple.py\n\"__main__\"\nâœ… Oui\n\n\nimport etl_simple\n\"etl_simple\"\nâŒ Non\n\n\n\nCela permet de : - RÃ©utiliser les fonctions dans dâ€™autres scripts (import etl_simple) - ExÃ©cuter le script directement (python etl_simple.py)\n\n\n\n\nğŸ“Œ Passer des arguments au script\n\nMÃ©thode simple : sys.argv\n# script_args.py\nimport sys\n\ndef main():\n    print(f\"Arguments reÃ§us : {sys.argv}\")\n    \n    # sys.argv[0] = nom du script\n    # sys.argv[1], sys.argv[2], ... = arguments\n    \n    if len(sys.argv) &lt; 2:\n        print(\"Usage : python script_args.py &lt;fichier&gt;\")\n        sys.exit(1)\n    \n    fichier = sys.argv[1]\n    print(f\"Traitement de : {fichier}\")\n\nif __name__ == \"__main__\":\n    main()\npython script_args.py data.csv\n# Arguments reÃ§us : ['script_args.py', 'data.csv']\n# Traitement de : data.csv\n\n\nMÃ©thode pro : argparse\n# etl_avec_args.py\nimport argparse\n\ndef main():\n    # CrÃ©er le parser\n    parser = argparse.ArgumentParser(\n        description=\"Pipeline ETL simple\"\n    )\n    \n    # DÃ©finir les arguments\n    parser.add_argument(\n        \"--input\", \"-i\",\n        required=True,\n        help=\"Fichier d'entrÃ©e (CSV)\"\n    )\n    parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.csv\",\n        help=\"Fichier de sortie (dÃ©faut: output.csv)\"\n    )\n    parser.add_argument(\n        \"--verbose\", \"-v\",\n        action=\"store_true\",\n        help=\"Mode verbeux\"\n    )\n    \n    # Parser les arguments\n    args = parser.parse_args()\n    \n    # Utiliser les arguments\n    print(f\"Input  : {args.input}\")\n    print(f\"Output : {args.output}\")\n    print(f\"Verbose: {args.verbose}\")\n\nif __name__ == \"__main__\":\n    main()\n# Afficher l'aide\npython etl_avec_args.py --help\n\n# ExÃ©cuter avec arguments\npython etl_avec_args.py --input data.csv --output result.csv --verbose\npython etl_avec_args.py -i data.csv -o result.csv -v\n\n\n\n\nğŸ“ Structure dâ€™un projet Data Engineering\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â”œâ”€â”€ transform.py\nâ”‚   â””â”€â”€ load.py\nâ”œâ”€â”€ scripts/\nâ”‚   â””â”€â”€ run_etl.py      â† Point d'entrÃ©e\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ data/               â† âš ï¸ Dans .gitignore\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ .gitignore\nâ””â”€â”€ README.md\n\n\n\nğŸ§ª Exercice pratique\nCrÃ©er un script compter_lignes.py qui : 1. Prend un fichier en argument 2. Compte le nombre de lignes 3. Affiche le rÃ©sultat\n\n\nğŸ’¡ Solution\n\n# compter_lignes.py\nimport argparse\nfrom pathlib import Path\n\ndef compter_lignes(chemin: str) -&gt; int:\n    \"\"\"Compte les lignes d'un fichier.\"\"\"\n    path = Path(chemin)\n    if not path.exists():\n        raise FileNotFoundError(f\"Fichier non trouvÃ© : {chemin}\")\n    \n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return sum(1 for _ in f)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Compte les lignes d'un fichier\")\n    parser.add_argument(\"fichier\", help=\"Chemin du fichier Ã  analyser\")\n    args = parser.parse_args()\n    \n    try:\n        nb_lignes = compter_lignes(args.fichier)\n        print(f\"Le fichier '{args.fichier}' contient {nb_lignes} lignes.\")\n    except FileNotFoundError as e:\n        print(f\"Erreur : {e}\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()\npython compter_lignes.py mon_fichier.csv",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” Context Managers â€” GÃ©rer les ressources proprement",
    "text": "ğŸ” Context Managers â€” GÃ©rer les ressources proprement\nTu as dÃ©jÃ  utilisÃ© with open(...) pour les fichiers. Câ€™est un context manager ! Ce pattern garantit que les ressources sont toujours libÃ©rÃ©es, mÃªme en cas dâ€™erreur.\n\n\nğŸ¤” Le problÃ¨me sans context manager\n# âŒ Risque : fichier non fermÃ© si erreur\nf = open(\"data.txt\", \"r\")\ndata = f.read()\n# Si une erreur se produit ici...\nprocess(data)  # Le fichier reste ouvert !\nf.close()\n# âœ… Avec context manager : fermeture garantie\nwith open(\"data.txt\", \"r\") as f:\n    data = f.read()\n    process(data)\n# Fichier automatiquement fermÃ©, mÃªme en cas d'erreur\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\n\n\n\nRessource\nPourquoi un context manager\n\n\n\n\nFichiers\nFermer aprÃ¨s lecture/Ã©criture\n\n\nConnexions DB\nLibÃ©rer la connexion\n\n\nTransactions\nCommit ou rollback\n\n\nFichiers temporaires\nSupprimer aprÃ¨s usage\n\n\nVerrous (locks)\nLibÃ©rer le verrou\n\n\n\n\n\n\nğŸ› ï¸ CrÃ©er son propre context manager\n\nMÃ©thode 1 : avec une classe\nclass DatabaseConnection:\n    def __init__(self, host: str, database: str):\n        self.host = host\n        self.database = database\n        self.connection = None\n    \n    def __enter__(self):\n        \"\"\"AppelÃ© au dÃ©but du bloc 'with'.\"\"\"\n        print(f\"ğŸ”Œ Connexion Ã  {self.host}/{self.database}...\")\n        self.connection = f\"Connection to {self.database}\"  # Simule une connexion\n        return self.connection\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"AppelÃ© Ã  la fin du bloc 'with' (mÃªme si erreur).\"\"\"\n        print(f\"ğŸ”Œ Fermeture de la connexion...\")\n        self.connection = None\n        # Retourner False pour propager les exceptions\n        return False\n\n# Utilisation\nwith DatabaseConnection(\"localhost\", \"warehouse\") as conn:\n    print(f\"Connexion active : {conn}\")\n    # Faire des requÃªtes...\n# Connexion automatiquement fermÃ©e ici\n\n\nMÃ©thode 2 : avec @contextmanager (plus simple)\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(nom: str):\n    \"\"\"Mesure le temps d'exÃ©cution d'un bloc.\"\"\"\n    import time\n    start = time.time()\n    print(f\"â±ï¸ DÃ©but : {nom}\")\n    \n    yield  # Le bloc 'with' s'exÃ©cute ici\n    \n    elapsed = time.time() - start\n    print(f\"â±ï¸ Fin : {nom} ({elapsed:.2f}s)\")\n\n# Utilisation\nwith timer(\"Traitement ETL\"):\n    # Simuler un traitement long\n    import time\n    time.sleep(1)\n    print(\"Traitement en cours...\")\n\n# Output:\n# â±ï¸ DÃ©but : Traitement ETL\n# Traitement en cours...\n# â±ï¸ Fin : Traitement ETL (1.00s)\n\n\n\n\nğŸ¯ Exemple complet : Transaction DB\nfrom contextlib import contextmanager\n\n@contextmanager\ndef transaction(connection):\n    \"\"\"GÃ¨re une transaction avec commit/rollback automatique.\"\"\"\n    try:\n        print(\"ğŸ”„ DÃ©but de la transaction\")\n        yield connection\n        print(\"âœ… COMMIT\")\n        # connection.commit()\n    except Exception as e:\n        print(f\"âŒ ROLLBACK : {e}\")\n        # connection.rollback()\n        raise\n\n# Utilisation\nwith transaction(\"ma_connexion\") as conn:\n    print(\"Insertion de donnÃ©es...\")\n    # Si une erreur ici â†’ rollback automatique\n\n\n\nğŸ’¡ Context managers utiles de la stdlib\nfrom contextlib import suppress, redirect_stdout\nimport tempfile\n\n# Ignorer silencieusement une erreur\nwith suppress(FileNotFoundError):\n    os.remove(\"fichier_peut_etre_absent.txt\")\n\n# Fichier temporaire (supprimÃ© automatiquement)\nwith tempfile.NamedTemporaryFile(mode=\"w\", delete=True) as tmp:\n    tmp.write(\"donnÃ©es temporaires\")\n    print(f\"Fichier temp : {tmp.name}\")\n# Fichier supprimÃ© ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "10. Logging ğŸ§¾ â€” TraÃ§abilitÃ© indispensable en Data Engineering",
    "text": "10. Logging ğŸ§¾ â€” TraÃ§abilitÃ© indispensable en Data Engineering\nDans un vrai pipeline Data (ingestion, nettoyage, transformationâ€¦), on doit suivre ce quâ€™il se passe :\n\nFichier introuvable ?\nAPI trop lente ?\nFormat JSON invalide ?\nDonnÃ©es anormales ?\nETL en retard ?\n\nâ¡ï¸ print() ne suffit pas, car il ne permet ni filtrage, ni niveaux, ni logs dans un fichier.\nâ¡ï¸ Le module logging est le standard utilisÃ© en entreprise.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” 10.1 Pourquoi utiliser logging ?",
    "text": "ğŸ” 10.1 Pourquoi utiliser logging ?\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nğŸšï¸ Niveaux de log\nDEBUG, INFO, WARNING, ERROR, CRITICAL\n\n\nğŸ¯ Filtrage des messages\nOn peut afficher seulement WARNING+\n\n\nğŸ’¾ Logs dans un fichier\nIndispensable en production\n\n\nğŸ§© Standard Python\nCompatible Airflow, FastAPI, ETL, microservices\n\n\nğŸ§µ Thread-safe\nFonctionne mÃªme avec du multithreading",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”§ Configuration minimale recommandÃ©e",
    "text": "ğŸ”§ Configuration minimale recommandÃ©e\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\nExplications : - level=INFO â†’ DEBUG est ignorÃ© - %(asctime)s â†’ timestamp (important dans les pipelines) - %(levelname)s â†’ niveau (INFO, ERRORâ€¦) - %(message)s â†’ contenu du message",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§ª Exemple dâ€™utilisation",
    "text": "ğŸ§ª Exemple dâ€™utilisation\nlogging.debug(\"Message DEBUG (non affichÃ© en mode INFO)\")\nlogging.info(\"DÃ©marrage du mini-script\")\nlogging.warning(\"Attention : donnÃ©es manquantes\")\nlogging.error(\"Erreur : Ã©chec de connexion API\")\nlogging.critical(\"CRITIQUE : le pipeline doit s'arrÃªter !\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ 10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)",
    "text": "ğŸ“ 10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"pipeline.log\",\n    filemode=\"a\",  # append au lieu de rÃ©Ã©crire\n)\nâ¡ï¸ TrÃ¨s utilisÃ© dans : - scripts Airflow - traitements batch - pipelines de production",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§° 10.3 Exemple typique dans une fonction",
    "text": "ğŸ§° 10.3 Exemple typique dans une fonction\ndef charger_json(chemin: str) -&gt; dict | None:\n    logging.info(f\"Chargement du fichier : {chemin}\")\n    try:\n        with open(chemin, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except FileNotFoundError:\n        logging.error(f\"Fichier introuvable : {chemin}\")\n    except json.JSONDecodeError:\n        logging.error(\"JSON invalide\")\n    return None",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nğŸ’¡ Pourquoi\nâœ… Bonne pratique\n\n\n\n\nUtiliser print() partout\nimpossible Ã  filtrer/logguer\nUtiliser logging.info()\n\n\nAppeler logging.basicConfig() plusieurs fois\nne fonctionne que la 1Ã¨re fois\nConfigurer un seul logger global\n\n\nLogger des donnÃ©es sensibles\nfuite de secrets/mots de passe\nFiltrer/masquer les champs sensibles\n\n\nLogs trop verbeux\nralentissent les pipelines\nUtiliser DEBUG seulement en dev\n\n\nLogs insuffisants\ndifficile de diagnostiquer\nLogger les erreurs + contexte",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce pro : Logger dans la console et dans un fichier",
    "text": "â­ Astuce pro : Logger dans la console et dans un fichier\nlogger = logging.getLogger(\"pipeline\")\nlogger.setLevel(logging.INFO)\n\n# Handler console\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\n# Handler fichier\nfile = logging.FileHandler(\"pipeline.log\")\nfile.setLevel(logging.INFO)\n\n# Format\nfmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nconsole.setFormatter(fmt)\nfile.setFormatter(fmt)\n\n# Ajouter handlers\nlogger.addHandler(console)\nlogger.addHandler(file)\n\nlogger.info(\"Pipeline dÃ©marrÃ©\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Ã€ retenir",
    "text": "ğŸ“ Ã€ retenir\n\nlogging = indispensable en Data Engineering\n\nToujours configurer : niveau + format\n\nUtiliser un fichier log en production\n\nJamais de print() dans un vrai pipeline\n\nBien choisir le niveau (INFO, WARNING, ERRORâ€¦)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "11. Exercices pratiques ğŸ§ª",
    "text": "11. Exercices pratiques ğŸ§ª\nEssaie de rÃ©soudre ces exercices avant dâ€™afficher les corrections.\n\nExercice 1 â€“ Validation dâ€™Ã¢ge\nÃ‰crire une fonction est_majeur(age: int) -&gt; bool qui : - retourne True si age est supÃ©rieur ou Ã©gal Ã  18 ; - sinon retourne False.\n# Ã€ toi de jouer\ndef est_majeur(age: int) -&gt; bool:\n    # TODO: complÃ©ter\n    pass\n\nprint(est_majeur(15))  # attendu: False\nprint(est_majeur(18))  # attendu: True\n\n\nAfficher une solution possible\n\ndef est_majeur(age: int) -&gt; bool:\n    return age &gt;= 18\n\nprint(est_majeur(15))  # False\nprint(est_majeur(18))  # True\n\n\n\nExercice 2 â€“ Compter les posts par utilisateur\nOn dispose dâ€™une liste de dictionnaires reprÃ©sentant des posts :\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\nÃ‰crire une fonction compter_posts_par_utilisateur(posts) qui retourne :\n{\"alice\": 2, \"bob\": 1}\n# Ã€ toi de jouer\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    # TODO: complÃ©ter\n    result = {}\n    return result\n\nprint(compter_posts_par_utilisateur(posts))\n\n\nAfficher une solution possible\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    result = {}\n    for p in posts:\n        user = p[\"user\"]\n        if user not in result:\n            result[user] = 0\n        result[user] += 1\n    return result\n\nprint(compter_posts_par_utilisateur(posts))  # {'alice': 2, 'bob': 1}\n\n\n\nExercice 3 â€“ Classe Post\nCrÃ©er une classe Post avec : - attributs : auteur (str), texte (str) ; - mÃ©thode longueur() qui retourne la longueur du texte.\n# Ã€ toi de jouer\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        # TODO: stocker les attributs\n        pass\n\n    def longueur(self) -&gt; int:\n        # TODO: retourner la longueur du texte\n        return 0\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())  # attendu: longueur de la phrase\n\n\nAfficher une solution possible\n\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        self.auteur = auteur\n        self.texte = texte\n\n    def longueur(self) -&gt; int:\n        return len(self.texte)\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§ª Introduction aux tests avec pytest",
    "text": "ğŸ§ª Introduction aux tests avec pytest\nTester son code est essentiel en Data Engineering. Un bug dans un pipeline peut corrompre des millions de lignes de donnÃ©es ! pytest est le framework de test standard en Python.\n\n\nğŸ“¦ Installation\npip install pytest\n\n\n\nğŸ¯ Premier test avec assert\nassert vÃ©rifie quâ€™une condition est vraie. Si elle est fausse â†’ erreur.\n# test_basics.py\n\ndef addition(a, b):\n    return a + b\n\ndef test_addition():\n    \"\"\"Test simple avec assert.\"\"\"\n    assert addition(2, 3) == 5\n    assert addition(0, 0) == 0\n    assert addition(-1, 1) == 0\n\ndef test_addition_floats():\n    \"\"\"Test avec des floats.\"\"\"\n    result = addition(0.1, 0.2)\n    assert abs(result - 0.3) &lt; 0.0001  # Comparaison floats\n# ExÃ©cuter les tests\npytest test_basics.py\n\n# Avec plus de dÃ©tails\npytest test_basics.py -v\n\n\n\nğŸ“Š Tester des fonctions Data Engineering\n# src/transform.py\ndef nettoyer_email(email: str) -&gt; str:\n    \"\"\"Nettoie un email : minuscules, strip.\"\"\"\n    if not email or \"@\" not in email:\n        raise ValueError(\"Email invalide\")\n    return email.strip().lower()\n\ndef filtrer_actifs(users: list[dict]) -&gt; list[dict]:\n    \"\"\"Garde uniquement les users actifs.\"\"\"\n    return [u for u in users if u.get(\"actif\", False)]\n# tests/test_transform.py\nimport pytest\nfrom src.transform import nettoyer_email, filtrer_actifs\n\nclass TestNettoyerEmail:\n    \"\"\"Tests pour nettoyer_email.\"\"\"\n    \n    def test_email_valide(self):\n        assert nettoyer_email(\"Alice@Test.COM\") == \"alice@test.com\"\n    \n    def test_email_avec_espaces(self):\n        assert nettoyer_email(\"  bob@test.com  \") == \"bob@test.com\"\n    \n    def test_email_invalide_sans_arobase(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"invalid_email\")\n    \n    def test_email_vide(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"\")\n\n\nclass TestFiltrerActifs:\n    \"\"\"Tests pour filtrer_actifs.\"\"\"\n    \n    def test_filtre_users_actifs(self):\n        users = [\n            {\"nom\": \"Alice\", \"actif\": True},\n            {\"nom\": \"Bob\", \"actif\": False},\n            {\"nom\": \"Charlie\", \"actif\": True}\n        ]\n        result = filtrer_actifs(users)\n        assert len(result) == 2\n        assert all(u[\"actif\"] for u in result)\n    \n    def test_liste_vide(self):\n        assert filtrer_actifs([]) == []\n    \n    def test_tous_inactifs(self):\n        users = [{\"nom\": \"X\", \"actif\": False}]\n        assert filtrer_actifs(users) == []\n\n\n\nğŸƒ ExÃ©cuter les tests\n# Tous les tests du dossier\npytest\n\n# Un fichier spÃ©cifique\npytest tests/test_transform.py\n\n# Une classe spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail\n\n# Un test spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail::test_email_valide\n\n# Avec couverture de code\npip install pytest-cov\npytest --cov=src\n\n\n\nğŸ“ Structure recommandÃ©e\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â””â”€â”€ transform.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ test_extract.py\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ pytest.ini          # Configuration pytest (optionnel)\nâ””â”€â”€ requirements.txt\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nPratique\nExplication\n\n\n\n\nNommer test_*.py\npytest les dÃ©tecte automatiquement\n\n\nUn assert par test\nPlus facile Ã  dÃ©buguer\n\n\nTester les cas limites\nListes vides, None, valeurs nÃ©gatives\n\n\nTester les erreurs\npytest.raises(Exception)\n\n\nExÃ©cuter avant chaque commit\nÃ‰vite les rÃ©gressions",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "12. Quiz final ğŸ¯",
    "text": "12. Quiz final ğŸ¯\nTeste tes connaissances ! RÃ©ponds mentalement puis vÃ©rifie les rÃ©ponses.\n\n\nâ“ Q1. Quel type correspond Ã  une chaÃ®ne de caractÃ¨res ?\n\nstr\n\ntext\n\nchar\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” str est le type pour les chaÃ®nes de caractÃ¨res.\n\n\n\n\nâ“ Q2. Quelle structure est la plus adaptÃ©e pour reprÃ©senter un objet JSON ?\n\nlist\n\ndict\n\ntuple\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” dict avec des paires clÃ©/valeur, comme JSON.\n\n\n\n\nâ“ Q3. Laquelle de ces boucles risque le plus de devenir infinie ?\n\nfor\n\nwhile\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” while continue tant que la condition est vraie.\n\n\n\n\nâ“ Q4. Quel mot-clÃ© permet de gÃ©rer une erreur ?\n\nerror\n\nexcept\n\ncatch\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” try/except pour gÃ©rer les exceptions.\n\n\n\n\nâ“ Q5. Quel module est utilisÃ© pour le logging ?\n\nlogs\n\nlogging\n\nlogger\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” import logging\n\n\n\n\nâ“ Q6. Comment crÃ©er un environnement virtuel avec venv ?\n\npython -m venv mon_env\n\npip create venv mon_env\n\npython --venv mon_env\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” python -m venv nom_environnement\n\n\n\n\nâ“ Q7. Quel fichier liste les dÃ©pendances dâ€™un projet Python ?\n\npackages.txt\n\nrequirements.txt\n\ndependencies.json\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” requirements.txt avec pip freeze",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Documentation officielle\n\nPython.org Documentation â€” RÃ©fÃ©rence complÃ¨te\nPython Tutorial â€” Tutoriel officiel\n\n\n\nğŸ“ Cours et tutoriels\n\nReal Python â€” Tutoriels de qualitÃ©\nPython for Data Engineering (DataCamp) â€” Cours interactifs\nAutomate the Boring Stuff â€” Livre gratuit\n\n\n\nğŸ› ï¸ Outils recommandÃ©s\n\nPyPI â€” Repository de packages Python\nRuff â€” Linter ultra-rapide\nBlack â€” Formateur de code\nmypy â€” VÃ©rification des types\n\n\n\nğŸ“Š Packages Data Engineering essentiels\n\n\n\nPackage\nUsage\n\n\n\n\npandas\nManipulation de donnÃ©es tabulaires\n\n\nnumpy\nCalcul numÃ©rique\n\n\nrequests\nAppels API HTTP\n\n\nsqlalchemy\nORM et connexions bases de donnÃ©es\n\n\npydantic\nValidation de donnÃ©es\n\n\npytest\nTests unitaires\n\n\nclick\nCLI (Command Line Interface)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les bases de Python, passons au traitement de donnÃ©es !\nğŸ‘‰ Module suivant : 05_python_data_processing_for_data_engineers.ipynb â€” Pandas, Matplotlib, Seaborn et ETL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Basics pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html",
    "href": "notebooks/beginner/03_git_for_data_engineers.html",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "",
    "text": "Objectif : Comprendre Git, GitHub et GitLab, savoir versionner ses scripts, notebooks et configurations.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 02_bash_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal\n\n\nâœ… Requis\nAvoir un compte GitHub (gratuit) â€” voir section ci-dessous",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les concepts de versioning\nInitialiser et configurer un dÃ©pÃ´t Git\nMaÃ®triser le workflow : add â†’ commit â†’ push\nTravailler avec les branches\nCollaborer efficacement avec une Ã©quipe\nUtiliser .gitignore pour protÃ©ger les donnÃ©es sensibles",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ§° Pourquoi Git est essentiel pour un Data Engineer ?",
    "text": "ğŸ§° Pourquoi Git est essentiel pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nVersionner les pipelines\nSuivre lâ€™Ã©volution de tes scripts ETL\n\n\nCollaborer en Ã©quipe\nTravailler Ã  plusieurs sur le mÃªme projet data\n\n\nRevenir en arriÃ¨re\nRestaurer une version qui fonctionnait aprÃ¨s un bug\n\n\nCode review\nValider les modifications avant mise en production\n\n\nCI/CD\nDÃ©clencher automatiquement des tests et dÃ©ploiements\n\n\nDocumentation\nHistorique complet de qui a fait quoi et pourquoi\n\n\n\n\nğŸ’¡ En bref : Git est le systÃ¨me nerveux de tout projet data moderne. Sans Git, pas de collaboration efficace ni de traÃ§abilitÃ©.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab",
    "text": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab\n\nğŸ§© Quâ€™est-ce que Git ?\nGit est un logiciel de gestion de versions distribuÃ©. Il permet de :\n\nSuivre lâ€™Ã©volution de vos fichiers au fil du temps\nCrÃ©er des points de restauration (commits)\nTravailler en parallÃ¨le sur diffÃ©rentes fonctionnalitÃ©s (branches)\nFusionner le travail de plusieurs personnes\n\nğŸ“ Ton projet\n    â”‚\n    â”œâ”€â”€ ğŸ“„ pipeline.py      â† VersionnÃ© par Git\n    â”œâ”€â”€ ğŸ“„ config.yaml      â† VersionnÃ© par Git  \n    â”œâ”€â”€ ğŸ“ .git/            â† Dossier cachÃ© contenant l'historique\n    â””â”€â”€ ğŸ“ data/            â† âš ï¸ Ã€ NE PAS versionner !\n\n\nâ˜ï¸ Quâ€™est-ce que GitHub ?\nGitHub est une plateforme cloud (propriÃ©tÃ© de Microsoft) qui hÃ©berge vos dÃ©pÃ´ts Git.\n\nInterface web moderne\nTrÃ¨s populaire pour lâ€™open source\nGitHub Actions pour la CI/CD\n\n\n\nğŸ—ï¸ Quâ€™est-ce que GitLab ?\nGitLab est une alternative open source Ã  GitHub :\n\nPeut Ãªtre auto-hÃ©bergÃ© (on-premise)\nCI/CD intÃ©grÃ© trÃ¨s puissant\nSouvent utilisÃ© en entreprise",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif",
    "text": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nGitHub\nGitLab\n\n\n\n\nğŸ  HÃ©bergement\nCloud (Microsoft)\nCloud ou auto-hÃ©bergÃ©\n\n\nâš™ï¸ CI/CD\nGitHub Actions\nGitLab CI/CD (intÃ©grÃ©)\n\n\nğŸ‘¥ CommunautÃ©\nTrÃ¨s vaste (open source)\nOrientÃ© entreprise\n\n\nğŸ¨ Interface\nModerne, simple\nComplÃ¨te, personnalisable\n\n\nğŸ’° Prix\nGratuit + plans payants\nGratuit + plans payants\n\n\nğŸ”’ SÃ©curitÃ©\nDÃ©pend du plan\nPeut Ãªtre auto-hÃ©bergÃ©\n\n\nğŸ“Š Usage recommandÃ©\nProjets publics, open source\nProjets internes, entreprise\n\n\n\n\nğŸ’¡ Pour ce cours, tu peux utiliser lâ€™un ou lâ€™autre. Les commandes Git sont identiques !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ†“ CrÃ©er un compte GitHub gratuit",
    "text": "ğŸ†“ CrÃ©er un compte GitHub gratuit\n\nğŸ“ Ã‰tape 1 : Inscription\n\nAller sur github.com\nCliquer sur â€œSign upâ€ (en haut Ã  droite)\nRemplir le formulaire :\n\nEmail : ton adresse email\nPassword : un mot de passe fort\nUsername : ton pseudo (visible publiquement)\n\nRÃ©soudre le puzzle de vÃ©rification\nValider lâ€™email (vÃ©rifier ta boÃ®te mail)\n\nâœ… Câ€™est gratuit ! Le plan gratuit inclut : - DÃ©pÃ´ts publics illimitÃ©s - DÃ©pÃ´ts privÃ©s illimitÃ©s - 500 Mo de stockage pour GitHub Packages - 2000 minutes/mois de GitHub Actions\n\n\n\nğŸ” Ã‰tape 2 : Configurer lâ€™authentification\nDepuis 2021, GitHub nâ€™accepte plus les mots de passe pour git push. Tu dois utiliser :\n\n\n\nMÃ©thode\nDifficultÃ©\nRecommandation\n\n\n\n\nHTTPS + Token\nâ­ Facile\nâœ… Pour dÃ©buter\n\n\nSSH\nâ­â­ Moyen\nâœ… Pour usage rÃ©gulier\n\n\n\n\n\n\nğŸ”‘ Option A : HTTPS + Personal Access Token (PAT)\nCrÃ©er un token :\n\nConnecte-toi sur github.com\nClique sur ta photo de profil â†’ Settings\nDans le menu gauche, descends jusquâ€™Ã  Developer settings\nClique sur Personal access tokens â†’ Tokens (classic)\nClique sur Generate new token â†’ Generate new token (classic)\nConfigure le token :\n\nNote : git-access (ou un nom descriptif)\nExpiration : 90 days (ou plus)\nScopes : cocher repo (accÃ¨s complet aux dÃ©pÃ´ts)\n\nClique sur Generate token\nâš ï¸ COPIE LE TOKEN MAINTENANT â€” tu ne pourras plus le voir aprÃ¨s !\n\nUtiliser le token :\n# Quand Git demande ton mot de passe, colle le TOKEN (pas ton mot de passe !)\ngit push origin main\nUsername: ton-username\nPassword: ghp_xxxxxxxxxxxxxxxxxxxx   # â† Coller le token ici\nSauvegarder le token (optionnel) :\n# MÃ©moriser les credentials pour 1 heure\ngit config --global credential.helper cache\n\n# Ou mÃ©moriser indÃ©finiment (moins sÃ©curisÃ©)\ngit config --global credential.helper store\n\n\n\nğŸ” Option B : ClÃ© SSH (recommandÃ© pour usage rÃ©gulier)\n1. GÃ©nÃ©rer une clÃ© SSH :\n# GÃ©nÃ©rer une paire de clÃ©s (appuie sur EntrÃ©e pour les valeurs par dÃ©faut)\nssh-keygen -t ed25519 -C \"ton.email@exemple.com\"\n\n# DÃ©marrer l'agent SSH\neval \"$(ssh-agent -s)\"\n\n# Ajouter la clÃ© Ã  l'agent\nssh-add ~/.ssh/id_ed25519\n2. Ajouter la clÃ© Ã  GitHub :\n# Copier la clÃ© publique\ncat ~/.ssh/id_ed25519.pub\n# Copie le rÃ©sultat (commence par ssh-ed25519...)\n\nSur GitHub : Settings â†’ SSH and GPG keys â†’ New SSH key\nColler la clÃ© publique et sauvegarder\n\n3. Tester la connexion :\nssh -T git@github.com\n# RÃ©ponse attendue : \"Hi username! You've successfully authenticated...\"\n4. Utiliser SSH pour cloner :\n# Cloner avec SSH (au lieu de HTTPS)\ngit clone git@github.com:username/repo.git\n\n# Ou changer un dÃ©pÃ´t existant vers SSH\ngit remote set-url origin git@github.com:username/repo.git\n\n\n\nğŸ“¦ Ã‰tape 3 : CrÃ©er ton premier dÃ©pÃ´t sur GitHub\nVia lâ€™interface web :\n\nConnecte-toi sur github.com\nClique sur â€œ+â€ (en haut Ã  droite) â†’ New repository\nConfigure le dÃ©pÃ´t :\n\nRepository name : mon-projet-data\nDescription : â€œMon premier projet Data Engineeringâ€\nVisibility : Public ou Private\nâŒ Ne PAS cocher â€œAdd a README fileâ€ (on le fera localement)\n\nClique sur Create repository\nGitHub affiche les commandes Ã  exÃ©cuter â€” copie-les !\n\nLier ton projet local au dÃ©pÃ´t GitHub :\n# Si tu as dÃ©jÃ  un projet local avec des commits\ncd mon_projet_data\ngit remote add origin https://github.com/ton-username/mon-projet-data.git\ngit branch -M main\ngit push -u origin main\n# Ou cloner un dÃ©pÃ´t existant\ngit clone https://github.com/ton-username/mon-projet-data.git\ncd mon-projet-data\n\n\n\nğŸ“Š RÃ©capitulatif : Workflow complet\n1. CrÃ©er compte GitHub â”€â”€â”€â”€â–º github.com/signup\n                                    â”‚\n2. Configurer auth â”€â”€â”€â”€â”€â”€â”€â”€â–º Token (HTTPS) ou SSH\n                                    â”‚\n3. CrÃ©er dÃ©pÃ´t sur GitHub â”€â–º github.com â†’ New repository\n                                    â”‚\n4. Lier projet local â”€â”€â”€â”€â”€â”€â–º git remote add origin ...\n                                    â”‚\n5. Pousser le code â”€â”€â”€â”€â”€â”€â”€â”€â–º git push -u origin main\n                                    â”‚\n                              âœ… Code sur GitHub !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "3ï¸âƒ£ Installation et configuration de Git",
    "text": "3ï¸âƒ£ Installation et configuration de Git\n\nğŸ§° Installation\n\n\n\nSystÃ¨me\nCommande\n\n\n\n\nğŸªŸ Windows\nTÃ©lÃ©charger depuis git-scm.com\n\n\nğŸ macOS\nbrew install git\n\n\nğŸ§ Linux (Debian/Ubuntu)\nsudo apt install git\n\n\nğŸ§ Linux (Fedora)\nsudo dnf install git\n\n\n\n\n\nCode\n%%bash\n# VÃ©rifier la version installÃ©e\ngit --version\n\n# Configuration obligatoire (identitÃ©)\ngit config --global user.name \"Ton Nom\"\ngit config --global user.email \"ton.email@exemple.com\"\n\n# Configuration recommandÃ©e\ngit config --global init.defaultBranch main    # Branche par dÃ©faut\ngit config --global core.editor \"code --wait\"  # Ã‰diteur (VS Code)\ngit config --global pull.rebase false          # Merge par dÃ©faut lors du pull\n\n# VÃ©rifier la configuration\ngit config --list",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "4ï¸âƒ£ Comprendre le workflow Git",
    "text": "4ï¸âƒ£ Comprendre le workflow Git\n\nğŸ“Š Les 4 zones de Git\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git add     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Working Dir    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Staging Area   â”‚\nâ”‚  (tes fichiers) â”‚                â”‚  (index)        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                            â”‚ git commit\n                                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git push    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Remote         â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Local Repo     â”‚\nâ”‚  (GitHub/GitLab)â”‚                â”‚  (.git)         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ”„ Cycle de vie dâ€™un fichier\n\n\n\nÃ‰tat\nDescription\nCommande pour passer Ã  lâ€™Ã©tat suivant\n\n\n\n\nUntracked\nNouveau fichier, non suivi\ngit add &lt;fichier&gt;\n\n\nStaged\nPrÃªt Ã  Ãªtre commitÃ©\ngit commit -m \"message\"\n\n\nCommitted\nEnregistrÃ© localement\ngit push\n\n\nPushed\nEnvoyÃ© sur le serveur distant\nâœ… TerminÃ©",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ–¥ï¸ Clients Git (alternatives aux commandes)",
    "text": "ğŸ–¥ï¸ Clients Git (alternatives aux commandes)\nTu nâ€™es pas obligÃ© dâ€™utiliser le terminal ! Il existe des interfaces graphiques (GUI) pour Git qui facilitent la visualisation et certaines opÃ©rations.\n\nğŸ“± Clients populaires\n\n\n\nClient\nPlateforme\nPoints forts\nPrix\n\n\n\n\nGitHub Desktop\nWindows, Mac\nSimple, parfait pour dÃ©buter, intÃ©gration GitHub\nGratuit\n\n\nGitKraken\nWindows, Mac, Linux\nInterface visuelle puissante, graphe des branches\nGratuit (public)\n\n\nSourcetree\nWindows, Mac\nComplet, supporte Git et Mercurial\nGratuit\n\n\nVS Code\nTous\nGit intÃ©grÃ© + extension GitLens\nGratuit\n\n\nPyCharm / IntelliJ\nTous\nGit intÃ©grÃ© dans lâ€™IDE\nGratuit / Payant\n\n\n\n\n\nğŸ¤” Terminal vs GUI : quand utiliser quoi ?\n\n\n\n\n\n\n\nSituation\nRecommandation\n\n\n\n\nVisualiser lâ€™historique et les branches\nğŸ–¥ï¸ GUI â€” Plus clair visuellement\n\n\nRÃ©soudre des conflits de merge\nğŸ–¥ï¸ GUI â€” Comparaison cÃ´te Ã  cÃ´te\n\n\nOpÃ©rations quotidiennes (add, commit, push)\nâŒ¨ï¸ Terminal ou ğŸ–¥ï¸ GUI â€” Au choix\n\n\nScripts et automatisation (CI/CD)\nâŒ¨ï¸ Terminal â€” Obligatoire\n\n\nServeurs distants (SSH)\nâŒ¨ï¸ Terminal â€” Pas de GUI disponible\n\n\nApprendre Git en profondeur\nâŒ¨ï¸ Terminal â€” Comprendre ce qui se passe\n\n\n\n\nğŸ’¡ Conseil : Apprends dâ€™abord les commandes pour comprendre Git, puis utilise un client GUI pour gagner en productivitÃ© au quotidien.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "5ï¸âƒ£ Utilisation de Git pas Ã  pas",
    "text": "5ï¸âƒ£ Utilisation de Git pas Ã  pas\n\nÃ‰tape 1 : CrÃ©er un projet et initialiser Git\n\n\nCode\n%%bash\n# CrÃ©er un dossier de projet\nmkdir mon_projet_data\ncd mon_projet_data\n\n# Initialiser Git (crÃ©e le dossier .git)\ngit init\n\n# VÃ©rifier le statut\ngit status\n\n\n\n\nÃ‰tape 2 : Ajouter des fichiers et commiter\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un fichier Python\necho \"print('Hello Data Engineering!')\" &gt; main.py\n\n# Voir le statut (fichier untracked)\ngit status\n\n# Ajouter le fichier Ã  la staging area\ngit add main.py\n\n# Voir le statut (fichier staged)\ngit status\n\n# Commiter avec un message descriptif\ngit commit -m \"feat: ajouter script principal\"\n\n# Voir l'historique\ngit log --oneline\n\n\n\n\nğŸ“ Conventions de commits (Conventional Commits)\nUtilise des prÃ©fixes standardisÃ©s pour des messages clairs :\n\n\n\n\n\n\n\n\nPrÃ©fixe\nUsage\nExemple\n\n\n\n\nfeat:\nNouvelle fonctionnalitÃ©\nfeat: ajouter extraction API\n\n\nfix:\nCorrection de bug\nfix: corriger parsing dates\n\n\ndocs:\nDocumentation\ndocs: mettre Ã  jour README\n\n\nrefactor:\nRefactoring (sans changer le comportement)\nrefactor: simplifier fonction ETL\n\n\ntest:\nAjout/modification de tests\ntest: ajouter tests unitaires\n\n\nchore:\nMaintenance, config\nchore: mettre Ã  jour dÃ©pendances\n\n\n\nExemple de bon message :\nfeat: ajouter pipeline d'extraction des donnÃ©es clients\n\n- Connexion Ã  l'API CRM\n- Transformation des donnÃ©es JSON\n- Export en format Parquet",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers",
    "text": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers\nLe .gitignore indique Ã  Git quels fichiers NE PAS versionner.\n\nâš ï¸ Ne JAMAIS versionner :\n\nğŸ“Š Fichiers de donnÃ©es (CSV, Parquet, JSON volumineux)\nğŸ”‘ Secrets et credentials (mots de passe, clÃ©s API)\nğŸ“¦ DÃ©pendances (node_modules, venv)\nğŸ—‘ï¸ Fichiers temporaires (cache, logs)\n\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un .gitignore pour projet Data Engineering\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# ==== DONNÃ‰ES ====\n*.csv\n*.parquet\n*.json\n*.xlsx\ndata/\nraw/\nprocessed/\n\n# ==== SECRETS ====\n.env\n*.pem\n*.key\ncredentials.json\nsecrets.yaml\n\n# ==== PYTHON ====\n__pycache__/\n*.py[cod]\nvenv/\n.venv/\n*.egg-info/\n.pytest_cache/\n\n# ==== JUPYTER ====\n.ipynb_checkpoints/\n*.ipynb_checkpoints\n\n# ==== IDE ====\n.idea/\n.vscode/\n*.swp\n\n# ==== LOGS ====\n*.log\nlogs/\n\n# ==== OS ====\n.DS_Store\nThumbs.db\nEOF\n\necho \"âœ… .gitignore crÃ©Ã©\"\ncat .gitignore\n\n\n\n\nğŸ’¡ Astuces .gitignore\n# Ignorer un dossier\ndata/\n\n# Ignorer tous les .csv sauf un\n*.csv\n!schema.csv\n\n# Ignorer les fichiers dans tous les sous-dossiers\n**/*.log\n\n# VÃ©rifier ce qui est ignorÃ©\ngit status --ignored\n\nğŸ”— GÃ©nÃ©rateur de .gitignore : gitignore.io",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)",
    "text": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)\n\nÃ‰tape 1 : CrÃ©er un dÃ©pÃ´t sur GitHub/GitLab\n\nVa sur github.com ou gitlab.com\nClique sur â€œNew repositoryâ€ / â€œNew projectâ€\nDonne un nom (ex: mon_projet_data)\nNe coche PAS â€œInitialize with READMEâ€ (on a dÃ©jÃ  un repo local)\nCopie lâ€™URL HTTPS\n\n\n\nÃ‰tape 2 : Connecter le dÃ©pÃ´t local\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# Ajouter le dÃ©pÃ´t distant (remplace par ton URL)\ngit remote add origin https://github.com/ton-username/mon_projet_data.git\n\n# VÃ©rifier les remotes\ngit remote -v\n\n# S'assurer d'Ãªtre sur la branche main\ngit branch -M main\n\n# Pousser le code (premiÃ¨re fois : -u pour lier la branche)\ngit push -u origin main\n\n\n\n\nğŸ“¥ Cloner un projet existant\n# Cloner un dÃ©pÃ´t\ngit clone https://github.com/username/projet.git\n\n# Cloner dans un dossier spÃ©cifique\ngit clone https://github.com/username/projet.git mon_dossier\n\n\nğŸ”„ Synchroniser avec le distant\n# RÃ©cupÃ©rer les modifications (fetch + merge)\ngit pull\n\n# Voir les modifications distantes sans les appliquer\ngit fetch\ngit log origin/main --oneline",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "8ï¸âƒ£ Travailler avec les branches",
    "text": "8ï¸âƒ£ Travailler avec les branches\nLes branches permettent de travailler sur des fonctionnalitÃ©s en parallÃ¨le sans affecter le code principal.\nmain         â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—  (code stable)\n                  â”‚               â†‘\nfeature/etl       â””â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”˜     (nouvelle fonctionnalitÃ©)\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# Voir les branches existantes\ngit branch\n\n# CrÃ©er une nouvelle branche\ngit branch feature/add-etl\n\n# Basculer sur la branche\ngit switch feature/add-etl\n\n# OU crÃ©er + basculer en une commande\ngit switch -c feature/add-validation\n\n# Faire des modifications\necho \"def validate(df): pass\" &gt; validation.py\ngit add validation.py\ngit commit -m \"feat: ajouter module de validation\"\n\n# Revenir sur main\ngit switch main\n\n# Fusionner la branche\ngit merge feature/add-validation\n\n# Supprimer la branche fusionnÃ©e\ngit branch -d feature/add-validation\n\n\n\nğŸŒ¿ Workflow de branches recommandÃ© pour Data Engineers\nmain (production)\n  â”‚\n  â”œâ”€â”€ develop (intÃ©gration)\n  â”‚     â”‚\n  â”‚     â”œâ”€â”€ feature/etl-clients\n  â”‚     â”œâ”€â”€ feature/dashboard-ventes  \n  â”‚     â””â”€â”€ fix/bug-parsing-dates\n  â”‚\n  â””â”€â”€ hotfix/critical-fix (urgences)\n\n\n\nBranche\nUsage\n\n\n\n\nmain\nCode en production, toujours stable\n\n\ndevelop\nIntÃ©gration des features avant release\n\n\nfeature/*\nNouvelles fonctionnalitÃ©s\n\n\nfix/*\nCorrections de bugs\n\n\nhotfix/*\nCorrections urgentes en production",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "9ï¸âƒ£ RÃ©soudre les conflits de merge",
    "text": "9ï¸âƒ£ RÃ©soudre les conflits de merge\nUn conflit survient quand deux personnes modifient la mÃªme ligne.\n\nğŸ” Ã€ quoi ressemble un conflit ?\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef process_data(df):\n    return df.dropna()\n=======\ndef process_data(dataframe):\n    return dataframe.fillna(0)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/autre-branche\n\n\nâœ… Comment rÃ©soudre ?\n\nOuvrir le fichier et choisir la bonne version\nSupprimer les marqueurs (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nTester que le code fonctionne\nCommiter la rÃ©solution\n\n# AprÃ¨s avoir Ã©ditÃ© le fichier\ngit add fichier_resolu.py\ngit commit -m \"fix: rÃ©soudre conflit sur process_data\"\n\nğŸ’¡ Astuce : Utilise un outil visuel comme VS Code pour rÃ©soudre les conflits plus facilement.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ”§ Commandes utiles avancÃ©es",
    "text": "ğŸ”§ Commandes utiles avancÃ©es\n\nğŸ“¦ git stash â€” Mettre de cÃ´tÃ© temporairement\n# Sauvegarder les modifications en cours\ngit stash\n\n# Voir les stash\ngit stash list\n\n# RÃ©cupÃ©rer le dernier stash\ngit stash pop\n\n# RÃ©cupÃ©rer un stash spÃ©cifique\ngit stash apply stash@{0}\n\n\nâ†©ï¸ï¸ Annuler des changements\n# Annuler les modifications d'un fichier (non commitÃ©)\ngit checkout -- fichier.py\n\n# Retirer un fichier de la staging area\ngit reset HEAD fichier.py\n\n# Annuler le dernier commit (garde les fichiers)\ngit reset --soft HEAD~1\n\n# Annuler le dernier commit (supprime les fichiers)\ngit reset --hard HEAD~1  # âš ï¸ DANGEREUX\n\n# CrÃ©er un commit qui annule un commit prÃ©cÃ©dent\ngit revert &lt;commit-hash&gt;\n\n\nğŸ” Inspecter lâ€™historique\n# Historique compact\ngit log --oneline\n\n# Historique graphique\ngit log --oneline --graph --all\n\n# Voir les modifications d'un commit\ngit show &lt;commit-hash&gt;\n\n# Voir qui a modifiÃ© chaque ligne\ngit blame fichier.py\n\n# Chercher un commit par message\ngit log --grep=\"ETL\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“‹ Cheatsheet â€” Commandes essentielles",
    "text": "ğŸ“‹ Cheatsheet â€” Commandes essentielles\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nSetup\ngit init\nInitialiser un dÃ©pÃ´t\n\n\n\ngit clone &lt;url&gt;\nCloner un dÃ©pÃ´t distant\n\n\n\ngit config --global user.name\nConfigurer son nom\n\n\nBasique\ngit status\nVoir lâ€™Ã©tat des fichiers\n\n\n\ngit add &lt;fichier&gt;\nAjouter Ã  la staging area\n\n\n\ngit add .\nAjouter tous les fichiers\n\n\n\ngit commit -m \"msg\"\nEnregistrer les modifications\n\n\nHistorique\ngit log --oneline\nVoir lâ€™historique compact\n\n\n\ngit diff\nVoir les modifications\n\n\n\ngit blame &lt;fichier&gt;\nVoir qui a modifiÃ© quoi\n\n\nBranches\ngit branch\nLister les branches\n\n\n\ngit switch -c &lt;nom&gt;\nCrÃ©er et basculer\n\n\n\ngit merge &lt;branche&gt;\nFusionner une branche\n\n\n\ngit branch -d &lt;nom&gt;\nSupprimer une branche\n\n\nRemote\ngit remote add origin &lt;url&gt;\nAjouter un dÃ©pÃ´t distant\n\n\n\ngit push\nEnvoyer sur le serveur\n\n\n\ngit pull\nRÃ©cupÃ©rer du serveur\n\n\n\ngit fetch\nVÃ©rifier les changements\n\n\nAnnuler\ngit stash\nMettre de cÃ´tÃ©\n\n\n\ngit reset --soft HEAD~1\nAnnuler dernier commit\n\n\n\ngit revert &lt;hash&gt;\nCrÃ©er un commit dâ€™annulation\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Git Cheatsheet officiel (PDF)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Git est un outil de :\n\nDesign graphique\n\nGestion de versions\n\nStockage cloud\n\nDÃ©ploiement automatique\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Git est un systÃ¨me de gestion de versions distribuÃ©.\n\n\n\n\nâ“ Q2. Quelle commande initialise un dÃ©pÃ´t Git ?\n\ngit start\n\ngit init\n\ngit create\n\ngit new\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” git init crÃ©e un nouveau dÃ©pÃ´t Git.\n\n\n\n\nâ“ Q3. Quelle est la diffÃ©rence entre git commit et git push ?\n\ncommit enregistre localement, push envoie au serveur distant\n\ncommit supprime des fichiers\n\npush crÃ©e un dÃ©pÃ´t local\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” commit sauvegarde localement, push synchronise avec le serveur.\n\n\n\n\nâ“ Q4. Que faut-il mettre dans le .gitignore pour un projet Data ?\n\nLes fichiers Python\n\nLes fichiers de donnÃ©es (CSV, Parquet) et les secrets\n\nLe README\n\nTous les fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Ne jamais versionner les donnÃ©es volumineuses ni les credentials !\n\n\n\n\nâ“ Q5. Quelle commande crÃ©e une nouvelle branche et bascule dessus ?\n\ngit branch new\n\ngit create-branch nom\n\ngit switch -c nom\n\ngit branch -m nom\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” git switch -c nom crÃ©e et bascule sur la branche.\n\n\n\n\nâ“ Q6. Comment annuler le dernier commit tout en gardant les fichiers ?\n\ngit delete commit\n\ngit reset --hard HEAD~1\n\ngit reset --soft HEAD~1\n\ngit undo\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” --soft garde les fichiers, --hard les supprime.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ’» Exercice pratique",
    "text": "ğŸ’» Exercice pratique\n\nğŸ¯ Objectif\nCrÃ©er un projet Data Engineering versionnÃ© et le pousser sur GitHub/GitLab.\n\n\nğŸ“ Instructions\n\nCrÃ©er un dossier projet_etl\nInitialiser Git\nCrÃ©er un .gitignore appropriÃ©\nCrÃ©er un fichier etl.py avec un script simple\nCrÃ©er un fichier README.md\nFaire un premier commit\nCrÃ©er une branche feature/add-config\nAjouter un fichier config.yaml sur cette branche\nMerger dans main\nCrÃ©er un dÃ©pÃ´t sur GitHub et pousser le code\n\n\n\nâœ… Solution\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n# 1. CrÃ©er le dossier\nmkdir projet_etl && cd projet_etl\n\n# 2. Initialiser Git\ngit init\n\n# 3. CrÃ©er le .gitignore\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# DonnÃ©es\n*.csv\n*.parquet\ndata/\n\n# Secrets\n.env\ncredentials.json\n\n# Python\n__pycache__/\nvenv/\nEOF\n\n# 4. CrÃ©er le script ETL\ncat &lt;&lt; 'EOF' &gt; etl.py\n#!/usr/bin/env python3\n\"\"\"Simple ETL Pipeline\"\"\"\n\ndef extract():\n    print(\"ğŸ“¥ Extracting data...\")\n    return {\"data\": [1, 2, 3]}\n\ndef transform(data):\n    print(\"ğŸ”„ Transforming data...\")\n    return {\"data\": [x * 2 for x in data[\"data\"]]}\n\ndef load(data):\n    print(\"ğŸ’¾ Loading data...\")\n    print(f\"Result: {data}\")\n\nif __name__ == \"__main__\":\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n    print(\"âœ… ETL completed!\")\nEOF\n\n# 5. CrÃ©er le README\ncat &lt;&lt; 'EOF' &gt; README.md\n# Projet ETL\n\nUn pipeline ETL simple pour apprendre Git.\n\n## Usage\n\n```bash\npython etl.py\nEOF",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ® Apprendre en pratiquant\n\nLearn Git Branching â€” Tutoriel interactif visuel\nOh My Git! â€” Jeu pour apprendre Git\nGit Katas â€” Exercices pratiques\n\n\n\nğŸ“– Documentation\n\nPro Git Book â€” Livre gratuit (en franÃ§ais)\nGitHub Docs\nGitLab Docs\n\n\n\nğŸ› ï¸ Outils\n\nGitHub Desktop â€” Interface graphique\nGitKraken â€” Client Git visuel\nConventional Commits â€” Standard de messages",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "âœ… Conclusion",
    "text": "âœ… Conclusion\nTu sais maintenant :\n\nâœ… Ce quâ€™est Git, GitHub et GitLab\nâœ… Comment initialiser et configurer un projet\nâœ… Le workflow : add â†’ commit â†’ push\nâœ… Travailler avec les branches\nâœ… Utiliser .gitignore pour protÃ©ger les donnÃ©es sensibles\nâœ… RÃ©soudre les conflits de merge\nâœ… Les commandes avancÃ©es (stash, reset, revert)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu sais versionner ton code, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 04_python_basics_for_data_engineers.ipynb â€” Les fondamentaux de Python\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Git pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html",
    "href": "notebooks/beginner/06_intro_relational_databases.html",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "",
    "text": "Ce module prÃ©sente les concepts fondamentaux des bases de donnÃ©es relationnelles.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 05_python_data_processing\n\n\nâœ… Requis\nComprendre les structures de donnÃ©es (listes, dictionnaires)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "href": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Expliquer ce quâ€™est une base de donnÃ©es\nâœ… Comprendre le modÃ¨le relationnel (tables, colonnes, lignes)\nâœ… DÃ©finir les clÃ©s primaires et Ã©trangÃ¨res\nâœ… Identifier les types de relations (1-1, 1-N, N-N)\nâœ… Comprendre les principes de normalisation\nâœ… Expliquer les propriÃ©tÃ©s ACID\nâœ… DiffÃ©rencier OLTP et OLAP\nâœ… Expliquer ce quâ€™est un Data Warehouse et un Data Mart\nâœ… Comprendre la modÃ©lisation dimensionnelle (Star/Snowflake Schema)\nâœ… Distinguer tables de faits et tables de dimensions\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique SQL viendra au module suivant !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ¯ 1. Câ€™est quoi une Base de DonnÃ©es ?",
    "text": "ğŸ¯ 1. Câ€™est quoi une Base de DonnÃ©es ?\n\nğŸ“– DÃ©finition\nUne base de donnÃ©es est un systÃ¨me organisÃ© pour :\n\n\n\nFonction\nDescription\n\n\n\n\nğŸ’¾ Stocker\nConserver des informations de faÃ§on permanente\n\n\nğŸ” Rechercher\nRetrouver rapidement nâ€™importe quelle donnÃ©e\n\n\nâœï¸ Modifier\nMettre Ã  jour les informations\n\n\nğŸ”’ SÃ©curiser\nContrÃ´ler lâ€™accÃ¨s aux donnÃ©es sensibles\n\n\nğŸ”— Relier\nConnecter diffÃ©rentes informations entre elles\n\n\n\n\n\n\nğŸŒŸ En une phrase\n\nâ€œUne base de donnÃ©es, câ€™est comme un classeur numÃ©rique gÃ©ant, ultra-organisÃ© et intelligent, capable de retrouver nâ€™importe quelle information parmi des milliards de donnÃ©es.â€\n\n\n\n\nğŸ“ Fichiers vs Base de donnÃ©es\nPourquoi ne pas simplement utiliser des fichiers CSV ou Excel ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nFichiers (CSV, Excel)\nBase de donnÃ©es\n\n\n\n\nAccÃ¨s concurrent\nâŒ Conflits si plusieurs utilisateurs\nâœ… GÃ©rÃ© automatiquement\n\n\nVolume\nâŒ Lent au-delÃ  de ~100K lignes\nâœ… Millions/milliards de lignes\n\n\nIntÃ©gritÃ©\nâŒ Pas de validation\nâœ… Contraintes, types\n\n\nRelations\nâŒ Difficile Ã  gÃ©rer\nâœ… Jointures natives\n\n\nSÃ©curitÃ©\nâŒ Tout ou rien\nâœ… Permissions fines\n\n\nSauvegarde\nâŒ Manuelle\nâœ… Automatique",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ–¥ï¸ 2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es",
    "text": "ğŸ–¥ï¸ 2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es\nUn SGBD (ou DBMS en anglais) est le logiciel qui gÃ¨re la base de donnÃ©es.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        APPLICATION                          â”‚\nâ”‚                  (Python, Java, Web...)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ RequÃªtes SQL\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          SGBD                               â”‚\nâ”‚              (PostgreSQL, MySQL, Oracle...)                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\nâ”‚  â”‚   Parser    â”‚  â”‚  Optimizer  â”‚  â”‚   Engine    â”‚         â”‚\nâ”‚  â”‚   (SQL)     â”‚  â”‚  (requÃªtes) â”‚  â”‚  (stockage) â”‚         â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     FICHIERS DISQUE                         â”‚\nâ”‚                   (donnÃ©es stockÃ©es)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ† SGBD Relationnels populaires\n\n\n\n\n\n\n\n\n\nSGBD\nType\nPoints forts\nCas dâ€™usage\n\n\n\n\nPostgreSQL\nOpen source\nPuissant, extensible, SQL avancÃ©\nProduction, analytics\n\n\nMySQL\nOpen source\nSimple, rapide, trÃ¨s rÃ©pandu\nWeb, startups\n\n\nSQLite\nEmbarquÃ©\nLÃ©ger, fichier unique, zÃ©ro config\nMobile, prototypage\n\n\nOracle\nCommercial\nEntreprise, haute disponibilitÃ©\nBanques, grandes entreprises\n\n\nSQL Server\nCommercial\nIntÃ©gration Microsoft\nEntreprises Windows\n\n\n\n\nğŸ’¡ Pour ce cours, on utilisera PostgreSQL â€” le plus complet et gratuit.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“Š 3. Le ModÃ¨le Relationnel",
    "text": "ğŸ“Š 3. Le ModÃ¨le Relationnel\nLe modÃ¨le relationnel a Ã©tÃ© inventÃ© par Edgar F. Codd (IBM) en 1970. Câ€™est le modÃ¨le le plus utilisÃ© depuis 50 ans !\n\nğŸ§± Vocabulaire de base\nBASE DE DONNÃ‰ES : ma_boutique\nâ”‚\nâ”œâ”€â”€ TABLE : clients\nâ”‚   â”‚\nâ”‚   â”‚    COLONNES (attributs)\nâ”‚   â”‚    â†“      â†“         â†“\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   â”‚  â”‚ id  â”‚  nom   â”‚     email       â”‚  â† EN-TÃŠTE\nâ”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   â”‚  â”‚  1  â”‚ Alice  â”‚ alice@mail.com  â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  2  â”‚ Bob    â”‚ bob@mail.com    â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  3  â”‚ Charlieâ”‚ charlie@mail.comâ”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚   â”‚    â†‘\nâ”‚   â”‚    CELLULE (valeur)\nâ”‚\nâ”œâ”€â”€ TABLE : produits\nâ”‚   â””â”€â”€ ...\nâ”‚\nâ””â”€â”€ TABLE : commandes\n    â””â”€â”€ ...\n\n\n\nğŸ“š Terminologie\n\n\n\nTerme technique\nTerme courant\nDescription\n\n\n\n\nRelation\nTable\nEnsemble de donnÃ©es du mÃªme type\n\n\nTuple\nLigne / Enregistrement\nUne entrÃ©e (ex: un client)\n\n\nAttribut\nColonne / Champ\nUne propriÃ©tÃ© (ex: nom, email)\n\n\nDomaine\nType\nValeurs possibles (INTEGER, VARCHARâ€¦)\n\n\nSchÃ©ma\nStructure\nDÃ©finition des colonnes et types",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”¢ 4. Types de donnÃ©es",
    "text": "ğŸ”¢ 4. Types de donnÃ©es\nChaque colonne a un type qui dÃ©finit les valeurs autorisÃ©es.\n\nğŸ“‹ Types courants (PostgreSQL)\n\n\n\nCatÃ©gorie\nType\nDescription\nExemple\n\n\n\n\nEntiers\nINTEGER\nNombre entier\n42\n\n\n\nBIGINT\nGrand entier\n9223372036854775807\n\n\n\nSMALLINT\nPetit entier\n32767 max\n\n\nDÃ©cimaux\nDECIMAL(p,s)\nPrÃ©cision exacte\n19.99\n\n\n\nFLOAT\nApproximatif\n3.14159\n\n\nTexte\nVARCHAR(n)\nTexte variable (max n)\n'Alice'\n\n\n\nTEXT\nTexte illimitÃ©\n'Long texte...'\n\n\n\nCHAR(n)\nTexte fixe (n caractÃ¨res)\n'FR'\n\n\nBoolÃ©en\nBOOLEAN\nVrai/Faux\nTRUE, FALSE\n\n\nDate/Heure\nDATE\nDate seule\n'2024-01-15'\n\n\n\nTIMESTAMP\nDate + heure\n'2024-01-15 14:30:00'\n\n\n\nTIME\nHeure seule\n'14:30:00'\n\n\nAutres\nUUID\nIdentifiant unique\n'a0eebc99-9c0b...'\n\n\n\nJSON\nDonnÃ©es JSON\n'{\"key\": \"value\"}'\n\n\n\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nDECIMAL pour lâ€™argent\nFLOAT pour lâ€™argent (imprÃ©cis)\n\n\nVARCHAR(255) pour emails\nTEXT partout (pas de limite)\n\n\nDATE pour les dates\nVARCHAR pour les dates\n\n\nTypes les plus petits possibles\nTypes trop grands â€œau cas oÃ¹â€",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”‘ 5. ClÃ© Primaire (Primary Key)",
    "text": "ğŸ”‘ 5. ClÃ© Primaire (Primary Key)\nLa clÃ© primaire (PK) identifie de faÃ§on unique chaque ligne dâ€™une table.\n\nğŸ“ RÃ¨gles\n\n\n\nRÃ¨gle\nDescription\n\n\n\n\nUnique\nDeux lignes ne peuvent pas avoir la mÃªme valeur\n\n\nNon NULL\nLa valeur doit toujours Ãªtre prÃ©sente\n\n\nImmuable\nNe devrait jamais changer\n\n\n\n\n\n\nğŸ“Š Exemple\nTable : clients\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚      email      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚ alice@mail.com  â”‚\nâ”‚    2    â”‚  Bob     â”‚ bob@mail.com    â”‚\nâ”‚    3    â”‚  Charlie â”‚ charlie@mail.comâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â†‘\n     ClÃ© primaire : garantit l'unicitÃ©\n\n\n\nğŸ”¢ Types de clÃ©s primaires\n\n\n\n\n\n\n\n\nType\nDescription\nExemple\n\n\n\n\nAuto-incrÃ©mentÃ©e\nGÃ©nÃ©rÃ©e automatiquement (1, 2, 3â€¦)\nSERIAL en PostgreSQL\n\n\nUUID\nIdentifiant universel unique\na0eebc99-9c0b-4ef8...\n\n\nNaturelle\nDonnÃ©e existante unique\nNumÃ©ro de sÃ©curitÃ© sociale\n\n\nComposite\nPlusieurs colonnes combinÃ©es\n(pays, code_postal)\n\n\n\n\nğŸ’¡ Recommandation : Utiliser SERIAL (auto-increment) ou UUID plutÃ´t quâ€™une clÃ© naturelle.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”— 6. ClÃ© Ã‰trangÃ¨re (Foreign Key)",
    "text": "ğŸ”— 6. ClÃ© Ã‰trangÃ¨re (Foreign Key)\nLa clÃ© Ã©trangÃ¨re (FK) crÃ©e un lien entre deux tables.\n\nğŸ“ Principe\nTable : clients                    Table : commandes\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚            â”‚ id (PK) â”‚ client_id(FK)â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    1    â”‚      1      â”‚ Clavier â”‚\nâ”‚    2    â”‚  Bob     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    2    â”‚      2      â”‚ Souris  â”‚\nâ”‚    3    â”‚  Charlie â”‚            â”‚    3    â”‚      1      â”‚ Ã‰cran   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                              â†‘\n                                    ClÃ© Ã©trangÃ¨re : rÃ©fÃ©rence clients.id\n\n\n\nâœ… Ce que garantit la FK\n\n\n\n\n\n\n\nGarantie\nDescription\n\n\n\n\nIntÃ©gritÃ© rÃ©fÃ©rentielle\nImpossible de rÃ©fÃ©rencer un client inexistant\n\n\nCohÃ©rence\nSi on supprime un client, que faire des commandes ?\n\n\n\n\n\n\nğŸ—‘ï¸ Actions en cascade\nQue se passe-t-il si on supprime ou modifie la ligne rÃ©fÃ©rencÃ©e ?\n\n\n\nAction\nComportement\n\n\n\n\nCASCADE\nSupprime/modifie aussi les lignes liÃ©es\n\n\nSET NULL\nMet la FK Ã  NULL\n\n\nSET DEFAULT\nMet une valeur par dÃ©faut\n\n\nRESTRICT\nInterdit la suppression/modification\n\n\nNO ACTION\nComme RESTRICT (vÃ©rification diffÃ©rÃ©e)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”€ 7. Types de Relations",
    "text": "ğŸ”€ 7. Types de Relations\n\n1ï¸âƒ£ Relation Un-Ã -Un (1:1)\nUne ligne dans A correspond Ã  exactement une ligne dans B.\nutilisateurs              profils\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  email  â”‚         â”‚ id â”‚ user_id â”‚   bio     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ a@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1  â”‚    1    â”‚ Dev...    â”‚\nâ”‚ 2  â”‚ b@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2  â”‚    2    â”‚ Designer..â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : SÃ©parer des donnÃ©es rarement utilisÃ©es (optimisation).\n\n\n\n1ï¸âƒ£â¡ï¸ğŸ”¢ Relation Un-Ã -Plusieurs (1:N)\nUne ligne dans A peut correspondre Ã  plusieurs lignes dans B.\nclients                   commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚         â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚  Alice  â”‚â—„â”€â”€â”€â”€â”¬â”€â”€â”€â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚  Bob    â”‚â—„â”€â”€â” â””â”€â”€â”€â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : Client â†’ Commandes, Auteur â†’ Articles, Pays â†’ Villes.\n\n\n\nğŸ”¢â†”ï¸ï¸ğŸ”¢ Relation Plusieurs-Ã -Plusieurs (N:N)\nPlusieurs lignes dans A correspondent Ã  plusieurs lignes dans B.\nNÃ©cessite une table de jonction !\netudiants          inscriptions         cours\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  nom  â”‚    â”‚ etudiant_idâ”‚ cours_id â”‚    â”‚ id â”‚   nom   â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice â”‚â—„â”€â”€â”€â”‚     1      â”‚    1     â”‚â”€â”€â”€â–ºâ”‚ 1  â”‚  Maths  â”‚\nâ”‚ 2  â”‚ Bob   â”‚â—„â”€â”¬â”€â”‚     1      â”‚    2     â”‚â”€â”¬â”€â–ºâ”‚ 2  â”‚ Python  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚     2      â”‚    1     â”‚ â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â””â”€â”‚     2      â”‚    2     â”‚â”€â”˜\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                       Table de jonction\nCas dâ€™usage : Ã‰tudiants â†”ï¸ Cours, Produits â†”ï¸ Tags, Acteurs â†”ï¸ Films.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "href": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“ 8. Normalisation",
    "text": "ğŸ“ 8. Normalisation\nLa normalisation consiste Ã  organiser les donnÃ©es pour : - âŒ Ã‰viter la redondance (donnÃ©es dupliquÃ©es) - âŒ Ã‰viter les anomalies (incohÃ©rences lors de modifications) - âœ… Garantir lâ€™intÃ©gritÃ© des donnÃ©es\n\n\nâŒ Exemple NON normalisÃ©\nTable : commandes (MAUVAIS)\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚ client  â”‚ client_email  â”‚ produit â”‚    ville     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com   â”‚ Clavier â”‚    Paris     â”‚\nâ”‚ 2  â”‚ Alice   â”‚ alice@m.com   â”‚ Souris  â”‚    Paris     â”‚  â† Redondance !\nâ”‚ 3  â”‚ Bob     â”‚ bob@m.com     â”‚ Ã‰cran   â”‚    Lyon      â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ProblÃ¨mes :\n  - Si Alice change d'email â†’ modifier TOUTES les lignes\n  - Risque d'incohÃ©rence si on oublie une ligne\n  - Espace gaspillÃ©\n\n\n\nâœ… Exemple normalisÃ©\nTable : clients               Table : commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚    email    â”‚   â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com â”‚   â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚ Bob     â”‚ bob@m.com   â”‚   â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                                 â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… Avantages :\n  - Email modifiÃ© une seule fois\n  - Pas de redondance\n  - DonnÃ©es cohÃ©rentes\n\n\n\nğŸ“Š Formes normales (rÃ©sumÃ©)\n\n\n\nForme\nRÃ¨gle principale\n\n\n\n\n1NF\nChaque cellule contient une seule valeur (pas de listes)\n\n\n2NF\n1NF + chaque colonne dÃ©pend de TOUTE la clÃ© primaire\n\n\n3NF\n2NF + pas de dÃ©pendance entre colonnes non-clÃ©s\n\n\n\n\nğŸ’¡ En pratique, la 3NF est gÃ©nÃ©ralement suffisante pour les bases OLTP.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "href": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ” 9. PropriÃ©tÃ©s ACID",
    "text": "ğŸ” 9. PropriÃ©tÃ©s ACID\nACID garantit la fiabilitÃ© des transactions dans une base relationnelle.\n\nğŸ“Š Les 4 propriÃ©tÃ©s\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   TRANSACTION   â”‚\n                    â”‚   (ex: virement â”‚\n                    â”‚    bancaire)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ ATOMICITÃ‰ â”‚      â”‚ COHÃ‰RENCE â”‚      â”‚ ISOLATION â”‚\n   â”‚  Tout ou  â”‚      â”‚   Ã‰tat    â”‚      â”‚Transactionsâ”‚\n   â”‚   rien    â”‚      â”‚  valide   â”‚      â”‚ sÃ©parÃ©es  â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ DURABILITÃ‰â”‚\n                      â”‚ Permanent â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“‹ DÃ©tail de chaque propriÃ©tÃ©\n\n\n\n\n\n\n\n\n\nLettre\nPropriÃ©tÃ©\nDescription\nExemple\n\n\n\n\nA\nAtomicitÃ©\nTout ou rien â€” si une partie Ã©choue, tout est annulÃ©\nVirement : dÃ©bit ET crÃ©dit rÃ©ussissent ou rien\n\n\nC\nCohÃ©rence\nLa base reste dans un Ã©tat valide\nLe solde ne peut pas Ãªtre nÃ©gatif\n\n\nI\nIsolation\nLes transactions concurrentes ne sâ€™interfÃ¨rent pas\nDeux virements simultanÃ©s ne se mÃ©langent pas\n\n\nD\nDurabilitÃ©\nUne fois validÃ©e, la transaction est permanente\nMÃªme aprÃ¨s un crash, le virement est enregistrÃ©\n\n\n\n\n\n\nğŸ’° Exemple : Virement bancaire\nTRANSACTION : Virer 100â‚¬ de Alice vers Bob\n\n  1. DÃ©biter 100â‚¬ du compte Alice\n  2. CrÃ©diter 100â‚¬ sur le compte Bob\n\nATOMICITÃ‰ :\n  âœ… Les deux opÃ©rations rÃ©ussissent â†’ COMMIT\n  âŒ Une opÃ©ration Ã©choue â†’ ROLLBACK (rien ne change)\n\nCOHÃ‰RENCE :\n  âœ… Alice : 500â‚¬ â†’ 400â‚¬\n  âœ… Bob   : 200â‚¬ â†’ 300â‚¬\n  âœ… Total : 700â‚¬ â†’ 700â‚¬ (inchangÃ©)\n\nISOLATION :\n  Un autre virement simultanÃ© ne voit pas l'Ã©tat intermÃ©diaire\n\nDURABILITÃ‰ :\n  MÃªme si le serveur crash juste aprÃ¨s le COMMIT,\n  le virement sera toujours lÃ  au redÃ©marrage",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#oltp-vs-olap-deux-mondes-diffÃ©rents",
    "href": "notebooks/beginner/06_intro_relational_databases.html#oltp-vs-olap-deux-mondes-diffÃ©rents",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "âš¡ 10. OLTP vs OLAP â€” Deux mondes diffÃ©rents",
    "text": "âš¡ 10. OLTP vs OLAP â€” Deux mondes diffÃ©rents\nLes bases de donnÃ©es relationnelles peuvent servir Ã  deux usages trÃ¨s diffÃ©rents. Comprendre cette distinction est fondamental en Data Engineering.\n\n\nğŸ”„ 10.1 OLTP â€” Online Transaction Processing\nLes bases OLTP gÃ¨rent les opÃ©rations quotidiennes dâ€™une entreprise.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nBut\nGÃ©rer les transactions courantes\n\n\nOpÃ©rations\nINSERT, UPDATE, DELETE frÃ©quents\n\n\nRequÃªtes\nSimples, sur peu de lignes\n\n\nUtilisateurs\nApplications, employÃ©s\n\n\nVolume par requÃªte\nQuelques lignes\n\n\nPrioritÃ©\nRapiditÃ©, disponibilitÃ©\n\n\nSchÃ©ma\nNormalisÃ© (3NF)\n\n\n\nExemples : - Application e-commerce (commandes, paiements) - SystÃ¨me bancaire (virements, retraits) - Gestion de stock (entrÃ©es, sorties) - RÃ©servation de billets\n\n\n\nğŸ“Š 10.2 OLAP â€” Online Analytical Processing\nLes bases OLAP sont conÃ§ues pour lâ€™analyse de donnÃ©es et le reporting.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nBut\nAnalyser les donnÃ©es historiques\n\n\nOpÃ©rations\nSELECT complexes (agrÃ©gations, jointures)\n\n\nRequÃªtes\nComplexes, sur des millions de lignes\n\n\nUtilisateurs\nAnalystes, Data Scientists, dirigeants\n\n\nVolume par requÃªte\nDes millions/milliards de lignes\n\n\nPrioritÃ©\nPerformance analytique\n\n\nSchÃ©ma\nDÃ©normalisÃ© (Star Schema, Snowflake)\n\n\n\nExemples : - Rapport des ventes par rÃ©gion/mois - Analyse du comportement client - Tableaux de bord (dashboards) - PrÃ©visions et tendances\n\n\n\nâš–ï¸ 10.3 Comparaison OLTP vs OLAP\n           OLTP                              OLAP\n    (Transactionnel)                    (Analytique)\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚  INSERT   â”‚                      â”‚  SELECT   â”‚\n    â”‚  UPDATE   â”‚                      â”‚  GROUP BY â”‚\n    â”‚  DELETE   â”‚                      â”‚  JOIN     â”‚\n    â”‚  (CRUD)   â”‚                      â”‚  (Analyse)â”‚\n    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚ Quelques  â”‚                      â”‚ Millions  â”‚\n    â”‚  lignes   â”‚                      â”‚ de lignes â”‚\n    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚   App     â”‚                      â”‚  Rapport  â”‚\n    â”‚   Web     â”‚                      â”‚ Dashboard â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nCritÃ¨re\nOLTP\nOLAP\n\n\n\n\nObjectif\nOpÃ©rations quotidiennes\nAnalyse, dÃ©cision\n\n\nDonnÃ©es\nActuelles\nHistoriques\n\n\nRequÃªtes\nSimples, frÃ©quentes\nComplexes, ponctuelles\n\n\nTemps de rÃ©ponse\nMillisecondes\nSecondes Ã  minutes\n\n\nUtilisateurs\nMilliers (applications)\nDizaines (analystes)\n\n\nSchÃ©ma\nNormalisÃ©\nDÃ©normalisÃ©\n\n\nExemples\nPostgreSQL, MySQL\nSnowflake, BigQuery, Redshift\n\n\n\n\n\n\nğŸ¢ 10.4 Data Warehouse â€” Lâ€™entrepÃ´t de donnÃ©es\nUn Data Warehouse (DWH) est une base de donnÃ©es OLAP centralisÃ©e qui stocke les donnÃ©es de toute lâ€™entreprise pour lâ€™analyse.\n\nğŸ“– DÃ©finition\n\nâ€œUn Data Warehouse est une copie des donnÃ©es transactionnelles, structurÃ©e spÃ©cifiquement pour lâ€™analyse et le reporting.â€ â€” Bill Inmon\n\n\n\nğŸ¯ CaractÃ©ristiques\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nOrientÃ© sujet\nOrganisÃ© par domaine mÃ©tier (ventes, clients, produits)\n\n\nIntÃ©grÃ©\nDonnÃ©es de sources multiples, harmonisÃ©es\n\n\nHistorisÃ©\nConserve lâ€™historique (pas de suppression)\n\n\nNon volatile\nDonnÃ©es en lecture seule (pas de UPDATE)\n\n\n\n\n\nğŸ­ Exemples de Data Warehouses\n\n\n\nProduit\nType\nParticularitÃ©\n\n\n\n\nSnowflake\nCloud\nSÃ©paration stockage/calcul, trÃ¨s scalable\n\n\nAmazon Redshift\nCloud AWS\nIntÃ©gration AWS native\n\n\nGoogle BigQuery\nCloud GCP\nServerless, pay-per-query\n\n\nAzure Synapse\nCloud Azure\nIntÃ©gration Microsoft\n\n\nTeradata\nOn-premise\nHistorique, grandes entreprises\n\n\n\n\n\n\n\nğŸ¯ 10.5 Data Mart â€” Le magasin spÃ©cialisÃ©\nUn Data Mart est un sous-ensemble du Data Warehouse, focalisÃ© sur un domaine mÃ©tier spÃ©cifique.\n\nğŸ“– DÃ©finition\n\nâ€œUn Data Mart est une vue spÃ©cialisÃ©e du Data Warehouse, optimisÃ©e pour les besoins dâ€™un dÃ©partement ou dâ€™une fonction mÃ©tier.â€\n\n\n\nğŸ¬ Exemples de Data Marts\n\n\n\nData Mart\nDonnÃ©es\nUtilisateurs\n\n\n\n\nMarketing\nCampagnes, conversions, segments\nÃ‰quipe Marketing\n\n\nFinance\nRevenus, coÃ»ts, budgets\nDirection financiÃ¨re\n\n\nRH\nEmployÃ©s, salaires, turnover\nRessources Humaines\n\n\nVentes\nCommandes, clients, produits\nÃ‰quipe commerciale\n\n\n\n\n\n\n\nğŸ–¼ï¸ 10.6 Architecture globale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         SOURCES DE DONNÃ‰ES                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\nâ”‚  â”‚   ERP    â”‚  â”‚   CRM    â”‚  â”‚  E-comm  â”‚  â”‚   Logs   â”‚  ...          â”‚\nâ”‚  â”‚  (OLTP)  â”‚  â”‚  (OLTP)  â”‚  â”‚  (OLTP)  â”‚  â”‚          â”‚               â”‚\nâ”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜\n        â”‚             â”‚             â”‚             â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   ETL / ELT     â”‚\n                    â”‚  (Extraction,   â”‚\n                    â”‚  Transformation,â”‚\n                    â”‚  Chargement)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  DATA WAREHOUSE â”‚\n                    â”‚     (OLAP)      â”‚\n                    â”‚                 â”‚\n                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n                    â”‚ â”‚  DonnÃ©es    â”‚ â”‚\n                    â”‚ â”‚  intÃ©grÃ©es  â”‚ â”‚\n                    â”‚ â”‚  historisÃ©esâ”‚ â”‚\n                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  DATA MART  â”‚     â”‚  DATA MART  â”‚     â”‚  DATA MART  â”‚\n  â”‚   Ventes    â”‚     â”‚  Marketing  â”‚     â”‚   Finance   â”‚\n  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  Dashboard  â”‚     â”‚  Rapports   â”‚     â”‚    KPIs     â”‚\n  â”‚   Power BI  â”‚     â”‚  Campaigns  â”‚     â”‚  Financiers â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ’¡ RÃ©sumÃ© OLTP / OLAP / DWH / Data Mart\n\n\n\nConcept\nRÃ´le\nAnalogie\n\n\n\n\nOLTP\nOpÃ©rations quotidiennes\nLa caisse enregistreuse\n\n\nOLAP\nAnalyse des donnÃ©es\nLe bureau de lâ€™analyste\n\n\nData Warehouse\nEntrepÃ´t centralisÃ©\nLe grand entrepÃ´t\n\n\nData Mart\nVue mÃ©tier spÃ©cialisÃ©e\nLe rayon dâ€™un magasin",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#modÃ©lisation-dimensionnelle-star-snowflake-schema",
    "href": "notebooks/beginner/06_intro_relational_databases.html#modÃ©lisation-dimensionnelle-star-snowflake-schema",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "â­ 11. ModÃ©lisation Dimensionnelle â€” Star & Snowflake Schema",
    "text": "â­ 11. ModÃ©lisation Dimensionnelle â€” Star & Snowflake Schema\nLa modÃ©lisation dimensionnelle est la technique utilisÃ©e pour structurer les donnÃ©es dans un Data Warehouse. Elle est optimisÃ©e pour lâ€™analyse (OLAP), pas pour les transactions (OLTP).\n\nğŸ’¡ Cette approche a Ã©tÃ© popularisÃ©e par Ralph Kimball dans les annÃ©es 1990.\n\n\n\nğŸ“Š 11.1 Tables de Faits (Fact Tables)\nUne table de faits contient les mesures (mÃ©triques) que lâ€™on veut analyser.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      TABLE DE FAITS                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  ğŸ¯ Contient :                                                  â”‚\nâ”‚     â€¢ Les MESURES (chiffres Ã  analyser)                        â”‚\nâ”‚     â€¢ Les CLÃ‰S Ã‰TRANGÃˆRES vers les dimensions                  â”‚\nâ”‚                                                                 â”‚\nâ”‚  ğŸ“Š Exemples de mesures :                                       â”‚\nâ”‚     â€¢ Montant de la vente                                      â”‚\nâ”‚     â€¢ QuantitÃ© vendue                                          â”‚\nâ”‚     â€¢ CoÃ»t                                                     â”‚\nâ”‚     â€¢ Nombre de clics                                          â”‚\nâ”‚                                                                 â”‚\nâ”‚  âš¡ CaractÃ©ristiques :                                          â”‚\nâ”‚     â€¢ TrÃ¨s grande (millions/milliards de lignes)               â”‚\nâ”‚     â€¢ Une ligne = un Ã‰VÃ‰NEMENT (une vente, un clic, etc.)      â”‚\nâ”‚     â€¢ GranularitÃ© fine (niveau de dÃ©tail)                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemple : Table de faits fact_sales\nfact_sales\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ sale_id(PK)â”‚ date_id(FK) â”‚ product_id(FK) â”‚ customer_id(FK)â”‚ quantity â”‚  amount  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     1      â”‚  20240115   â”‚      101       â”‚      501       â”‚    2     â”‚  199.98  â”‚\nâ”‚     2      â”‚  20240115   â”‚      102       â”‚      502       â”‚    1     â”‚   49.99  â”‚\nâ”‚     3      â”‚  20240116   â”‚      101       â”‚      501       â”‚    1     â”‚   99.99  â”‚\nâ”‚    ...     â”‚    ...      â”‚      ...       â”‚      ...       â”‚   ...    â”‚   ...    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚              â”‚                â”‚\n                FK vers        FK vers          FK vers\n               dim_date     dim_product      dim_customer\n\n\n\nğŸ“ 11.2 Tables de Dimensions (Dimension Tables)\nUne table de dimension contient le contexte descriptif des mesures.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    TABLE DE DIMENSION                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  ğŸ¯ Contient :                                                  â”‚\nâ”‚     â€¢ Les ATTRIBUTS descriptifs (le \"quoi\", \"qui\", \"oÃ¹\", \"quand\") â”‚\nâ”‚     â€¢ La clÃ© primaire rÃ©fÃ©rencÃ©e par la table de faits         â”‚\nâ”‚                                                                 â”‚\nâ”‚  ğŸ“Š Exemples de dimensions :                                    â”‚\nâ”‚     â€¢ dim_date : annÃ©e, mois, jour, trimestre, jour_semaine    â”‚\nâ”‚     â€¢ dim_product : nom, catÃ©gorie, marque, prix_liste         â”‚\nâ”‚     â€¢ dim_customer : nom, segment, ville, pays                 â”‚\nâ”‚     â€¢ dim_store : nom, rÃ©gion, type, surface                   â”‚\nâ”‚                                                                 â”‚\nâ”‚  âš¡ CaractÃ©ristiques :                                          â”‚\nâ”‚     â€¢ Relativement petite (milliers Ã  millions de lignes)      â”‚\nâ”‚     â€¢ DÃ©normalisÃ©e (toutes les infos dans une seule table)     â”‚\nâ”‚     â€¢ Permet le \"slicing & dicing\" (filtres et regroupements)  â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemples de tables de dimensions :\ndim_date                                    dim_product\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_id  â”‚ year â”‚monthâ”‚ day â”‚quarterâ”‚   â”‚ product_id â”‚   name   â”‚  category â”‚  brand  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 20240115 â”‚ 2024 â”‚  1  â”‚ 15  â”‚  Q1  â”‚    â”‚    101     â”‚ iPhone 15â”‚ Smartphoneâ”‚  Apple  â”‚\nâ”‚ 20240116 â”‚ 2024 â”‚  1  â”‚ 16  â”‚  Q1  â”‚    â”‚    102     â”‚Galaxy S24â”‚ Smartphoneâ”‚ Samsung â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\ndim_customer\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ customer_id â”‚   name   â”‚ segment  â”‚  city   â”‚ country â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    501      â”‚  Alice   â”‚ Premium  â”‚  Paris  â”‚ France  â”‚\nâ”‚    502      â”‚   Bob    â”‚ Standard â”‚  Lyon   â”‚ France  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nâ­ 11.3 Star Schema (SchÃ©ma en Ã©toile)\nLe Star Schema est le modÃ¨le dimensionnel le plus simple et le plus utilisÃ©.\n                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                            â”‚    dim_date     â”‚\n                            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                            â”‚ date_id (PK)    â”‚\n                            â”‚ year            â”‚\n                            â”‚ month           â”‚\n                            â”‚ quarter         â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   dim_customer  â”‚         â”‚   fact_sales    â”‚         â”‚   dim_product   â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\nâ”‚ customer_id(PK) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ sale_id (PK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ product_id (PK) â”‚\nâ”‚ name            â”‚         â”‚ date_id (FK)    â”‚         â”‚ name            â”‚\nâ”‚ segment         â”‚         â”‚ product_id (FK) â”‚         â”‚ category        â”‚\nâ”‚ city            â”‚         â”‚ customer_id(FK) â”‚         â”‚ brand           â”‚\nâ”‚ country         â”‚         â”‚ store_id (FK)   â”‚         â”‚ price           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ quantity        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ amount          â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â”‚\n                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                            â”‚   dim_store     â”‚\n                            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                            â”‚ store_id (PK)   â”‚\n                            â”‚ name            â”‚\n                            â”‚ region          â”‚\n                            â”‚ type            â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nPourquoi â€œÃ©toileâ€ ? La table de faits est au centre, entourÃ©e des dimensions â€” comme une Ã©toile !\n\n\n\n\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nSimple Ã  comprendre\nRedondance dans les dimensions\n\n\nRequÃªtes rapides (peu de jointures)\nDimensions peuvent Ãªtre grandes\n\n\nFacile Ã  maintenir\nPas adaptÃ© aux hiÃ©rarchies trÃ¨s complexes\n\n\n\n\n\n\nâ„ï¸ 11.4 Snowflake Schema (SchÃ©ma en flocon)\nLe Snowflake Schema est une variante oÃ¹ les dimensions sont normalisÃ©es (dÃ©coupÃ©es en sous-tables).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   dim_year    â”‚     â”‚   dim_month     â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\nâ”‚ year_id (PK)  â”‚â—„â”€â”€â”€â”€â”‚ month_id (PK)   â”‚\nâ”‚ year          â”‚     â”‚ year_id (FK)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ month           â”‚\n                      â”‚ quarter         â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚    dim_date     â”‚         â”‚  dim_category   â”‚\n                      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                      â”‚ date_id (PK)    â”‚         â”‚ category_id(PK) â”‚\n                      â”‚ month_id (FK)   â”‚         â”‚ category_name   â”‚\n                      â”‚ day             â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n                               â”‚                           â”‚\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚   fact_sales    â”‚         â”‚   dim_product   â”‚\n                      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                      â”‚ date_id (FK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ product_id (PK) â”‚\n                      â”‚ product_id (FK) â”‚         â”‚ category_id(FK) â”‚\n                      â”‚ quantity        â”‚         â”‚ name            â”‚\n                      â”‚ amount          â”‚         â”‚ brand           â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nPourquoi â€œfloconâ€ ? Les branches se subdivisent comme un flocon de neige.\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nMoins de redondance\nPlus de jointures (plus lent)\n\n\nÃ‰conomie dâ€™espace\nPlus complexe Ã  comprendre\n\n\nMeilleur pour hiÃ©rarchies\nMaintenance plus difficile\n\n\n\n\n\n\nâš–ï¸ 11.5 Star vs Snowflake â€” Quand utiliser quoi ?\n\n\n\nCritÃ¨re\nStar Schema â­\nSnowflake Schema â„ï¸\n\n\n\n\nPerformance\nâš¡ Plus rapide\nğŸ¢ Plus de jointures\n\n\nSimplicitÃ©\nâœ… Simple\nâš ï¸ Complexe\n\n\nRedondance\nâš ï¸ Plus de duplication\nâœ… Moins de duplication\n\n\nCas dâ€™usage\nReporting, dashboards\nHiÃ©rarchies complexes\n\n\n\n\nğŸ’¡ En pratique : Le Star Schema est recommandÃ© dans 90% des cas. Utilise Snowflake uniquement si tu as des contraintes spÃ©cifiques de stockage ou des hiÃ©rarchies trÃ¨s profondes.\n\n\n\n\nğŸ¯ 11.6 Exemple de requÃªte analytique\nAvec un Star Schema, les requÃªtes analytiques sont simples et intuitives :\n-- Ventes par catÃ©gorie de produit et par trimestre\nSELECT \n    d.year,\n    d.quarter,\n    p.category,\n    SUM(f.amount) as total_sales,\n    COUNT(*) as num_transactions\nFROM fact_sales f\nJOIN dim_date d ON f.date_id = d.date_id\nJOIN dim_product p ON f.product_id = p.product_id\nWHERE d.year = 2024\nGROUP BY d.year, d.quarter, p.category\nORDER BY d.quarter, total_sales DESC;\n\n\n\nyear\nquarter\ncategory\ntotal_sales\nnum_transactions\n\n\n\n\n2024\nQ1\nSmartphone\n1,250,000\n8,500\n\n\n2024\nQ1\nLaptop\n980,000\n3,200\n\n\n2024\nQ1\nAccessoires\n450,000\n15,000\n\n\n\n\nğŸ”® Preview : Dans le module 08, tu dÃ©couvriras les concepts de Data Lake, Data Lakehouse, Medallion Architecture (Bronze/Silver/Gold) et les architectures Lambda/Kappa pour le Big Data !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne-oltp",
    "href": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne-oltp",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ—ï¸ 12. Exemple complet : SchÃ©ma dâ€™une boutique en ligne (OLTP)",
    "text": "ğŸ—ï¸ 12. Exemple complet : SchÃ©ma dâ€™une boutique en ligne (OLTP)\nVoici un exemple de schÃ©ma OLTP normalisÃ© pour les opÃ©rations quotidiennes :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        BASE DE DONNÃ‰ES : ma_boutique                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚    CLIENTS      â”‚       â”‚    COMMANDES    â”‚       â”‚   PRODUITS    â”‚ â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\nâ”‚  â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)    â”‚ â”‚\nâ”‚  â”‚ nom             â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ ğŸ”— client_id(FK)â”‚       â”‚ nom           â”‚ â”‚\nâ”‚  â”‚ email           â”‚       â”‚ date            â”‚       â”‚ prix          â”‚ â”‚\nâ”‚  â”‚ telephone       â”‚       â”‚ statut          â”‚       â”‚ stock         â”‚ â”‚\nâ”‚  â”‚ created_at      â”‚       â”‚ total           â”‚       â”‚ categorie     â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                     â”‚                        â”‚         â”‚\nâ”‚                                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                                     â”‚    â”‚                             â”‚\nâ”‚                                     â–¼    â–¼                             â”‚\nâ”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                            â”‚   LIGNES_COMMANDE   â”‚                     â”‚\nâ”‚                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚\nâ”‚                            â”‚ ğŸ”‘ id (PK)          â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— commande_id (FK) â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— produit_id (FK)  â”‚                     â”‚\nâ”‚                            â”‚ quantite            â”‚                     â”‚\nâ”‚                            â”‚ prix_unitaire       â”‚                     â”‚\nâ”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRELATIONS :\n  â€¢ clients (1) â”€â”€â”€â–º (N) commandes        : Un client a plusieurs commandes\n  â€¢ commandes (1) â”€â”€â”€â–º (N) lignes_commande : Une commande a plusieurs lignes\n  â€¢ produits (1) â”€â”€â”€â–º (N) lignes_commande  : Un produit dans plusieurs lignes\nCe schÃ©ma est normalisÃ© (3NF) car câ€™est un systÃ¨me OLTP pour les opÃ©rations quotidiennes.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nğŸ§± Vocabulaire relationnel\n\n\n\nTerme\nDescription\n\n\n\n\nTable\nCollection de donnÃ©es structurÃ©es\n\n\nColonne\nAttribut (nom, email, prixâ€¦)\n\n\nLigne\nUn enregistrement (un client, une commandeâ€¦)\n\n\nClÃ© Primaire (PK)\nIdentifiant unique dâ€™une ligne\n\n\nClÃ© Ã‰trangÃ¨re (FK)\nRÃ©fÃ©rence vers une autre table\n\n\nSGBD\nLogiciel de gestion (PostgreSQL, MySQLâ€¦)\n\n\n\n\n\nğŸ”€ Relations\n\n\n\nType\nNotation\nExemple\n\n\n\n\nUn-Ã -Un\n1:1\nUtilisateur â†”ï¸ Profil\n\n\nUn-Ã -Plusieurs\n1:N\nClient â†’ Commandes\n\n\nPlusieurs-Ã -Plusieurs\nN:N\nÃ‰tudiants â†”ï¸ Cours\n\n\n\n\n\nğŸ” ACID\n\n\n\nLettre\nPropriÃ©tÃ©\n\n\n\n\nA\nAtomicitÃ© â€” Tout ou rien\n\n\nC\nCohÃ©rence â€” Ã‰tat valide\n\n\nI\nIsolation â€” Transactions sÃ©parÃ©es\n\n\nD\nDurabilitÃ© â€” Permanent\n\n\n\n\n\nâš¡ OLTP vs OLAP\n\n\n\nConcept\nOLTP\nOLAP\n\n\n\n\nUsage\nTransactions\nAnalyse\n\n\nRequÃªtes\nSimples, rapides\nComplexes, agrÃ©gÃ©es\n\n\nSchÃ©ma\nNormalisÃ©\nDÃ©normalisÃ©\n\n\n\n\n\nğŸ¢ Data Warehouse & Data Mart\n\n\n\nConcept\nDescription\n\n\n\n\nData Warehouse\nEntrepÃ´t centralisÃ© pour lâ€™analyse\n\n\nData Mart\nSous-ensemble orientÃ© mÃ©tier\n\n\n\n\n\nâ­ ModÃ©lisation dimensionnelle\n\n\n\nConcept\nDescription\n\n\n\n\nTable de faits\nMesures numÃ©riques (ventes, quantitÃ©s)\n\n\nTable de dimension\nContexte descriptif (date, produit, client)\n\n\nStar Schema\nFaits au centre, dimensions autour\n\n\nSnowflake Schema\nStar avec dimensions normalisÃ©es",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "href": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quâ€™est-ce quâ€™une clÃ© primaire ?\n\nUne clÃ© de chiffrement\n\nUn identifiant unique pour chaque ligne\n\nLe nom de la premiÃ¨re colonne\n\nUn mot de passe\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© primaire identifie de faÃ§on unique chaque ligne dâ€™une table.\n\n\n\n\nâ“ Q2. Ã€ quoi sert une clÃ© Ã©trangÃ¨re ?\n\nÃ€ chiffrer les donnÃ©es\n\nÃ€ crÃ©er un lien entre deux tables\n\nÃ€ indexer les colonnes\n\nÃ€ supprimer des lignes\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© Ã©trangÃ¨re rÃ©fÃ©rence la clÃ© primaire dâ€™une autre table pour crÃ©er une relation.\n\n\n\n\nâ“ Q3. Quelle relation nÃ©cessite une table de jonction ?\n\nUn-Ã -Un (1:1)\n\nUn-Ã -Plusieurs (1:N)\n\nPlusieurs-Ã -Plusieurs (N:N)\n\nAucune\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Les relations N:N nÃ©cessitent une table intermÃ©diaire (jonction) pour stocker les associations.\n\n\n\n\nâ“ Q4. Que signifie le A de ACID ?\n\nAuthentification\n\nAtomicitÃ©\n\nAutomatisation\n\nArchivage\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” AtomicitÃ© : une transaction est indivisible (tout rÃ©ussit ou tout Ã©choue).\n\n\n\n\nâ“ Q5. Pourquoi normaliser une base de donnÃ©es ?\n\nPour la rendre plus rapide\n\nPour Ã©viter la redondance et les incohÃ©rences\n\nPour ajouter du chiffrement\n\nPour compresser les donnÃ©es\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La normalisation Ã©vite les donnÃ©es dupliquÃ©es et garantit la cohÃ©rence.\n\n\n\n\nâ“ Q6. Quel SGBD est open source ET trÃ¨s complet ?\n\nOracle\n\nSQL Server\n\nPostgreSQL\n\nAccess\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” PostgreSQL est open source, gratuit et trÃ¨s complet (le plus recommandÃ©).\n\n\n\n\nâ“ Q7. Quel type utiliser pour stocker des montants en euros ?\n\nFLOAT\n\nINTEGER\n\nDECIMAL\n\nVARCHAR\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” DECIMAL garantit une prÃ©cision exacte pour les montants financiers (FLOAT est imprÃ©cis).\n\n\n\n\nâ“ Q8. Quelle est la diffÃ©rence principale entre OLTP et OLAP ?\n\nOLTP est plus rÃ©cent\n\nOLTP gÃ¨re les transactions, OLAP lâ€™analyse\n\nOLAP est plus rapide\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” OLTP (Online Transaction Processing) gÃ¨re les opÃ©rations quotidiennes, OLAP (Online Analytical Processing) est optimisÃ© pour lâ€™analyse et le reporting.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Data Warehouse ?\n\nUn serveur trÃ¨s puissant\n\nUn entrepÃ´t centralisÃ© pour lâ€™analyse des donnÃ©es\n\nUn type de base de donnÃ©es NoSQL\n\nUn logiciel de visualisation\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Warehouse est une base de donnÃ©es OLAP centralisÃ©e qui stocke les donnÃ©es de lâ€™entreprise pour lâ€™analyse et le reporting.\n\n\n\n\nâ“ Q10. Quâ€™est-ce quâ€™un Data Mart ?\n\nUn magasin de donnÃ©es brutes\n\nUn sous-ensemble du Data Warehouse orientÃ© mÃ©tier\n\nUne base de donnÃ©es transactionnelle\n\nUn outil ETL\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Mart est une vue spÃ©cialisÃ©e du Data Warehouse, focalisÃ©e sur un domaine mÃ©tier (Marketing, Finance, Ventesâ€¦).\n\n\n\n\nâ“ Q11. Dans un Star Schema, que contient la table de faits ?\n\nLes descriptions des produits\n\nLes mesures (mÃ©triques) Ã  analyser\n\nLes informations clients\n\nLes dates uniquement\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La table de faits contient les mesures numÃ©riques (montants, quantitÃ©s, clicsâ€¦) et les clÃ©s Ã©trangÃ¨res vers les dimensions.\n\n\n\n\nâ“ Q12. Quelle est la diffÃ©rence entre Star Schema et Snowflake Schema ?\n\nStar Schema est plus rÃ©cent\n\nDans Snowflake, les dimensions sont normalisÃ©es (dÃ©coupÃ©es)\n\nSnowflake nâ€™a pas de table de faits\n\nIls sont identiques\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Dans un Snowflake Schema, les tables de dimensions sont normalisÃ©es (dÃ©coupÃ©es en sous-tables), ce qui ressemble Ã  un flocon de neige.\n\n\n\n\nâ“ Q13. Une table de dimension contientâ€¦\n\nLes chiffres Ã  analyser\n\nLe contexte descriptif (qui, quoi, oÃ¹, quand)\n\nLes clÃ©s Ã©trangÃ¨res uniquement\n\nLes donnÃ©es en temps rÃ©el\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Une table de dimension contient les attributs descriptifs qui donnent du contexte aux mesures (produit, client, date, magasinâ€¦).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "href": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nBases de donnÃ©es relationnelles\n\nPostgreSQL Documentation\nSQLBolt â€” Tutoriel interactif\nDB Diagram â€” CrÃ©er des schÃ©mas visuels\nDatabase Normalization (Wikipedia)\n\n\n\nOLTP, OLAP et Data Warehousing\n\nOLTP vs OLAP (AWS)\nWhat is a Data Warehouse? (Snowflake)\nData Mart vs Data Warehouse (IBM)\n\n\n\nModÃ©lisation dimensionnelle\n\nStar Schema vs Snowflake Schema (Databricks)\nThe Data Warehouse Toolkit â€” Ralph Kimball\nDimensional Modeling (Kimball Group)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu connais maintenant la thÃ©orie des bases de donnÃ©es relationnelles, OLTP/OLAP, Data Warehouse, et la modÃ©lisation dimensionnelle. Passons Ã  la pratique !\nğŸ‘‰ Module suivant : 07_sql_for_data_engineers.ipynb â€” Ã‰crire des requÃªtes SQL\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises les concepts fondamentaux des bases relationnelles et de lâ€™architecture Data.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "",
    "text": "Ce module couvre le traitement de donnÃ©es avancÃ© avec Python : Pandas, visualisation, APIs, et pipelines ETL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 04_python_basics_for_data_engineers\n\n\nâœ… Requis\nMaÃ®triser les bases de Python (variables, fonctions, boucles)\n\n\nâœ… Requis\nSavoir utiliser pip et les environnements virtuels",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Manipuler des donnÃ©es avec Pandas (DataFrames, nettoyage, agrÃ©gations)\nâœ… Visualiser des donnÃ©es avec Matplotlib\nâœ… CrÃ©er des graphiques statistiques avec Seaborn\nâœ… Traiter du texte et utiliser les regex\nâœ… Consommer des APIs REST\nâœ… Valider la qualitÃ© des donnÃ©es\nâœ… Construire un pipeline ETL complet\nâœ… GÃ©rer les configurations et secrets",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“¦ Installation des dÃ©pendances",
    "text": "ğŸ“¦ Installation des dÃ©pendances\nAvant de commencer, assurons-nous dâ€™avoir toutes les librairies nÃ©cessaires.\n\n\nCode\n# Installation des packages (Ã  exÃ©cuter une seule fois)\n!pip install pandas numpy requests python-dotenv pytest pandera pyarrow openpyxl matplotlib seaborn\n\n\n\n\nCode\n# Imports de base\nimport pandas as pd\nimport numpy as np\nimport json\nimport requests\nfrom datetime import datetime\nimport time\nimport logging\nimport re\nfrom pathlib import Path\n\n# Configuration de l'affichage\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\n\nprint(\"âœ… Imports rÃ©ussis !\")\nprint(f\"Version Pandas : {pd.__version__}\")\nprint(f\"Version NumPy : {np.__version__}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.1 CrÃ©er et lire des donnÃ©es",
    "text": "1.1 CrÃ©er et lire des donnÃ©es\n\n\nCode\n# CrÃ©er un DataFrame simple\ndata = {\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'age': [25, 30, 35, None, 28],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon'],\n    'salaire': [45000, 55000, 60000, 50000, None]\n}\n\ndf = pd.DataFrame(data)\nprint(\"ğŸ“Š DataFrame crÃ©Ã© :\")\nprint(df)\n\n\n\n\nCode\n# Sauvegarder en CSV\ndf.to_csv('exemple_employes.csv', index=False)\nprint(\"âœ… Fichier CSV sauvegardÃ©\")\n\n# Lire depuis CSV\ndf_from_csv = pd.read_csv('exemple_employes.csv')\nprint(\"\\nğŸ“‚ Lecture depuis CSV :\")\nprint(df_from_csv.head())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.2 Exploration des donnÃ©es",
    "text": "1.2 Exploration des donnÃ©es\n\n\nCode\n# Informations gÃ©nÃ©rales\nprint(\"ğŸ“‹ Informations du DataFrame :\")\nprint(df.info())\nprint(\"\\n\" + \"=\"*50)\n\n# Statistiques descriptives\nprint(\"\\nğŸ“Š Statistiques descriptives :\")\nprint(df.describe())\n\n# PremiÃ¨res lignes\nprint(\"\\nğŸ” PremiÃ¨res lignes :\")\nprint(df.head(3))\n\n# DerniÃ¨res lignes\nprint(\"\\nğŸ”š DerniÃ¨res lignes :\")\nprint(df.tail(2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¤– Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es",
    "text": "ğŸ¤– Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es\nJulius.ai est une plateforme dâ€™IA qui permet dâ€™analyser des donnÃ©es en langage naturel, sans Ã©crire de code.\n\nğŸŒ AccÃ¨s\nğŸ‘‰ julius.ai â€” Gratuit avec limitations, plans payants disponibles\n\n\nâœ¨ FonctionnalitÃ©s\n\n\n\n\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nUpload de fichiers\nCSV, Excel, JSON, bases de donnÃ©es\n\n\nQuestions en franÃ§ais\nâ€œQuelle est la moyenne des salaires par ville ?â€\n\n\nGÃ©nÃ©ration de code\nPython/Pandas gÃ©nÃ©rÃ© automatiquement\n\n\nVisualisations\nGraphiques crÃ©Ã©s Ã  la demande\n\n\nExport\nCode Python, graphiques, rapports\n\n\n\n\n\nğŸ“ Exemples de questions Ã  poser\n- \"Montre-moi les 10 premiÃ¨res lignes\"\n- \"Combien de valeurs manquantes par colonne ?\"\n- \"CrÃ©e un graphique des ventes par mois\"\n- \"Quelle est la corrÃ©lation entre age et salaire ?\"\n- \"Nettoie les doublons et les valeurs aberrantes\"\n- \"GÃ©nÃ¨re un rapport de qualitÃ© des donnÃ©es\"\n\n\nğŸ’¡ Cas dâ€™usage Data Engineering\n\n\n\nSituation\nComment Julius aide\n\n\n\n\nNouveau dataset inconnu\nExploration rapide sans code\n\n\nRÃ©union avec non-techniques\nDÃ©mo interactive\n\n\nPrototypage rapide\nGÃ©nÃ©rer du code Pandas Ã  rÃ©utiliser\n\n\nDebugging\nâ€œPourquoi jâ€™ai des NaN dans cette colonne ?â€\n\n\n\n\n\nâš ï¸ Limitations\n\nDonnÃ©es envoyÃ©es dans le cloud (attention aux donnÃ©es sensibles)\nGratuit limitÃ© en nombre de requÃªtes\nPas adaptÃ© pour la production (utiliser le code gÃ©nÃ©rÃ© plutÃ´t)\n\n\nğŸ’¡ Astuce : Utilise Julius pour explorer, puis copie le code Python gÃ©nÃ©rÃ© dans ton pipeline !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š ydata-profiling â€” Rapport complet en 1 ligne",
    "text": "ğŸ“Š ydata-profiling â€” Rapport complet en 1 ligne\nydata-profiling (anciennement pandas-profiling) gÃ©nÃ¨re un rapport HTML interactif complet sur ton DataFrame.\n\nğŸ“¦ Installation\npip install ydata-profiling\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install ydata-profiling\n\nfrom ydata_profiling import ProfileReport\n\n# CrÃ©er un dataset d'exemple\ndf_exemple = pd.DataFrame({\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n    'age': [25, 30, 35, None, 28, 45, 32, 29],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris', 'Lyon', 'Paris'],\n    'salaire': [45000, 55000, 60000, 50000, None, 75000, 52000, 48000],\n    'experience': [2, 5, 8, 3, 4, 15, 7, 3],\n    'date_embauche': pd.to_datetime(['2022-01-15', '2019-06-20', '2016-03-10', \n                                      '2021-09-01', '2020-04-15', '2010-01-01',\n                                      '2017-08-20', '2021-11-30'])\n})\n\n# GÃ©nÃ©rer le rapport (mode minimal pour rapiditÃ©)\nprofile = ProfileReport(\n    df_exemple, \n    title=\"Rapport EmployÃ©s\",\n    minimal=True,  # Mode rapide\n    explorative=True\n)\n\n# Afficher dans le notebook\nprofile.to_notebook_iframe()\n\n# Ou sauvegarder en HTML\n# profile.to_file(\"rapport_employes.html\")\n\n\n\n\nğŸ“‹ Ce que contient le rapport\n\n\n\n\n\n\n\nSection\nContenu\n\n\n\n\nOverview\nNombre de lignes, colonnes, types, taille mÃ©moire\n\n\nVariables\nStats par colonne (min, max, mean, distribution)\n\n\nInteractions\nCorrÃ©lations entre variables\n\n\nCorrelations\nMatrices de corrÃ©lation (Pearson, Spearman)\n\n\nMissing values\nVisualisation des valeurs manquantes\n\n\nDuplicates\nDÃ©tection des doublons\n\n\nAlerts\nâš ï¸ Alertes automatiques (haute cardinalitÃ©, skewness, etc.)\n\n\n\n\n\nâš™ï¸ Options utiles\n# Rapport complet (plus lent)\nprofile = ProfileReport(df, minimal=False)\n\n# Comparer deux datasets\nprofile_train = ProfileReport(df_train, title=\"Train\")\nprofile_test = ProfileReport(df_test, title=\"Test\")\ncomparison = profile_train.compare(profile_test)\ncomparison.to_file(\"comparison.html\")\n\n# Exclure certaines analyses (plus rapide)\nprofile = ProfileReport(\n    df,\n    correlations=None,  # DÃ©sactiver les corrÃ©lations\n    interactions=None   # DÃ©sactiver les interactions\n)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¬ Sweetviz â€” Comparaison visuelle de datasets",
    "text": "ğŸ¬ Sweetviz â€” Comparaison visuelle de datasets\nSweetviz est spÃ©cialisÃ© dans la comparaison de datasets (train vs test, avant vs aprÃ¨s nettoyage).\n\nğŸ“¦ Installation\npip install sweetviz\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install sweetviz\n\nimport sweetviz as sv\n\n# Rapport simple\nreport = sv.analyze(df_exemple)\nreport.show_notebook()  # Afficher dans le notebook\n# report.show_html(\"sweetviz_report.html\")  # Ou sauvegarder\n\n\n\n\nCode\n# Comparaison de deux datasets (ex: train vs test)\ndf_train = df_exemple.iloc[:5]\ndf_test = df_exemple.iloc[5:]\n\n# GÃ©nÃ©rer le rapport de comparaison\ncomparison_report = sv.compare([df_train, \"Train\"], [df_test, \"Test\"])\ncomparison_report.show_notebook()\n\n# Analyse avec variable cible (pour ML)\n# report = sv.analyze(df, target_feat=\"salaire\")\n\n\n\n\nâœ¨ Points forts de Sweetviz\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nComparaison cÃ´te Ã  cÃ´te\nVoir les diffÃ©rences entre 2 datasets\n\n\nVariable cible\nAnalyse par rapport Ã  une target (ML)\n\n\nDesign moderne\nRapports visuellement attractifs\n\n\nRapide\nPlus lÃ©ger que ydata-profiling\n\n\n\n\n\nğŸ†š ydata-profiling vs Sweetviz\n\n\n\nCritÃ¨re\nydata-profiling\nSweetviz\n\n\n\n\nProfondeur dâ€™analyse\nâ­â­â­ TrÃ¨s dÃ©taillÃ©\nâ­â­ Essentiel\n\n\nVitesse\nğŸ¢ Plus lent\nğŸ‡ Plus rapide\n\n\nComparaison\nâœ… Possible\nâ­â­â­ Excellent\n\n\nDesign\nClassique\nModerne\n\n\nAlertes\nâœ… Oui\nâŒ Non",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ–¥ï¸ D-Tale â€” Exploration interactive (comme Excel)",
    "text": "ğŸ–¥ï¸ D-Tale â€” Exploration interactive (comme Excel)\nD-Tale lance une interface web interactive pour explorer tes donnÃ©es comme dans Excel/Google Sheets, mais avec la puissance de Python derriÃ¨re.\n\nğŸ“¦ Installation\npip install dtale\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install dtale\n\nimport dtale\n\n# Lancer D-Tale\nd = dtale.show(df_exemple)\n\n# Afficher dans le notebook (ou ouvre un nouvel onglet)\nd.notebook()\n\n\n\n\nâœ¨ FonctionnalitÃ©s D-Tale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  D-Tale                                         [Export] [Code]â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  [Filters] [Sort] [Charts] [Correlations] [Describe] [Missing] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    nom    â”‚  age  â”‚  ville   â”‚ salaire â”‚ experience â”‚          â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚\nâ”‚  Alice    â”‚  25   â”‚  Paris   â”‚  45000  â”‚     2      â”‚          â”‚\nâ”‚  Bob      â”‚  30   â”‚  Lyon    â”‚  55000  â”‚     5      â”‚          â”‚\nâ”‚  Charlie  â”‚  35   â”‚  Paris   â”‚  60000  â”‚     8      â”‚          â”‚\nâ”‚  ...      â”‚  ...  â”‚  ...     â”‚  ...    â”‚    ...     â”‚          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nAction\nComment\n\n\n\n\nFiltrer\nCliquer sur une colonne â†’ Filter\n\n\nTrier\nCliquer sur lâ€™en-tÃªte de colonne\n\n\nGraphiques\nMenu Charts â†’ choisir le type\n\n\nStats\nMenu Describe â†’ stats par colonne\n\n\nExporter le code\nBouton â€œCode Exportâ€ â†’ copier le Pandas gÃ©nÃ©rÃ©\n\n\n\n\nğŸ’¡ Killer feature : D-Tale gÃ©nÃ¨re le code Pandas de toutes tes manipulations !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“ˆ Pygwalker â€” Interface Tableau dans Jupyter",
    "text": "ğŸ“ˆ Pygwalker â€” Interface Tableau dans Jupyter\nPygwalker transforme ton DataFrame en une interface drag & drop comme Tableau/Power BI, directement dans Jupyter.\n\nğŸ“¦ Installation\npip install pygwalker\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install pygwalker\n\nimport pygwalker as pyg\n\n# Lancer l'interface interactive\nwalker = pyg.walk(df_exemple)\n\n\n\n\nâœ¨ Comment utiliser Pygwalker\n\nGlisser-dÃ©poser les colonnes sur les axes X, Y, Color, Size\nChoisir le type de graphique (bar, line, scatter, heatmapâ€¦)\nFiltrer les donnÃ©es visuellement\nExporter la configuration pour la rÃ©utiliser\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Pygwalker                                                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   FIELDS         â”‚                                              â”‚\nâ”‚                  â”‚         [Graphique interactif]               â”‚\nâ”‚   ğŸ“Š nom         â”‚                                              â”‚\nâ”‚   ğŸ“Š age         â”‚              â–ˆâ–ˆâ–ˆâ–ˆ                            â”‚\nâ”‚   ğŸ“Š ville       â”‚         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚\nâ”‚   ğŸ“Š salaire     â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚\nâ”‚   ğŸ“Š experience  â”‚                                              â”‚\nâ”‚                  â”‚                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  X: ville        â”‚  Y: salaire    Color: experience             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Cas dâ€™usage\n\nExploration visuelle rapide sans Ã©crire de code matplotlib\nPrÃ©sentation Ã  des non-techniques\nPrototypage de dashboards avant de coder",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“‹ RÃ©capitulatif â€” Quel outil choisir ?",
    "text": "ğŸ“‹ RÃ©capitulatif â€” Quel outil choisir ?\n\n\n\n\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nPremier aperÃ§u rapide dâ€™un dataset\nydata-profiling (minimal=True)\n\n\nComparer data1 vs data2\nSweetviz\n\n\nExploration interactive (comme Excel)\nD-Tale\n\n\nCrÃ©er des graphiques sans code\nPygwalker\n\n\nPoser des questions en franÃ§ais\nJulius.ai\n\n\nDonnÃ©es sensibles (pas de cloud)\nD-Tale ou ydata-profiling (tout local)\n\n\nGÃ©nÃ©rer du code Pandas\nJulius.ai ou D-Tale\n\n\n\n\nğŸ“¦ Installation complÃ¨te\npip install ydata-profiling sweetviz dtale pygwalker\n\n\nâš ï¸ Bonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nUtiliser ces outils pour explorer\nLes utiliser en production\n\n\nCopier le code gÃ©nÃ©rÃ© dans ton pipeline\nDÃ©pendre de lâ€™interface pour le traitement\n\n\nPartager les rapports HTML avec lâ€™Ã©quipe\nEnvoyer des donnÃ©es sensibles sur Julius.ai\n\n\nCombiner plusieurs outils\nSe limiter Ã  un seul\n\n\n\n\nğŸ’¡ Workflow recommandÃ© : 1. Julius.ai pour les premiÃ¨res questions 2. ydata-profiling pour un rapport complet 3. D-Tale pour explorer interactivement 4. Copier le code dans ton pipeline Pandas",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.3 Nettoyage des donnÃ©es",
    "text": "1.3 Nettoyage des donnÃ©es\n\n\nCode\n# DÃ©tecter les valeurs manquantes\nprint(\"â“ Valeurs manquantes par colonne :\")\nprint(df.isnull().sum())\nprint(f\"\\nTotal de valeurs manquantes : {df.isnull().sum().sum()}\")\n\n# Visualiser les lignes avec des valeurs manquantes\nprint(\"\\nğŸ” Lignes avec des NaN :\")\nprint(df[df.isnull().any(axis=1)])\n\n\n\n\nCode\n# StratÃ©gies de gestion des valeurs manquantes\n\n# 1. Supprimer les lignes avec des NaN\ndf_drop = df.dropna()\nprint(\"ğŸ—‘ï¸ AprÃ¨s suppression des lignes avec NaN :\")\nprint(df_drop)\n\n# 2. Remplir avec une valeur par dÃ©faut\ndf_fill = df.fillna({\n    'age': df['age'].median(),\n    'salaire': df['salaire'].mean()\n})\nprint(\"\\nâœ¨ AprÃ¨s remplissage des NaN :\")\nprint(df_fill)\n\n# 3. Forward fill (propager la valeur prÃ©cÃ©dente)\ndf_ffill = df.fillna(method='ffill')\nprint(\"\\nâ¡ï¸ AprÃ¨s forward fill :\")\nprint(df_ffill)\n\n\n\n\nCode\n# Supprimer les doublons\ndf_with_duplicates = pd.DataFrame({\n    'id': [1, 2, 3, 2, 4],\n    'nom': ['Alice', 'Bob', 'Charlie', 'Bob', 'David']\n})\n\nprint(\"Avant suppression des doublons :\")\nprint(df_with_duplicates)\n\ndf_no_duplicates = df_with_duplicates.drop_duplicates()\nprint(\"\\nAprÃ¨s suppression :\")\nprint(df_no_duplicates)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.4 SÃ©lection et filtrage",
    "text": "1.4 SÃ©lection et filtrage\n\n\nCode\n# Utilisons le DataFrame nettoyÃ©\ndf_clean = df_fill.copy()\n\n# SÃ©lectionner une colonne\nprint(\"ğŸ“Œ Colonne 'nom' :\")\nprint(df_clean['nom'])\n\n# SÃ©lectionner plusieurs colonnes\nprint(\"\\nğŸ“Œ Colonnes 'nom' et 'ville' :\")\nprint(df_clean[['nom', 'ville']])\n\n# Filtrer les lignes\nprint(\"\\nğŸ” EmployÃ©s de Paris :\")\nprint(df_clean[df_clean['ville'] == 'Paris'])\n\n# Filtres multiples\nprint(\"\\nğŸ” EmployÃ©s de Paris avec salaire &gt; 50000 :\")\nprint(df_clean[(df_clean['ville'] == 'Paris') & (df_clean['salaire'] &gt; 50000)])\n\n\n\n\nCode\n# Indexation avancÃ©e avec loc et iloc\n\n# loc : par label/nom\nprint(\"ğŸ“ loc[0, 'nom'] :\")\nprint(df_clean.loc[0, 'nom'])\n\n# iloc : par position numÃ©rique\nprint(\"\\nğŸ“ iloc[0, 0] (premiÃ¨re ligne, premiÃ¨re colonne) :\")\nprint(df_clean.iloc[0, 0])\n\n# SÃ©lection de plages\nprint(\"\\nğŸ“ loc[0:2, ['nom', 'age']] :\")\nprint(df_clean.loc[0:2, ['nom', 'age']])",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.5 GroupBy et agrÃ©gations",
    "text": "1.5 GroupBy et agrÃ©gations\n\n\nCode\n# Grouper par ville et calculer des statistiques\nprint(\"ğŸ“Š Statistiques par ville :\")\ngrouped = df_clean.groupby('ville').agg({\n    'nom': 'count',\n    'age': ['mean', 'min', 'max'],\n    'salaire': ['mean', 'sum']\n})\nprint(grouped)\n\n# Renommer les colonnes pour plus de clartÃ©\nprint(\"\\nğŸ“Š Salaire moyen par ville :\")\nsalaire_moyen = df_clean.groupby('ville')['salaire'].mean().round(2)\nprint(salaire_moyen)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.6 Apply vs Vectorisation",
    "text": "1.6 Apply vs Vectorisation\n\n\nCode\n# CrÃ©er une colonne calculÃ©e\n\n# MÃ©thode 1 : Apply (plus lent mais flexible)\ndef categoriser_age(age):\n    if age &lt; 30:\n        return 'Junior'\n    elif age &lt; 40:\n        return 'Senior'\n    else:\n        return 'Expert'\n\ndf_clean['categorie_apply'] = df_clean['age'].apply(categoriser_age)\n\n# MÃ©thode 2 : Vectorisation (plus rapide)\ndf_clean['categorie_vect'] = pd.cut(\n    df_clean['age'],\n    bins=[0, 30, 40, 100],\n    labels=['Junior', 'Senior', 'Expert']\n)\n\nprint(\"ğŸ”§ Colonnes calculÃ©es :\")\nprint(df_clean[['nom', 'age', 'categorie_apply', 'categorie_vect']])\n\n\n\n\nCode\n# Comparaison de performance (sur un grand dataset)\nimport time\n\n# CrÃ©er un grand DataFrame\nbig_df = pd.DataFrame({\n    'valeur': np.random.randint(1, 100, 100000)\n})\n\n# MÃ©thode Apply\nstart = time.time()\nbig_df['double_apply'] = big_df['valeur'].apply(lambda x: x * 2)\ntime_apply = time.time() - start\n\n# MÃ©thode VectorisÃ©e\nstart = time.time()\nbig_df['double_vect'] = big_df['valeur'] * 2\ntime_vect = time.time() - start\n\nprint(f\"â±ï¸ Temps Apply : {time_apply:.4f}s\")\nprint(f\"â±ï¸ Temps Vectorisation : {time_vect:.4f}s\")\nprint(f\"ğŸš€ Vectorisation est {time_apply/time_vect:.1f}x plus rapide !\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.7 Gestion de la mÃ©moire",
    "text": "1.7 Gestion de la mÃ©moire\n\n\nCode\n# VÃ©rifier l'utilisation mÃ©moire\nprint(\"ğŸ’¾ Utilisation mÃ©moire par colonne :\")\nprint(df_clean.memory_usage(deep=True))\nprint(f\"\\nTotal : {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n\n\n\nCode\n# Optimiser les types de donnÃ©es\ndf_optimized = df_clean.copy()\n\n# Avant optimisation\nprint(\"Avant optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n# Convertir en types plus efficaces\ndf_optimized['age'] = df_optimized['age'].astype('int8')\ndf_optimized['salaire'] = df_optimized['salaire'].astype('int32')\ndf_optimized['ville'] = df_optimized['ville'].astype('category')\n\nprint(\"\\nAprÃ¨s optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.8 Manipulation de dates",
    "text": "1.8 Manipulation de dates\n\n\nCode\n# CrÃ©er un DataFrame avec des dates\ndf_dates = pd.DataFrame({\n    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],\n    'montant': [1000, 1500, 1200, 1800]\n})\n\n# Convertir en datetime\ndf_dates['date'] = pd.to_datetime(df_dates['date_str'])\n\n# Extraire des composantes\ndf_dates['annee'] = df_dates['date'].dt.year\ndf_dates['mois'] = df_dates['date'].dt.month\ndf_dates['jour'] = df_dates['date'].dt.day\ndf_dates['nom_mois'] = df_dates['date'].dt.month_name()\ndf_dates['jour_semaine'] = df_dates['date'].dt.day_name()\n\nprint(\"ğŸ“… DataFrame avec dates extraites :\")\nprint(df_dates)\n\n\n\n\nCode\n# Calculs avec les dates\ndf_dates['jours_depuis_debut'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n# Ajouter/soustraire des pÃ©riodes\ndf_dates['date_plus_30j'] = df_dates['date'] + pd.Timedelta(days=30)\ndf_dates['date_moins_1mois'] = df_dates['date'] - pd.DateOffset(months=1)\n\nprint(\"ğŸ“… Calculs de dates :\")\nprint(df_dates[['date', 'jours_depuis_debut', 'date_plus_30j', 'date_moins_1mois']])",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.9 Export de donnÃ©es",
    "text": "1.9 Export de donnÃ©es\n\n\nCode\n# Export CSV\ndf_clean.to_csv('employes_clean.csv', index=False)\nprint(\"âœ… Export CSV rÃ©ussi\")\n\n# Export JSON\ndf_clean.to_json('employes_clean.json', orient='records', indent=2)\nprint(\"âœ… Export JSON rÃ©ussi\")\n\n# Export Parquet (format columnar, trÃ¨s efficace)\ndf_clean.to_parquet('employes_clean.parquet', index=False)\nprint(\"âœ… Export Parquet rÃ©ussi\")\n\n# Export Excel\ndf_clean.to_excel('employes_clean.xlsx', index=False, sheet_name='EmployÃ©s')\nprint(\"âœ… Export Excel rÃ©ussi\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 1 : Pandas",
    "text": "ğŸ¯ Exercice Pratique 1 : Pandas\nObjectif : Analyser un fichier de ventes\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, quantitÃ©, prix, date)\nCalculer le chiffre dâ€™affaires total\nTrouver le produit le plus vendu\nCalculer les ventes mensuelles\nExporter le rÃ©sultat en CSV\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques linÃ©aires (Line Plots)",
    "text": "ğŸ“Š Graphiques linÃ©aires (Line Plots)\n\n\nCode\n# DonnÃ©es pour un graphique linÃ©aire\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# CrÃ©ation du graphique\nplt.figure(figsize=(12, 6))\nplt.plot(x, y1, label='Sin(x)', color='blue', linewidth=2)\nplt.plot(x, y2, label='Cos(x)', color='red', linestyle='--', linewidth=2)\n\n# Personnalisation\nplt.title('Fonctions trigonomÃ©triques', fontsize=16, fontweight='bold')\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques Ã  barres (Bar Plots)",
    "text": "ğŸ“Š Graphiques Ã  barres (Bar Plots)\n\n\nCode\n# DonnÃ©es de ventes par mois\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\nventes_2023 = [1200, 1500, 1800, 1600, 2000, 2200]\nventes_2024 = [1400, 1700, 1900, 1800, 2300, 2500]\n\nx = np.arange(len(mois))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Barres groupÃ©es\nbars1 = ax.bar(x - width/2, ventes_2023, width, label='2023', color='steelblue')\nbars2 = ax.bar(x + width/2, ventes_2024, width, label='2024', color='coral')\n\n# Personnalisation\nax.set_title('Comparaison des ventes 2023 vs 2024', fontsize=16, fontweight='bold')\nax.set_xlabel('Mois', fontsize=12)\nax.set_ylabel('Ventes (â‚¬)', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(mois)\nax.legend()\n\n# Ajouter les valeurs sur les barres\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height}', xy=(bar.get_x() + bar.get_width()/2, height),\n                xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Nuages de points (Scatter Plots)",
    "text": "ğŸ“Š Nuages de points (Scatter Plots)\n\n\nCode\n# DonnÃ©es alÃ©atoires avec corrÃ©lation\nnp.random.seed(42)\nx = np.random.randn(100)\ny = 2 * x + np.random.randn(100) * 0.5\ncolors = np.random.rand(100)\nsizes = np.random.rand(100) * 200\n\n# Scatter plot avec couleurs et tailles variables\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(x, y, c=colors, s=sizes, alpha=0.6, cmap='viridis')\n\n# Ajouter une ligne de tendance\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x, p(x), 'r--', linewidth=2, label=f'Tendance: y = {z[0]:.2f}x + {z[1]:.2f}')\n\nplt.colorbar(scatter, label='Valeur')\nplt.title('Nuage de points avec ligne de tendance', fontsize=16, fontweight='bold')\nplt.xlabel('Variable X')\nplt.ylabel('Variable Y')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Histogrammes",
    "text": "ğŸ“Š Histogrammes\n\n\nCode\n# DonnÃ©es de distribution\nnp.random.seed(42)\ndata_normal = np.random.normal(loc=50, scale=10, size=1000)\ndata_skewed = np.random.exponential(scale=10, size=1000)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme distribution normale\naxes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\naxes[0].set_title('Distribution Normale', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Valeur')\naxes[0].set_ylabel('FrÃ©quence')\naxes[0].legend()\n\n# Histogramme distribution exponentielle\naxes[1].hist(data_skewed, bins=30, color='coral', edgecolor='black', alpha=0.7)\naxes[1].axvline(data_skewed.mean(), color='red', linestyle='--', label=f'Moyenne: {data_skewed.mean():.1f}')\naxes[1].set_title('Distribution Exponentielle', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Valeur')\naxes[1].set_ylabel('FrÃ©quence')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques circulaires (Pie Charts)",
    "text": "ğŸ“Š Graphiques circulaires (Pie Charts)\n\n\nCode\n# DonnÃ©es de rÃ©partition\ncategories = ['Produit A', 'Produit B', 'Produit C', 'Produit D', 'Autres']\nparts = [35, 25, 20, 15, 5]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\nexplode = (0.05, 0, 0, 0, 0)  # Mettre en Ã©vidence le premier segment\n\nplt.figure(figsize=(10, 8))\nwedges, texts, autotexts = plt.pie(parts, labels=categories, colors=colors, explode=explode,\n                                    autopct='%1.1f%%', startangle=90, shadow=True)\n\n# AmÃ©liorer l'apparence du texte\nfor autotext in autotexts:\n    autotext.set_fontsize(11)\n    autotext.set_fontweight('bold')\n\nplt.title('RÃ©partition des ventes par produit', fontsize=16, fontweight='bold')\nplt.axis('equal')  # Assure que le cercle est bien rond\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Sous-graphiques (Subplots)",
    "text": "ğŸ“Š Sous-graphiques (Subplots)\n\n\nCode\n# CrÃ©er une grille de sous-graphiques\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# DonnÃ©es\nx = np.linspace(0, 10, 50)\n\n# Graphique 1: Ligne\naxes[0, 0].plot(x, np.sin(x), 'b-', linewidth=2)\naxes[0, 0].set_title('Graphique linÃ©aire')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Sin(X)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Graphique 2: Barres\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 56, 78]\naxes[0, 1].bar(categories, values, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\naxes[0, 1].set_title('Graphique Ã  barres')\n\n# Graphique 3: Scatter\nx_scatter = np.random.rand(50)\ny_scatter = np.random.rand(50)\naxes[1, 0].scatter(x_scatter, y_scatter, c='purple', alpha=0.6, s=100)\naxes[1, 0].set_title('Nuage de points')\n\n# Graphique 4: Histogramme\ndata = np.random.randn(1000)\naxes[1, 1].hist(data, bins=30, color='orange', edgecolor='black', alpha=0.7)\naxes[1, 1].set_title('Histogramme')\n\nplt.suptitle('Tableau de bord - Vue d\\'ensemble', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ’¾ Sauvegarder des graphiques",
    "text": "ğŸ’¾ Sauvegarder des graphiques\n\n\nCode\n# CrÃ©er un graphique Ã  sauvegarder\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), 'b-', linewidth=2, label='Sin(x)')\nax.set_title('Graphique Ã  exporter', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Sauvegarder dans diffÃ©rents formats\nfig.savefig('graphique.png', dpi=300, bbox_inches='tight')\nfig.savefig('graphique.pdf', bbox_inches='tight')\nfig.savefig('graphique.svg', bbox_inches='tight')\n\nprint(\"âœ… Graphiques sauvegardÃ©s en PNG, PDF et SVG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique : Matplotlib",
    "text": "ğŸ¯ Exercice Pratique : Matplotlib\nObjectif : CrÃ©er un tableau de bord de visualisation\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, mois, ventes, profit)\nCrÃ©er 4 sous-graphiques montrant :\n\nÃ‰volution des ventes mensuelles (ligne)\nVentes par produit (barres)\nRelation ventes/profit (scatter)\nDistribution des profits (histogramme)\n\nPersonnaliser les couleurs et ajouter des titres\nSauvegarder le rÃ©sultat en PNG\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“¦ Charger des jeux de donnÃ©es intÃ©grÃ©s",
    "text": "ğŸ“¦ Charger des jeux de donnÃ©es intÃ©grÃ©s\n\n\nCode\n# Seaborn propose des jeux de donnÃ©es pour s'entraÃ®ner\ntips = sns.load_dataset('tips')\nprint(\"ğŸ“Š Dataset 'tips' :\")\nprint(tips.head())\nprint(f\"\\nDimensions : {tips.shape}\")\nprint(f\"\\nColonnes : {list(tips.columns)}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des distributions",
    "text": "ğŸ“Š Visualisation des distributions\n\n\nCode\n# Histogramme avec KDE (Kernel Density Estimation)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme simple\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0], color='steelblue')\naxes[0].set_title('Distribution du montant total', fontsize=14, fontweight='bold')\n\n# Histogramme avec hue (groupement)\nsns.histplot(data=tips, x='total_bill', hue='time', kde=True, ax=axes[1])\naxes[1].set_title('Distribution par moment de la journÃ©e', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# KDE plot (densitÃ© de probabilitÃ©)\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=tips, x='total_bill', hue='day', fill=True, alpha=0.5)\nplt.title('DensitÃ© du montant total par jour', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des donnÃ©es catÃ©gorielles",
    "text": "ğŸ“Š Visualisation des donnÃ©es catÃ©gorielles\n\n\nCode\n# Box plot - Distribution par catÃ©gorie\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Box plot simple\nsns.boxplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Set2')\naxes[0].set_title('Montant total par jour', fontsize=14, fontweight='bold')\n\n# Box plot avec hue\nsns.boxplot(data=tips, x='day', y='total_bill', hue='sex', ax=axes[1], palette='Set1')\naxes[1].set_title('Montant total par jour et sexe', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Violin plot - Combine box plot et KDE\nplt.figure(figsize=(12, 6))\nsns.violinplot(data=tips, x='day', y='total_bill', hue='sex', split=True, palette='muted')\nplt.title('Distribution du montant par jour et sexe (Violin Plot)', fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nCode\n# Bar plot avec estimation statistique\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Bar plot avec intervalle de confiance\nsns.barplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Blues_d', errorbar='ci')\naxes[0].set_title('Montant moyen par jour (avec IC 95%)', fontsize=14, fontweight='bold')\n\n# Count plot (compte les occurrences)\nsns.countplot(data=tips, x='day', hue='time', ax=axes[1], palette='Set2')\naxes[1].set_title('Nombre de repas par jour', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des relations entre variables",
    "text": "ğŸ“Š Visualisation des relations entre variables\n\n\nCode\n# Scatter plot avec rÃ©gression\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Scatter plot simple avec rÃ©gression\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[0], color='coral')\naxes[0].set_title('Relation montant/pourboire avec rÃ©gression', fontsize=14, fontweight='bold')\n\n# Scatter plot avec hue et style\nsns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', style='sex', \n                size='size', sizes=(50, 200), ax=axes[1], palette='Set1')\naxes[1].set_title('Relation multidimensionnelle', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# lmplot - RÃ©gression avec facettes\ng = sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', col='time', \n               height=5, aspect=1.2, palette='Set1')\ng.fig.suptitle('RÃ©gression par moment et statut fumeur', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Heatmaps et matrices de corrÃ©lation",
    "text": "ğŸ“Š Heatmaps et matrices de corrÃ©lation\n\n\nCode\n# Matrice de corrÃ©lation\n# SÃ©lectionner uniquement les colonnes numÃ©riques\nnumeric_cols = tips.select_dtypes(include=[np.number])\ncorrelation_matrix = numeric_cols.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', linewidths=0.5, square=True)\nplt.title('Matrice de corrÃ©lation', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Heatmap de donnÃ©es pivotÃ©es\npivot_data = tips.pivot_table(values='tip', index='day', columns='time', aggfunc='mean')\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)\nplt.title('Pourboire moyen par jour et moment', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Pair plots (visualisation multivariÃ©e)",
    "text": "ğŸ“Š Pair plots (visualisation multivariÃ©e)\n\n\nCode\n# Pair plot - Toutes les combinaisons de variables\ng = sns.pairplot(tips, hue='time', palette='Set1', diag_kind='kde', \n                 plot_kws={'alpha': 0.6}, height=2.5)\ng.fig.suptitle('Pair Plot - Dataset Tips', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š FacetGrid - Graphiques multi-facettes",
    "text": "ğŸ“Š FacetGrid - Graphiques multi-facettes\n\n\nCode\n# CrÃ©er une grille de facettes\ng = sns.FacetGrid(tips, col='time', row='smoker', height=4, aspect=1.2)\ng.map_dataframe(sns.histplot, x='total_bill', kde=True)\ng.add_legend()\ng.fig.suptitle('Distribution du montant par temps et statut fumeur', y=1.02, \n               fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Joint plots - Distributions jointes",
    "text": "ğŸ“Š Joint plots - Distributions jointes\n\n\nCode\n# Joint plot avec distributions marginales\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', \n                  height=8, ratio=4, color='coral')\ng.fig.suptitle('Distribution jointe montant/pourboire', y=1.02, fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nCode\n# Joint plot avec hexbin (pour grandes quantitÃ©s de donnÃ©es)\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='hex', \n                  height=8, ratio=4, cmap='Blues')\ng.fig.suptitle('Distribution jointe (Hexbin)', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¨ Personnalisation des styles",
    "text": "ğŸ¨ Personnalisation des styles\n\n\nCode\n# Explorer diffÃ©rents styles\nstyles = ['white', 'dark', 'whitegrid', 'darkgrid', 'ticks']\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\n\nfor ax, style in zip(axes, styles):\n    with sns.axes_style(style):\n        sns.histplot(tips['total_bill'], ax=ax, color='steelblue')\n        ax.set_title(f\"Style: {style}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Palettes de couleurs\npalettes = ['deep', 'muted', 'bright', 'pastel', 'dark', 'colorblind']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor ax, palette in zip(axes, palettes):\n    sns.barplot(data=tips, x='day', y='total_bill', palette=palette, ax=ax)\n    ax.set_title(f\"Palette: {palette}\", fontsize=12)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Exemple : Tableau de bord complet",
    "text": "ğŸ“Š Exemple : Tableau de bord complet\n\n\nCode\n# CrÃ©er un tableau de bord d'analyse complet\nsns.set_theme(style='whitegrid')\n\nfig = plt.figure(figsize=(16, 12))\n\n# CrÃ©er une grille personnalisÃ©e\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Distribution des montants\nax1 = fig.add_subplot(gs[0, 0])\nsns.histplot(data=tips, x='total_bill', kde=True, ax=ax1, color='steelblue')\nax1.set_title('Distribution des montants', fontweight='bold')\n\n# 2. Distribution des pourboires\nax2 = fig.add_subplot(gs[0, 1])\nsns.histplot(data=tips, x='tip', kde=True, ax=ax2, color='coral')\nax2.set_title('Distribution des pourboires', fontweight='bold')\n\n# 3. Relation montant/pourboire\nax3 = fig.add_subplot(gs[0, 2])\nsns.regplot(data=tips, x='total_bill', y='tip', ax=ax3, color='purple', scatter_kws={'alpha':0.5})\nax3.set_title('Montant vs Pourboire', fontweight='bold')\n\n# 4. Boxplot par jour\nax4 = fig.add_subplot(gs[1, 0])\nsns.boxplot(data=tips, x='day', y='total_bill', ax=ax4, palette='Set2')\nax4.set_title('Montants par jour', fontweight='bold')\n\n# 5. Violin plot par temps\nax5 = fig.add_subplot(gs[1, 1])\nsns.violinplot(data=tips, x='time', y='total_bill', hue='sex', split=True, ax=ax5, palette='muted')\nax5.set_title('Distribution par temps et sexe', fontweight='bold')\n\n# 6. Count plot\nax6 = fig.add_subplot(gs[1, 2])\nsns.countplot(data=tips, x='day', hue='time', ax=ax6, palette='Set1')\nax6.set_title('Nombre de repas', fontweight='bold')\n\n# 7. Heatmap de corrÃ©lation (grande)\nax7 = fig.add_subplot(gs[2, :])\npivot = tips.pivot_table(values='tip', index='day', columns='size', aggfunc='mean')\nsns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax7, linewidths=0.5)\nax7.set_title('Pourboire moyen par jour et taille de groupe', fontweight='bold')\n\nplt.suptitle('ğŸ“Š Tableau de bord - Analyse des pourboires', fontsize=18, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.savefig('dashboard_seaborn.png', dpi=300, bbox_inches='tight')\nprint(\"âœ… Dashboard sauvegardÃ© en PNG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique : Seaborn",
    "text": "ğŸ¯ Exercice Pratique : Seaborn\nObjectif : Analyser le dataset â€˜titanicâ€™ de Seaborn\n\nCharger le dataset avec sns.load_dataset('titanic')\nCrÃ©er un tableau de bord avec :\n\nDistribution des Ã¢ges par classe (violin plot)\nTaux de survie par sexe et classe (bar plot)\nMatrice de corrÃ©lation des variables numÃ©riques (heatmap)\nRelation Ã¢ge/tarif avec survie en couleur (scatter plot)\n\nUtiliser FacetGrid pour analyser les survivants par sexe et classe\nSauvegarder votre tableau de bord\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Charger le dataset\ntitanic = sns.load_dataset('titanic')\nprint(titanic.head())\nprint(f\"\\nDimensions : {titanic.shape}\")\n\n# Votre code de visualisation ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-de-base-avec-matplotlib",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-de-base-avec-matplotlib",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques de base avec Matplotlib",
    "text": "ğŸ“Š Graphiques de base avec Matplotlib\n\n\nCode\n# DonnÃ©es pour les graphiques\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\nventes = [1200, 1500, 1800, 1600, 2000, 2200]\n\n# CrÃ©er une figure avec 2x2 sous-graphiques\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Graphique linÃ©aire\naxes[0, 0].plot(mois, ventes, marker='o', linewidth=2, color='steelblue')\naxes[0, 0].set_title('ğŸ“ˆ Ã‰volution des ventes', fontweight='bold')\naxes[0, 0].set_xlabel('Mois')\naxes[0, 0].set_ylabel('Ventes (â‚¬)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Graphique Ã  barres\ncolors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\naxes[0, 1].bar(mois, ventes, color=colors, edgecolor='black')\naxes[0, 1].set_title('ğŸ“Š Ventes par mois', fontweight='bold')\naxes[0, 1].set_xlabel('Mois')\naxes[0, 1].set_ylabel('Ventes (â‚¬)')\n\n# 3. Graphique circulaire\naxes[1, 0].pie(ventes, labels=mois, autopct='%1.1f%%', colors=colors)\naxes[1, 0].set_title('ğŸ¥§ RÃ©partition des ventes', fontweight='bold')\n\n# 4. Nuage de points\nnp.random.seed(42)\nx = np.random.randn(50)\ny = 2 * x + np.random.randn(50) * 0.5\naxes[1, 1].scatter(x, y, c='coral', alpha=0.7, s=100)\naxes[1, 1].set_title('ğŸ“ Nuage de points', fontweight='bold')\naxes[1, 1].set_xlabel('Variable X')\naxes[1, 1].set_ylabel('Variable Y')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('graphiques_matplotlib.png', dpi=150)\nprint(\"âœ… Graphique sauvegardÃ© en PNG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes-et-distributions",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes-et-distributions",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Histogrammes et distributions",
    "text": "ğŸ“Š Histogrammes et distributions\n\n\nCode\n# GÃ©nÃ©rer des donnÃ©es de distribution\nnp.random.seed(42)\ndata_normal = np.random.normal(loc=50, scale=10, size=1000)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme simple\naxes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\naxes[0].set_title('Distribution des donnÃ©es', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Valeur')\naxes[0].set_ylabel('FrÃ©quence')\naxes[0].legend()\n\n# Box plot\ndata_multi = [np.random.normal(50, 10, 100), np.random.normal(60, 15, 100), np.random.normal(45, 8, 100)]\nbp = axes[1].boxplot(data_multi, labels=['Groupe A', 'Groupe B', 'Groupe C'], patch_artist=True)\ncolors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\naxes[1].set_title('Comparaison des groupes', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('Valeur')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisations-avec-seaborn",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisations-avec-seaborn",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisations avec Seaborn",
    "text": "ğŸ“Š Visualisations avec Seaborn\n\n\nCode\n# CrÃ©er un tableau de bord avec Seaborn\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Distribution avec KDE\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0, 0], color='steelblue')\naxes[0, 0].set_title('Distribution du montant', fontweight='bold')\n\n# 2. Box plot par catÃ©gorie\nsns.boxplot(data=tips, x='day', y='total_bill', palette='Set2', ax=axes[0, 1])\naxes[0, 1].set_title('Montant par jour', fontweight='bold')\n\n# 3. Scatter plot avec rÃ©gression\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[1, 0], color='coral')\naxes[1, 0].set_title('Relation montant/pourboire', fontweight='bold')\n\n# 4. Count plot\nsns.countplot(data=tips, x='day', hue='time', palette='Set1', ax=axes[1, 1])\naxes[1, 1].set_title('Nombre de repas par jour', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Heatmap de corrÃ©lation\nplt.figure(figsize=(8, 6))\nnumeric_cols = tips.select_dtypes(include=[np.number])\ncorrelation = numeric_cols.corr()\n\nsns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', linewidths=0.5, square=True)\nplt.title('Matrice de corrÃ©lation', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-visualisation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-visualisation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice : Visualisation",
    "text": "ğŸ¯ Exercice : Visualisation\nCrÃ©er un tableau de bord de visualisation avec les donnÃ©es df_clean (employÃ©s) :\n\nDistribution des salaires (histogramme)\nSalaire moyen par ville (bar plot)\nRelation Ã¢ge/salaire (scatter plot)\nRÃ©partition par ville (pie chart)\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# CrÃ©ez votre tableau de bord ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.1 Nettoyage de base",
    "text": "2.1 Nettoyage de base\n\n\nCode\n# DonnÃ©es textuelles brutes\ntextes = pd.DataFrame({\n    'texte': [\n        '  BONJOUR   ',\n        'Salut tout le monde!',\n        'Python_est_gÃ©nial',\n        'Data-Engineering-2024'\n    ]\n})\n\n# Nettoyage basique\ntextes['clean'] = textes['texte'].str.strip()  # Supprimer espaces\ntextes['lower'] = textes['texte'].str.lower()  # Minuscules\ntextes['upper'] = textes['texte'].str.upper()  # Majuscules\ntextes['replace'] = textes['texte'].str.replace('_', ' ')  # Remplacer\n\nprint(\"ğŸ§¹ Nettoyage de texte :\")\nprint(textes)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.2 MÃ©thodes Pandas string (.str accessor)",
    "text": "2.2 MÃ©thodes Pandas string (.str accessor)\n\n\nCode\n# DonnÃ©es d'exemple\ndf_text = pd.DataFrame({\n    'email': ['alice@example.com', 'bob@test.org', 'charlie@mail.fr'],\n    'nom_complet': ['Jean Dupont', 'Marie Martin', 'Pierre Durand'],\n    'telephone': ['0612345678', '06-98-76-54-32', '06 11 22 33 44']\n})\n\n# VÃ©rifier si contient\ndf_text['email_gmail'] = df_text['email'].str.contains('gmail')\n\n# Commencer/finir par\ndf_text['email_com'] = df_text['email'].str.endswith('.com')\n\n# Extraire le domaine\ndf_text['domaine'] = df_text['email'].str.split('@').str[1]\n\n# SÃ©parer nom et prÃ©nom\ndf_text[['prenom', 'nom']] = df_text['nom_complet'].str.split(' ', expand=True)\n\n# Longueur\ndf_text['longueur_nom'] = df_text['nom_complet'].str.len()\n\nprint(\"ğŸ”¤ MÃ©thodes string :\")\nprint(df_text)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.3 Expressions rÃ©guliÃ¨res (Regex)",
    "text": "2.3 Expressions rÃ©guliÃ¨res (Regex)\n\n\nCode\nimport re\n\n# Exemples de regex courantes\ntexte_test = \"\"\"\nContact: alice@example.com ou bob@test.org\nTÃ©lÃ©phones: 06.12.34.56.78, 01-23-45-67-89\nURL: https://www.example.com\nPrix: 29.99â‚¬, 15.50â‚¬, 100â‚¬\n\"\"\"\n\n# Extraire les emails\nemails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', texte_test)\nprint(\"ğŸ“§ Emails trouvÃ©s :\")\nprint(emails)\n\n# Extraire les tÃ©lÃ©phones\ntelephones = re.findall(r'\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}', texte_test)\nprint(\"\\nğŸ“ TÃ©lÃ©phones trouvÃ©s :\")\nprint(telephones)\n\n# Extraire les URLs\nurls = re.findall(r'https?://[^\\s]+', texte_test)\nprint(\"\\nğŸ”— URLs trouvÃ©es :\")\nprint(urls)\n\n# Extraire les prix\nprix = re.findall(r'\\d+\\.?\\d*â‚¬', texte_test)\nprint(\"\\nğŸ’° Prix trouvÃ©s :\")\nprint(prix)\n\n\n\n\nCode\n# Validation avec regex\ndef valider_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\n# Test\nemails_test = ['alice@example.com', 'bob@invalid', 'charlie.fr', 'david@test.org']\nfor email in emails_test:\n    valide = \"âœ…\" if valider_email(email) else \"âŒ\"\n    print(f\"{valide} {email}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs",
    "text": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs\n\n\nCode\n# Exemple de logs Apache/Nginx\nlogs = \"\"\"\n192.168.1.1 - - [01/Dec/2024:10:15:30 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.2 - - [01/Dec/2024:10:16:45 +0000] \"POST /api/login HTTP/1.1\" 401 567\n192.168.1.3 - - [01/Dec/2024:10:17:20 +0000] \"GET /api/products HTTP/1.1\" 200 8901\n\"\"\"\n\n# Pattern pour parser les logs\npattern = r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)'\n\n# Extraire les informations\nmatches = re.findall(pattern, logs)\n\n# CrÃ©er un DataFrame\ndf_logs = pd.DataFrame(matches, columns=[\n    'ip', 'timestamp', 'methode', 'endpoint', 'protocole', 'status', 'bytes'\n])\n\n# Convertir les types\ndf_logs['status'] = df_logs['status'].astype(int)\ndf_logs['bytes'] = df_logs['bytes'].astype(int)\n\nprint(\"ğŸ“‹ Logs parsÃ©s :\")\nprint(df_logs)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.5 Gestion de lâ€™encodage",
    "text": "2.5 Gestion de lâ€™encodage\n\n\nCode\n# CrÃ©er un fichier avec encodage spÃ©cifique\ntexte_accentue = \"Voici du texte avec des accents : Ã©Ã Ã¹Ã´ Ã§Ã±\"\n\n# Sauvegarder en UTF-8\nwith open('test_utf8.txt', 'w', encoding='utf-8') as f:\n    f.write(texte_accentue)\n\n# Sauvegarder en Latin-1\nwith open('test_latin1.txt', 'w', encoding='latin-1') as f:\n    f.write(texte_accentue)\n\n# Lire avec le bon encodage\nprint(\"âœ… Lecture UTF-8 :\")\nwith open('test_utf8.txt', 'r', encoding='utf-8') as f:\n    print(f.read())\n\nprint(\"\\nâœ… Lecture Latin-1 :\")\nwith open('test_latin1.txt', 'r', encoding='latin-1') as f:\n    print(f.read())\n\n\n\n\nCode\n# DÃ©tecter l'encodage automatiquement\n!pip install chardet\n\nimport chardet\n\n# DÃ©tecter l'encodage d'un fichier\nwith open('test_utf8.txt', 'rb') as f:\n    result = chardet.detect(f.read())\n    print(f\"Encodage dÃ©tectÃ© : {result['encoding']} (confiance: {result['confidence']*100:.1f}%)\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 2 : Texte et Regex",
    "text": "ğŸ¯ Exercice Pratique 2 : Texte et Regex\nObjectif : Nettoyer et valider des donnÃ©es clients\n\nCrÃ©er un DataFrame avec nom, email, tÃ©lÃ©phone\nNettoyer les noms (trim, capitaliser)\nValider les emails avec regex\nNormaliser les numÃ©ros de tÃ©lÃ©phone (format uniforme)\nExporter les donnÃ©es valides uniquement\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.1 Manipulation de JSON",
    "text": "3.1 Manipulation de JSON\n\n\nCode\nimport json\n\n# CrÃ©er un dictionnaire Python\ndata = {\n    \"nom\": \"Alice\",\n    \"age\": 30,\n    \"competences\": [\"Python\", \"SQL\", \"Pandas\"],\n    \"actif\": True\n}\n\n# Convertir en JSON\njson_str = json.dumps(data, indent=2)\nprint(\"ğŸ“„ JSON formatÃ© :\")\nprint(json_str)\n\n# Reconvertir en dictionnaire\ndata_reloaded = json.loads(json_str)\nprint(\"\\nğŸ”„ RechargÃ© :\")\nprint(data_reloaded)\n\n\n\n\nCode\n# Sauvegarder et lire des fichiers JSON\n\n# Sauvegarder\nwith open('data.json', 'w', encoding='utf-8') as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\nprint(\"âœ… JSON sauvegardÃ©\")\n\n# Lire\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data_loaded = json.load(f)\nprint(\"\\nğŸ“‚ JSON chargÃ© :\")\nprint(data_loaded)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.2 Appels API avec requests",
    "text": "3.2 Appels API avec requests\n\n\nCode\nimport requests\n\n# API publique gratuite : JSONPlaceholder\nurl = \"https://jsonplaceholder.typicode.com/users\"\n\n# GET Request\nresponse = requests.get(url)\n\n# VÃ©rifier le statut\nprint(f\"Status code: {response.status_code}\")\n\nif response.status_code == 200:\n    users = response.json()\n    print(f\"\\nâœ… {len(users)} utilisateurs rÃ©cupÃ©rÃ©s\")\n    print(\"\\nPremier utilisateur :\")\n    print(json.dumps(users[0], indent=2))\nelse:\n    print(\"âŒ Erreur lors de la requÃªte\")\n\n\n\n\nCode\n# Convertir en DataFrame\ndf_users = pd.json_normalize(users)\nprint(\"ğŸ‘¥ DataFrame des utilisateurs :\")\nprint(df_users.head())\nprint(f\"\\nColonnes : {df_users.columns.tolist()}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.3 Gestion des erreurs HTTP",
    "text": "3.3 Gestion des erreurs HTTP\n\n\nCode\ndef fetch_data_safe(url):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec gestion d'erreurs\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # LÃ¨ve une exception si statut &gt;= 400\n        return response.json()\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : le serveur met trop de temps Ã  rÃ©pondre\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        print(f\"âŒ Erreur HTTP : {e}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        print(f\"âŒ Erreur de connexion : {e}\")\n        return None\n\n# Test avec une URL valide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"âœ… DonnÃ©es rÃ©cupÃ©rÃ©es :\")\n    print(json.dumps(data, indent=2))\n\n# Test avec une URL invalide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/invalid\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.4 Authentification API",
    "text": "3.4 Authentification API\n\n\nCode\n# Exemple 1 : API Key dans les headers\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY_HERE\",\n    \"Content-Type\": \"application/json\"\n}\n\n# response = requests.get(url, headers=headers)\n\n# Exemple 2 : API Key dans les paramÃ¨tres\nparams = {\n    \"api_key\": \"YOUR_API_KEY_HERE\",\n    \"format\": \"json\"\n}\n\n# response = requests.get(url, params=params)\n\n# Exemple 3 : Basic Auth\nfrom requests.auth import HTTPBasicAuth\n\n# response = requests.get(url, auth=HTTPBasicAuth('username', 'password'))\n\nprint(\"ğŸ’¡ Les exemples ci-dessus montrent diffÃ©rentes mÃ©thodes d'authentification\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.5 Pagination dâ€™APIs",
    "text": "3.5 Pagination dâ€™APIs\n\n\nCode\ndef fetch_all_pages(base_url, max_pages=5):\n    \"\"\"RÃ©cupÃ¨re toutes les pages d'une API paginÃ©e\"\"\"\n    all_data = []\n    \n    for page in range(1, max_pages + 1):\n        url = f\"{base_url}?_page={page}&_limit=10\"\n        print(f\"ğŸ“„ RÃ©cupÃ©ration page {page}...\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            if not data:  # Plus de donnÃ©es\n                break\n            all_data.extend(data)\n        else:\n            print(f\"âŒ Erreur page {page}\")\n            break\n    \n    return all_data\n\n# Test avec JSONPlaceholder\nposts = fetch_all_pages(\"https://jsonplaceholder.typicode.com/posts\", max_pages=3)\nprint(f\"\\nâœ… Total rÃ©cupÃ©rÃ© : {len(posts)} posts\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.6 Rate Limiting et Retry Logic",
    "text": "3.6 Rate Limiting et Retry Logic\n\n\nCode\nimport time\nfrom datetime import datetime\n\ndef fetch_with_retry(url, max_retries=3, delay=2):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec retry et backoff exponentiel\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"ğŸ”„ Tentative {attempt + 1}/{max_retries}\")\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            print(f\"âŒ Erreur : {e}\")\n            if attempt &lt; max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Backoff exponentiel\n                print(f\"â³ Attente de {wait_time}s avant nouvelle tentative...\")\n                time.sleep(wait_time)\n            else:\n                print(\"âŒ Ã‰chec aprÃ¨s toutes les tentatives\")\n                return None\n\n# Test\ndata = fetch_with_retry(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"\\nâœ… SuccÃ¨s !\")\n\n\n\n\nCode\n# Rate limiting simple\ndef fetch_with_rate_limit(urls, requests_per_second=2):\n    \"\"\"RÃ©cupÃ¨re plusieurs URLs en respectant un rate limit\"\"\"\n    delay = 1.0 / requests_per_second\n    results = []\n    \n    for url in urls:\n        start = time.time()\n        print(f\"â¬ RÃ©cupÃ©ration : {url}\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            results.append(response.json())\n        \n        elapsed = time.time() - start\n        sleep_time = max(0, delay - elapsed)\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n    \n    return results\n\n# Test\nurls = [\n    \"https://jsonplaceholder.typicode.com/users/1\",\n    \"https://jsonplaceholder.typicode.com/users/2\",\n    \"https://jsonplaceholder.typicode.com/users/3\"\n]\n\nstart_time = time.time()\nresults = fetch_with_rate_limit(urls, requests_per_second=1)\ntotal_time = time.time() - start_time\n\nprint(f\"\\nâœ… {len(results)} URLs rÃ©cupÃ©rÃ©es en {total_time:.2f}s\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.7 JSON imbriquÃ© complexe",
    "text": "3.7 JSON imbriquÃ© complexe\n\n\nCode\n# JSON complexe imbriquÃ©\ncomplex_json = {\n    \"id\": 1,\n    \"nom\": \"Entreprise A\",\n    \"employes\": [\n        {\n            \"id\": 101,\n            \"nom\": \"Alice\",\n            \"competences\": [\"Python\", \"SQL\"],\n            \"adresse\": {\"ville\": \"Paris\", \"code_postal\": \"75001\"}\n        },\n        {\n            \"id\": 102,\n            \"nom\": \"Bob\",\n            \"competences\": [\"Java\", \"Docker\"],\n            \"adresse\": {\"ville\": \"Lyon\", \"code_postal\": \"69001\"}\n        }\n    ]\n}\n\n# Normaliser avec json_normalize\ndf_complex = pd.json_normalize(\n    complex_json,\n    record_path='employes',\n    meta=['nom'],\n    meta_prefix='entreprise_'\n)\n\nprint(\"ğŸ”„ JSON normalisÃ© :\")\nprint(df_complex)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 3 : APIs",
    "text": "ğŸ¯ Exercice Pratique 3 : APIs\nObjectif : RÃ©cupÃ©rer et analyser des donnÃ©es dâ€™une API publique\n\nUtiliser lâ€™API JSONPlaceholder pour rÃ©cupÃ©rer les posts\nConvertir en DataFrame\nCompter le nombre de posts par utilisateur\nRÃ©cupÃ©rer les dÃ©tails des 5 utilisateurs les plus actifs\nExporter le rÃ©sultat en JSON\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.1 VÃ©rifications basiques",
    "text": "4.1 VÃ©rifications basiques\n\n\nCode\n# CrÃ©er des donnÃ©es de test\ntest_data = pd.DataFrame({\n    'user_id': [1, 2, 3, 2, 5],\n    'email': ['alice@test.com', 'bob@test', None, 'bob@test', 'eve@test.com'],\n    'age': [25, 150, -5, 30, 28],\n    'salaire': [45000, 55000, 60000, 55000, None]\n})\n\nprint(\"ğŸ“Š DonnÃ©es de test :\")\nprint(test_data)\n\n# VÃ©rifications\nprint(\"\\nğŸ” VÃ©rifications :\")\nprint(f\"Colonnes manquantes : {set(['user_id', 'email', 'age', 'salaire']) - set(test_data.columns)}\")\nprint(f\"Valeurs nulles : {test_data.isnull().sum().sum()}\")\nprint(f\"Doublons : {test_data.duplicated().sum()}\")\nprint(f\"Types : \\n{test_data.dtypes}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.2 Classe de validation complÃ¨te",
    "text": "4.2 Classe de validation complÃ¨te\n\n\nCode\nclass DataValidator:\n    \"\"\"Validateur simple pour DataFrames\"\"\"\n    \n    def __init__(self, df):\n        self.df = df\n        self.errors = []\n    \n    def check_columns(self, required_columns):\n        \"\"\"VÃ©rifie prÃ©sence des colonnes requises\"\"\"\n        missing = set(required_columns) - set(self.df.columns)\n        if missing:\n            self.errors.append(f\"Colonnes manquantes: {missing}\")\n            return False\n        return True\n    \n    def check_nulls(self, max_null_pct=10):\n        \"\"\"VÃ©rifie le pourcentage de valeurs nulles\"\"\"\n        null_pct = (self.df.isnull().sum() / len(self.df)) * 100\n        violations = null_pct[null_pct &gt; max_null_pct]\n        if not violations.empty:\n            self.errors.append(f\"Trop de nulls: {violations.to_dict()}\")\n            return False\n        return True\n    \n    def check_range(self, column, min_val, max_val):\n        \"\"\"VÃ©rifie que les valeurs sont dans une plage\"\"\"\n        if column in self.df.columns:\n            violations = self.df[(self.df[column] &lt; min_val) | (self.df[column] &gt; max_val)]\n            if len(violations) &gt; 0:\n                self.errors.append(f\"{column}: {len(violations)} valeurs hors plage [{min_val}, {max_val}]\")\n                return False\n        return True\n    \n    def check_duplicates(self, subset=None):\n        \"\"\"VÃ©rifie les doublons\"\"\"\n        duplicates = self.df.duplicated(subset=subset).sum()\n        if duplicates &gt; 0:\n            self.errors.append(f\"{duplicates} doublons trouvÃ©s\")\n            return False\n        return True\n    \n    def check_types(self, column, expected_type):\n        \"\"\"VÃ©rifie le type d'une colonne\"\"\"\n        if column in self.df.columns:\n            if self.df[column].dtype != expected_type:\n                self.errors.append(f\"{column}: type attendu {expected_type}, obtenu {self.df[column].dtype}\")\n                return False\n        return True\n    \n    def validate(self):\n        \"\"\"Retourne True si valide, False sinon\"\"\"\n        return len(self.errors) == 0\n    \n    def report(self):\n        \"\"\"GÃ©nÃ¨re un rapport de validation\"\"\"\n        return {\n            'is_valid': self.validate(),\n            'total_errors': len(self.errors),\n            'errors': self.errors\n        }\n\n# Utilisation\nvalidator = DataValidator(test_data)\nvalidator.check_columns(['user_id', 'email', 'age'])\nvalidator.check_nulls(max_null_pct=15)\nvalidator.check_range('age', 0, 120)\nvalidator.check_duplicates(subset=['user_id', 'email'])\n\nreport = validator.report()\nprint(\"\\nğŸ“‹ Rapport de validation:\")\nprint(json.dumps(report, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.3 Validation avec schÃ©ma",
    "text": "4.3 Validation avec schÃ©ma\n\n\nCode\n# DÃ©finir un schÃ©ma de validation\nschema = {\n    'user_id': {'type': 'int64', 'nullable': False, 'unique': True},\n    'email': {'type': 'object', 'nullable': False, 'pattern': r'.+@.+\\..+'},\n    'age': {'type': 'int64', 'nullable': False, 'min': 0, 'max': 120},\n    'salaire': {'type': 'int64', 'nullable': True, 'min': 0}\n}\n\ndef validate_schema(df, schema):\n    \"\"\"Valide un DataFrame contre un schÃ©ma\"\"\"\n    errors = []\n    \n    for column, rules in schema.items():\n        # VÃ©rifier si la colonne existe\n        if column not in df.columns:\n            errors.append(f\"Colonne manquante: {column}\")\n            continue\n        \n        # VÃ©rifier les nulls\n        if not rules.get('nullable', True) and df[column].isnull().any():\n            errors.append(f\"{column}: contient des valeurs nulles\")\n        \n        # VÃ©rifier l'unicitÃ©\n        if rules.get('unique', False) and df[column].duplicated().any():\n            errors.append(f\"{column}: contient des doublons\")\n        \n        # VÃ©rifier la plage\n        if 'min' in rules:\n            violations = df[df[column] &lt; rules['min']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &lt; {rules['min']}\")\n        \n        if 'max' in rules:\n            violations = df[df[column] &gt; rules['max']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &gt; {rules['max']}\")\n        \n        # VÃ©rifier le pattern (pour les strings)\n        if 'pattern' in rules:\n            pattern = rules['pattern']\n            invalid = df[column].dropna()[~df[column].dropna().str.match(pattern)]\n            if len(invalid) &gt; 0:\n                errors.append(f\"{column}: {len(invalid)} valeurs ne matchent pas le pattern\")\n    \n    return {\n        'is_valid': len(errors) == 0,\n        'errors': errors\n    }\n\n# Test\nresult = validate_schema(test_data, schema)\nprint(\"\\nğŸ“‹ Validation avec schÃ©ma :\")\nprint(json.dumps(result, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 4 : Validation",
    "text": "ğŸ¯ Exercice Pratique 4 : Validation\nObjectif : CrÃ©er un validateur pour des transactions\n\nCrÃ©er un DataFrame de transactions (id, date, montant, type)\nDÃ©finir un schÃ©ma de validation\nValider que toutes les transactions ont un montant positif\nVÃ©rifier quâ€™il nâ€™y a pas de doublons dâ€™ID\nGÃ©nÃ©rer un rapport de qualitÃ©\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.1 Architecture du pipeline",
    "text": "5.1 Architecture du pipeline\n\n\nCode\n# CrÃ©er la structure de dossiers\nfrom pathlib import Path\n\ndirs = ['data/raw', 'data/processed', 'data/output', 'logs']\nfor dir_path in dirs:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"âœ… Structure de dossiers crÃ©Ã©e\")\nprint(\"\\nğŸ“ Structure :\")\nprint(\"\"\"\nproject/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”œâ”€â”€ processed/\nâ”‚   â””â”€â”€ output/\nâ””â”€â”€ logs/\n\"\"\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.2 Configuration et Logging",
    "text": "5.2 Configuration et Logging\n\n\nCode\nimport logging\nfrom datetime import datetime\n\n# Configuration du logging\nlog_file = f\"logs/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger('ETL_Pipeline')\nlogger.info(\"ğŸš€ Pipeline dÃ©marrÃ©\")\n\n\n\n\nCode\n# Configuration centralisÃ©e\nclass Config:\n    \"\"\"Configuration du pipeline\"\"\"\n    # Chemins\n    RAW_DATA_DIR = 'data/raw'\n    PROCESSED_DATA_DIR = 'data/processed'\n    OUTPUT_DIR = 'data/output'\n    \n    # API\n    API_URL = 'https://jsonplaceholder.typicode.com'\n    API_TIMEOUT = 10\n    API_MAX_RETRIES = 3\n    \n    # Validation\n    MAX_NULL_PCT = 10\n    \n    # Export\n    EXPORT_FORMATS = ['csv', 'parquet', 'json']\n\nconfig = Config()\nlogger.info(\"âš™ï¸ Configuration chargÃ©e\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.3 Ã‰tape 1 : Extract",
    "text": "5.3 Ã‰tape 1 : Extract\n\n\nCode\ndef extract_from_api(url, max_retries=3):\n    \"\"\"Extrait des donnÃ©es depuis une API\"\"\"\n    logger.info(f\"ğŸ“¥ Extraction depuis {url}\")\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=config.API_TIMEOUT)\n            response.raise_for_status()\n            data = response.json()\n            logger.info(f\"âœ… {len(data)} enregistrements extraits\")\n            return data\n        except Exception as e:\n            logger.warning(f\"âš ï¸ Tentative {attempt + 1}/{max_retries} Ã©chouÃ©e: {e}\")\n            if attempt == max_retries - 1:\n                logger.error(\"âŒ Extraction Ã©chouÃ©e\")\n                raise\n            time.sleep(2 ** attempt)\n\n# Test extraction\nusers_data = extract_from_api(f\"{config.API_URL}/users\")\ndf_raw = pd.DataFrame(users_data)\n\n# Sauvegarder les donnÃ©es brutes\nraw_file = f\"{config.RAW_DATA_DIR}/users_raw_{datetime.now().strftime('%Y%m%d')}.csv\"\ndf_raw.to_csv(raw_file, index=False)\nlogger.info(f\"ğŸ’¾ DonnÃ©es brutes sauvegardÃ©es: {raw_file}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.4 Ã‰tape 2 : Transform",
    "text": "5.4 Ã‰tape 2 : Transform\n\n\nCode\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    logger.info(\"ğŸ”„ DÃ©but de la transformation\")\n    \n    df_transformed = df.copy()\n    \n    # 1. Normaliser les colonnes imbriquÃ©es\n    if 'address' in df.columns:\n        address_df = pd.json_normalize(df['address'])\n        address_df.columns = ['address_' + col for col in address_df.columns]\n        df_transformed = pd.concat([df_transformed.drop('address', axis=1), address_df], axis=1)\n        logger.info(\"âœ… Colonnes adresse normalisÃ©es\")\n    \n    # 2. Nettoyer les noms de colonnes\n    df_transformed.columns = df_transformed.columns.str.lower().str.replace('.', '_')\n    logger.info(\"âœ… Noms de colonnes nettoyÃ©s\")\n    \n    # 3. GÃ©rer les valeurs manquantes\n    null_counts = df_transformed.isnull().sum()\n    if null_counts.sum() &gt; 0:\n        logger.warning(f\"âš ï¸ {null_counts.sum()} valeurs manquantes dÃ©tectÃ©es\")\n        df_transformed = df_transformed.dropna()\n        logger.info(\"âœ… Valeurs manquantes supprimÃ©es\")\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    if 'name' in df_transformed.columns:\n        df_transformed['name_length'] = df_transformed['name'].str.len()\n        logger.info(\"âœ… Colonne dÃ©rivÃ©e 'name_length' crÃ©Ã©e\")\n    \n    # 5. Ajouter metadata\n    df_transformed['processed_at'] = datetime.now().isoformat()\n    \n    logger.info(f\"âœ… Transformation terminÃ©e: {len(df_transformed)} lignes\")\n    return df_transformed\n\n# Test transformation\ndf_transformed = transform_data(df_raw)\nprint(\"\\nğŸ“Š DonnÃ©es transformÃ©es :\")\nprint(df_transformed.head())\nprint(f\"\\nColonnes: {df_transformed.columns.tolist()}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.5 Ã‰tape 3 : Validate",
    "text": "5.5 Ã‰tape 3 : Validate\n\n\nCode\ndef validate_data(df):\n    \"\"\"Valide la qualitÃ© des donnÃ©es\"\"\"\n    logger.info(\"ğŸ” DÃ©but de la validation\")\n    \n    validator = DataValidator(df)\n    \n    # DÃ©finir les rÃ¨gles de validation\n    required_columns = ['id', 'name', 'email']\n    validator.check_columns(required_columns)\n    validator.check_nulls(max_null_pct=config.MAX_NULL_PCT)\n    validator.check_duplicates(subset=['id'])\n    \n    # GÃ©nÃ©rer le rapport\n    report = validator.report()\n    \n    if report['is_valid']:\n        logger.info(\"âœ… Validation rÃ©ussie\")\n    else:\n        logger.error(f\"âŒ Validation Ã©chouÃ©e: {report['total_errors']} erreurs\")\n        for error in report['errors']:\n            logger.error(f\"  - {error}\")\n    \n    return report\n\n# Test validation\nvalidation_report = validate_data(df_transformed)\nprint(\"\\nğŸ“‹ Rapport de validation :\")\nprint(json.dumps(validation_report, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.6 Ã‰tape 4 : Load",
    "text": "5.6 Ã‰tape 4 : Load\n\n\nCode\ndef load_data(df, base_filename):\n    \"\"\"Exporte les donnÃ©es dans plusieurs formats\"\"\"\n    logger.info(\"ğŸ’¾ DÃ©but de l'export\")\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    files_created = []\n    \n    for format_type in config.EXPORT_FORMATS:\n        filename = f\"{config.OUTPUT_DIR}/{base_filename}_{timestamp}.{format_type}\"\n        \n        try:\n            if format_type == 'csv':\n                df.to_csv(filename, index=False)\n            elif format_type == 'parquet':\n                df.to_parquet(filename, index=False)\n            elif format_type == 'json':\n                df.to_json(filename, orient='records', indent=2)\n            \n            file_size = Path(filename).stat().st_size / 1024  # KB\n            logger.info(f\"âœ… Export {format_type.upper()}: {filename} ({file_size:.2f} KB)\")\n            files_created.append(filename)\n        except Exception as e:\n            logger.error(f\"âŒ Erreur export {format_type}: {e}\")\n    \n    return files_created\n\n# Test export\nexported_files = load_data(df_transformed, 'users_processed')\nprint(\"\\nğŸ“¦ Fichiers exportÃ©s :\")\nfor file in exported_files:\n    print(f\"  - {file}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.7 Pipeline complet",
    "text": "5.7 Pipeline complet\n\n\nCode\ndef run_pipeline():\n    \"\"\"ExÃ©cute le pipeline complet\"\"\"\n    start_time = time.time()\n    logger.info(\"=\"*50)\n    logger.info(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE\")\n    logger.info(\"=\"*50)\n    \n    try:\n        # EXTRACT\n        logger.info(\"\\nğŸ“¥ PHASE 1: EXTRACTION\")\n        data = extract_from_api(f\"{config.API_URL}/users\")\n        df = pd.DataFrame(data)\n        logger.info(f\"Lignes extraites: {len(df)}\")\n        \n        # TRANSFORM\n        logger.info(\"\\nğŸ”„ PHASE 2: TRANSFORMATION\")\n        df_clean = transform_data(df)\n        logger.info(f\"Lignes aprÃ¨s transformation: {len(df_clean)}\")\n        \n        # VALIDATE\n        logger.info(\"\\nğŸ” PHASE 3: VALIDATION\")\n        validation = validate_data(df_clean)\n        \n        if not validation['is_valid']:\n            logger.error(\"âŒ Validation Ã©chouÃ©e, arrÃªt du pipeline\")\n            return False\n        \n        # LOAD\n        logger.info(\"\\nğŸ’¾ PHASE 4: EXPORT\")\n        files = load_data(df_clean, 'users_final')\n        \n        # STATISTIQUES\n        duration = time.time() - start_time\n        logger.info(\"\\n\" + \"=\"*50)\n        logger.info(\"ğŸ“Š STATISTIQUES DU PIPELINE\")\n        logger.info(\"=\"*50)\n        logger.info(f\"DurÃ©e totale: {duration:.2f}s\")\n        logger.info(f\"Lignes traitÃ©es: {len(df_clean)}\")\n        logger.info(f\"Fichiers crÃ©Ã©s: {len(files)}\")\n        logger.info(f\"Taux de rÃ©ussite: 100%\")\n        logger.info(\"=\"*50)\n        logger.info(\"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        logger.info(\"=\"*50)\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"âŒ ERREUR FATALE: {e}\")\n        logger.exception(\"Stack trace:\")\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Final : Pipeline Complet",
    "text": "ğŸ¯ Exercice Final : Pipeline Complet\nObjectif : CrÃ©er votre propre pipeline ETL\n\nExtraire des donnÃ©es de posts depuis JSONPlaceholder\nEnrichir avec les donnÃ©es utilisateurs\nCalculer des statistiques (posts par utilisateur, mots par post, etc.)\nValider la qualitÃ©\nExporter dans tous les formats\nAjouter un logging complet\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# CrÃ©ez votre pipeline complet ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "6ï¸âƒ£ Gestion des configurations",
    "text": "6ï¸âƒ£ Gestion des configurations\n\n\nCode\n# Installer python-dotenv\n!pip install python-dotenv\n\n# CrÃ©er un fichier .env (Ã  ne JAMAIS commiter)\nenv_content = \"\"\"\nAPI_KEY=votre_cle_api_secrete\nDATABASE_URL=postgresql://user:password@localhost:5432/db\nENVIRONMENT=development\n\"\"\"\n\nwith open('.env', 'w') as f:\n    f.write(env_content)\n\nprint(\"âœ… Fichier .env crÃ©Ã©\")\nprint(\"âš ï¸ N'oubliez pas d'ajouter .env Ã  votre .gitignore !\")\n\n\n\n\nCode\nfrom dotenv import load_dotenv\nimport os\n\n# Charger les variables d'environnement\nload_dotenv()\n\n# AccÃ©der aux variables\napi_key = os.getenv('API_KEY')\ndb_url = os.getenv('DATABASE_URL')\nenv = os.getenv('ENVIRONMENT')\n\nprint(f\"ğŸ”‘ API Key: {api_key[:10]}...\")\nprint(f\"ğŸ—„ï¸ Database URL: {db_url[:30]}...\")\nprint(f\"ğŸŒ Environment: {env}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "7ï¸âƒ£ Tests unitaires basiques",
    "text": "7ï¸âƒ£ Tests unitaires basiques\n\n\nCode\n# Exemple de fonction Ã  tester\ndef calculer_age_moyen(df, colonne='age'):\n    \"\"\"Calcule l'Ã¢ge moyen d'un DataFrame\"\"\"\n    if colonne not in df.columns:\n        raise ValueError(f\"Colonne '{colonne}' introuvable\")\n    return df[colonne].mean()\n\n# Tests\ndef test_calculer_age_moyen():\n    # Test avec donnÃ©es valides\n    df_test = pd.DataFrame({'age': [20, 30, 40]})\n    assert calculer_age_moyen(df_test) == 30, \"Test 1 Ã©chouÃ©\"\n    print(\"âœ… Test 1: donnÃ©es valides\")\n    \n    # Test avec colonne manquante\n    try:\n        calculer_age_moyen(pd.DataFrame({'nom': ['Alice']}), 'age')\n        print(\"âŒ Test 2 Ã©chouÃ©: devrait lever une exception\")\n    except ValueError:\n        print(\"âœ… Test 2: exception levÃ©e correctement\")\n    \n    # Test avec valeurs nulles\n    df_null = pd.DataFrame({'age': [20, None, 40]})\n    result = calculer_age_moyen(df_null)\n    assert result == 30, \"Test 3 Ã©chouÃ©\"\n    print(\"âœ… Test 3: gestion des nulls\")\n    \n    print(\"\\nğŸ‰ Tous les tests passent !\")\n\n# ExÃ©cuter les tests\ntest_calculer_age_moyen()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "Ce que tu as appris âœ…",
    "text": "Ce que tu as appris âœ…\n\n\n\n\n\n\n\nSection\nCompÃ©tences acquises\n\n\n\n\nPandas\nManipulation de donnÃ©es, nettoyage, agrÃ©gations, merges\n\n\nMatplotlib\nGraphiques de base, personnalisation, export\n\n\nSeaborn\nVisualisations statistiques, heatmaps, pair plots\n\n\nTexte & Regex\nNettoyage, parsing de logs, expressions rÃ©guliÃ¨res\n\n\nAPIs\nAppels REST, pagination, retry logic\n\n\nValidation\nSchÃ©mas, checks de qualitÃ©\n\n\nPipeline ETL\nArchitecture complÃ¨te Extract-Transform-Load\n\n\nBonnes pratiques\nLogging, configuration, tests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nDocumentation officielle\n\nPandas Documentation\nMatplotlib Documentation\nSeaborn Documentation\nRequests Documentation\n\n\n\nTutoriels et cours\n\nReal Python - Pandas\nKaggle Learn\nDataCamp\n\n\n\nOutils avancÃ©s Ã  explorer\n\nPolars â€” Alternative plus rapide Ã  Pandas\nGreat Expectations â€” Validation de donnÃ©es avancÃ©e\nPandera â€” SchÃ©mas de validation pour DataFrames",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises le traitement de donnÃ©es, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 06_intro_databases.ipynb â€” Introduction aux bases de donnÃ©es\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Data Processing pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "",
    "text": "Ce module couvre les fondamentaux et concepts avancÃ©s de SQL, avec une pratique directe via Python + DuckDB.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "âš™ï¸ Approche de ce cours",
    "text": "âš™ï¸ Approche de ce cours\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ğŸ¯ FOCUS DU COURS                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Ce cours utilise Python + DuckDB pour apprendre SQL          â”‚\nâ”‚                                                                 â”‚\nâ”‚   âœ… SQL standard (compatible PostgreSQL, MySQL, etc.)          â”‚\nâ”‚   âœ… ExÃ©cution directe dans Jupyter (pas de serveur)            â”‚\nâ”‚   âœ… IntÃ©gration native avec Pandas                             â”‚\nâ”‚   âœ… CompÃ©tences transfÃ©rables Ã  tout SGBD                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Le SQL appris ici fonctionne sur PostgreSQL, MySQL, SQLite, BigQuery, Snowflake, etc. avec des variations mineures de syntaxe.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nComprendre les concepts de tables, colonnes, clÃ©s\n\n\nâœ… Requis\nBases de Python (module 04 et 05)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Ã‰crire des requÃªtes SELECT, WHERE, ORDER BY\nâœ… Utiliser les fonctions dâ€™agrÃ©gation (COUNT, SUM, AVG)\nâœ… MaÃ®triser GROUP BY et HAVING\nâœ… Faire des jointures (JOIN, LEFT JOIN)\nâœ… Utiliser les fonctions de date (DATE_TRUNC, EXTRACT)\nâœ… Utiliser CASE, les CTEs et les Window Functions\nâœ… ExÃ©cuter du SQL depuis Python avec DuckDB\n\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter les cellules de code directement !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§  Câ€™est quoi SQL ?",
    "text": "ğŸ§  Câ€™est quoi SQL ?\nSQL (Structured Query Language) est un langage de requÃªtes pour interagir avec des bases de donnÃ©es relationnelles.\n\n\n\nAction\nCommande SQL\n\n\n\n\nğŸ” Rechercher\nSELECT\n\n\nâœï¸ InsÃ©rer\nINSERT\n\n\nğŸ”„ Modifier\nUPDATE\n\n\nğŸ—‘ï¸ Supprimer\nDELETE\n\n\nğŸ“Š AgrÃ©ger\nCOUNT, SUM, AVG\n\n\nğŸ”— Joindre\nJOIN",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§° Pourquoi câ€™est essentiel pour un Data Engineer ?",
    "text": "ğŸ§° Pourquoi câ€™est essentiel pour un Data Engineer ?\n\nInterroger les data warehouses (BigQuery, Snowflake, Redshiftâ€¦)\nExtraire / filtrer les donnÃ©es pour les pipelines\nVÃ©rifier la qualitÃ© des donnÃ©es\nCrÃ©er des vues et agrÃ©gats pour les dashboards",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸŒ Outils en ligne pour tester du SQL",
    "text": "ğŸŒ Outils en ligne pour tester du SQL\nAvant de plonger dans Python + DuckDB, sache quâ€™il existe de nombreux outils en ligne gratuits pour tester des requÃªtes SQL sur diffÃ©rents SGBD. Câ€™est utile pour :\n\nğŸ§ª Tester rapidement une requÃªte\nğŸ”„ VÃ©rifier la compatibilitÃ© entre SGBD\nğŸ“š Sâ€™entraÃ®ner sans rien installer\nğŸ¤ Partager des exemples avec des collÃ¨gues\n\n\nğŸ› ï¸ Outils recommandÃ©s\n\n\n\nOutil\nURL\nSGBD supportÃ©s\nPoints forts\n\n\n\n\nDB Fiddle\ndb-fiddle.com\nPostgreSQL, MySQL, SQLite\nInterface claire, partage facile\n\n\nSQL Fiddle\nsqlfiddle.com\nMySQL, PostgreSQL, Oracle, SQL Server\nLe classique, multi-SGBD\n\n\nSQLite Online\nsqliteonline.com\nSQLite, PostgreSQL, MySQL\nTrÃ¨s simple, import CSV\n\n\nProgramiz SQL\nprogramiz.com/sql/online-compiler\nSQLite\nIdÃ©al dÃ©butants, tutoriels intÃ©grÃ©s\n\n\nOneCompiler\nonecompiler.com/mysql\nMySQL, PostgreSQL, SQL Server\nRapide, moderne\n\n\nReplit\nreplit.com\nSQLite, PostgreSQL\nEnvironnement complet, collaboratif\n\n\n\n\n\nğŸ’¡ Exemple avec DB Fiddle\n\nAller sur db-fiddle.com\nChoisir le SGBD (ex: PostgreSQL 15)\nDans le panneau gauche, crÃ©er le schÃ©ma :\n\nCREATE TABLE employes (\n    id INT PRIMARY KEY,\n    nom VARCHAR(50),\n    salaire INT\n);\n\nINSERT INTO employes VALUES \n(1, 'Alice', 50000),\n(2, 'Bob', 60000),\n(3, 'Charlie', 55000);\n\nDans le panneau droit, Ã©crire ta requÃªte :\n\nSELECT nom, salaire \nFROM employes \nWHERE salaire &gt; 52000;\n\nCliquer sur Run et voir le rÃ©sultat !\n\n\n\nâš ï¸ DiffÃ©rences entre SGBD\nLe SQL est standardisÃ©, mais chaque SGBD a ses particularitÃ©s :\n\n\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nPostgreSQL\nMySQL\nSQL Server\nSQLite\n\n\n\n\nConcatÃ©nation\n\\|\\| ou CONCAT()\nCONCAT()\n+ ou CONCAT()\n\\|\\|\n\n\nLimite rÃ©sultats\nLIMIT 10\nLIMIT 10\nTOP 10\nLIMIT 10\n\n\nAuto-increment\nSERIAL\nAUTO_INCREMENT\nIDENTITY\nAUTOINCREMENT\n\n\nDate actuelle\nCURRENT_DATE\nCURDATE()\nGETDATE()\nDATE('now')\n\n\nBoolÃ©ens\nTRUE/FALSE\n1/0\n1/0\n1/0\n\n\n\n\nğŸ’¡ DuckDB utilise une syntaxe proche de PostgreSQL, ce qui est idÃ©al car PostgreSQL est le standard de facto en Data Engineering.\n\n\n\n\nğŸ¯ Pourquoi on utilise Python + DuckDB dans ce cours ?\n\n\n\nAvantage\nExplication\n\n\n\n\nTout-en-un\nSQL + Python dans le mÃªme notebook\n\n\nPas dâ€™installation serveur\nDuckDB tourne en mÃ©moire\n\n\nIntÃ©gration Pandas\nRÃ©sultats directement en DataFrame\n\n\nFichiers directs\nLire CSV/Parquet/JSON sans import\n\n\nSyntaxe standard\nCompatible PostgreSQL â†’ transfÃ©rable\n\n\nPerformance\nOptimisÃ© pour lâ€™analytics (OLAP)\n\n\n\n# Exemple : SQL â†’ DataFrame en 1 ligne\ndf = con.execute(\"SELECT * FROM clients WHERE pays = 'France'\").df()\n\nğŸ“ Les compÃ©tences SQL apprises ici sont directement applicables Ã  PostgreSQL, BigQuery, Snowflake, Redshift, et tout autre SGBD en production.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¦† Python + DuckDB â€” SQL dans ton notebook !",
    "text": "ğŸ¦† Python + DuckDB â€” SQL dans ton notebook !\n\nQuâ€™est-ce que DuckDB ?\nDuckDB est une base de donnÃ©es analytique embarquÃ©e (comme SQLite, mais optimisÃ©e pour lâ€™analytics).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    POURQUOI DUCKDB ?                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  âœ… ZÃ©ro installation serveur (fonctionne en mÃ©moire)       â”‚\nâ”‚  âœ… Syntaxe SQL standard (PostgreSQL-like)                  â”‚\nâ”‚  âœ… IntÃ©gration native avec Pandas                         â”‚\nâ”‚  âœ… TrÃ¨s rapide pour l'analytics (colonnar storage)        â”‚\nâ”‚  âœ… Lit directement CSV, Parquet, JSON                      â”‚\nâ”‚  âœ… Parfait pour apprendre SQL dans un notebook             â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“Š DuckDB vs autres bases\n\n\n\n\n\n\n\n\n\nCritÃ¨re\nSQLite\nDuckDB\nPostgreSQL\n\n\n\n\nInstallation\nEmbarquÃ©\nEmbarquÃ©\nServeur\n\n\nOptimisÃ© pour\nOLTP (transactions)\nOLAP (analytics)\nLes deux\n\n\nIntÃ©gration Pandas\nâš ï¸ Via SQLAlchemy\nâœ… Native\nâš ï¸ Via psycopg2\n\n\nFichiers CSV/Parquet\nâŒ Non\nâœ… Direct\nâŒ Non\n\n\nUsage\nMobile, embarquÃ©\nData Science, ETL\nProduction\n\n\n\n\n\nCode\n# ğŸ“¦ Installation de DuckDB\n!pip install duckdb --quiet\n\nprint(\"âœ… DuckDB installÃ© !\")\n\n\n\n\nCode\nimport duckdb\n\n# CrÃ©er une connexion en mÃ©moire\ncon = duckdb.connect(database=':memory:')\n\nprint(\"âœ… Connexion DuckDB crÃ©Ã©e !\")\nprint(f\"Version : {duckdb.__version__}\")\n\n\n\n\nCode\n# ğŸ“Š CrÃ©ation des tables de dÃ©monstration\n\n# Table clients\ncon.execute(\"\"\"\nCREATE TABLE clients (\n    id_client INTEGER PRIMARY KEY,\n    nom VARCHAR(50),\n    email VARCHAR(100),\n    pays VARCHAR(50),\n    date_inscription DATE\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO clients VALUES\n    (1, 'Alice', 'alice@mail.com', 'France', '2023-01-15'),\n    (2, 'Bob', 'bob@mail.com', 'France', '2023-02-20'),\n    (3, 'Charlie', 'charlie@mail.com', 'Allemagne', '2023-03-10'),\n    (4, 'Diana', 'diana@mail.com', 'Belgique', '2023-04-05'),\n    (5, 'Eve', 'eve@mail.com', 'France', '2023-05-12');\n\"\"\")\n\n# Table commandes\ncon.execute(\"\"\"\nCREATE TABLE commandes (\n    id_commande INTEGER PRIMARY KEY,\n    id_client INTEGER,\n    produit VARCHAR(50),\n    montant DECIMAL(10,2),\n    date_commande DATE,\n    FOREIGN KEY (id_client) REFERENCES clients(id_client)\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO commandes VALUES\n    (1, 1, 'Clavier', 50.00, '2023-07-12'),\n    (2, 1, 'Souris', 25.00, '2023-07-15'),\n    (3, 2, 'Ã‰cran', 120.00, '2023-08-01'),\n    (4, 1, 'Webcam', 45.00, '2023-08-20'),\n    (5, 4, 'Casque', 80.00, '2023-09-05'),\n    (6, 2, 'Clavier', 55.00, '2023-09-15'),\n    (7, 5, 'Souris', 30.00, '2023-10-01'),\n    (8, 4, 'Ã‰cran', 150.00, '2023-10-20');\n\"\"\")\n\nprint(\"âœ… Tables crÃ©Ã©es : clients (5 lignes), commandes (8 lignes)\")\n\n\n\n\nCode\n# ğŸ‘€ VÃ©rifier les donnÃ©es\nprint(\"ğŸ“‹ Table clients :\")\nprint(con.execute(\"SELECT * FROM clients\").fetchdf())\n\nprint(\"\\nğŸ“‹ Table commandes :\")\nprint(con.execute(\"SELECT * FROM commandes\").fetchdf())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet SQL â€” Commandes essentielles",
    "text": "ğŸ“‹ Cheatsheet SQL â€” Commandes essentielles\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\nExemple\n\n\n\n\nLecture\nSELECT\nSÃ©lectionner des colonnes\nSELECT nom, email FROM clients\n\n\n\nSELECT *\nToutes les colonnes\nSELECT * FROM clients\n\n\n\nDISTINCT\nValeurs uniques\nSELECT DISTINCT pays FROM clients\n\n\nFiltrage\nWHERE\nFiltrer les lignes\nWHERE pays = 'France'\n\n\n\nAND / OR\nConditions multiples\nWHERE age &gt; 18 AND pays = 'France'\n\n\n\nIN\nListe de valeurs\nWHERE pays IN ('France', 'Belgique')\n\n\n\nBETWEEN\nPlage de valeurs\nWHERE montant BETWEEN 50 AND 100\n\n\n\nLIKE\nRecherche pattern\nWHERE nom LIKE 'A%'\n\n\n\nIS NULL\nValeurs nulles\nWHERE email IS NULL\n\n\nTri\nORDER BY\nTrier les rÃ©sultats\nORDER BY nom ASC\n\n\n\nLIMIT\nLimiter le nombre\nLIMIT 10\n\n\nAgrÃ©gation\nCOUNT()\nCompter\nSELECT COUNT(*) FROM clients\n\n\n\nSUM()\nSomme\nSELECT SUM(montant) FROM commandes\n\n\n\nAVG()\nMoyenne\nSELECT AVG(montant) FROM commandes\n\n\n\nMIN() / MAX()\nMin / Max\nSELECT MAX(montant) FROM commandes\n\n\nGroupement\nGROUP BY\nRegrouper\nGROUP BY pays\n\n\n\nHAVING\nFiltrer aprÃ¨s agrÃ©gation\nHAVING COUNT(*) &gt; 5\n\n\nJointures\nJOIN\nJointure interne\nJOIN commandes ON ...\n\n\n\nLEFT JOIN\nJointure gauche\nLEFT JOIN commandes ON ...",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ” 1. SELECT â€” SÃ©lectionner des donnÃ©es",
    "text": "ğŸ” 1. SELECT â€” SÃ©lectionner des donnÃ©es\n\nSyntaxe de base\nSELECT colonne1, colonne2\nFROM table\nWHERE condition\nORDER BY colonne;\n\n\nCode\n# ğŸ” SELECT * â€” Toutes les colonnes\nquery = \"SELECT * FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” SELECT colonnes spÃ©cifiques\nquery = \"SELECT nom, email, pays FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” WHERE â€” Filtrer les lignes\nquery = \"\"\"\nSELECT nom, email, pays\nFROM clients\nWHERE pays = 'France'\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” ORDER BY + LIMIT\nquery = \"\"\"\nSELECT nom, montant, date_commande\nFROM commandes\nJOIN clients ON clients.id_client = commandes.id_client\nORDER BY montant DESC\nLIMIT 3\n\"\"\"\nprint(\"ğŸ† Top 3 des commandes les plus chÃ¨res :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“Š 2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX",
    "text": "ğŸ“Š 2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX\n\n\nCode\n# ğŸ“Š COUNT â€” Compter les lignes\nquery = \"SELECT COUNT(*) AS nb_clients FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“Š SUM, AVG, MIN, MAX\nquery = \"\"\"\nSELECT \n    COUNT(*) AS nb_commandes,\n    SUM(montant) AS total,\n    AVG(montant) AS moyenne,\n    MIN(montant) AS min,\n    MAX(montant) AS max\nFROM commandes\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“¦ 3. GROUP BY â€” Regrouper les donnÃ©es",
    "text": "ğŸ“¦ 3. GROUP BY â€” Regrouper les donnÃ©es\n\n\nCode\n# ğŸ“¦ GROUP BY â€” Nombre de clients par pays\nquery = \"\"\"\nSELECT pays, COUNT(*) AS nb_clients\nFROM clients\nGROUP BY pays\nORDER BY nb_clients DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“¦ Total des achats par client\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“¦ HAVING â€” Filtrer aprÃ¨s agrÃ©gation\n# Clients ayant dÃ©pensÃ© plus de 100â‚¬\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nHAVING SUM(cmd.montant) &gt; 100\nORDER BY total_achats DESC\n\"\"\"\nprint(\"ğŸ† Clients ayant dÃ©pensÃ© plus de 100â‚¬ :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ”— 4. JOIN â€” Combiner les tables",
    "text": "ğŸ”— 4. JOIN â€” Combiner les tables\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ INNER JOIN (JOIN)         â”‚ Seulement les correspondances      â”‚\nâ”‚ A âˆ© B                      â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ LEFT JOIN                 â”‚ Tout A + correspondances B         â”‚\nâ”‚ A + (A âˆ© B)               â”‚ (NULL si pas de correspondance)    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ RIGHT JOIN                â”‚ Tout B + correspondances A         â”‚\nâ”‚ B + (A âˆ© B)               â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ FULL OUTER JOIN           â”‚ Tout A + Tout B                    â”‚\nâ”‚ A âˆª B                      â”‚                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n# ğŸ”— INNER JOIN â€” Clients avec leurs commandes\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ”— LEFT JOIN â€” Tous les clients, mÃªme sans commande\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\nprint(\"ğŸ‘€ Remarque : Charlie n'a pas de commande (NULL)\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” Trouver les clients sans commande\nquery = \"\"\"\nSELECT c.nom, c.email\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nWHERE cmd.id_commande IS NULL\n\"\"\"\nprint(\"ğŸ˜´ Clients sans aucune commande :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“… 5. Fonctions de date",
    "text": "ğŸ“… 5. Fonctions de date\n\n\n\n\n\n\n\n\nFonction\nDescription\nExemple\n\n\n\n\nCURRENT_DATE\nDate du jour\n2024-01-15\n\n\nEXTRACT()\nExtraire une partie\nEXTRACT(YEAR FROM date) â†’ 2024\n\n\nDATE_TRUNC()\nTronquer Ã  une pÃ©riode\nDATE_TRUNC('month', date) â†’ 2024-01-01\n\n\nDATE_DIFF()\nDiffÃ©rence entre dates\nDATE_DIFF('day', date1, date2)\n\n\n\n\n\nCode\n# ğŸ“… Ventes par mois\nquery = \"\"\"\nSELECT \n    DATE_TRUNC('month', date_commande) AS mois,\n    SUM(montant) AS total_ventes,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois\n\"\"\"\nprint(\"ğŸ“Š Ventes mensuelles :\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“… Commandes par jour de la semaine\nquery = \"\"\"\nSELECT \n    EXTRACT(DOW FROM date_commande) AS jour_num,\n    CASE EXTRACT(DOW FROM date_commande)\n        WHEN 0 THEN 'Dimanche'\n        WHEN 1 THEN 'Lundi'\n        WHEN 2 THEN 'Mardi'\n        WHEN 3 THEN 'Mercredi'\n        WHEN 4 THEN 'Jeudi'\n        WHEN 5 THEN 'Vendredi'\n        WHEN 6 THEN 'Samedi'\n    END AS jour,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY EXTRACT(DOW FROM date_commande)\nORDER BY jour_num\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ”€ 6. CASE â€” Conditions dans les requÃªtes",
    "text": "ğŸ”€ 6. CASE â€” Conditions dans les requÃªtes\n\n\nCode\n# ğŸ”€ CASE â€” CatÃ©goriser les clients\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    CASE\n        WHEN SUM(cmd.montant) &gt;= 150 THEN 'ğŸ¥‡ Premium'\n        WHEN SUM(cmd.montant) &gt;= 100 THEN 'ğŸ¥ˆ Standard'\n        ELSE 'ğŸ¥‰ Basique'\n    END AS categorie\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“¦ 7. CTE (Common Table Expression) â€” RequÃªtes lisibles",
    "text": "ğŸ“¦ 7. CTE (Common Table Expression) â€” RequÃªtes lisibles\nLes CTEs permettent de crÃ©er des â€œtables temporairesâ€ pour clarifier les requÃªtes complexes.\nWITH nom_cte AS (\n    SELECT ...\n)\nSELECT * FROM nom_cte;\n\n\nCode\n# ğŸ“¦ CTE â€” Calcul intermÃ©diaire rÃ©utilisable\nquery = \"\"\"\nWITH total_par_client AS (\n    SELECT \n        c.id_client,\n        c.nom,\n        c.pays,\n        SUM(cmd.montant) AS total\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n    GROUP BY c.id_client, c.nom, c.pays\n)\nSELECT \n    nom,\n    pays,\n    total,\n    CASE WHEN total &gt; 100 THEN 'âœ… VIP' ELSE 'âŒ' END AS vip\nFROM total_par_client\nORDER BY total DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸªŸ 8. Window Functions â€” Calculs avancÃ©s",
    "text": "ğŸªŸ 8. Window Functions â€” Calculs avancÃ©s\nLes Window Functions permettent de faire des calculs sur un groupe de lignes sans les regrouper.\n\n\n\nFonction\nDescription\n\n\n\n\nROW_NUMBER()\nNumÃ©ro de ligne\n\n\nRANK()\nClassement (avec ex-aequo)\n\n\nDENSE_RANK()\nClassement sans saut\n\n\nLAG()\nValeur de la ligne prÃ©cÃ©dente\n\n\nLEAD()\nValeur de la ligne suivante\n\n\nSUM() OVER()\nSomme cumulative\n\n\n\n\n\nCode\n# ğŸ† RANK â€” Classement des clients par total d'achats\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    RANK() OVER (ORDER BY SUM(cmd.montant) DESC) AS rang\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“ˆ SUM OVER â€” Total cumulatif\nquery = \"\"\"\nSELECT \n    date_commande,\n    montant,\n    SUM(montant) OVER (ORDER BY date_commande) AS cumul\nFROM commandes\nORDER BY date_commande\n\"\"\"\nprint(\"ğŸ“ˆ Ã‰volution cumulative des ventes :\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“Š LAG â€” Comparer avec la pÃ©riode prÃ©cÃ©dente\nquery = \"\"\"\nWITH mensuel AS (\n    SELECT \n        DATE_TRUNC('month', date_commande) AS mois,\n        SUM(montant) AS total\n    FROM commandes\n    GROUP BY DATE_TRUNC('month', date_commande)\n)\nSELECT \n    mois,\n    total,\n    LAG(total) OVER (ORDER BY mois) AS total_precedent,\n    total - LAG(total) OVER (ORDER BY mois) AS evolution\nFROM mensuel\nORDER BY mois\n\"\"\"\nprint(\"ğŸ“Š Ã‰volution mensuelle :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¼ 9. IntÃ©gration avec Pandas",
    "text": "ğŸ¼ 9. IntÃ©gration avec Pandas\nDuckDB sâ€™intÃ¨gre parfaitement avec Pandas !\n\n\nCode\nimport pandas as pd\n\n# ğŸ¼ RÃ©cupÃ©rer le rÃ©sultat en DataFrame Pandas\nquery = \"\"\"\nSELECT c.nom, c.pays, SUM(cmd.montant) AS total\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom, c.pays\nORDER BY total DESC\n\"\"\"\n\ndf = con.execute(query).fetchdf()\nprint(type(df))  # C'est un DataFrame Pandas !\ndf\n\n\n\n\nCode\n# ğŸ¦† RequÃªter directement un DataFrame Pandas avec SQL !\ndf_exemple = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'A', 'B'],\n    'ventes': [100, 200, 150, 120, 180]\n})\n\n# DuckDB peut requÃªter le DataFrame directement\nresult = duckdb.query(\"\"\"\n    SELECT produit, SUM(ventes) AS total\n    FROM df_exemple\n    GROUP BY produit\n    ORDER BY total DESC\n\"\"\").fetchdf()\n\nprint(\"ğŸ”¥ SQL directement sur un DataFrame Pandas :\")\nresult\n\n\n\n\nCode\n# ğŸ“ DuckDB peut aussi lire directement des fichiers CSV/Parquet\n# Exemple (si tu as un fichier) :\n# duckdb.query(\"SELECT * FROM 'mon_fichier.csv' LIMIT 10\")\n# duckdb.query(\"SELECT * FROM 'data.parquet' WHERE date &gt; '2024-01-01'\")\n\nprint(\"ğŸ’¡ DuckDB peut lire directement :\")\nprint(\"   - CSV  : SELECT * FROM 'fichier.csv'\")\nprint(\"   - Parquet : SELECT * FROM 'fichier.parquet'\")\nprint(\"   - JSON : SELECT * FROM 'fichier.json'\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¯ 10. Exercices pratiques",
    "text": "ğŸ¯ 10. Exercices pratiques\nEssaie de rÃ©soudre ces exercices dans les cellules de code ci-dessous !\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nAfficher tous les clients de France, triÃ©s par nom.\n\n\nğŸ’¡ Solution\n\nSELECT * FROM clients WHERE pays = 'France' ORDER BY nom;\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nCalculer le montant moyen des commandes.\n\n\nğŸ’¡ Solution\n\nSELECT AVG(montant) AS montant_moyen FROM commandes;\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nAfficher le total des achats par client, y compris ceux sans commande (afficher 0).\n\n\nğŸ’¡ Solution\n\nSELECT c.nom, COALESCE(SUM(cmd.montant), 0) AS total\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total DESC;\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nCalculer les ventes par mois avec DATE_TRUNC.\n\n\nğŸ’¡ Solution\n\nSELECT DATE_TRUNC('month', date_commande) AS mois, SUM(montant) AS total\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois;\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nPour chaque client, afficher sa derniÃ¨re commande (produit et date). Indice : ROW_NUMBER()\n\n\nğŸ’¡ Solution\n\nWITH derniere AS (\n    SELECT c.nom, cmd.produit, cmd.date_commande,\n        ROW_NUMBER() OVER (PARTITION BY c.id_client ORDER BY cmd.date_commande DESC) AS rn\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n)\nSELECT nom, produit, date_commande FROM derniere WHERE rn = 1;\n\n\n\nCode\n# âœï¸ Espace pour tes exercices\nquery = \"\"\"\n-- Ã‰cris ta requÃªte ici\nSELECT * FROM clients LIMIT 5\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quelle commande affiche toutes les colonnes dâ€™une table clients ?\n\nSHOW * FROM clients;\n\nSELECT * FROM clients;\n\nLIST * FROM clients;\n\nDISPLAY * FROM clients;\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” SELECT * FROM clients; affiche toutes les colonnes.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre WHERE et HAVING ?\n\nAucune diffÃ©rence\n\nWHERE sâ€™utilise avant GROUP BY, HAVING aprÃ¨s\n\nHAVING est plus rapide\n\nWHERE ne peut filtrer que les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” WHERE filtre avant agrÃ©gation, HAVING filtre aprÃ¨s.\n\n\n\n\nâ“ Q3. Quel JOIN retourne UNIQUEMENT les lignes qui ont une correspondance ?\n\nLEFT JOIN\n\nRIGHT JOIN\n\nINNER JOIN\n\nFULL OUTER JOIN\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” INNER JOIN ne garde que les correspondances.\n\n\n\n\nâ“ Q4. Ã€ quoi sert une CTE (WITH) ?\n\nCrÃ©er une table permanente\n\nDÃ©finir une sous-requÃªte rÃ©utilisable\n\nSupprimer des donnÃ©es\n\nCrÃ©er un index\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les CTEs crÃ©ent des â€œtables temporairesâ€ pour clarifier les requÃªtes.\n\n\n\n\nâ“ Q5. Quelle Window Function permet de classer les lignes ?\n\nCOUNT()\n\nRANK()\n\nSUM()\n\nGROUP BY\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” RANK() OVER (ORDER BY ...) attribue un classement.\n\n\n\n\nâ“ Q6. Quel avantage de DuckDB pour un Data Engineer ?\n\nIl nÃ©cessite un serveur\n\nIl peut requÃªter directement des DataFrames Pandas\n\nIl ne supporte pas SQL standard\n\nIl ne lit pas les fichiers CSV\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” DuckDB peut requÃªter directement des DataFrames Pandas avec SQL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nğŸ® Pratiquer SQL en ligne\n\nSQLBolt â€” Tutoriel interactif\nMode SQL Tutorial â€” Cas business rÃ©els\nLeetCode SQL â€” DÃ©fis progressifs\nHackerRank SQL â€” Exercices variÃ©s\n\n\n\nğŸ¦† DuckDB\n\nDuckDB Documentation\nDuckDB SQL Reference\n\n\n\nğŸ“– Documentation SQL\n\nPostgreSQL Documentation\nW3Schools SQL",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant SQL et sais lâ€™exÃ©cuter depuis Python ! DÃ©couvrons les concepts du Big Data et les bases NoSQL.\nğŸ‘‰ Module suivant : 08_intro_big_data_nosql.ipynb â€” Big Data, traitement distribuÃ© et NoSQL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module SQL pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nAvoir suivi le module 07_sql_for_data_engineers",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre ce quâ€™est le Big Data et ses caractÃ©ristiques (5V)\nâœ… Expliquer pourquoi le traitement distribuÃ© est nÃ©cessaire\nâœ… DÃ©crire lâ€™architecture Hadoop (HDFS, MapReduce, YARN)\nâœ… Comprendre le modÃ¨le MapReduce\nâœ… Expliquer pourquoi Spark a remplacÃ© MapReduce\nâœ… DiffÃ©rencier Data Lake et Data Lakehouse\nâœ… Comprendre les architectures Lambda et Kappa\nâœ… ConnaÃ®tre le concept de Data Mesh\nâœ… Comprendre lâ€™architecture Medallion (Bronze/Silver/Gold)\nâœ… Comprendre le NoSQL et ses diffÃ©rents types\nâœ… Savoir quand utiliser SQL vs NoSQL\nâœ… ConnaÃ®tre le thÃ©orÃ¨me CAP\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique viendra avec MongoDB (module suivant) et PySpark !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸŒ 1. Câ€™est quoi le Big Data ?",
    "text": "ğŸŒ 1. Câ€™est quoi le Big Data ?\n\nğŸ“– DÃ©finition\nLe Big Data dÃ©signe des ensembles de donnÃ©es tellement volumineux et complexes quâ€™ils ne peuvent pas Ãªtre traitÃ©s par des outils traditionnels (Excel, bases SQL classiques, un seul serveur).\n\n\nğŸ“… Origine\nLe terme a Ã©mergÃ© dans les annÃ©es 2000 avec lâ€™explosion :\n\nğŸŒ Dâ€™Internet et des rÃ©seaux sociaux\nğŸ“± Des smartphones\nğŸ“¡ Des capteurs IoT\nğŸ’³ Des transactions en ligne\n\n\n\nğŸ“Š Ordres de grandeur\n1 Ko  (Kilooctet)   = 1 page de texte\n1 Mo  (MÃ©gaoctet)   = 1 photo HD\n1 Go  (Gigaoctet)   = 1 film HD\n1 To  (TÃ©raoctet)   = 1 000 films HD\n1 Po  (PÃ©taoctet)   = 1 000 To = 1 million de Go\n1 Eo  (Exaoctet)    = 1 000 Po\n1 Zo  (Zettaoctet)  = 1 000 Eo\n\n\nğŸ¢ Exemples concrets\n\n\n\nEntreprise\nVolume de donnÃ©es\n\n\n\n\nFacebook\n~4 Po gÃ©nÃ©rÃ©s par jour\n\n\nGoogle\n~20 Po traitÃ©s par jour\n\n\nNetflix\n~60 Po de vidÃ©os stockÃ©es\n\n\nCERN (LHC)\n~1 Po par seconde pendant les expÃ©riences",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“Š 2. Les 5V du Big Data",
    "text": "ğŸ“Š 2. Les 5V du Big Data\nLes caractÃ©ristiques du Big Data sont souvent rÃ©sumÃ©es par les 5V :\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   VOLUME    â”‚\n                    â”‚  (quantitÃ©) â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚                  â”‚                  â”‚\n        â–¼                  â–¼                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   VELOCITY    â”‚  â”‚   VARIETY     â”‚  â”‚   VERACITY    â”‚\nâ”‚   (vitesse)   â”‚  â”‚  (diversitÃ©)  â”‚  â”‚  (fiabilitÃ©)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                           â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚    VALUE    â”‚\n                    â”‚  (valeur)   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“¦ Volume â€” La quantitÃ© massive de donnÃ©es\n\n\n\n\n\n\n\nDÃ©fi\nSolution\n\n\n\n\nImpossible de stocker sur un seul disque\nStockage distribuÃ© (HDFS, S3)\n\n\nImpossible de charger en RAM\nTraitement par partitions\n\n\n\n\n\n\nâš¡ Velocity â€” La vitesse de gÃ©nÃ©ration et traitement\n\n\n\nType\nExemple\nLatence\n\n\n\n\nBatch\nRapport mensuel\nHeures\n\n\nNear real-time\nDashboard\nMinutes\n\n\nReal-time / Streaming\nDÃ©tection de fraude\nMillisecondes\n\n\n\n\n\n\nğŸ¨ Variety â€” La diversitÃ© des formats\n\nğŸ’¡ Rappel : Tu as dÃ©jÃ  vu Ã§a dans le module 06 !\n\n\n\n\n\n\n\n\n\nType\nFormat\nExemple\n\n\n\n\nStructurÃ©\nTables, colonnes fixes\nSQL, CSV\n\n\nSemi-structurÃ©\nSchÃ©ma flexible\nJSON, XML (MongoDB, Elasticsearch)\n\n\nNon-structurÃ©\nPas de schÃ©ma\nImages, vidÃ©os, texte libre\n\n\n\n\n\n\nâœ… Veracity â€” La fiabilitÃ© des donnÃ©es\n\n\n\nProblÃ¨me\nImpact\n\n\n\n\nDonnÃ©es manquantes\nRÃ©sultats biaisÃ©s\n\n\nDoublons\nComptages faux\n\n\nErreurs de saisie\nMauvaises dÃ©cisions\n\n\nDonnÃ©es obsolÃ¨tes\nAnalyses non pertinentes\n\n\n\n\nğŸ’¡ Câ€™est lÃ  que le Data Engineer intervient : nettoyer, valider, transformer !\n\n\n\n\nğŸ’ Value â€” La valeur extraite\nLes donnÃ©es nâ€™ont de valeur que si on peut en tirer des insights : - ğŸ“ˆ PrÃ©dictions (ML) - ğŸ“Š Dashboards - ğŸ”” Alertes - ğŸ’° Optimisation business",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš™ï¸ 3. Pourquoi le traitement distribuÃ© ?",
    "text": "âš™ï¸ 3. Pourquoi le traitement distribuÃ© ?\n\nğŸš« Limites dâ€™une machine unique\nImaginons que tu dois traiter 10 To de logs :\n\n\n\nRessource\nLimite typique\nProblÃ¨me\n\n\n\n\nRAM\n64-256 Go\n10 To ne tient pas en mÃ©moire\n\n\nCPU\n8-64 cÅ“urs\nTraitement sÃ©quentiel = trop lent\n\n\nDisque\n500 Mo/s lecture\n10 To = 5+ heures juste pour lire\n\n\nRÃ©seau\nGoulot dâ€™Ã©tranglement\nTransfÃ©rer 10 To = des heures\n\n\n\n\n\n\nğŸ“ˆ Scale-Up vs Scale-Out\nSCALE-UP (vertical)              SCALE-OUT (horizontal)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”\n   â”‚             â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚   MEGA      â”‚               â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚\n   â”‚  SERVEUR    â”‚      vs       â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚\n   â”‚   ğŸ’ªğŸ’ªğŸ’ª    â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚             â”‚               â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               \n                                 Cluster de serveurs\n   + Plus de RAM                 + Moins cher (commodity)\n   + Plus de CPU                 + ScalabilitÃ© infinie\n   - TrÃ¨s cher $$$               + TolÃ©rance aux pannes\n   - Limite physique             - Plus complexe\n\n\nâœ… Le Big Data utilise le Scale-Out !\nAu lieu dâ€™une machine surpuissante, on utilise un cluster de machines ordinaires.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ§  4. Concepts clÃ©s du traitement distribuÃ©",
    "text": "ğŸ§  4. Concepts clÃ©s du traitement distribuÃ©\n\nğŸ”„ ParallÃ©lisme vs Distribution\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nParallÃ©lisme\nPlusieurs tÃ¢ches en mÃªme temps sur une machine\nMulti-threading\n\n\nDistribution\nTÃ¢ches rÃ©parties sur plusieurs machines\nCluster Hadoop/Spark\n\n\n\n\n\n\nğŸ“ Data Locality â€” â€œAmener le code aux donnÃ©esâ€\nâŒ MAUVAIS : DÃ©placer les donnÃ©es vers le code\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â•â•10 Toâ•â•â–º    â”‚   Serveur 2    â”‚\nâ”‚   (donnÃ©es)    â”‚   rÃ©seau       â”‚    (calcul)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   lent !       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… BON : DÃ©placer le code vers les donnÃ©es\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â—„â•â•codeâ•â•     â”‚   Serveur 2    â”‚\nâ”‚ donnÃ©es+calcul â”‚   (petit)      â”‚    (master)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Câ€™est le principe fondamental de Hadoop et Spark !\n\n\n\n\nğŸ›¡ï¸ Fault Tolerance â€” TolÃ©rance aux pannes\nDans un cluster de 1000 machines, des pannes arrivent tous les jours !\n\n\n\n\n\n\n\nStratÃ©gie\nDescription\n\n\n\n\nRÃ©plication\nCopier les donnÃ©es sur plusieurs nÅ“uds (HDFS : 3 copies)\n\n\nCheckpointing\nSauvegarder lâ€™Ã©tat intermÃ©diaire\n\n\nLineage\nRecalculer les donnÃ©es perdues (Spark RDD)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ˜ 5. Lâ€™Ã©cosystÃ¨me Hadoop",
    "text": "ğŸ˜ 5. Lâ€™Ã©cosystÃ¨me Hadoop\nHadoop est un framework open-source crÃ©Ã© par Yahoo (2006), inspirÃ© des papiers de Google (GFS, MapReduce).\n\nğŸ—ï¸ Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Ã‰COSYSTÃˆME HADOOP                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚   â”‚  Hive   â”‚  â”‚   Pig   â”‚  â”‚  HBase  â”‚  â”‚  Sqoop  â”‚  ...  â”‚\nâ”‚   â”‚  (SQL)  â”‚  â”‚(scripts)â”‚  â”‚ (NoSQL) â”‚  â”‚ (import)â”‚       â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚\nâ”‚        â”‚            â”‚            â”‚            â”‚             â”‚\nâ”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚  MapReduce  â”‚  â—„â”€â”€ Traitement         â”‚\nâ”‚                    â”‚  (calcul)   â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    YARN     â”‚  â—„â”€â”€ Ressources         â”‚\nâ”‚                    â”‚ (scheduler) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    HDFS     â”‚  â—„â”€â”€ Stockage           â”‚\nâ”‚                    â”‚  (fichiers) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“ HDFS â€” Hadoop Distributed File System\nSystÃ¨me de fichiers distribuÃ© qui stocke les donnÃ©es sur plusieurs machines.\nFichier original : data.csv (300 Mo)\n                      â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â–¼             â–¼             â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ Block 1 â”‚   â”‚ Block 2 â”‚   â”‚ Block 3 â”‚   (128 Mo chacun)\n   â”‚ 128 Mo  â”‚   â”‚ 128 Mo  â”‚   â”‚  44 Mo  â”‚\n   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n        â”‚             â”‚             â”‚\n   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x\n\n\n\nCaractÃ©ristique\nValeur par dÃ©faut\n\n\n\n\nTaille de bloc\n128 Mo\n\n\nFacteur de rÃ©plication\n3\n\n\nType dâ€™accÃ¨s\nWrite once, read many\n\n\n\n\n\n\nğŸ›ï¸ YARN â€” Yet Another Resource Negotiator\nGestionnaire de ressources du cluster :\n\nAlloue CPU/RAM aux applications\nGÃ¨re la file dâ€™attente des jobs\nSurveille lâ€™Ã©tat des nÅ“uds",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ”„ 6. MapReduce â€” Le modÃ¨le de traitement",
    "text": "ğŸ”„ 6. MapReduce â€” Le modÃ¨le de traitement\nMapReduce est un modÃ¨le de programmation pour traiter de grandes quantitÃ©s de donnÃ©es en parallÃ¨le.\n\nğŸ“ Les 3 Ã©tapes\n\n\n\nÃ‰tape\nAction\nParallÃ©lisme\n\n\n\n\nMap\nTransformer chaque Ã©lÃ©ment\nâœ… ParallÃ¨le\n\n\nShuffle\nRegrouper par clÃ©\nâš ï¸ RÃ©seau\n\n\nReduce\nAgrÃ©ger les valeurs\nâœ… ParallÃ¨le\n\n\n\n\n\n\nğŸ“ Exemple : Word Count\nCompter les occurrences de chaque mot dans un texte.\nENTRÃ‰E : \"hello world hello\"\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                              MAP\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  \"hello world hello\"   \n         â”‚\n         â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  (hello, 1)  (world, 1)  (hello, 1)  â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            SHUFFLE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  Regrouper par clÃ© (mot) :\n  \n  hello â†’ [1, 1]\n  world â†’ [1]\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            REDUCE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  hello â†’ sum([1, 1]) â†’ 2\n  world â†’ sum([1])    â†’ 1\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSORTIE : { \"hello\": 2, \"world\": 1 }\n\n\n\nğŸ–¥ï¸ MapReduce sur un cluster\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚   MASTER    â”‚\n                         â”‚  (Driver)   â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â”‚                    â”‚                    â”‚\n           â–¼                    â–¼                    â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚   NODE 1    â”‚      â”‚   NODE 2    â”‚      â”‚   NODE 3    â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚\n    â”‚ \"hello\"     â”‚      â”‚ \"world\"     â”‚      â”‚ \"hello\"     â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ MAP:        â”‚      â”‚ MAP:        â”‚      â”‚ MAP:        â”‚\n    â”‚ (hello, 1)  â”‚      â”‚ (world, 1)  â”‚      â”‚ (hello, 1)  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                    â”‚                    â”‚\n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n                         SHUFFLE (rÃ©seau)\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â–¼                                         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  REDUCER 1  â”‚                           â”‚  REDUCER 2  â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ hello: [1,1]â”‚                           â”‚ world: [1]  â”‚\n    â”‚ â†’ hello: 2  â”‚                           â”‚ â†’ world: 1  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Chaque nÅ“ud traite ses donnÃ©es localement (data locality) !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš ï¸ 7. Limites de MapReduce",
    "text": "âš ï¸ 7. Limites de MapReduce\n\nğŸ˜“ ProblÃ¨mes de MapReduce\n\n\n\n\n\n\n\n\nProblÃ¨me\nCause\nImpact\n\n\n\n\nLent\nÃ‰crit sur disque entre chaque Ã©tape\nI/O intensif\n\n\nVerbose\nCode Java complexe\nProductivitÃ© basse\n\n\nBatch only\nPas de streaming\nPas de temps rÃ©el\n\n\nPas de cache\nRelit les donnÃ©es Ã  chaque job\nItÃ©rations lentes (ML)\n\n\n\n\n\nğŸ’¾ Le problÃ¨me du disque\nMapReduce : DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                 â†‘           â†‘              â†‘              â†‘\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              LENT ! (I/O disque)\n\nSpark :     DISQUE â†’ Map â†’ MÃ‰MOIRE â†’ Shuffle â†’ MÃ‰MOIRE â†’ Reduce\n                            â†‘                    â†‘\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              RAPIDE ! (in-memory)\n\n\n\nğŸ”¥ Exemple : Algorithme itÃ©ratif (ML)\nPour un algorithme qui fait 10 itÃ©rations sur les mÃªmes donnÃ©es :\n\n\n\nFramework\nComportement\nTemps\n\n\n\n\nMapReduce\nRelit les donnÃ©es du disque 10 fois\nğŸ˜“\n\n\nSpark\nGarde les donnÃ©es en mÃ©moire, itÃ¨re 10 fois\nâš¡",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš¡ 8. Spark â€” Lâ€™Ã©volution",
    "text": "âš¡ 8. Spark â€” Lâ€™Ã©volution\nApache Spark (2014) a Ã©tÃ© crÃ©Ã© pour rÃ©soudre les limitations de MapReduce.\n\nğŸ“Š Comparaison Hadoop MapReduce vs Spark\n\n\n\nCritÃ¨re\nHadoop MapReduce\nApache Spark\n\n\n\n\nVitesse\nLent (disque)\n100x plus rapide (mÃ©moire)\n\n\nFacilitÃ©\nJava verbeux\nPython, Scala, SQL\n\n\nTraitement\nBatch only\nBatch + Streaming\n\n\nItÃ©rations\nLent (relit le disque)\nRapide (cache en RAM)\n\n\nÃ‰cosystÃ¨me\nHive, Pig, etc.\nSpark SQL, MLlib, GraphX\n\n\nStockage\nHDFS\nHDFS, S3, Cassandra, etc.\n\n\n\n\n\n\nğŸ—ï¸ Architecture Spark\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      APACHE SPARK                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚ Spark SQL â”‚ â”‚ Streaming â”‚ â”‚   MLlib   â”‚ â”‚  GraphX   â”‚  â”‚\nâ”‚   â”‚  (SQL)    â”‚ â”‚ (temps    â”‚ â”‚   (ML)    â”‚ â”‚ (graphes) â”‚  â”‚\nâ”‚   â”‚           â”‚ â”‚   rÃ©el)   â”‚ â”‚           â”‚ â”‚           â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚         â”‚             â”‚             â”‚             â”‚        â”‚\nâ”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚                       â”‚ Spark Core  â”‚                      â”‚\nâ”‚                       â”‚   (RDD)     â”‚                      â”‚\nâ”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚         â–¼                    â–¼                    â–¼        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚   YARN   â”‚        â”‚   Mesos  â”‚        â”‚Standaloneâ”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Spark peut fonctionner sur YARN (cluster Hadoop existant) ou en mode standalone.\n\n\n\n\nğŸ”® Concepts Spark Ã  venir\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nRDD\nResilient Distributed Dataset â€” collection distribuÃ©e\n\n\nDataFrame\nComme un tableau avec colonnes (similaire Ã  Pandas)\n\n\nTransformation\nOpÃ©ration lazy (map, filter, groupBy)\n\n\nAction\nDÃ©clenche le calcul (collect, count, show)\n\n\nLazy Evaluation\nRien ne sâ€™exÃ©cute tant quâ€™une action nâ€™est pas appelÃ©e\n\n\nPartition\nMorceau de donnÃ©es traitÃ© par un worker\n\n\n\n\n\n\nğŸ¯ Ce que tu vas apprendre avec PySpark\n\nCrÃ©er et manipuler des DataFrames distribuÃ©s\nÃ‰crire des transformations SQL-like\nLire/Ã©crire des fichiers (CSV, Parquet, JSON)\nOptimiser les performances\nConstruire des pipelines de donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-stockage-modernes",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-stockage-modernes",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ—ï¸ 9. Architectures de Stockage Modernes",
    "text": "ğŸ—ï¸ 9. Architectures de Stockage Modernes\nMaintenant que tu connais le Big Data et le traitement distribuÃ©, voyons comment organiser et stocker ces donnÃ©es.\n\nğŸ’¡ Rappel : Tu as vu Data Warehouse et Data Mart dans le module 06. Ici, on complÃ¨te avec Data Lake et Lakehouse !\n\n\n\nğŸŒŠ 9.1 Data Lake â€” Le lac de donnÃ©es\nUn Data Lake est un systÃ¨me de stockage qui contient une grande quantitÃ© de donnÃ©es brutes dans leur format natif.\n\nğŸ“– DÃ©finition\n\nâ€œUn Data Lake est un rÃ©fÃ©rentiel centralisÃ© qui permet de stocker toutes les donnÃ©es structurÃ©es et non structurÃ©es Ã  nâ€™importe quelle Ã©chelle.â€\n\n\n\nğŸ¯ CaractÃ©ristiques\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nSchema-on-read\nPas de schÃ©ma dÃ©fini Ã  lâ€™Ã©criture, appliquÃ© Ã  la lecture\n\n\nTous formats\nStructurÃ©, semi-structurÃ©, non-structurÃ©\n\n\nStockage brut\nDonnÃ©es dans leur format original\n\n\nScalabilitÃ©\nPÃ©taoctets de donnÃ©es\n\n\nCoÃ»t faible\nStockage object (S3, ADLS, GCS)\n\n\n\n\n\nğŸ­ Exemples de Data Lakes\n\n\n\nProduit\nProvider\nStockage\n\n\n\n\nAmazon S3\nAWS\nObject storage\n\n\nAzure Data Lake Storage (ADLS)\nAzure\nObject storage\n\n\nGoogle Cloud Storage (GCS)\nGCP\nObject storage\n\n\nHDFS\nOn-premise\nDistributed filesystem\n\n\n\n\n\nâš ï¸ Le piÃ¨ge : Data Swamp (MarÃ©cage de donnÃ©es)\nâŒ DATA SWAMP â€” Ce qu'il faut Ã©viter :\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       DATA LAKE                                 â”‚\nâ”‚                                                                 â”‚\nâ”‚   ğŸ“ old_data/                  ğŸ“ test_final_v2_FINAL/         â”‚\nâ”‚   ğŸ“ backup_2019/               ğŸ“ john_analysis/               â”‚\nâ”‚   ğŸ“„ data.csv                   ğŸ“„ data_new.csv                 â”‚\nâ”‚   ğŸ“„ data_copy.csv              ğŸ“„ ???.parquet                  â”‚\nâ”‚   ğŸ“ temp/                      ğŸ“ DO_NOT_DELETE/               â”‚\nâ”‚                                                                 â”‚\nâ”‚   ğŸ˜± Personne ne sait ce que contiennent ces fichiers !        â”‚\nâ”‚   ğŸ˜± Pas de documentation                                       â”‚\nâ”‚   ğŸ˜± DonnÃ©es dupliquÃ©es, obsolÃ¨tes, invalides                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Sans gouvernance, un Data Lake devient un Data Swamp inutilisable !\n\n\n\n\n\nğŸ  9.2 Data Lakehouse â€” Le meilleur des deux mondes\nUn Data Lakehouse combine les avantages du Data Lake (flexibilitÃ©, coÃ»t) et du Data Warehouse (performance, ACID).\n\nğŸ“– DÃ©finition\n\nâ€œUn Lakehouse est une nouvelle architecture qui combine les meilleurs Ã©lÃ©ments des Data Lakes et des Data Warehouses.â€\n\n\n\nğŸ”‘ Technologies clÃ©s\n\n\n\nTechnologie\nDescription\nCrÃ©ateur\n\n\n\n\nDelta Lake\nFormat de table ACID sur Data Lake\nDatabricks\n\n\nApache Iceberg\nFormat de table open source\nNetflix\n\n\nApache Hudi\nFormat de table pour upserts\nUber\n\n\n\n\n\nâœ… Ce que le Lakehouse apporte\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      DATA LAKEHOUSE                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   âœ… Transactions ACID (comme un Data Warehouse)               â”‚\nâ”‚   âœ… Schema enforcement & evolution                             â”‚\nâ”‚   âœ… Time Travel (historique des versions)                      â”‚\nâ”‚   âœ… Stockage sur object storage (comme un Data Lake)          â”‚\nâ”‚   âœ… Support batch ET streaming                                â”‚\nâ”‚   âœ… Format ouvert (Parquet + mÃ©tadonnÃ©es)                     â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\nâš–ï¸ 9.3 Comparaison : Warehouse vs Lake vs Lakehouse\n\n\n\nCritÃ¨re\nData Warehouse\nData Lake\nData Lakehouse\n\n\n\n\nDonnÃ©es\nStructurÃ©es\nToutes\nToutes\n\n\nSchÃ©ma\nSchema-on-write\nSchema-on-read\nSchema-on-write\n\n\nACID\nâœ… Oui\nâŒ Non\nâœ… Oui\n\n\nCoÃ»t\nğŸ’°ğŸ’°ğŸ’° Ã‰levÃ©\nğŸ’° Faible\nğŸ’° Faible\n\n\nPerformance\nâš¡ Haute\nâš ï¸ Variable\nâš¡ Haute\n\n\nUse cases\nBI, Reporting\nML, Data Science\nBI + ML\n\n\nTime Travel\nâš ï¸ LimitÃ©\nâŒ Non\nâœ… Oui\n\n\nExemples\nSnowflake, Redshift\nS3, ADLS\nDelta Lake, Iceberg\n\n\n\nğŸ• Ã‰volution historique :\n\n    1990s-2000s           2010s              2020s+\n         â”‚                  â”‚                  â”‚\n         â–¼                  â–¼                  â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚    DATA     â”‚    â”‚    DATA     â”‚    â”‚    DATA     â”‚\n  â”‚  WAREHOUSE  â”‚ â†’  â”‚    LAKE     â”‚ â†’  â”‚  LAKEHOUSE  â”‚\n  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚\n  â”‚ StructurÃ©   â”‚    â”‚ Tout format â”‚    â”‚ Best of     â”‚\n  â”‚ CoÃ»teux     â”‚    â”‚ Pas d'ACID  â”‚    â”‚ both worlds â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ”® Ã€ venir : Tu apprendras Delta Lake et Iceberg en dÃ©tail dans le module intermÃ©diaire !\n\n\n\n\nğŸ¥‰ğŸ¥ˆğŸ¥‡ 9.4 Medallion Architecture â€” Bronze, Silver, Gold\nLa Medallion Architecture (ou architecture en mÃ©daillons) est un pattern dâ€™organisation des donnÃ©es dans un Data Lakehouse. Elle structure les donnÃ©es en 3 couches de qualitÃ© croissante.\n\nğŸ’¡ Cette architecture est trÃ¨s populaire avec Delta Lake et Databricks, mais sâ€™applique Ã  tout Lakehouse.\n\n\n\nğŸ—ï¸ Vue dâ€™ensemble\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     MEDALLION ARCHITECTURE                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚   SOURCES              BRONZE            SILVER            GOLD         â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€         â”‚\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚   API   â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  Raw    â”‚       â”‚ Cleaned â”‚       â”‚ Curated â”‚    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  Data   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚  Data   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚  Data   â”‚    â”‚\nâ”‚  â”‚   DB    â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  ğŸ¥‰     â”‚       â”‚  ğŸ¥ˆ     â”‚       â”‚  ğŸ¥‡     â”‚    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ BRONZE  â”‚       â”‚ SILVER  â”‚       â”‚  GOLD   â”‚    â”‚\nâ”‚  â”‚  Files  â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚        â”‚\nâ”‚  â”‚ Streams â”‚â”€â”€â”€â”€â”€â”€â”€â–º        ...                               â–¼        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚                                                        â”‚ Dashboard â”‚   â”‚\nâ”‚                                                        â”‚ ML Models â”‚   â”‚\nâ”‚                                                        â”‚ Reports   â”‚   â”‚\nâ”‚                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ¥‰ Couche Bronze â€” DonnÃ©es brutes\nLa couche Bronze contient les donnÃ©es exactement comme elles arrivent des sources.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nDonnÃ©es brutes, non transformÃ©es\n\n\nFormat\nCopie exacte des sources (JSON, CSV, logsâ€¦)\n\n\nQualitÃ©\nAucun nettoyage, peut contenir des erreurs\n\n\nBut\nHistorique complet, traÃ§abilitÃ©, replay\n\n\nRÃ©tention\nLongue durÃ©e (mois/annÃ©es)\n\n\n\nğŸ¥‰ BRONZE â€” Exemple de donnÃ©es brutes\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ {                                                              â”‚\nâ”‚   \"event_id\": \"abc123\",                                       â”‚\nâ”‚   \"timestamp\": \"2024-01-15T14:30:00Z\",                        â”‚\nâ”‚   \"user_id\": \"usr_456\",                                       â”‚\nâ”‚   \"action\": \"purchase\",                                       â”‚\nâ”‚   \"amount\": \"99.99\",      â† String au lieu de number         â”‚\nâ”‚   \"product\": null,         â† Valeur manquante                 â”‚\nâ”‚   \"_ingested_at\": \"2024-01-15T14:30:05Z\"  â† MÃ©tadonnÃ©e ajoutÃ©eâ”‚\nâ”‚ }                                                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ¥ˆ Couche Silver â€” DonnÃ©es nettoyÃ©es\nLa couche Silver contient les donnÃ©es nettoyÃ©es, validÃ©es et conformÃ©es.\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nDonnÃ©es nettoyÃ©es et validÃ©es\n\n\nTransformations\nTypes corrigÃ©s, doublons supprimÃ©s, nulls gÃ©rÃ©s\n\n\nQualitÃ©\nDonnÃ©es fiables, schÃ©ma appliquÃ©\n\n\nBut\nSource de vÃ©ritÃ© pour les analyses\n\n\nStructure\nTables normalisÃ©es ou semi-normalisÃ©es\n\n\n\nğŸ¥ˆ SILVER â€” DonnÃ©es nettoyÃ©es\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Table: silver_events                                          â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚ â”‚ event_id  â”‚     timestamp      â”‚ user_id  â”‚ action  â”‚amountâ”‚â”‚\nâ”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤â”‚\nâ”‚ â”‚ abc123    â”‚ 2024-01-15 14:30:00â”‚ usr_456  â”‚purchase â”‚ 99.99â”‚â”‚\nâ”‚ â”‚ def789    â”‚ 2024-01-15 14:31:00â”‚ usr_789  â”‚ view    â”‚  0.00â”‚â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚                                                                â”‚\nâ”‚ âœ… Types corrects (DECIMAL pour amount)                       â”‚\nâ”‚ âœ… Pas de doublons                                            â”‚\nâ”‚ âœ… Valeurs nulles remplacÃ©es par dÃ©fauts                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ¥‡ Couche Gold â€” DonnÃ©es agrÃ©gÃ©es pour le business\nLa couche Gold contient les donnÃ©es agrÃ©gÃ©es et optimisÃ©es pour des cas dâ€™usage mÃ©tier spÃ©cifiques.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nAgrÃ©gations, KPIs, mÃ©triques business\n\n\nStructure\nTables dÃ©normalisÃ©es (Star Schema)\n\n\nQualitÃ©\nDonnÃ©es prÃªtes pour dashboards et ML\n\n\nBut\nConsommation directe par les analystes\n\n\nPerformance\nOptimisÃ©es pour les requÃªtes\n\n\n\nğŸ¥‡ GOLD â€” DonnÃ©es agrÃ©gÃ©es\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Table: gold_daily_sales                                       â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚ â”‚    date    â”‚   region    â”‚ category  â”‚total_salesâ”‚num_ordersâ”‚â”‚\nâ”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   France    â”‚Electronicsâ”‚ 125,430.50â”‚    1,245â”‚â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   Germany   â”‚Electronicsâ”‚  98,200.00â”‚      987â”‚â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   France    â”‚  Fashion  â”‚  45,670.00â”‚      567â”‚â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚                                                                â”‚\nâ”‚ â†’ PrÃªt pour Power BI, Tableau, Looker                         â”‚\nâ”‚ â†’ RequÃªtes ultra-rapides                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“Š RÃ©sumÃ© des 3 couches\n\n\n\n\n\n\n\n\n\n\nCouche\nQualitÃ©\nTransformations\nUtilisateurs\nFormat typique\n\n\n\n\nğŸ¥‰ Bronze\nBrute\nAucune\nData Engineers\nJSON, CSV, Parquet brut\n\n\nğŸ¥ˆ Silver\nNettoyÃ©e\nCleaning, validation, typing\nData Engineers, Analysts\nDelta Lake, Iceberg\n\n\nğŸ¥‡ Gold\nBusiness-ready\nAgrÃ©gations, dÃ©normalisation\nAnalysts, Dashboard, ML\nTables Star Schema\n\n\n\n\n\n\nâœ… Avantages de Medallion Architecture\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nTraÃ§abilitÃ©\nOn peut toujours revenir aux donnÃ©es brutes (Bronze)\n\n\nQualitÃ© progressive\nChaque couche amÃ©liore la qualitÃ©\n\n\nRÃ©utilisabilitÃ©\nSilver sert de source commune Ã  plusieurs tables Gold\n\n\nDebugging\nFacile de trouver oÃ¹ une erreur sâ€™est introduite\n\n\nPerformance\nGold optimisÃ© pour les requÃªtes analytiques\n\n\n\n\n\n\nğŸ”„ Flux de donnÃ©es typique\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         PIPELINE ETL/ELT                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                          â”‚\nâ”‚   SOURCE          BRONZE              SILVER              GOLD           â”‚\nâ”‚      â”‚               â”‚                   â”‚                  â”‚            â”‚\nâ”‚      â”‚   Ingestion   â”‚    Cleaning       â”‚   Aggregation    â”‚            â”‚\nâ”‚      â”‚   (raw copy)  â”‚    Validation     â”‚   Denormalizationâ”‚            â”‚\nâ”‚      â”‚               â”‚    Deduplication  â”‚   Business rules â”‚            â”‚\nâ”‚      â–¼               â–¼                   â–¼                  â–¼            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚  API  â”‚â”€â”€â”€â”€â”€â–ºâ”‚ bronze_â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ silver_  â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚  â”‚ logs  â”‚      â”‚ events â”‚         â”‚ events   â”‚   â”Œâ”€â”€â–ºâ”‚ daily_   â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ metrics  â”‚      â”‚\nâ”‚                                         â”‚        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚  DB   â”‚â”€â”€â”€â”€â”€â–ºâ”‚ bronze_â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ silver_  â”‚â”€â”€â”€â”¼â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚  â”‚ users â”‚      â”‚ users  â”‚         â”‚ users    â”‚   â”‚   â”‚ user_    â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ segments â”‚      â”‚\nâ”‚                                                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚                                                   â””â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚                                                       â”‚ funnel   â”‚      â”‚\nâ”‚                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ”® Ã€ venir : Tu implÃ©menteras cette architecture avec Delta Lake dans le module intermÃ©diaire !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-traitement-lambda-kappa",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-traitement-lambda-kappa",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ”„ 10. Architectures de Traitement â€” Lambda & Kappa",
    "text": "ğŸ”„ 10. Architectures de Traitement â€” Lambda & Kappa\nComment organiser le traitement des donnÃ©es ? Deux grandes architectures dominent.\n\n\nâš¡ 10.1 Lambda Architecture â€” Batch + Streaming\nLâ€™architecture Lambda (Nathan Marz, 2011) combine traitement batch et streaming pour avoir le meilleur des deux.\n\nğŸ—ï¸ Structure\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚            DATA SOURCES             â”‚\n                         â”‚     (Events, Logs, Transactions)    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚                                             â”‚\n                    â–¼                                             â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚    BATCH LAYER      â”‚                      â”‚    SPEED LAYER      â”‚\n         â”‚   (Traitement lot)  â”‚                      â”‚   (Temps rÃ©el)      â”‚\n         â”‚                     â”‚                      â”‚                     â”‚\n         â”‚  â€¢ Hadoop/Spark     â”‚                      â”‚  â€¢ Kafka Streams    â”‚\n         â”‚  â€¢ DonnÃ©es complÃ¨tesâ”‚                      â”‚  â€¢ Spark Streaming  â”‚\n         â”‚  â€¢ Haute prÃ©cision  â”‚                      â”‚  â€¢ Flink            â”‚\n         â”‚  â€¢ Latence: heures  â”‚                      â”‚  â€¢ Latence: secondesâ”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                    â”‚                                             â”‚\n                    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n                    â”‚     â”‚     SERVING LAYER       â”‚             â”‚\n                    â””â”€â”€â”€â”€â–ºâ”‚  (Couche de service)    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚                         â”‚\n                          â”‚  â€¢ Combine les deux     â”‚\n                          â”‚  â€¢ RequÃªtes rapides     â”‚\n                          â”‚  â€¢ Vue unifiÃ©e          â”‚\n                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                      â”‚\n                                      â–¼\n                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                          â”‚      APPLICATIONS       â”‚\n                          â”‚   Dashboards, APIs, ML  â”‚\n                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“‹ Les 3 couches\n\n\n\n\n\n\n\n\nCouche\nRÃ´le\nCaractÃ©ristiques\n\n\n\n\nBatch Layer\nTraitement complet, prÃ©cis\nLatence haute, donnÃ©es historiques\n\n\nSpeed Layer\nTraitement temps rÃ©el\nLatence basse, donnÃ©es rÃ©centes\n\n\nServing Layer\nCombine et sert les donnÃ©es\nRequÃªtes rapides, vue unifiÃ©e\n\n\n\n\n\nâœ… Avantages / âŒ InconvÃ©nients\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nRobuste (double traitement)\nComplexe (2 systÃ¨mes Ã  maintenir)\n\n\nDonnÃ©es historiques complÃ¨tes\nCode dupliquÃ© (batch + streaming)\n\n\nFaible latence possible\nCoÃ»t Ã©levÃ© (2 pipelines)\n\n\n\n\n\n\n\nğŸŒ€ 10.2 Kappa Architecture â€” Streaming-first\nLâ€™architecture Kappa (Jay Kreps, 2014) simplifie Lambda en utilisant uniquement du streaming.\n\nğŸ—ï¸ Structure\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚            DATA SOURCES             â”‚\n                         â”‚     (Events, Logs, Transactions)    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚       MESSAGE QUEUE / LOG           â”‚\n                         â”‚           (Apache Kafka)            â”‚\n                         â”‚                                     â”‚\n                         â”‚   â€¢ Stocke TOUS les Ã©vÃ©nements      â”‚\n                         â”‚   â€¢ RÃ©tention longue (jours/mois)   â”‚\n                         â”‚   â€¢ Replay possible                 â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚        STREAM PROCESSING            â”‚\n                         â”‚    (Kafka Streams, Flink, Spark)    â”‚\n                         â”‚                                     â”‚\n                         â”‚   â€¢ Une seule codebase              â”‚\n                         â”‚   â€¢ Traite events en temps rÃ©el     â”‚\n                         â”‚   â€¢ Reprocessing = replay du log    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚         SERVING LAYER               â”‚\n                         â”‚   (Base de donnÃ©es, Cache, API)     â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ”„ Le concept clÃ© : Replay\nBesoin de recalculer les donnÃ©es ?\n\nLambda : Relance le batch job (heures)\nKappa  : Rejoue le log Kafka (mÃªme vitesse que le streaming)\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Kafka Log (rÃ©tention 30 jours)                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚ E1 â”‚ E2 â”‚ E3 â”‚ E4 â”‚ E5 â”‚ E6 â”‚ E7 â”‚ E8 â”‚ E9 â”‚E10 â”‚ ...   â”‚\nâ”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜        â”‚\nâ”‚                              â–²                               â”‚\nâ”‚                              â”‚                               â”‚\nâ”‚                    Consumer peut \"remonter\"                  â”‚\nâ”‚                    et rejouer les Ã©vÃ©nements                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâœ… Avantages / âŒ InconvÃ©nients\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nSimple (1 seul systÃ¨me)\nStockage Kafka coÃ»teux (rÃ©tention longue)\n\n\nUne seule codebase\nPas idÃ©al pour analytics trÃ¨s complexes\n\n\nReprocessing facile\nNÃ©cessite Kafka bien configurÃ©\n\n\n\n\n\n\n\nâš–ï¸ 10.3 Lambda vs Kappa â€” Quand utiliser quoi ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nLambda\nKappa\n\n\n\n\nComplexitÃ©\nHaute (2 systÃ¨mes)\nMoyenne (1 systÃ¨me)\n\n\nMaintenance\n2 codebases\n1 codebase\n\n\nReprocessing\nBatch job (lent mais fiable)\nReplay Kafka (rapide)\n\n\nUse case idÃ©al\nAnalytics historiques + temps rÃ©el\nEvent-driven, streaming-first\n\n\nCoÃ»t\nÃ‰levÃ©\nMoyen\n\n\n\n\nğŸ¯ Recommandations\n\n\n\nSituation\nArchitecture recommandÃ©e\n\n\n\n\nBesoin de rapports historiques prÃ©cis + temps rÃ©el\nLambda\n\n\nApplication principalement event-driven\nKappa\n\n\nÃ‰quipe petite, budget limitÃ©\nKappa\n\n\nDonnÃ©es complexes nÃ©cessitant du batch lourd\nLambda\n\n\nStartup, MVP, itÃ©ration rapide\nKappa\n\n\n\n\nğŸ’¡ En pratique : Beaucoup dâ€™entreprises utilisent des architectures hybrides adaptÃ©es Ã  leurs besoins spÃ©cifiques.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#data-mesh-lapproche-dÃ©centralisÃ©e",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#data-mesh-lapproche-dÃ©centralisÃ©e",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ•¸ï¸ 11. Data Mesh â€” Lâ€™approche dÃ©centralisÃ©e",
    "text": "ğŸ•¸ï¸ 11. Data Mesh â€” Lâ€™approche dÃ©centralisÃ©e\nLe Data Mesh est une approche organisationnelle (pas technique) pour gÃ©rer les donnÃ©es Ã  grande Ã©chelle.\n\nğŸ“– Origine du problÃ¨me\nArchitecture centralisÃ©e traditionnelle :\n\n    Domain A        Domain B        Domain C\n        â”‚               â”‚               â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                        â”‚\n                        â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚   DATA TEAM         â”‚\n              â”‚   CENTRALISÃ‰E       â”‚  â† Goulot d'Ã©tranglement !\n              â”‚                     â”‚\n              â”‚ â€¢ Pipeline A        â”‚\n              â”‚ â€¢ Pipeline B        â”‚\n              â”‚ â€¢ Pipeline C        â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                        â”‚\n                        â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚   DATA WAREHOUSE    â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ProblÃ¨mes :\n  â€¢ L'Ã©quipe data devient un goulot d'Ã©tranglement\n  â€¢ Elle ne comprend pas tous les domaines mÃ©tiers\n  â€¢ Temps de livraison trÃ¨s long\n\n\n\nğŸ¯ Les 4 principes du Data Mesh\n\n\n\n\n\n\n\nPrincipe\nDescription\n\n\n\n\n1. Domain Ownership\nChaque domaine mÃ©tier est responsable de ses donnÃ©es\n\n\n2. Data as a Product\nLes donnÃ©es sont traitÃ©es comme des produits avec SLA, qualitÃ©, documentation\n\n\n3. Self-serve Platform\nPlateforme en libre-service pour crÃ©er des data products\n\n\n4. Federated Governance\nGouvernance fÃ©dÃ©rÃ©e avec des standards communs\n\n\n\n\n\n\nğŸ—ï¸ Architecture Data Mesh\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         DATA MESH                                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚   DOMAIN: Sales   â”‚  â”‚  DOMAIN: Product  â”‚  â”‚ DOMAIN: Customer  â”‚   â”‚\nâ”‚  â”‚                   â”‚  â”‚                   â”‚  â”‚                   â”‚   â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚\nâ”‚  â”‚  â”‚ Data Productâ”‚  â”‚  â”‚  â”‚ Data Productâ”‚  â”‚  â”‚  â”‚ Data Productâ”‚  â”‚   â”‚\nâ”‚  â”‚  â”‚ \"Orders\"    â”‚  â”‚  â”‚  â”‚ \"Catalog\"   â”‚  â”‚  â”‚  â”‚ \"Profiles\"  â”‚  â”‚   â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚\nâ”‚  â”‚                   â”‚  â”‚                   â”‚  â”‚                   â”‚   â”‚\nâ”‚  â”‚  Owner: Sales Teamâ”‚  â”‚  Owner: Product   â”‚  â”‚  Owner: CRM Team  â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚            â”‚                      â”‚                      â”‚             â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚                                   â”‚                                     â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                    â”‚    SELF-SERVE PLATFORM      â”‚                     â”‚\nâ”‚                    â”‚  (Infrastructure commune)   â”‚                     â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                   â”‚                                     â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                    â”‚   FEDERATED GOVERNANCE      â”‚                     â”‚\nâ”‚                    â”‚  (Standards, SÃ©curitÃ©, ...)â”‚                     â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ’¡ Data Mesh vs Architecture CentralisÃ©e\n\n\n\n\n\n\n\n\nAspect\nCentralisÃ©e\nData Mesh\n\n\n\n\nOwnership\nÃ‰quipe data centrale\nÃ‰quipes domaines\n\n\nScalabilitÃ©\nLimitÃ©e par lâ€™Ã©quipe data\nScale avec lâ€™organisation\n\n\nConnaissance mÃ©tier\nFaible\nForte (experts domaine)\n\n\nTime-to-market\nLent\nRapide\n\n\nComplexitÃ©\nTechnique\nOrganisationnelle\n\n\n\n\nâš ï¸ Attention : Data Mesh nâ€™est pas pour tout le monde. Il convient aux grandes organisations avec de nombreux domaines mÃ©tiers distincts.\n\n\nğŸ”® Pour approfondir : Le Data Mesh est un concept avancÃ© que tu exploreras au niveau expert du bootcamp !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ¤” Pourquoi NoSQL ?",
    "text": "ğŸ¤” Pourquoi NoSQL ?\n\nLes limites des bases relationnelles face au Big Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PROBLÃˆMES DES BASES SQL CLASSIQUES                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  âŒ SchÃ©ma rigide â†’ Difficile de changer la structure           â”‚\nâ”‚  âŒ Scale-up only â†’ Un seul serveur (coÃ»teux)                   â”‚\nâ”‚  âŒ Jointures â†’ Lentes sur des milliards de lignes              â”‚\nâ”‚  âŒ ACID strict â†’ Latence Ã©levÃ©e                                â”‚\nâ”‚  âŒ DonnÃ©es variÃ©es â†’ JSON, graphes mal gÃ©rÃ©s                   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLa solution NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    AVANTAGES NoSQL                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  âœ… SchÃ©ma flexible â†’ S'adapte aux donnÃ©es                      â”‚\nâ”‚  âœ… Scale-out natif â†’ Ajout de serveurs facile                  â”‚\nâ”‚  âœ… Pas de jointures â†’ DonnÃ©es dÃ©normalisÃ©es, rapides           â”‚\nâ”‚  âœ… Haute disponibilitÃ© â†’ TolÃ©rance aux pannes                  â”‚\nâ”‚  âœ… ModÃ¨les variÃ©s â†’ Document, clÃ©-valeur, graphe...            â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ NoSQL = â€œNot Only SQLâ€ (pas seulement SQL), pas â€œNo SQLâ€ !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“Š Les 4 types de bases NoSQL",
    "text": "ğŸ“Š Les 4 types de bases NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         TYPES DE BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     ğŸ“„ DOCUMENT     â”‚   ğŸ”‘ CLÃ‰-VALEUR    â”‚    ğŸ“Š COLONNES      â”‚  ğŸ”— GRAPHE â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”‚  {                  â”‚   key1 â†’ value1     â”‚  Row1: col1, col2   â”‚   (A)â”€â”€â†’(B)â”‚\nâ”‚    \"nom\": \"Alice\", â”‚   key2 â†’ value2     â”‚  Row2: col1, col3   â”‚    â”‚       â”‚\nâ”‚    \"age\": 30        â”‚   key3 â†’ value3     â”‚  Row3: col2, col4   â”‚    â†“       â”‚\nâ”‚  }                  â”‚                     â”‚                     â”‚   (C)     â”‚\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  MongoDB            â”‚  Redis              â”‚  Cassandra          â”‚ Neo4j     â”‚\nâ”‚  Couchbase          â”‚  Memcached          â”‚  HBase              â”‚ Amazon    â”‚\nâ”‚  Firestore          â”‚  DynamoDB           â”‚  ScyllaDB           â”‚ Neptune   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“„ 1. Bases Document (MongoDB, Couchbase)\nStockent des documents JSON/BSON avec un schÃ©ma flexible.\n// Document MongoDB\n{\n  \"_id\": \"user123\",\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@example.com\",\n  \"adresses\": [\n    {\"type\": \"domicile\", \"ville\": \"Paris\"},\n    {\"type\": \"travail\", \"ville\": \"Lyon\"}\n  ]\n}\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nSchÃ©ma flexible\nPas de jointures natives\n\n\nDocuments imbriquÃ©s\nDonnÃ©es dupliquÃ©es\n\n\nRequÃªtes riches\nTransactions limitÃ©es\n\n\n\nCas dâ€™usage : Catalogues produits, profils utilisateurs, CMS, logs\n\n\n\nğŸ”‘ 2. Bases ClÃ©-Valeur (Redis, Memcached)\nStockage ultra-simple : une clÃ© â†’ une valeur.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      CLÃ‰        â”‚          VALEUR            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  user:123       â”‚  {\"nom\": \"Alice\", ...}    â”‚\nâ”‚  session:abc    â”‚  {\"user_id\": 123, ...}    â”‚\nâ”‚  cache:page:42  â”‚  \"&lt;html&gt;...&lt;/html&gt;\"       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : Cache, sessions, compteurs, files dâ€™attente\n\n\n\nğŸ“Š 3. Bases Colonnes (Cassandra, HBase)\nOptimisÃ©es pour les Ã©critures massives et les requÃªtes analytiques.\nCas dâ€™usage : IoT, time-series, logs, analytics\n\n\n\nğŸ”— 4. Bases Graphe (Neo4j, Amazon Neptune)\nOptimisÃ©es pour les relations complexes entre entitÃ©s.\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  Alice  â”‚\n         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n              â”‚ KNOWS\n              â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       WORKS_AT       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚   Bob   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  TechCorp   â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : RÃ©seaux sociaux, recommandations, fraude, connaissances",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ”º Le thÃ©orÃ¨me CAP",
    "text": "ğŸ”º Le thÃ©orÃ¨me CAP\nLe thÃ©orÃ¨me CAP (Eric Brewer, 2000) est fondamental pour comprendre les choix de design des bases distribuÃ©es.\n                         C\n                    Consistency\n                    (CohÃ©rence)\n                        /\\\n                       /  \\\n                      /    \\\n                     /  CA  \\\n                    /________\\\n                   /\\        /\\\n                  /  \\      /  \\\n                 / CP \\    / AP \\\n                /______\\  /______\\\n               A                    P\n          Availability          Partition\n         (DisponibilitÃ©)        Tolerance\n\nğŸ“– Les 3 propriÃ©tÃ©s\n\n\n\nPropriÃ©tÃ©\nSignification\n\n\n\n\nConsistency\nTous les nÅ“uds voient les mÃªmes donnÃ©es\n\n\nAvailability\nLe systÃ¨me rÃ©pond toujours\n\n\nPartition Tolerance\nFonctionne malgrÃ© des pannes rÃ©seau\n\n\n\n\n\nâš ï¸ Le thÃ©orÃ¨me dit :\n\nEn cas de partition rÃ©seau, tu dois choisir entre CohÃ©rence et DisponibilitÃ©.\n\nTu ne peux avoir que 2 sur 3 !\n\n\n\nğŸ—‚ï¸ Classification des bases selon CAP\n\n\n\n\n\n\n\n\n\nType\nChoix CAP\nBases\nComportement\n\n\n\n\nCP\nCohÃ©rence + Partition\nMongoDB, HBase, Redis\nPeut refuser des requÃªtes si partition\n\n\nAP\nDisponibilitÃ© + Partition\nCassandra, DynamoDB, CouchDB\nRÃ©pond toujours, cohÃ©rence Ã©ventuelle\n\n\nCA\nCohÃ©rence + DisponibilitÃ©\nPostgreSQL, MySQL (single node)\nPas de tolÃ©rance aux partitions",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš–ï¸ SQL vs NoSQL â€” Quand utiliser quoi ?",
    "text": "âš–ï¸ SQL vs NoSQL â€” Quand utiliser quoi ?\n\nğŸ“Š Tableau comparatif\n\n\n\nCritÃ¨re\nSQL (Relationnel)\nNoSQL\n\n\n\n\nSchÃ©ma\nFixe, dÃ©fini Ã  lâ€™avance\nFlexible, dynamique\n\n\nDonnÃ©es\nStructurÃ©es, normalisÃ©es\nSemi/non-structurÃ©es\n\n\nRelations\nJointures natives\nDonnÃ©es embarquÃ©es\n\n\nTransactions\nACID complet\nBASE (Ã©ventuel)\n\n\nScalabilitÃ©\nVerticale (scale-up)\nHorizontale (scale-out)\n\n\n\n\n\n\nâœ… Utilise SQL quandâ€¦\n\nDonnÃ©es trÃ¨s structurÃ©es (comptabilitÃ©, RH)\nRelations complexes (ERP, CRM)\nTransactions critiques (banque, e-commerce)\n\n\n\nâœ… Utilise NoSQL quandâ€¦\n\nSchÃ©ma variable (catalogues, profils)\nCache haute performance (sessions)\nÃ‰critures massives (IoT, logs)\nScale-out nÃ©cessaire (Big Data)\n\n\n\nğŸ¯ En rÃ©alitÃ© : on utilise les deux !\n\nğŸ’¡ Polyglot Persistence : Utiliser la bonne base pour le bon cas dâ€™usage !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nLes 5V du Big Data\n\n\n\nV\nDÃ©fi\n\n\n\n\nVolume\nStocker et traiter des To/Po\n\n\nVelocity\nTraiter en temps rÃ©el\n\n\nVariety\nGÃ©rer tous les formats\n\n\nVeracity\nAssurer la qualitÃ©\n\n\nValue\nExtraire des insights\n\n\n\n\n\nTraitement distribuÃ©\n\n\n\nConcept\nRetenir\n\n\n\n\nScale-out\nCluster de machines ordinaires\n\n\nData locality\nAmener le code aux donnÃ©es\n\n\nFault tolerance\nRÃ©plication, recalcul\n\n\n\n\n\nHadoop vs Spark\n\n\n\n\nHadoop MR\nSpark\n\n\n\n\nStockage intermÃ©diaire\nDisque\nMÃ©moire\n\n\nVitesse\nLent\n100x plus rapide\n\n\nLangages\nJava\nPython, Scala, SQL\n\n\n\n\n\nArchitectures de stockage\n\n\n\nType\nCaractÃ©ristique clÃ©\n\n\n\n\nData Lake\nStockage brut, schema-on-read\n\n\nData Lakehouse\nLake + ACID + Performance\n\n\n\n\n\nArchitectures de traitement\n\n\n\nArchitecture\nApproche\n\n\n\n\nLambda\nBatch + Streaming (2 systÃ¨mes)\n\n\nKappa\nStreaming-first (1 systÃ¨me)\n\n\n\n\n\nSQL vs NoSQL\n\n\n\nType\nQuand utiliser\n\n\n\n\nSQL\nDonnÃ©es structurÃ©es, transactions, intÃ©gritÃ©\n\n\nNoSQL Document\nSchÃ©ma flexible, JSON (MongoDB)\n\n\nNoSQL ClÃ©-valeur\nCache, sessions (Redis)\n\n\nNoSQL Colonnes\nÃ‰critures massives, IoT (Cassandra)\n\n\nNoSQL Graphe\nRelations complexes (Neo4j)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quels sont les 3V originaux du Big Data ?\n\nVitesse, Valeur, VÃ©ritÃ©\n\nVolume, Velocity, Variety\n\nVolume, Validation, Visualisation\n\nVÃ©locitÃ©, VÃ©racitÃ©, Valorisation\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les 3V originaux (Doug Laney, 2001) sont Volume, Velocity, Variety.\n\n\n\n\nâ“ Q2. Que signifie â€œScale-outâ€ ?\n\nAugmenter la RAM dâ€™un serveur\n\nAjouter des machines au cluster\n\nCompresser les donnÃ©es\n\nRÃ©duire la taille du cluster\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Scale-out (horizontal) = ajouter des machines. Scale-up (vertical) = augmenter les ressources dâ€™une machine.\n\n\n\n\nâ“ Q3. Quel est le principe de â€œData Localityâ€ ?\n\nStocker les donnÃ©es localement sur son PC\n\nDÃ©placer les donnÃ©es vers le serveur de calcul\n\nDÃ©placer le code vers les donnÃ©es\n\nCompresser les donnÃ©es pour les transfÃ©rer\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” On envoie le code (petit) vers les donnÃ©es (grosses), pas lâ€™inverse.\n\n\n\n\nâ“ Q4. Quels sont les 3 composants principaux de Hadoop ?\n\nHDFS, Spark, Kafka\n\nHDFS, MapReduce, YARN\n\nHive, Pig, HBase\n\nMap, Shuffle, Reduce\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” HDFS (stockage), MapReduce (calcul), YARN (ressources).\n\n\n\n\nâ“ Q5. Pourquoi Spark est plus rapide que MapReduce ?\n\nIl utilise un meilleur algorithme\n\nIl stocke les donnÃ©es intermÃ©diaires en mÃ©moire\n\nIl compresse les donnÃ©es\n\nIl utilise plus de serveurs\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Spark garde les donnÃ©es en mÃ©moire (RAM) au lieu dâ€™Ã©crire sur disque entre chaque Ã©tape.\n\n\n\n\nâ“ Q6. Quelle est la diffÃ©rence entre Data Lake et Data Lakehouse ?\n\nAucune diffÃ©rence\n\nLe Lakehouse ajoute les transactions ACID au Data Lake\n\nLe Data Lake est plus rÃ©cent\n\nLe Lakehouse ne supporte que les donnÃ©es structurÃ©es\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Le Data Lakehouse combine la flexibilitÃ© du Data Lake avec les transactions ACID et les performances du Data Warehouse.\n\n\n\n\nâ“ Q7. Quelles sont les 3 couches de lâ€™architecture Lambda ?\n\nBronze, Silver, Gold\n\nBatch, Speed, Serving\n\nExtract, Transform, Load\n\nInput, Process, Output\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Lambda Architecture = Batch Layer + Speed Layer + Serving Layer.\n\n\n\n\nâ“ Q8. Quel est lâ€™avantage principal de lâ€™architecture Kappa sur Lambda ?\n\nPlus rapide\n\nPlus simple (une seule codebase)\n\nMoins de stockage\n\nMeilleure prÃ©cision\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Kappa simplifie lâ€™architecture en nâ€™utilisant quâ€™un seul systÃ¨me (streaming), Ã©vitant de maintenir deux codebases.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Data Swamp ?\n\nUn type de Data Lake optimisÃ©\n\nUn Data Lake mal gÃ©rÃ© et inutilisable\n\nUne architecture de streaming\n\nUn Data Warehouse sur le cloud\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Swamp est un Data Lake devenu ingÃ©rable par manque de gouvernance (donnÃ©es non documentÃ©es, dupliquÃ©es, obsolÃ¨tes).\n\n\n\n\nâ“ Q10. Quel principe du Data Mesh dit que chaque domaine mÃ©tier est responsable de ses donnÃ©es ?\n\nData as a Product\n\nSelf-serve Platform\n\nDomain Ownership\n\nFederated Governance\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Domain Ownership : chaque domaine mÃ©tier possÃ¨de et gÃ¨re ses propres donnÃ©es.\n\n\n\n\nâ“ Q11. Combien de copies HDFS fait-il par dÃ©faut ?\n\n1\n\n2\n\n3\n\n5\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” HDFS rÃ©plique chaque bloc 3 fois par dÃ©faut pour la tolÃ©rance aux pannes.\n\n\n\n\nâ“ Q12. Quelle base NoSQL est idÃ©ale pour le cache et les sessions ?\n\nMongoDB\n\nRedis\n\nCassandra\n\nNeo4j\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Redis est une base in-memory clÃ©-valeur, idÃ©ale pour le cache et les sessions.\n\n\n\n\nâ“ Q13. Dans lâ€™architecture Medallion, quelle couche contient les donnÃ©es brutes non transformÃ©es ?\n\nGold\n\nSilver\n\nBronze\n\nPlatinum\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Bronze contient les donnÃ©es brutes exactement comme elles arrivent des sources.\n\n\n\n\nâ“ Q14. Quel est lâ€™ordre correct de qualitÃ© croissante dans lâ€™architecture Medallion ?\n\nGold â†’ Silver â†’ Bronze\n\nBronze â†’ Gold â†’ Silver\n\nSilver â†’ Bronze â†’ Gold\n\nBronze â†’ Silver â†’ Gold\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… d â€” Bronze (brut) â†’ Silver (nettoyÃ©) â†’ Gold (agrÃ©gÃ©/business-ready). La qualitÃ© augmente Ã  chaque couche.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nBig Data & Hadoop\n\nHadoop Documentation\nSpark Documentation\nThe Google File System (paper)\nMapReduce: Simplified Data Processing (paper)\n\n\n\nArchitectures modernes\n\nDelta Lake Documentation\nMedallion Architecture (Databricks)\nApache Iceberg\nLambda Architecture (Nathan Marz)\nKappa Architecture (Jay Kreps)\nData Mesh (Zhamak Dehghani)\n\n\n\nNoSQL\n\nMongoDB University â€” Cours gratuits\nRedis Documentation\nCAP Theorem Explained",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu as maintenant les bases thÃ©oriques du Big Data, du traitement distribuÃ©, des architectures modernes et du NoSQL. Place Ã  la pratique avec MongoDB !\nğŸ‘‰ Module suivant : 09_mongodb_for_data_engineers.ipynb â€” MongoDB pour les Data Engineers\nTu apprendras Ã  : - Installer et utiliser MongoDB - CrÃ©er des collections et documents - Ã‰crire des requÃªtes MQL (MongoDB Query Language) - Utiliser PyMongo depuis Python - ModÃ©liser des donnÃ©es pour MongoDB\n\nğŸ‰ FÃ©licitations ! Tu comprends maintenant le Big Data, les systÃ¨mes distribuÃ©s, les architectures modernes et le NoSQL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "index.html#pourquoi-choisir-ce-bootcamp",
    "href": "index.html#pourquoi-choisir-ce-bootcamp",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ¯ Pourquoi choisir ce Bootcamp ?",
    "text": "ğŸ¯ Pourquoi choisir ce Bootcamp ?\nCe programme a Ã©tÃ© conÃ§u par et pour des Data Engineers, en se concentrant sur les exigences rÃ©elles du marchÃ© du travail. Nous transformons votre ambition en expertise pratique.\n\nâœ” Expertise Industrielle : MaÃ®trisez les outils modernes utilisÃ©s par les gÃ©ants de la Tech (Spark 3.x, Kafka, Docker, Kubernetes, dbt, Cloud multi-plateforme).\nâœ” Architectures AvancÃ©es : Devenez architecte de vos propres pipelines en comprenant les modÃ¨les Lakehouse (Delta/Iceberg), Medaillon, et les architectures Kappa/Lambda modernes.\nâœ” Focus Production : DÃ©ployez des pipelines batch et streaming de bout en bout, de lâ€™ingestion brute Ã  la table Gold optimisÃ©e.\nâœ” Progression MesurÃ©e : Le parcours est divisÃ© en trois niveaux clairs : DÃ©butant â†’ IntermÃ©diaire â†’ AvancÃ©, pour une montÃ©e en compÃ©tence garantie.\n\n\nğŸ‘¥ Ã€ qui sâ€™adresse ce Bootcamp ?\n\nÃ‰tudiants & DÃ©veloppeurs : Vous voulez une porte dâ€™entrÃ©e structurÃ©e dans le domaine de la Data Engineering.\nAnalystes BI : Vous souhaitez migrer vers le Big Data, le Cloud et lâ€™industrialisation des pipelines.\nProfessionnels : Vous cherchez Ã  structurer, optimiser et gouverner une plateforme Data moderne en entreprise.\n\n\n\n\nğŸ› ï¸ RÃ©sumÃ© des Technologies (Votre BoÃ®te Ã  Outils)\n\n\n\n\n\n\n\nDomaine\nOutils & Technologies\n\n\n\n\nBig Data Processing\nSpark (PySpark), Spark SQL, Polars, Dask, Vaex\n\n\nData Lakehouse\nDelta Lake, Apache Iceberg, Hudi\n\n\nStreaming\nApache Kafka, Spark Structured Streaming, Flink\n\n\nDevOps / Infra\nDocker, Kubernetes (K8s), Spark Operator, Helm\n\n\nCloud Computing\nAWS S3, GCP GCS, Azure Blob, IAM, MinIO\n\n\nData Quality & ETL\ndbt (Data Build Tool), Great Expectations\n\n\nMLOps & Architecture\nMLflow, Feature Store, Data Mesh, Observability",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#la-structure-complÃ¨te-du-programme",
    "href": "index.html#la-structure-complÃ¨te-du-programme",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ“˜ La Structure ComplÃ¨te du Programme",
    "text": "ğŸ“˜ La Structure ComplÃ¨te du Programme\nLa progression est claire : Fondations â†’ Industrialisation â†’ Architecture.",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#comment-utiliser-ce-bootcamp",
    "href": "index.html#comment-utiliser-ce-bootcamp",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ“Œ Comment utiliser ce Bootcamp ?",
    "text": "ğŸ“Œ Comment utiliser ce Bootcamp ?\nLe Bootcamp est organisÃ© dans lâ€™ordre pÃ©dagogique exact nÃ©cessaire pour devenir Data Engineer :\n\nOuvrez le menu latÃ©ral gauche.\n\nCliquez sur le premier module : 01 â€“ Introduction au Data Engineering.\n\nProgressez ensuite dans lâ€™ordre recommandÃ© (DÃ©butant â†’ IntermÃ©diaire â†’ AvancÃ©).\n\nğŸ‘‰ Pour commencer tout de suite :\nğŸš€ DÃ©marrer le Module 01",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#ready-to-start",
    "href": "index.html#ready-to-start",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸš€ Ready to Start?",
    "text": "ğŸš€ Ready to Start?\nCe bootcamp est conÃ§u pour vous emmener de la thÃ©orie Ã  la pratique industrielle.\nğŸ‘‰ Commencez aujourdâ€™hui pour dÃ©marrer votre transition.\nğŸ‘‰ Le programme est 100% pratique, progressif et centrÃ© sur le monde professionnel.\nBonne montÃ©e en compÃ©tence ! ğŸ’ª",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente MongoDB, la base de donnÃ©es NoSQL documentaire la plus populaire.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Configurer MongoDB Atlas (gratuit, cloud)\nâœ… MaÃ®triser le CRUD (Create, Read, Update, Delete)\nâœ… Utiliser les opÃ©rateurs de filtrage et comparaison\nâœ… Ã‰crire des agrÃ©gations\nâœ… CrÃ©er des index pour optimiser les performances",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ MongoDB dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ MongoDB dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module prÃ©cÃ©dent que MongoDB est une base NoSQL documentaire. Voici comment elle rÃ©pond aux dÃ©fis du Big Data :\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment MongoDB rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs serveurs)\n\n\nVelocity\nÃ‰critures rapides, rÃ©plication temps rÃ©el\n\n\nVariety\nSchÃ©ma flexible (JSON), pas de structure rigide\n\n\nVeracity\nValidation de schÃ©ma optionnelle\n\n\nValue\nAgrÃ©gations puissantes, intÃ©gration BI\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nMongoDB\n\n\n\n\nCAP\nCP (Consistency + Partition tolerance) par dÃ©faut\n\n\nBASE\nCohÃ©rence configurable (forte ou Ã©ventuelle)\n\n\n\n\nğŸ’¡ Pas dâ€™installation nÃ©cessaire : Tout se fait dans le cloud avec MongoDB Atlas !\nğŸ“ Note : Les commandes sâ€™exÃ©cutent dans MongoDB Shell ou lâ€™interface web Atlas.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.1 Explication simple",
    "text": "1.1 Explication simple\nMongoDB est une base de donnÃ©es qui stocke les donnÃ©es au format JSON (comme des fichiers texte structurÃ©s).\n\nComparaison : SQL vs MongoDB\nBase de donnÃ©es SQL (MySQL, PostgreSQL) :\nTable : employes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom    â”‚ age â”‚ ville â”‚ salaire â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice    â”‚ 25  â”‚ Paris â”‚ 45000   â”‚\nâ”‚ 2  â”‚ Bob      â”‚ 30  â”‚ Lyon  â”‚ 50000   â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n-- RequÃªte SQL\nSELECT * FROM employes WHERE ville = 'Paris';\nMongoDB (NoSQL) :\nCollection : employes\n[\n  {\n    \"_id\": 1,\n    \"nom\": \"Alice\",\n    \"age\": 25,\n    \"ville\": \"Paris\",\n    \"salaire\": 45000,\n    \"competences\": [\"Python\", \"SQL\"]\n  },\n  {\n    \"_id\": 2,\n    \"nom\": \"Bob\",\n    \"age\": 30,\n    \"ville\": \"Lyon\",\n    \"salaire\": 50000,\n    \"competences\": [\"Java\", \"Docker\"]\n  }\n]\n\n// RequÃªte MongoDB\ndb.employes.find({ ville: \"Paris\" })\n\n\nğŸ”‘ DiffÃ©rences principales :\n\n\n\n\n\n\n\n\nAspect\nSQL\nMongoDB\n\n\n\n\nStructure\nTableaux avec colonnes fixes\nDocuments JSON flexibles\n\n\nSchÃ©ma\nRigide (dÃ©fini Ã  lâ€™avance)\nFlexible (peut changer)\n\n\nFormat\nLignes et colonnes\nDocuments JSON\n\n\nTableaux\nDifficile (tables sÃ©parÃ©es)\nFacile (intÃ©grÃ©)\n\n\nLangage\nSQL\nJavaScript/JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.2 Vocabulaire de base",
    "text": "1.2 Vocabulaire de base\n\n\n\n\n\n\n\n\n\nSQL\nMongoDB\nExplication\nExemple\n\n\n\n\nDatabase\nDatabase\nConteneur principal\nentreprise\n\n\nTable\nCollection\nGroupe de donnÃ©es similaires\nemployes, produits\n\n\nRow\nDocument\nUne entrÃ©e de donnÃ©es\n{nom: \"Alice\", age: 25}\n\n\nColumn\nField\nUn attribut\nnom, age, ville\n\n\n\n\nStructure visuelle :\nMongoDB Server\n  â””â”€â”€ Database: ma_boutique\n       â”œâ”€â”€ Collection: clients\n       â”‚    â”œâ”€â”€ Document 1: { nom: \"Alice\", email: \"alice@email.com\" }\n       â”‚    â””â”€â”€ Document 2: { nom: \"Bob\", email: \"bob@email.com\" }\n       â”‚\n       â””â”€â”€ Collection: produits\n            â”œâ”€â”€ Document 1: { nom: \"Laptop\", prix: 899 }\n            â””â”€â”€ Document 2: { nom: \"Souris\", prix: 29 }",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.3 Format des documents",
    "text": "1.3 Format des documents\nMongoDB stocke les donnÃ©es en BSON (Binary JSON).\n\nExemple de document complet :\n{\n  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  // ID unique automatique\n  \"nom\": \"Alice Dupont\",                        // String (texte)\n  \"age\": 28,                                     // Number (entier)\n  \"salaire\": 55000.50,                           // Number (dÃ©cimal)\n  \"actif\": true,                                 // Boolean (vrai/faux)\n  \"date_embauche\": ISODate(\"2022-01-15\"),       // Date\n  \"competences\": [\"Python\", \"SQL\", \"MongoDB\"],  // Array (tableau)\n  \"adresse\": {                                   // Object (objet imbriquÃ©)\n    \"rue\": \"123 Main St\",\n    \"ville\": \"Paris\",\n    \"code_postal\": \"75001\"\n  },\n  \"notes\": null                                  // Null (vide)\n}\n\n\nPoints importants :\n\nChaque document a un champ _id unique (crÃ©Ã© automatiquement si absent)\nOn peut imbriquer des objets et des tableaux\nPas besoin que tous les documents aient les mÃªmes champs\nLa syntaxe ressemble Ã  JavaScript/JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?",
    "text": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?\n\nâœ… Avantages :\n1. FlexibilitÃ© du schÃ©ma\n\nPas besoin de dÃ©finir la structure Ã  lâ€™avance\nParfait pour des donnÃ©es qui changent souvent\nChaque document peut Ãªtre diffÃ©rent\n\n// Document 1 : simple\n{ nom: \"Alice\", age: 25 }\n\n// Document 2 : plus dÃ©taillÃ© (dans la mÃªme collection !)\n{ nom: \"Bob\", age: 30, ville: \"Lyon\", competences: [\"Python\"], manager: \"Alice\" }\n2. Format JSON natif\n\nLes APIs web utilisent JSON\nPas de conversion nÃ©cessaire\nFacile Ã  lire et manipuler\n\n3. Performance\n\nLectures et Ã©critures trÃ¨s rapides\nGÃ¨re facilement des millions de documents\nScalabilitÃ© horizontale (ajouter des serveurs)\n\n4. DonnÃ©es imbriquÃ©es\n\nStocke des structures complexes facilement\nPas besoin de multiples tables et jointures\n\n// Tout dans un seul document !\n{\n  \"commande_id\": \"CMD001\",\n  \"client\": { \"nom\": \"Alice\", \"email\": \"alice@example.com\" },\n  \"articles\": [\n    { \"produit\": \"Laptop\", \"quantite\": 1, \"prix\": 899 },\n    { \"produit\": \"Souris\", \"quantite\": 2, \"prix\": 29 }\n  ],\n  \"total\": 957\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.5 Quand utiliser MongoDB ?",
    "text": "1.5 Quand utiliser MongoDB ?\n\nâœ… Utilisez MongoDB pour :\n\n\n\nCas dâ€™usage\nPourquoi\n\n\n\n\nğŸ“± Applications web/mobile\nAPI JSON, scalabilitÃ©\n\n\nğŸ“Š Collecte de logs\nVolume Ã©levÃ©, structure flexible\n\n\nğŸ›’ E-commerce (catalogues)\nProduits avec attributs variables\n\n\nğŸ“¡ DonnÃ©es IoT\nMillions dâ€™Ã©critures/seconde\n\n\nğŸ“ CMS (contenu)\nStructures diverses\n\n\nğŸ”„ Prototypage rapide\nPas de schÃ©ma prÃ©dÃ©fini\n\n\nğŸŒ DonnÃ©es JSON dâ€™APIs\nFormat natif\n\n\n\n\n\nâŒ Nâ€™utilisez PAS MongoDB pour :\n\n\n\nCas dâ€™usage\nPrÃ©fÃ©rez SQL\n\n\n\n\nğŸ’° Transactions bancaires\nBesoin dâ€™ACID strict\n\n\nğŸ”— Relations multiples complexes\nNombreuses jointures\n\n\nğŸ“ˆ Reporting BI traditionnel\nRequÃªtes SQL ad-hoc\n\n\nğŸ“Š Data Warehouse\nSchÃ©ma en Ã©toile\n\n\n\n\n\nğŸ’¡ RÃ¨gle simple :\n\nDonnÃ©es flexibles, JSON, volume Ã©levÃ© â†’ MongoDB\n\nDonnÃ©es structurÃ©es, relations strictes, transactions complexes â†’ SQL (PostgreSQL, MySQL)",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.1 Pourquoi MongoDB Atlas ?",
    "text": "2.1 Pourquoi MongoDB Atlas ?\nMongoDB Atlas = MongoDB hÃ©bergÃ© dans le cloud (gratuit)\n\nAvantages :\nâœ… Gratuit : Tier M0 avec 512 MB de stockage\nâœ… Pas dâ€™installation : Tout dans le navigateur\nâœ… Interface graphique : Data Explorer facile\nâœ… SÃ©curisÃ© : Backup automatique\nâœ… MongoDB Shell intÃ©grÃ© : Testez vos commandes directement",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.2 CrÃ©er votre compte (5 minutes)",
    "text": "2.2 CrÃ©er votre compte (5 minutes)\n\nâ–¶ï¸ Ã‰tape 1 : Inscription\n\nAllez sur ğŸ‘‰ cloud.mongodb.com\nCliquez sur â€œTry Freeâ€\nInscrivez-vous avec email/mot de passe (ou Google)\n\n\n\nâ–¶ï¸ Ã‰tape 2 : CrÃ©er un cluster gratuit\n\nChoisissez M0 (FREE) ğŸ‰\nProvider : AWS (ou Google Cloud/Azure)\nRÃ©gion : Choisissez proche de vous\n\nEurope : Frankfurt, Paris, London\nAmÃ©rique : N. Virginia, Oregon\nAsie : Singapore, Mumbai\n\nCluster Name : Cluster0 (par dÃ©faut, vous pouvez changer)\nCliquez â€œCreateâ€\n\nâ³ Attendez 2-3 minutes que le cluster se crÃ©e\n\n\nâ–¶ï¸ Ã‰tape 3 : CrÃ©er un utilisateur\n\nDans la popup de sÃ©curitÃ© :\nAuthentication Method : Username and Password\nUsername : admin (ou votre choix)\nPassword : CrÃ©ez un mot de passe fort\n\nâš ï¸ NOTEZ-LE BIEN quelque part !\n\nCliquez â€œCreate Userâ€\n\n\n\nâ–¶ï¸ Ã‰tape 4 : Autoriser lâ€™accÃ¨s\n\nDans la popup : â€œWhere would you like to connect from?â€\nChoisissez â€œMy Local Environmentâ€\nOption 1 (recommandÃ©e pour apprendre) :\n\nCliquez â€œAdd My Current IP Addressâ€\n\nOption 2 (plus simple mais moins sÃ©curisÃ©e) :\n\nMettez 0.0.0.0/0 pour autoriser toutes les IPs\n\nCliquez â€œFinish and Closeâ€\n\nâœ… Votre cluster est prÃªt !",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.3 AccÃ©der Ã  MongoDB Shell",
    "text": "2.3 AccÃ©der Ã  MongoDB Shell\n\nMÃ©thode 1 : MongoDB Shell Web (le plus simple)\n\nDans MongoDB Atlas, allez sur â€œDatabaseâ€ (menu gauche)\nCliquez sur â€œBrowse Collectionsâ€ sur votre cluster\nEn bas de page, cliquez sur lâ€™onglet â€œMongoDB Shellâ€ ou â€œ&gt;_ mongoshâ€\nUne console sâ€™ouvre dans le navigateur âœ…\n\n\n\nMÃ©thode 2 : Data Explorer (interface graphique)\n\nCliquez sur â€œBrowse Collectionsâ€\nVous pouvez crÃ©er des bases de donnÃ©es et collections visuellement\nPratique pour visualiser, mais on va utiliser des commandes\n\n\n\nğŸ’¡ Dans ce tutoriel\nToutes les commandes ci-dessous sont Ã  taper dans : - MongoDB Shell Web (dans Atlas) - Ou mongosh (si vous lâ€™installez localement) - Ou Data Explorer â†’ Insert Document (pour ajouter des donnÃ©es)\nOn utilise la syntaxe JavaScript/MongoDB, pas Python !",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.1 CrÃ©er une base de donnÃ©es et une collection",
    "text": "3.1 CrÃ©er une base de donnÃ©es et une collection\n\nCrÃ©er/SÃ©lectionner une base de donnÃ©es\n// CrÃ©er ou utiliser la base de donnÃ©es \"ma_premiere_db\"\nuse ma_premiere_db\nNote : La base de donnÃ©es nâ€™est crÃ©Ã©e rÃ©ellement que quand vous insÃ©rez des donnÃ©es.\n\n\nVoir la base de donnÃ©es actuelle\ndb\n\n\nLister toutes les bases de donnÃ©es\nshow dbs\n\n\nLister les collections\nshow collections",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.2 CREATE - InsÃ©rer des documents",
    "text": "3.2 CREATE - InsÃ©rer des documents\n\nInsÃ©rer UN document\n// CrÃ©er la collection \"employes\" et insÃ©rer un document\ndb.employes.insertOne({\n  nom: \"Alice Dupont\",\n  age: 28,\n  poste: \"Data Engineer\",\n  salaire: 55000,\n  ville: \"Paris\"\n})\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedId: ObjectId(\"507f1f77bcf86cd799439011\")\n}\n\n\nInsÃ©rer PLUSIEURS documents\ndb.employes.insertMany([\n  {\n    nom: \"Bob Martin\",\n    age: 32,\n    poste: \"Data Analyst\",\n    salaire: 48000,\n    ville: \"Lyon\"\n  },\n  {\n    nom: \"Charlie Dubois\",\n    age: 35,\n    poste: \"Data Scientist\",\n    salaire: 65000,\n    ville: \"Paris\"\n  },\n  {\n    nom: \"David Laurent\",\n    age: 29,\n    poste: \"Data Engineer\",\n    salaire: 58000,\n    ville: \"Marseille\"\n  },\n  {\n    nom: \"Eve Bernard\",\n    age: 26,\n    poste: \"Data Analyst\",\n    salaire: 45000,\n    ville: \"Lyon\"\n  }\n])\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedIds: {\n    '0': ObjectId(\"...\"),\n    '1': ObjectId(\"...\"),\n    '2': ObjectId(\"...\"),\n    '3': ObjectId(\"...\")\n  }\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.3 READ - Lire des documents",
    "text": "3.3 READ - Lire des documents\n\nLire TOUS les documents\ndb.employes.find()\n\n\nLire avec un affichage formatÃ© (pretty)\ndb.employes.find().pretty()\n\n\nLire UN seul document\ndb.employes.findOne()\n\n\nFiltrer : employÃ©s Ã  Paris\ndb.employes.find({ ville: \"Paris\" })\n\n\nFiltrer : salaire supÃ©rieur Ã  50000\ndb.employes.find({ salaire: { $gt: 50000 } })\nOpÃ©rateurs de comparaison : - $gt : greater than (&gt;) - $gte : greater than or equal (&gt;=) - $lt : less than (&lt;) - $lte : less than or equal (&lt;=) - $eq : equal (=) - $ne : not equal (!=)\n\n\nFiltrer avec plusieurs conditions (AND)\n// EmployÃ©s Ã  Paris avec salaire &gt; 50000\ndb.employes.find({\n  ville: \"Paris\",\n  salaire: { $gt: 50000 }\n})\n\n\nFiltrer avec OR\n// EmployÃ©s Ã  Paris OU Lyon\ndb.employes.find({\n  $or: [\n    { ville: \"Paris\" },\n    { ville: \"Lyon\" }\n  ]\n})\n\n\nSÃ©lectionner certains champs seulement (projection)\n// Afficher seulement nom et salaire (sans _id)\ndb.employes.find(\n  {},\n  { nom: 1, salaire: 1, _id: 0 }\n)\nNote : 1 = inclure, 0 = exclure",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.4 Trier, limiter, compter",
    "text": "3.4 Trier, limiter, compter\n\nTrier par salaire (croissant)\ndb.employes.find().sort({ salaire: 1 })\n\n\nTrier par salaire (dÃ©croissant)\ndb.employes.find().sort({ salaire: -1 })\nNote : 1 = croissant, -1 = dÃ©croissant\n\n\nLimiter les rÃ©sultats (top 3)\n// Top 3 salaires\ndb.employes.find().sort({ salaire: -1 }).limit(3)\n\n\nSauter des rÃ©sultats (pagination)\n// Sauter les 2 premiers, afficher les 3 suivants\ndb.employes.find().skip(2).limit(3)\n\n\nCompter les documents\n// Compter tous les employÃ©s\ndb.employes.countDocuments()\n\n// Compter les employÃ©s Ã  Paris\ndb.employes.countDocuments({ ville: \"Paris\" })\n\n// Compter les employÃ©s avec salaire &gt; 50000\ndb.employes.countDocuments({ salaire: { $gt: 50000 } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.5 UPDATE - Modifier des documents",
    "text": "3.5 UPDATE - Modifier des documents\n\nModifier UN document\n// Augmenter le salaire d'Alice\ndb.employes.updateOne(\n  { nom: \"Alice Dupont\" },           // Condition\n  { $set: { salaire: 60000 } }       // Modification\n)\nRÃ©sultat :\n{\n  acknowledged: true,\n  matchedCount: 1,\n  modifiedCount: 1\n}\n\n\nModifier PLUSIEURS documents\n// Augmenter tous les salaires de Lyon de 2000â‚¬\ndb.employes.updateMany(\n  { ville: \"Lyon\" },\n  { $inc: { salaire: 2000 } }       // $inc = incrÃ©menter\n)\n\n\nOpÃ©rateurs de modification\n// $set : dÃ©finir une valeur\n{ $set: { ville: \"Paris\" } }\n\n// $inc : incrÃ©menter\n{ $inc: { age: 1 } }\n\n// $mul : multiplier\n{ $mul: { salaire: 1.1 } }  // Augmentation de 10%\n\n// $unset : supprimer un champ\n{ $unset: { notes: \"\" } }\n\n// $rename : renommer un champ\n{ $rename: { \"nom\": \"nom_complet\" } }\n\n\nAjouter un champ Ã  tous les documents\ndb.employes.updateMany(\n  {},\n  { $set: { actif: true } }\n)",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.6 DELETE - Supprimer des documents",
    "text": "3.6 DELETE - Supprimer des documents\n\nSupprimer UN document\ndb.employes.deleteOne({ nom: \"Test User\" })\n\n\nSupprimer PLUSIEURS documents\n// Supprimer tous les employÃ©s de Test\ndb.employes.deleteMany({ ville: \"Test\" })\n\n\nâš ï¸ Supprimer TOUS les documents\n// ATTENTION : Supprime tout !\ndb.employes.deleteMany({})\n\n\nSupprimer une collection entiÃ¨re\ndb.employes.drop()\n\n\nSupprimer une base de donnÃ©es\ndb.dropDatabase()",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.1 Grouper et compter",
    "text": "4.1 Grouper et compter\n\nCompter les employÃ©s par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",              // Grouper par ville\n      nombre: { $sum: 1 }         // Compter\n    }\n  }\n])\nRÃ©sultat :\n[\n  { _id: \"Paris\", nombre: 2 },\n  { _id: \"Lyon\", nombre: 2 },\n  { _id: \"Marseille\", nombre: 1 }\n]\n\n\nSalaire moyen par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  },\n  {\n    $sort: { salaire_moyen: -1 }   // Trier par salaire moyen dÃ©croissant\n  }\n])\n\n\nGrouper par poste\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$poste\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.2 Filtrer avant dâ€™agrÃ©ger",
    "text": "4.2 Filtrer avant dâ€™agrÃ©ger\n// Statistiques pour les employÃ©s avec salaire &gt; 50000\ndb.employes.aggregate([\n  {\n    $match: { salaire: { $gt: 50000 } }   // Filtrer d'abord\n  },\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.3 Statistiques globales",
    "text": "4.3 Statistiques globales\n// Statistiques sur tous les salaires\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: null,                          // null = pas de groupement\n      total_employes: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_total: { $sum: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "Ce que vous avez appris âœ…",
    "text": "Ce que vous avez appris âœ…\n\nConcepts\n\nMongoDB = Base de donnÃ©es NoSQL (documents JSON)\nVocabulaire : Database â†’ Collection â†’ Document â†’ Field\nDiffÃ©rence SQL/NoSQL : Structure fixe vs flexible\nQuand utiliser MongoDB : APIs, logs, IoT, e-commerce\n\n\n\nCommandes essentielles\n// BASES\nuse ma_db                    // CrÃ©er/utiliser DB\nshow dbs                     // Lister les DBs\nshow collections             // Lister les collections\n\n// CREATE\ndb.collection.insertOne({...})\ndb.collection.insertMany([{...}, {...}])\n\n// READ\ndb.collection.find()                     // Tout\ndb.collection.find({ ville: \"Paris\" })   // FiltrÃ©\ndb.collection.findOne()\ndb.collection.find().sort({ age: -1 })   // TriÃ©\ndb.collection.find().limit(5)            // LimitÃ©\ndb.collection.countDocuments()\n\n// UPDATE\ndb.collection.updateOne({ _id: 1 }, { $set: { age: 30 } })\ndb.collection.updateMany({ ville: \"Lyon\" }, { $inc: { salaire: 1000 } })\n\n// DELETE\ndb.collection.deleteOne({ _id: 1 })\ndb.collection.deleteMany({ ville: \"Test\" })\n\n// AGGREGATE\ndb.collection.aggregate([\n  { $group: { _id: \"$ville\", count: { $sum: 1 } } }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "OpÃ©rateurs importants ğŸ“",
    "text": "OpÃ©rateurs importants ğŸ“\n\nComparaison\n\n$gt : &gt; (greater than)\n$gte : &gt;= (greater than or equal)\n$lt : &lt; (less than)\n$lte : &lt;= (less than or equal)\n$eq : = (equal)\n$ne : != (not equal)\n\n\n\nModification\n\n$set : DÃ©finir une valeur\n$inc : IncrÃ©menter\n$mul : Multiplier\n$unset : Supprimer un champ\n\n\n\nAgrÃ©gation\n\n$sum : Somme\n$avg : Moyenne\n$max : Maximum\n$min : Minimum\n$group : Grouper\n$match : Filtrer",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.1 Pourquoi les index ?",
    "text": "5.1 Pourquoi les index ?\nSans index, MongoDB doit scanner tous les documents pour trouver ceux qui correspondent au filtre. Avec un index, la recherche est beaucoup plus rapide.\n\n\n\nSans index\nAvec index\n\n\n\n\nScan complet (lent)\nRecherche directe (rapide)\n\n\nO(n)\nO(log n)\n\n\n1M docs = 1M comparaisons\n1M docs = ~20 comparaisons",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.2 CrÃ©er un index",
    "text": "5.2 CrÃ©er un index\n\nIndex simple (un champ)\n// Index sur le champ \"email\" (croissant)\ndb.employes.createIndex({ email: 1 })\n\n// Index sur le champ \"salaire\" (dÃ©croissant)\ndb.employes.createIndex({ salaire: -1 })\n\n\nIndex composÃ© (plusieurs champs)\n// Index sur ville + poste (pour requÃªtes frÃ©quentes)\ndb.employes.createIndex({ ville: 1, poste: 1 })\n\n\nIndex unique\n// EmpÃªche les doublons sur email\ndb.employes.createIndex({ email: 1 }, { unique: true })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.3 GÃ©rer les index",
    "text": "5.3 GÃ©rer les index\n// Lister tous les index\ndb.employes.getIndexes()\n\n// Supprimer un index\ndb.employes.dropIndex({ email: 1 })\n\n// Supprimer tous les index (sauf _id)\ndb.employes.dropIndexes()",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.4 Analyser les performances",
    "text": "5.4 Analyser les performances\n// Voir le plan d'exÃ©cution\ndb.employes.find({ ville: \"Paris\" }).explain(\"executionStats\")\nRegarder :\n\ntotalDocsExamined : Combien de documents scannÃ©s\nexecutionTimeMillis : Temps dâ€™exÃ©cution\nstage: \"IXSCAN\" = Index utilisÃ© âœ…\nstage: \"COLLSCAN\" = Scan complet âŒ",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.5 Bonnes pratiques",
    "text": "5.5 Bonnes pratiques\n\n\n\n\n\n\n\nRÃ¨gle\nExplication\n\n\n\n\nâœ… Indexer les champs de filtrage frÃ©quents\nville, email, date\n\n\nâœ… Indexer les champs de tri\nORDER BY = index\n\n\nâš ï¸ Pas trop dâ€™index\nChaque index ralentit les Ã©critures\n\n\nâš ï¸ Index composÃ© : ordre important\nLe champ le plus filtrant en premier",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.1 Recherche avec expressions rÃ©guliÃ¨res",
    "text": "6.1 Recherche avec expressions rÃ©guliÃ¨res\n// Noms commenÃ§ant par \"A\"\ndb.employes.find({ nom: { $regex: \"^A\" } })\n\n// Noms contenant \"dupont\" (insensible Ã  la casse)\ndb.employes.find({ nom: { $regex: \"dupont\", $options: \"i\" } })\n\n// Emails Gmail\ndb.employes.find({ email: { $regex: \"@gmail\\.com$\" } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.2 OpÃ©rateurs de tableau",
    "text": "6.2 OpÃ©rateurs de tableau\n// Document avec un tableau\ndb.employes.insertOne({\n  nom: \"Alice\",\n  competences: [\"Python\", \"SQL\", \"MongoDB\"]\n})\n\n// Trouver ceux qui ont \"Python\" dans leurs compÃ©tences\ndb.employes.find({ competences: \"Python\" })\n\n// Trouver ceux qui ont Python ET SQL\ndb.employes.find({ competences: { $all: [\"Python\", \"SQL\"] } })\n\n// Trouver ceux qui ont au moins 3 compÃ©tences\ndb.employes.find({ competences: { $size: 3 } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.3 Recherche dans les objets imbriquÃ©s",
    "text": "6.3 Recherche dans les objets imbriquÃ©s\n// Document avec objet imbriquÃ©\ndb.employes.insertOne({\n  nom: \"Bob\",\n  adresse: {\n    ville: \"Paris\",\n    code_postal: \"75001\"\n  }\n})\n\n// Rechercher par ville imbriquÃ©e (notation pointÃ©e)\ndb.employes.find({ \"adresse.ville\": \"Paris\" })\n\n// Rechercher par code postal\ndb.employes.find({ \"adresse.code_postal\": { $regex: \"^75\" } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet : SQL vs MongoDB",
    "text": "ğŸ“‹ Cheatsheet : SQL vs MongoDB\n\n\n\n\n\n\n\n\nOpÃ©ration\nSQL\nMongoDB\n\n\n\n\nTout sÃ©lectionner\nSELECT * FROM table\ndb.collection.find()\n\n\nFiltrer\nWHERE col = 'val'\nfind({ col: 'val' })\n\n\nSupÃ©rieur Ã \nWHERE col &gt; 10\nfind({ col: { $gt: 10 } })\n\n\nET\nWHERE a = 1 AND b = 2\nfind({ a: 1, b: 2 })\n\n\nOU\nWHERE a = 1 OR b = 2\nfind({ $or: [{a:1}, {b:2}] })\n\n\nIN\nWHERE col IN (1,2,3)\nfind({ col: { $in: [1,2,3] } })\n\n\nLIKE\nWHERE col LIKE '%val%'\nfind({ col: { $regex: 'val' } })\n\n\nProjection\nSELECT a, b FROM\nfind({}, { a:1, b:1 })\n\n\nTrier\nORDER BY col ASC\n.sort({ col: 1 })\n\n\nLimiter\nLIMIT 10\n.limit(10)\n\n\nCompter\nSELECT COUNT(*)\n.countDocuments()\n\n\nGrouper\nGROUP BY col\naggregate([{$group:{_id:'$col'}}])\n\n\nInsert\nINSERT INTO ... VALUES\ninsertOne({...})\n\n\nUpdate\nUPDATE ... SET ... WHERE\nupdateOne({filter}, {$set:{...}})\n\n\nDelete\nDELETE FROM ... WHERE\ndeleteOne({filter})",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ Exercices pratiques â€” Ã€ toi de jouer !",
    "text": "ğŸ¯ Exercices pratiques â€” Ã€ toi de jouer !\n\nCollection disponible : employes\n{ nom, age, poste, salaire, ville, competences: [...] }\n\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nTrouver tous les employÃ©s qui habitent Ã  Lyon.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({ ville: \"Lyon\" })\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nCompter le nombre total dâ€™employÃ©s.\n\n\nğŸ’¡ Solution\n\ndb.employes.countDocuments()\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nTrouver les 3 employÃ©s les mieux payÃ©s (nom et salaire uniquement).\n\n\nğŸ’¡ Solution\n\ndb.employes.find({}, { nom: 1, salaire: 1, _id: 0 })\n  .sort({ salaire: -1 })\n  .limit(3)\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nAugmenter de 5% le salaire de tous les Data Engineers.\n\n\nğŸ’¡ Solution\n\ndb.employes.updateMany(\n  { poste: \"Data Engineer\" },\n  { $mul: { salaire: 1.05 } }\n)\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nCalculer le salaire moyen par poste, triÃ© du plus Ã©levÃ© au plus bas.\n\n\nğŸ’¡ Solution\n\ndb.employes.aggregate([\n  { $group: { _id: \"$poste\", salaire_moyen: { $avg: \"$salaire\" } } },\n  { $sort: { salaire_moyen: -1 } }\n])\n\n\n\n\nğŸ‹ï¸ Exercice 6 â€” AvancÃ©\nTrouver les employÃ©s qui ont â€œPythonâ€ dans leurs compÃ©tences et gagnent plus de 50000â‚¬.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({\n  competences: \"Python\",\n  salaire: { $gt: 50000 }\n})",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert MongoDB ! Vous Ãªtes prÃªt pour le niveau intermÃ©diaire\n7-9/10 : ğŸŒŸ TrÃ¨s bien ! Relisez les sections oÃ¹ vous avez hÃ©sitÃ©\n5-6/10 : ğŸ’ª Bon dÃ©but ! Pratiquez les commandes dans Atlas\n&lt; 5/10 : ğŸ“š Relisez le notebook et testez les exemples",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant MongoDB ! Passons Ã  une autre base NoSQL trÃ¨s utilisÃ©e en Data Engineering.\nğŸ‘‰ Module suivant : 10_elasticsearch_for_data_engineers.ipynb â€” Elasticsearch pour la recherche et lâ€™analytics\n\n\nğŸ“š Ressources\n\nMongoDB Atlas â€” Votre compte\nMongoDB Documentation â€” Doc officielle\nMongoDB University â€” Cours gratuits\nMongoDB Cheat Sheet\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les bases de MongoDB.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "",
    "text": "Ce module bonus couvre FastAPI, le framework Python moderne pour crÃ©er des APIs REST performantes. En tant que Data Engineer, tu auras souvent besoin dâ€™exposer des donnÃ©es via des APIs.",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#prÃ©requis",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 04_python_basics (fonctions, classes, type hints)\n\n\nâœ… Requis\nModule 05_python_data_processing (Pandas, JSON)\n\n\nâ­ Bonus\nModule 09_mongodb (pour les exemples avec DB)",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… CrÃ©er une API REST avec FastAPI\nâœ… DÃ©finir des modÃ¨les de donnÃ©es avec Pydantic\nâœ… CrÃ©er des endpoints CRUD (Create, Read, Update, Delete)\nâœ… Valider automatiquement les donnÃ©es entrantes\nâœ… Servir des donnÃ©es Pandas via API\nâœ… Connecter une API Ã  une base de donnÃ©es\nâœ… GÃ©nÃ©rer une documentation Swagger automatique\nâœ… DÃ©ployer une API avec Uvicorn\n\n\n\nğŸ’¡ Note : FastAPI sâ€™exÃ©cute comme un serveur. Certains exemples seront Ã  tester hors du notebook, dans des fichiers .py.",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ¤” Pourquoi FastAPI pour un Data Engineer ?",
    "text": "ğŸ¤” Pourquoi FastAPI pour un Data Engineer ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              CAS D'USAGE DATA ENGINEERING                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  ğŸ“Š Exposer des donnÃ©es traitÃ©es (datamart â†’ API)               â”‚\nâ”‚  ğŸ”„ CrÃ©er des webhooks pour dÃ©clencher des pipelines            â”‚\nâ”‚  ğŸ¤– Servir des prÃ©dictions ML en production                     â”‚\nâ”‚  ğŸ“ˆ Fournir des mÃ©triques pour les dashboards                   â”‚\nâ”‚  ğŸ”— CrÃ©er des microservices data                                â”‚\nâ”‚  âœ… Valider des donnÃ©es avant ingestion                         â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ†š FastAPI vs autres frameworks\n\n\n\nFramework\nPerformance\nTyping\nDoc auto\nAsync\nComplexitÃ©\n\n\n\n\nFastAPI\nâ­â­â­\nâœ… Natif\nâœ… Swagger\nâœ…\nSimple\n\n\nFlask\nâ­â­\nâŒ Manuel\nâŒ Extension\nâŒ\nSimple\n\n\nDjango REST\nâ­â­\nâŒ Manuel\nâœ…\nâš ï¸\nComplexe\n\n\nExpress (Node)\nâ­â­â­\nâŒ\nâŒ\nâœ…\nSimple\n\n\n\n\nğŸ’¡ FastAPI combine le meilleur : performance, simplicitÃ©, et documentation automatique.",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#installation",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#installation",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“¦ Installation",
    "text": "ğŸ“¦ Installation\n# Installation de base\npip install fastapi uvicorn\n\n# Avec toutes les dÃ©pendances optionnelles\npip install \"fastapi[all]\"\n\n\n\nPackage\nRÃ´le\n\n\n\n\nfastapi\nLe framework\n\n\nuvicorn\nServeur ASGI (pour lancer lâ€™API)\n\n\npydantic\nValidation de donnÃ©es (inclus avec FastAPI)\n\n\n\n\n\nCode\n# Installation\n!pip install fastapi uvicorn pydantic pandas --quiet\n\nprint(\"âœ… Packages installÃ©s !\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-minimale",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-minimale",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ Structure minimale",
    "text": "ğŸ“ Structure minimale\nCrÃ©er un fichier main.py :\n# main.py\nfrom fastapi import FastAPI\n\n# CrÃ©er l'application\napp = FastAPI()\n\n# Premier endpoint\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Hello Data Engineer!\"}\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\"}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#lancer-le-serveur",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#lancer-le-serveur",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸš€ Lancer le serveur",
    "text": "ğŸš€ Lancer le serveur\n# Dans le terminal\nuvicorn main:app --reload\n\n# main = fichier main.py\n# app = variable FastAPI()\n# --reload = redÃ©marre automatiquement si le code change\nRÃ©sultat :\nINFO:     Uvicorn running on http://127.0.0.1:8000\nINFO:     Started reloader process",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#tester-lapi",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#tester-lapi",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸŒ Tester lâ€™API",
    "text": "ğŸŒ Tester lâ€™API\n\n\n\nURL\nRÃ©sultat\n\n\n\n\nhttp://127.0.0.1:8000\n{\"message\": \"Hello Data Engineer!\"}\n\n\nhttp://127.0.0.1:8000/health\n{\"status\": \"healthy\"}\n\n\nhttp://127.0.0.1:8000/docs\nğŸ“„ Documentation Swagger\n\n\nhttp://127.0.0.1:8000/redoc\nğŸ“„ Documentation ReDoc",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ”„ Les mÃ©thodes HTTP",
    "text": "ğŸ”„ Les mÃ©thodes HTTP\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    MÃ‰THODES HTTP (CRUD)                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ MÃ©thode  â”‚ Action           â”‚ Exemple                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ GET      â”‚ Lire (Read)      â”‚ GET /users â†’ Liste des users      â”‚\nâ”‚ POST     â”‚ CrÃ©er (Create)   â”‚ POST /users â†’ CrÃ©er un user       â”‚\nâ”‚ PUT      â”‚ Remplacer        â”‚ PUT /users/1 â†’ Remplacer user 1   â”‚\nâ”‚ PATCH    â”‚ Modifier         â”‚ PATCH /users/1 â†’ Modifier user 1  â”‚\nâ”‚ DELETE   â”‚ Supprimer        â”‚ DELETE /users/1 â†’ Supprimer       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items\")\ndef get_items():\n    return {\"action\": \"Liste des items\"}\n\n@app.post(\"/items\")\ndef create_item():\n    return {\"action\": \"CrÃ©ation d'un item\"}\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int):\n    return {\"action\": f\"Mise Ã  jour de l'item {item_id}\"}\n\n@app.delete(\"/items/{item_id}\")\ndef delete_item(item_id: int):\n    return {\"action\": f\"Suppression de l'item {item_id}\"}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ ParamÃ¨tres de chemin (Path Parameters)",
    "text": "ğŸ“ ParamÃ¨tres de chemin (Path Parameters)\nLes paramÃ¨tres dans lâ€™URL permettent dâ€™identifier une ressource spÃ©cifique.\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# ParamÃ¨tre simple\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):  # Typage automatique !\n    return {\"user_id\": user_id}\n\n# Plusieurs paramÃ¨tres\n@app.get(\"/users/{user_id}/orders/{order_id}\")\ndef get_user_order(user_id: int, order_id: int):\n    return {\n        \"user_id\": user_id,\n        \"order_id\": order_id\n    }\n\nâœ… Validation automatique\nGET /users/42        â†’ {\"user_id\": 42}       âœ…\nGET /users/abc       â†’ Erreur 422            âŒ (pas un int)\nGET /users/-1        â†’ {\"user_id\": -1}       âœ… (mais logiquement faux)\n\n\nğŸ”’ Validation avancÃ©e avec Path\nfrom fastapi import FastAPI, Path\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(\n    user_id: int = Path(\n        ...,  # Requis\n        title=\"User ID\",\n        description=\"L'identifiant unique de l'utilisateur\",\n        ge=1,  # Greater or Equal (&gt;= 1)\n        le=10000  # Less or Equal (&lt;= 10000)\n    )\n):\n    return {\"user_id\": user_id}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ” ParamÃ¨tres de requÃªte (Query Parameters)",
    "text": "ğŸ” ParamÃ¨tres de requÃªte (Query Parameters)\nLes paramÃ¨tres aprÃ¨s le ? dans lâ€™URL pour filtrer, paginer, etc.\nGET /users?skip=0&limit=10&active=true\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              Query parameters\nfrom fastapi import FastAPI\nfrom typing import Optional\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(\n    skip: int = 0,           # DÃ©faut = 0\n    limit: int = 10,         # DÃ©faut = 10\n    active: bool = True,     # DÃ©faut = True\n    search: Optional[str] = None  # Optionnel\n):\n    return {\n        \"skip\": skip,\n        \"limit\": limit,\n        \"active\": active,\n        \"search\": search\n    }\n\nğŸ“Š Cas dâ€™usage Data Engineering : Pagination\n@app.get(\"/data\")\ndef get_data(\n    page: int = 1,\n    page_size: int = 100,\n    sort_by: str = \"created_at\",\n    order: str = \"desc\"\n):\n    # Calculer l'offset\n    offset = (page - 1) * page_size\n    \n    return {\n        \"page\": page,\n        \"page_size\": page_size,\n        \"offset\": offset,\n        \"sort_by\": sort_by,\n        \"order\": order\n    }",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ DÃ©finir un modÃ¨le",
    "text": "ğŸ“ DÃ©finir un modÃ¨le\nfrom pydantic import BaseModel, Field, EmailStr\nfrom typing import Optional\nfrom datetime import datetime\n\nclass User(BaseModel):\n    \"\"\"ModÃ¨le utilisateur pour l'API.\"\"\"\n    \n    id: Optional[int] = None\n    nom: str = Field(..., min_length=2, max_length=50)\n    email: EmailStr\n    age: int = Field(..., ge=0, le=150)\n    actif: bool = True\n    created_at: datetime = Field(default_factory=datetime.now)\n    \n    class Config:\n        # Exemple pour la documentation\n        json_schema_extra = {\n            \"example\": {\n                \"nom\": \"Alice Dupont\",\n                \"email\": \"alice@example.com\",\n                \"age\": 30,\n                \"actif\": True\n            }\n        }\n\nğŸ”’ Validateurs disponibles\n\n\n\nValidateur\nExemple\nDescription\n\n\n\n\nmin_length\nField(min_length=2)\nLongueur min string\n\n\nmax_length\nField(max_length=50)\nLongueur max string\n\n\nge\nField(ge=0)\nGreater or Equal\n\n\nle\nField(le=100)\nLess or Equal\n\n\ngt\nField(gt=0)\nGreater Than\n\n\nlt\nField(lt=100)\nLess Than\n\n\nregex\nField(regex='^[A-Z]')\nPattern regex\n\n\nEmailStr\nType spÃ©cial\nValide format email\n\n\n\n\n\nCode\n# Exemple Pydantic dans le notebook\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import Optional\nfrom datetime import datetime\n\nclass Transaction(BaseModel):\n    \"\"\"ModÃ¨le de transaction pour pipeline ETL.\"\"\"\n    \n    id: Optional[int] = None\n    montant: float = Field(..., gt=0, description=\"Montant positif\")\n    devise: str = Field(..., min_length=3, max_length=3)\n    description: str = Field(..., min_length=1, max_length=200)\n    timestamp: datetime = Field(default_factory=datetime.now)\n    \n    @field_validator('devise')\n    @classmethod\n    def devise_uppercase(cls, v):\n        return v.upper()\n\n# Test de validation\ntry:\n    # âœ… Valide\n    tx1 = Transaction(montant=100.50, devise=\"eur\", description=\"Achat\")\n    print(f\"âœ… Transaction valide : {tx1}\")\n    print(f\"   Devise convertie : {tx1.devise}\")\n    \n    # âŒ Invalide\n    tx2 = Transaction(montant=-50, devise=\"EUR\", description=\"Test\")\nexcept Exception as e:\n    print(f\"\\nâŒ Erreur de validation : {e}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“¨ Corps de requÃªte (Request Body)",
    "text": "ğŸ“¨ Corps de requÃªte (Request Body)\nPour les requÃªtes POST/PUT, les donnÃ©es sont envoyÃ©es dans le body (corps) en JSON.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserCreate(BaseModel):\n    nom: str\n    email: str\n    age: int\n\nclass UserResponse(BaseModel):\n    id: int\n    nom: str\n    email: str\n    age: int\n\n# Base de donnÃ©es fictive\nusers_db = []\nuser_id_counter = 1\n\n@app.post(\"/users\", response_model=UserResponse)\ndef create_user(user: UserCreate):\n    global user_id_counter\n    \n    # CrÃ©er le user avec un ID\n    new_user = {\n        \"id\": user_id_counter,\n        \"nom\": user.nom,\n        \"email\": user.email,\n        \"age\": user.age\n    }\n    \n    users_db.append(new_user)\n    user_id_counter += 1\n    \n    return new_user\n\nğŸ“¤ RequÃªte avec curl\ncurl -X POST \"http://127.0.0.1:8000/users\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}'\n\n\nğŸ“¤ RequÃªte avec Python requests\nimport requests\n\nresponse = requests.post(\n    \"http://127.0.0.1:8000/users\",\n    json={\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}\n)\nprint(response.json())",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "âš ï¸ Gestion des erreurs",
    "text": "âš ï¸ Gestion des erreurs\nFastAPI utilise HTTPException pour retourner des erreurs HTTP propres.\nfrom fastapi import FastAPI, HTTPException, status\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    user = find_user_by_id(user_id)  # Fonction fictive\n    \n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User {user_id} non trouvÃ©\",\n            headers={\"X-Error\": \"User not found\"}\n        )\n    \n    return user\n\nğŸ“‹ Codes HTTP courants\n\n\n\nCode\nNom\nUsage\n\n\n\n\n200\nOK\nSuccÃ¨s (GET, PUT)\n\n\n201\nCreated\nCrÃ©ation rÃ©ussie (POST)\n\n\n204\nNo Content\nSuppression rÃ©ussie (DELETE)\n\n\n400\nBad Request\nRequÃªte invalide\n\n\n401\nUnauthorized\nNon authentifiÃ©\n\n\n403\nForbidden\nNon autorisÃ©\n\n\n404\nNot Found\nRessource introuvable\n\n\n422\nUnprocessable Entity\nErreur de validation\n\n\n500\nInternal Server Error\nErreur serveur",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ—„ï¸ Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)",
    "text": "ğŸ—„ï¸ Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)\n# database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# URL de connexion\nDATABASE_URL = \"postgresql://user:password@localhost:5432/mydb\"\n# Pour SQLite: \"sqlite:///./data.db\"\n\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Dependency pour injecter la session\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n# models.py\nfrom sqlalchemy import Column, Integer, String, Float, DateTime\nfrom database import Base\nfrom datetime import datetime\n\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    nom = Column(String(100), nullable=False)\n    email = Column(String(100), unique=True, index=True)\n    age = Column(Integer)\n    created_at = Column(DateTime, default=datetime.now)\n# main.py\nfrom fastapi import FastAPI, Depends\nfrom sqlalchemy.orm import Session\nfrom database import get_db\nimport models\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(db: Session = Depends(get_db), skip: int = 0, limit: int = 10):\n    users = db.query(models.User).offset(skip).limit(limit).all()\n    return users",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸƒ Avec MongoDB (PyMongo)",
    "text": "ğŸƒ Avec MongoDB (PyMongo)\n# mongodb_api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nfrom pymongo import MongoClient\nfrom bson import ObjectId\nfrom typing import Optional\n\napp = FastAPI(title=\"MongoDB API\")\n\n# Connexion MongoDB\nclient = MongoClient(\"mongodb://localhost:27017\")\ndb = client[\"data_engineering\"]\ncollection = db[\"pipelines\"]\n\n# Helper pour ObjectId\nclass PyObjectId(ObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n    \n    @classmethod\n    def validate(cls, v, field):\n        if not ObjectId.is_valid(v):\n            raise ValueError(\"Invalid ObjectId\")\n        return ObjectId(v)\n\n# ModÃ¨le\nclass Pipeline(BaseModel):\n    id: Optional[str] = Field(default=None, alias=\"_id\")\n    nom: str\n    description: str\n    schedule: str\n    \n    class Config:\n        populate_by_name = True\n\n# CRUD\n@app.get(\"/pipelines\")\ndef list_pipelines():\n    pipelines = []\n    for doc in collection.find():\n        doc[\"_id\"] = str(doc[\"_id\"])\n        pipelines.append(doc)\n    return pipelines\n\n@app.post(\"/pipelines\")\ndef create_pipeline(pipeline: Pipeline):\n    result = collection.insert_one(pipeline.model_dump(exclude={\"id\"}))\n    return {\"id\": str(result.inserted_id)}\n\n@app.get(\"/pipelines/{pipeline_id}\")\ndef get_pipeline(pipeline_id: str):\n    doc = collection.find_one({\"_id\": ObjectId(pipeline_id)})\n    if not doc:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    doc[\"_id\"] = str(doc[\"_id\"])\n    return doc\n\n@app.delete(\"/pipelines/{pipeline_id}\")\ndef delete_pipeline(pipeline_id: str):\n    result = collection.delete_one({\"_id\": ObjectId(pipeline_id)})\n    if result.deleted_count == 0:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    return {\"deleted\": True}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "â° Background Tasks (tÃ¢ches en arriÃ¨re-plan)",
    "text": "â° Background Tasks (tÃ¢ches en arriÃ¨re-plan)\nUtile pour les webhooks et le dÃ©clenchement de pipelines.\nfrom fastapi import FastAPI, BackgroundTasks\nimport time\n\napp = FastAPI()\n\ndef run_etl_pipeline(pipeline_id: int):\n    \"\"\"Simule l'exÃ©cution d'un pipeline ETL.\"\"\"\n    print(f\"ğŸš€ DÃ©marrage du pipeline {pipeline_id}\")\n    time.sleep(10)  # Simule un traitement long\n    print(f\"âœ… Pipeline {pipeline_id} terminÃ©\")\n\n@app.post(\"/pipelines/{pipeline_id}/run\")\ndef trigger_pipeline(pipeline_id: int, background_tasks: BackgroundTasks):\n    \"\"\"DÃ©clenche un pipeline en arriÃ¨re-plan.\"\"\"\n    \n    # Ajoute la tÃ¢che en arriÃ¨re-plan\n    background_tasks.add_task(run_etl_pipeline, pipeline_id)\n    \n    # RÃ©pond immÃ©diatement\n    return {\n        \"message\": f\"Pipeline {pipeline_id} dÃ©clenchÃ©\",\n        \"status\": \"running\"\n    }\n\nğŸ”— Webhook pour dÃ©clencher un pipeline\n@app.post(\"/webhook/data-arrival\")\ndef webhook_data_arrival(\n    payload: dict,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"Webhook appelÃ© quand de nouvelles donnÃ©es arrivent.\"\"\"\n    \n    source = payload.get(\"source\")\n    file_path = payload.get(\"file_path\")\n    \n    # DÃ©clencher le pipeline appropriÃ©\n    background_tasks.add_task(\n        process_new_data, \n        source=source, \n        file_path=file_path\n    )\n    \n    return {\"status\": \"accepted\", \"message\": \"Pipeline dÃ©clenchÃ©\"}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#upload-de-fichiers",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#upload-de-fichiers",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ Upload de fichiers",
    "text": "ğŸ“ Upload de fichiers\nPermet dâ€™ingÃ©rer des fichiers (CSV, JSON, Parquet) via API.\nfrom fastapi import FastAPI, UploadFile, File\nimport pandas as pd\nimport io\n\napp = FastAPI()\n\n@app.post(\"/upload/csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    \"\"\"Upload et analyse un fichier CSV.\"\"\"\n    \n    # VÃ©rifier l'extension\n    if not file.filename.endswith('.csv'):\n        raise HTTPException(status_code=400, detail=\"Fichier CSV requis\")\n    \n    # Lire le contenu\n    contents = await file.read()\n    \n    # Charger dans Pandas\n    df = pd.read_csv(io.StringIO(contents.decode('utf-8')))\n    \n    return {\n        \"filename\": file.filename,\n        \"rows\": len(df),\n        \"columns\": list(df.columns),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n        \"preview\": df.head(5).to_dict(orient=\"records\")\n    }\n\n@app.post(\"/upload/batch\")\nasync def upload_multiple(files: list[UploadFile] = File(...)):\n    \"\"\"Upload plusieurs fichiers.\"\"\"\n    \n    results = []\n    for file in files:\n        results.append({\n            \"filename\": file.filename,\n            \"size\": len(await file.read())\n        })\n    \n    return {\"uploaded\": len(files), \"files\": results}",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#documentation-automatique",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#documentation-automatique",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“„ Documentation automatique",
    "text": "ğŸ“„ Documentation automatique\nFastAPI gÃ©nÃ¨re automatiquement une documentation interactive !\n\nğŸŒ URLs de documentation\n\n\n\nURL\nType\nDescription\n\n\n\n\n/docs\nSwagger UI\nInterface interactive pour tester\n\n\n/redoc\nReDoc\nDocumentation lisible\n\n\n/openapi.json\nOpenAPI\nSchÃ©ma JSON de lâ€™API\n\n\n\n\n\nğŸ¨ Personnaliser la documentation\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Data Pipeline API\",\n    description=\"\"\"\n    ## ğŸš€ API pour gÃ©rer les pipelines de donnÃ©es\n    \n    Cette API permet de :\n    - GÃ©rer les pipelines ETL\n    - Consulter les donnÃ©es\n    - DÃ©clencher des exÃ©cutions\n    \n    ### Authentification\n    Utilise un token Bearer dans le header `Authorization`.\n    \"\"\",\n    version=\"1.0.0\",\n    contact={\n        \"name\": \"Data Team\",\n        \"email\": \"data@company.com\"\n    },\n    license_info={\n        \"name\": \"MIT\"\n    }\n)\n\n# Documenter un endpoint\n@app.get(\n    \"/pipelines\",\n    summary=\"Liste les pipelines\",\n    description=\"Retourne tous les pipelines avec pagination\",\n    response_description=\"Liste des pipelines\",\n    tags=[\"Pipelines\"]\n)\ndef list_pipelines():\n    pass",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-docker",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-docker",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ³ Avec Docker",
    "text": "ğŸ³ Avec Docker\n# Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n# requirements.txt\nfastapi==0.109.0\nuvicorn==0.27.0\npydantic==2.5.0\npandas==2.1.0\n# Build et run\ndocker build -t data-api .\ndocker run -p 8000:8000 data-api",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#options-de-dÃ©ploiement-cloud",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#options-de-dÃ©ploiement-cloud",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "â˜ï¸ Options de dÃ©ploiement cloud",
    "text": "â˜ï¸ Options de dÃ©ploiement cloud\n\n\n\nService\nDifficultÃ©\nCoÃ»t\nNotes\n\n\n\n\nRailway\nâ­ Facile\nGratuit/Payant\nIdÃ©al pour dÃ©buter\n\n\nRender\nâ­ Facile\nGratuit/Payant\nAuto-deploy depuis Git\n\n\nFly.io\nâ­â­ Moyen\nGratuit/Payant\nDocker natif\n\n\nAWS Lambda\nâ­â­ Moyen\nPay-per-use\nAvec Mangum adapter\n\n\nGCP Cloud Run\nâ­â­ Moyen\nPay-per-use\nContainer serverless\n\n\nKubernetes\nâ­â­â­ AvancÃ©\nVariable\nProduction enterprise",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "âœ… Structure de projet recommandÃ©e",
    "text": "âœ… Structure de projet recommandÃ©e\nmy_api/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e FastAPI\nâ”‚   â”œâ”€â”€ config.py            # Configuration (env vars)\nâ”‚   â”œâ”€â”€ database.py          # Connexion DB\nâ”‚   â”œâ”€â”€ models/              # ModÃ¨les SQLAlchemy/Pydantic\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ schemas/             # SchÃ©mas Pydantic (request/response)\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ routers/             # Endpoints par domaine\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ pipelines.py\nâ”‚   â”‚   â””â”€â”€ data.py\nâ”‚   â””â”€â”€ services/            # Logique mÃ©tier\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ etl.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ test_pipelines.py\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ” SÃ©curitÃ©",
    "text": "ğŸ” SÃ©curitÃ©\n# Ne jamais hardcoder les secrets !\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nAPI_KEY = os.getenv(\"API_KEY\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#checklist-production",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#checklist-production",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ Checklist production",
    "text": "ğŸ“‹ Checklist production\n\n\n\nâœ…\nItem\n\n\n\n\nâ˜\nVariables dâ€™environnement pour les secrets\n\n\nâ˜\nValidation des entrÃ©es (Pydantic)\n\n\nâ˜\nGestion des erreurs (HTTPException)\n\n\nâ˜\nLogging structurÃ©\n\n\nâ˜\nRate limiting\n\n\nâ˜\nCORS configurÃ©\n\n\nâ˜\nTests automatisÃ©s\n\n\nâ˜\nDocumentation Ã  jour\n\n\nâ˜\nHealth check endpoint",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nCe que tu as appris\n\n\n\nConcept\nDescription\n\n\n\n\nFastAPI\nFramework moderne pour APIs REST\n\n\nPydantic\nValidation automatique des donnÃ©es\n\n\nEndpoints CRUD\nGET, POST, PUT, PATCH, DELETE\n\n\nPath/Query params\nParamÃ¨tres dâ€™URL et de requÃªte\n\n\nPandas + API\nServir des donnÃ©es via HTTP\n\n\nBackground tasks\nExÃ©cuter des tÃ¢ches asynchrones\n\n\nDocumentation\nSwagger automatique\n\n\n\n\n\nğŸ¯ Cas dâ€™usage Data Engineering\n\n\n\nCas\nExemple\n\n\n\n\nExposer des donnÃ©es\nAPI pour dashboard/BI\n\n\nWebhook\nDÃ©clencher un pipeline Ã  lâ€™arrivÃ©e de donnÃ©es\n\n\nValidation\nVÃ©rifier les donnÃ©es avant ingestion\n\n\nMonitoring\nAPI de statut des pipelines\n\n\nML Serving\nExposer des prÃ©dictions\n\n\n\n\n\nğŸ“¦ Commandes essentielles\n# Installation\npip install fastapi uvicorn\n\n# Lancer en dev\nuvicorn main:app --reload\n\n# Lancer en prod\nuvicorn main:app --host 0.0.0.0 --port 8000 --workers 4",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#ressources",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation officielle\n\nFastAPI Documentation â€” Excellente et complÃ¨te\nPydantic Documentation\nUvicorn Documentation\n\n\n\nTutoriels\n\nFastAPI Tutorial â€” Tutoriel officiel\nReal Python - FastAPI\nTestDriven.io - FastAPI\n\n\n\nOutils complÃ©mentaires\n\nSQLModel â€” ORM par le crÃ©ateur de FastAPI\nStrawberry â€” GraphQL avec FastAPI\nCelery â€” TÃ¢ches asynchrones avancÃ©es\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant FastAPI pour le Data Engineering.",
    "crumbs": [
      "DÃ©butant",
      "ğŸš€ BONUS : FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html",
    "href": "notebooks/beginner/01_intro_data_engineering.html",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "",
    "text": "Bienvenue dans cette premiÃ¨re leÃ§on du parcours Data Engineering â€” From Zero to Hero.\nDans ce notebook, nous allons dÃ©couvrir :",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\nAvant de commencer ce parcours, il est recommandÃ© dâ€™avoir :\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Basique\nSavoir utiliser un ordinateur et naviguer dans des fichiers\n\n\nâœ… Basique\nConnaÃ®tre les concepts de base dâ€™internet\n\n\n\n\nğŸ’¡ Pas de panique ! Ce parcours est conÃ§u pour les dÃ©butants. Nous couvrirons tout ce dont tu as besoin.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "href": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“Œ 1. Câ€™est quoi le Data Engineering ?",
    "text": "ğŸ“Œ 1. Câ€™est quoi le Data Engineering ?\nLe Data Engineering (ou ingÃ©nierie des donnÃ©es) est une discipline qui consiste Ã  concevoir, construire, maintenir et optimiser les systÃ¨mes de traitement de donnÃ©es.\nLe Data Engineer, spÃ©cialiste en gestion des donnÃ©es, conÃ§oit et maintient lâ€™infrastructure data (bases de donnÃ©es, entrepÃ´ts de donnÃ©es, lacs de donnÃ©es) et dÃ©veloppe des pipelines automatisÃ©s qui extraient, transforment et chargent les donnÃ©es dans des systÃ¨mes adaptÃ©s.\nCâ€™est le socle technique qui garantit la qualitÃ©, la disponibilitÃ© et la sÃ©curitÃ© des donnÃ©es utilisÃ©es par les Data Analysts et Data Scientists pour gÃ©nÃ©rer des insights et orienter les stratÃ©gies dâ€™entreprise.\n\nğŸ¢ Exemples concrets en entreprise\n\n\n\n\n\n\n\nEntreprise\nCas dâ€™usage Data Engineering\n\n\n\n\nNetflix\nPipeline qui collecte les donnÃ©es de visionnage de millions dâ€™utilisateurs pour alimenter le systÃ¨me de recommandation\n\n\nUber\nTraitement en temps rÃ©el des donnÃ©es GPS de milliers de chauffeurs pour optimiser les trajets et les prix\n\n\nSpotify\nAgrÃ©gation des donnÃ©es dâ€™Ã©coute pour gÃ©nÃ©rer les playlists â€œDiscover Weeklyâ€ personnalisÃ©es\n\n\nAirbnb\nPipeline de donnÃ©es pour analyser les prix du marchÃ© et suggÃ©rer des tarifs aux hÃ´tes\n\n\nE-commerce\nSynchronisation des stocks entre le site web, lâ€™ERP et les entrepÃ´ts en temps rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "href": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ‘¨ğŸ½â€ğŸ’» 2. DiffÃ©rences entre mÃ©tiers",
    "text": "ğŸ‘¨ğŸ½â€ğŸ’» 2. DiffÃ©rences entre mÃ©tiers\n\n\n\n\n\n\n\n\n\nMÃ©tier\nRÃ´le Principal\nFocus\nOutils ClÃ©s\n\n\n\n\nğŸ”§ Data Engineer\nConstruire et maintenir lâ€™infrastructure de donnÃ©es\nInfrastructure & Pipelines\nApache Airflow, Apache Spark, Kafka, Snowflake, dbt, Python, SQL, Prefect, Docker, Kubernetes, etcâ€¦\n\n\nğŸ”¬ Data Scientist\nExtraire des insights et crÃ©er des modÃ¨les prÃ©dictifs\nModÃ©lisation & ML\nPython, R, scikit-learn, TensorFlow, PyTorch, Jupyter, MLflow, Pandas, XGBoost, etcâ€¦\n\n\nğŸ“ˆ Data Analyst\nTransformer les donnÃ©es en insights actionnables\nBusiness Intelligence\nSQL, Excel, Power BI, Tableau, Looker, Google Analytics, Python (basique)\n\n\n\n\nğŸ”„ Comment ces rÃ´les collaborent ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Data Engineer  â”‚ â”€â”€â–¶ â”‚  Data Scientist â”‚ â”€â”€â–¶ â”‚  Data Analyst   â”‚\nâ”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚\nâ”‚ â€¢ Collecte      â”‚     â”‚ â€¢ ModÃ©lisation  â”‚     â”‚ â€¢ Visualisation â”‚\nâ”‚ â€¢ Pipelines     â”‚     â”‚ â€¢ PrÃ©dictions   â”‚     â”‚ â€¢ Reporting     â”‚\nâ”‚ â€¢ Infrastructureâ”‚     â”‚ â€¢ ML/AI         â”‚     â”‚ â€¢ Insights      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                                               â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback & Besoins â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "href": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ—ï¸ 3. Architecture typique dâ€™un pipeline de donnÃ©es moderne",
    "text": "ğŸ—ï¸ 3. Architecture typique dâ€™un pipeline de donnÃ©es moderne\nUn pipeline de donnÃ©es moderne suit gÃ©nÃ©ralement ce flux :\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚                ğŸ›¡ï¸  GOUVERNANCE DATA                    â”‚\n                 â”‚ Catalogue, QualitÃ©, Lineage, SÃ©curitÃ©, RBAC, Privacy   â”‚\n                 â”‚ Compliance, MDM                                        â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   ğŸ“¥ SOURCES      â”‚   â†’     â”‚               ğŸ’¾ STOCKAGE                 â”‚\nâ”‚ API, DB, Logs,    â”‚         â”‚         (Data Lakehouse : S3, ADLSâ€¦)     â”‚\nâ”‚ CSV, IoT          â”‚         â”‚                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   BRONZE : DonnÃ©es brutes (Raw)           â”‚\n                             â”‚   - IngÃ©rÃ©es telles quelles                â”‚\n                             â”‚   - Formats variÃ©s (JSON, CSV, Parquet)    â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   - Pas de qualitÃ© garantie                â”‚\n   â”‚   ğŸ”„ INGESTION        â”‚  â”‚                                           â”‚\n   â”‚ ETL/ELT, Streams     â”‚  â”‚   SILVER : DonnÃ©es nettoyÃ©es (Clean)       â”‚\n   â”‚ Airbyte, Fivetran    â”‚  â”‚   - NormalisÃ©es, typÃ©es, dÃ©dupliquÃ©es      â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   - QualitÃ© contrÃ´lÃ©e                      â”‚\n                             â”‚   - Jointures simples / enrichissement     â”‚\n                             â”‚                                           â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   GOLD : DonnÃ©es business (Curated)        â”‚\n   â”‚ âš™ï¸ TRANSFORMATION      â”‚  â”‚   - ModÃ¨les analytiques (Star Schema)      â”‚\n   â”‚ dbt, SQL, Spark,      â”‚  â”‚   - KPIs, mÃ©triques, tables prÃªtes BI     â”‚\n   â”‚ Pandas, MLflow        â”‚  â”‚   - Haute qualitÃ© et gouvernance forte     â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                              â”‚        ğŸ“Š EXPOSITION         â”‚\n                              â”‚ Dashboards, ML, APIs, Apps   â”‚\n                              â”‚ Power BI, Tableau, Looker    â”‚\n                              â”‚ (âš ï¸ Toujours Ã  partir du GOLD)â”‚\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ” ETL vs ELT â€” Quelle est la diffÃ©rence ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nETL (Extract â†’ Transform â†’ Load)\nELT (Extract â†’ Load â†’ Transform)\n\n\n\n\nğŸ”„ Ordre\nExtraction â†’ Transformation â†’ Chargement\nExtraction â†’ Chargement â†’ Transformation\n\n\nğŸ“ Lieu de la transformation\nEn dehors du stockage (dans un script ou un outil)\nDirectement dans le data warehouse\n\n\nâœ… Avantages\nPlus de contrÃ´le sur la transformation\nPlus rapide sur des gros volumes\n\n\nâš ï¸ InconvÃ©nients\nPeut surcharger les outils intermÃ©diaires\nBesoin dâ€™un entrepÃ´t puissant (coÃ»ts)\n\n\nğŸ› ï¸ Outils typiques\nInformatica, Talend, scripts Python, â€¦\ndbt, Snowflake, BigQuery, â€¦\n\n\nğŸ“… Cas dâ€™usage\nDonnÃ©es sensibles nÃ©cessitant un prÃ©-traitement\nAnalytics modernes sur le cloud",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "href": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ•˜ 4. Pipelines batch vs streaming",
    "text": "ğŸ•˜ 4. Pipelines batch vs streaming\n\n\n\n\n\n\n\n\n\nMode\nDÃ©finition\nLatence\nExemples\n\n\n\n\nBatch\nTraitement pÃ©riodique (toutes les heures, tous les joursâ€¦)\nMinutes Ã  heures\nRapport quotidien, import CSV, agrÃ©gations nocturnes\n\n\nStreaming\nTraitement en temps rÃ©el, Ã©vÃ©nement par Ã©vÃ©nement\nMillisecondes Ã  secondes\nLogs serveurs, capteurs IoT, transactions bancaires, dÃ©tection de fraude\n\n\n\n\nğŸ¤” Comment choisir ?\n\n\n\n\n\n\n\nQuestion\nSi oui â†’\n\n\n\n\nLes donnÃ©es doivent-elles Ãªtre traitÃ©es immÃ©diatement ?\nStreaming\n\n\nLe volume est-il trÃ¨s Ã©levÃ© mais la latence peu critique ?\nBatch\n\n\nAvez-vous besoin de dÃ©tecter des anomalies en temps rÃ©el ?\nStreaming\n\n\nSâ€™agit-il de rapports quotidiens/hebdomadaires ?\nBatch\n\n\n\n\n\nğŸ§± Fondations des pipelines de donnÃ©es\nTout pipeline de donnÃ©es repose sur plusieurs piliers fondamentaux :\n\n\n\n\n\n\n\n\nğŸ”¢ Pilier\nğŸ’¡ Description\nğŸ“š Module associÃ©\n\n\n\n\n1. Data Collecting\nComment collecter les donnÃ©es brutes (fichiers, API, capteursâ€¦)\nPython, APIs\n\n\n2. Data Ingestion\nComment les intÃ©grer dans un systÃ¨me (DB, data lakeâ€¦)\nETL, Airbyte\n\n\n3. Data Storage\nComment et oÃ¹ les stocker (SQL, NoSQL, S3â€¦)\nDatabases, Cloud\n\n\n4. Data Processing\nComment les transformer, nettoyer, agrÃ©ger\nPython, Spark, dbt\n\n\n5. Data Modeling\nComment organiser les donnÃ©es pour lâ€™analyse\nSQL, dbt\n\n\n6. Data Quality & Governance\nComment garantir la fiabilitÃ© et la traÃ§abilitÃ©\nGreat Expectations\n\n\n7. Data Orchestration\nComment automatiser les tÃ¢ches et gÃ©rer les dÃ©pendances\nAirflow, Prefect\n\n\n8. ScalabilitÃ© & Performance\nComment faire face Ã  de gros volumes ou Ã  la charge\nSpark, Kubernetes\n\n\n9. SÃ©curitÃ© des donnÃ©es\nChiffrement, contrÃ´le dâ€™accÃ¨s, audit\nIAM, Vault\n\n\n10. DevOps pour la Data\nConteneurisation, CI/CD, monitoring\nDocker, GitHub Actions\n\n\n\n\nğŸ“˜ Ces concepts seront abordÃ©s progressivement dans le parcours.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§° 5. Panorama des outils du Data Engineer",
    "text": "ğŸ§° 5. Panorama des outils du Data Engineer\n\n\n\n\n\n\n\n\nDomaine\nOutils populaires\nNiveau\n\n\n\n\nğŸ“¥ Collecte\nPython, API REST, Scrapy, Kafka\nDÃ©butant â†’ IntermÃ©diaire\n\n\nğŸ”„ Ingestion\nAirbyte, Fivetran, Python scripts\nDÃ©butant\n\n\nğŸ’¾ Stockage\nPostgreSQL, Snowflake, S3, Delta Lake\nDÃ©butant â†’ AvancÃ©\n\n\nâš™ï¸ Traitement (Batch)\nPandas, Spark, dbt, SQL\nDÃ©butant â†’ AvancÃ©\n\n\nâš¡ Traitement (Streaming)\nKafka, Spark Streaming, Flink\nIntermÃ©diaire â†’ AvancÃ©\n\n\nğŸ¼ Orchestration\nApache Airflow, Prefect, Dagster\nIntermÃ©diaire\n\n\nğŸ³ DevOps & CI/CD\nDocker, GitHub Actions, Terraform\nIntermÃ©diaire\n\n\nğŸ“Š Monitoring\nGrafana, Prometheus, ELK Stack\nIntermÃ©diaire\n\n\n\n\nğŸ—ºï¸ Stack moderne typique (2024)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    STACK DATA MODERNE                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Orchestration    â”‚  Airflow / Prefect / Dagster            â”‚\nâ”‚  Transformation   â”‚  dbt / Spark / Python                   â”‚\nâ”‚  Warehouse        â”‚  Snowflake / BigQuery / Redshift        â”‚\nâ”‚  Ingestion        â”‚  Airbyte / Fivetran / Stitch            â”‚\nâ”‚  Sources          â”‚  APIs / Databases / SaaS / Files        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§  6. Soft Skills & Mindset du Data Engineer",
    "text": "ğŸ§  6. Soft Skills & Mindset du Data Engineer\nLe mÃ©tier de Data Engineer nâ€™est pas uniquement technique. Pour rÃ©ussir dans ce domaine, il faut aussi dÃ©velopper des compÃ©tences humaines et professionnelles essentielles :\n\n\n\n\n\n\n\n\nCompÃ©tence\nDescription\nPourquoi câ€™est important\n\n\n\n\nğŸ“ Documenter\nÃ‰crire une documentation claire pour son code et ses pipelines\nFacilite la maintenance et lâ€™onboarding des nouveaux membres\n\n\nğŸ¤ Collaborer\nTravailler avec les Ã©quipes Data Science, BI, Produit, DevOps\nLes donnÃ©es traversent toute lâ€™organisation\n\n\nğŸ¯ ÃŠtre rigoureux\nGarantir la qualitÃ©, la fiabilitÃ© et la traÃ§abilitÃ© des donnÃ©es\nUne erreur de donnÃ©es peut avoir des consÃ©quences business majeures\n\n\nğŸ•µğŸ½â€â™‚ï¸ Investiguer\nSavoir dÃ©bugger des anomalies, logs ou Ã©checs de pipeline\nLes pipelines cassent, il faut savoir pourquoi rapidement\n\n\nğŸ“š Apprendre en continu\nSe tenir Ã  jour sur les nouveaux outils et pratiques\nLe domaine Ã©volue trÃ¨s rapidement\n\n\nğŸ’¬ Communiquer\nExpliquer des concepts techniques Ã  des non-techniques\nAlignement avec les Ã©quipes mÃ©tier",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour valider tes acquis ğŸ‘‡ğŸ½\n\n\nâ“ Q1. Quel est le rÃ´le principal dâ€™un Data Engineer ?\n\nCrÃ©er des modÃ¨les prÃ©dictifs\n\nVisualiser les donnÃ©es dans Power BI\n\nConcevoir et maintenir des pipelines de donnÃ©es\n\nFaire des analyses statistiques dans Excel\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le Data Engineer conÃ§oit et maintient des pipelines de donnÃ©es.\n\n\n\n\nâ“ Q2. Dans un pipeline ETL, que signifie le â€œTâ€ ?\n\nTransfer\n\nTrigger\n\nTransform\n\nTransport\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” â€œTâ€ signifie Transform, câ€™est lâ€™Ã©tape de transformation des donnÃ©es.\n\n\n\n\nâ“ Q3. Quelle est la principale diffÃ©rence entre ETL et ELT ?\n\nELT ne fait pas de transformation\n\nELT transforme les donnÃ©es aprÃ¨s les avoir chargÃ©es\n\nETL est utilisÃ© uniquement pour les fichiers CSV\n\nELT est un outil comme Apache Airflow\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ELT charge dâ€™abord les donnÃ©es, puis les transforme dans le data warehouse.\n\n\n\n\nâ“ Q4. Lequel de ces outils est utilisÃ© pour orchestrer des pipelines ?\n\nApache Kafka\n\nApache Airflow\n\nTableau\n\nPostgreSQL\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Apache Airflow est un outil dâ€™orchestration de pipelines.\n\n\n\n\nâ“ Q5. Le traitement batch consiste Ã  :\n\nTraiter les donnÃ©es en continu\n\nTraiter les donnÃ©es en petits lots Ã  la volÃ©e\n\nTraiter les donnÃ©es par groupe, Ã  intervalle rÃ©gulier\n\nTraiter uniquement les donnÃ©es texte\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le batch consiste Ã  traiter les donnÃ©es par lots Ã  intervalles dÃ©finis.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "href": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§  Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL",
    "text": "ğŸ§  Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL\n\n\nâ“ Q6. Que signifie ETLt ?\n\nUne erreur dans la chaÃ®ne ETL\n\nUne combinaison hybride entre ETL et ELT\n\nUne technique de transfert via email\n\nUne transformation uniquement aprÃ¨s chargement\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ETLt correspond Ã  : Extract â†’ Transform (1) â†’ Load â†’ Transform (2).\nCâ€™est une approche hybride oÃ¹ une partie des transformations est faite avant le chargement, et une autre aprÃ¨s.\n\n\n\n\nâ“ Q7. Le Reverse ETL est utilisÃ© pour :\n\nRecharger les donnÃ©es sources depuis le warehouse\n\nSupprimer les donnÃ©es invalides dans un lac de donnÃ©es\n\nPousser les donnÃ©es du data warehouse vers les outils mÃ©tiers\n\nTransformer les donnÃ©es en reverse-engineering\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Reverse ETL consiste Ã  extraire les donnÃ©es dâ€™un data warehouse (ex. BigQuery) pour les charger dans des outils mÃ©tiers comme Salesforce, Notion, Slack, etc.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Lectures recommandÃ©es\n\nFundamentals of Data Engineering â€” Joe Reis & Matt Housley\nThe Data Warehouse Toolkit â€” Ralph Kimball\nDesigning Data-Intensive Applications â€” Martin Kleppmann\n\n\n\nğŸŒ Sites & Blogs\n\nData Engineering Weekly â€” Newsletter hebdomadaire\nSeattle Data Guy â€” ChaÃ®ne YouTube\nStart Data Engineering â€” Tutoriels pratiques\n\n\n\nğŸ“ Certifications\n\nGoogle Cloud Professional Data Engineer\nAWS Certified Data Engineer\nDatabricks Certified Data Engineer\n\n\n\nğŸ› ï¸ Outils Ã  explorer\n\ndbt â€” Transformation de donnÃ©es\nAirbyte â€” Ingestion de donnÃ©es (open source)\nApache Airflow â€” Orchestration",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu as une vue dâ€™ensemble du Data Engineering, passons Ã  la pratique !\nğŸ‘‰ Module suivant : 02_bash_for_data_engineers.ipynb â€” MaÃ®triser la ligne de commande\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le premier module du parcours Data Engineering.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html",
    "href": "notebooks/beginner/12_orchestration_pipelines.html",
    "title": "â° Orchestration de Pipelines Data",
    "section": "",
    "text": "Ce module prÃ©sente les outils dâ€™orchestration pour automatiser lâ€™exÃ©cution de vos pipelines de donnÃ©es.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 11_pyspark_for_data_engineering\n\n\nâœ… Requis\nComprendre les pipelines ETL\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nConnaÃ®tre les bases de Linux (ligne de commande)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre ce quâ€™est lâ€™orchestration de pipelines\nâœ… Utiliser le Planificateur Windows (niveau dÃ©butant)\nâœ… Configurer des tÃ¢ches avec Crontab (niveau intermÃ©diaire)\nâœ… Comprendre lâ€™architecture dâ€™Apache Airflow\nâœ… CrÃ©er des DAGs avec Apache Airflow\nâœ… Utiliser les diffÃ©rents types dâ€™Operators\nâœ… GÃ©rer les dÃ©pendances et le passage de donnÃ©es (XCom)\nâœ… Configurer les alertes et le monitoring\nâœ… Choisir le bon outil selon ton besoin",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering",
    "text": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering\nTu as appris Ã  crÃ©er des pipelines ETL avec PySpark. Mais comment les automatiser pour quâ€™ils sâ€™exÃ©cutent rÃ©guliÃ¨rement sans intervention manuelle ?\n\nLe problÃ¨me\nSans orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   ğŸ˜° \"Il faut que je lance mon script tous les jours...\"   â”‚\nâ”‚   ğŸ˜° \"J'ai oubliÃ© de lancer le pipeline hier !\"            â”‚\nâ”‚   ğŸ˜° \"Le script B a plantÃ© car A n'avait pas fini...\"      â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nAvec orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   âœ… Scripts exÃ©cutÃ©s automatiquement                       â”‚\nâ”‚   âœ… Alertes en cas d'Ã©chec                                 â”‚\nâ”‚   âœ… DÃ©pendances respectÃ©es (A â†’ B â†’ C)                     â”‚\nâ”‚   âœ… Logs et monitoring centralisÃ©s                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPosition dans le pipeline Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     PIPELINE DATA                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources        ETL              Destination                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚\nâ”‚                                                                 â”‚\nâ”‚   APIs     â”€â”                 â”Œâ”€â–º  Data Warehouse               â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  PySpark  â”€â”€â”€â”¼â”€â–º  Data Lake                    â”‚\nâ”‚   BDD      â”€â”˜                 â””â”€â–º  Dashboard                    â”‚\nâ”‚                                                                 â”‚\nâ”‚            â–²                                                    â”‚\nâ”‚            â”‚                                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\nâ”‚   â”‚  ORCHESTRATION  â”‚  â—„â”€â”€ Quand ? Dans quel ordre ?           â”‚\nâ”‚   â”‚  (Airflow/Cron) â”‚                                           â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Comparaison rapide des outils",
    "text": "ğŸ“Š Comparaison rapide des outils\n\n\n\nCritÃ¨re\nWindows Task\nCrontab\nAirflow\n\n\n\n\nFacilitÃ©\nâ­â­â­\nâ­â­\nâ­\n\n\nInterface\nGUI\nCLI\nWeb\n\n\nDÃ©pendances\nâŒ\nâŒ\nâœ…\n\n\nMonitoring\nBasique\nNon\nComplet\n\n\nRetry auto\nâŒ\nâŒ\nâœ…\n\n\nIdÃ©al pour\n1-5 scripts\n5-15 scripts\n10+ pipelines",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn outil intÃ©grÃ© Ã  Windows pour exÃ©cuter des programmes automatiquement.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n\nOuvrir le planificateur :\nWindows + R â†’ Taper 'taskschd.msc' â†’ EntrÃ©e\n\n\nCrÃ©er une tÃ¢che :\n\nAction â†’ CrÃ©er une tÃ¢che de base\nNom : â€œMon script quotidienâ€\nDÃ©clencheur : Quotidien Ã  2h du matin\nAction : DÃ©marrer python.exe avec C:\\scripts\\mon_script.py\nTerminer\n\nâœ… VoilÃ  ! Votre script sâ€™exÃ©cutera automatiquement tous les jours Ã  2h.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces",
    "text": "âœ… Forces\nâœ… TrÃ¨s facile - Interface graphique intuitive\nâœ… DÃ©jÃ  installÃ© - Pas de setup\nâœ… Parfait pour dÃ©buter - Pas de code complexe",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses",
    "text": "âŒ Faiblesses\nâŒ Pas de dÃ©pendances - Si tÃ¢che A doit finir avant tÃ¢che B â†’ compliquÃ©\nâŒ Monitoring limitÃ© - Difficile de voir lâ€™Ã©tat global\nâŒ Windows uniquement - Ne fonctionne pas sur Linux",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nâœ… OUI : Vous avez 1-5 scripts simples sur Windows\nâŒ NON : Vous avez besoin que script B attende script A",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nLe planificateur standard sur Linux/Mac.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Syntaxe de base",
    "text": "Syntaxe de base\nminute heure jour mois jour_semaine commande\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande\n\nExemples simples :\n# Tous les jours Ã  2h du matin\n0 2 * * * python3 /home/user/script.py\n\n# Toutes les heures\n0 * * * * python3 /home/user/hourly.py\n\n# Lundi Ã  vendredi Ã  9h\n0 9 * * 1-5 python3 /home/user/weekday.py\n\n# Toutes les 15 minutes\n*/15 * * * * python3 /home/user/check.py\n\n# Le 1er de chaque mois Ã  minuit\n0 0 1 * * python3 /home/user/monthly.py",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n# Ã‰diter votre crontab\ncrontab -e\n\n# Voir les tÃ¢ches planifiÃ©es\ncrontab -l\n\n# Ajouter vos lignes\n0 2 * * * python3 /home/user/backup.py &gt;&gt; /var/log/backup.log 2&gt;&1\n\n# Sauvegarder et quitter\n# âœ… C'est fait !\n\nğŸ’¡ Astuce : Utilise crontab.guru pour tester tes expressions !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces",
    "text": "âœ… Forces\nâœ… Universel - Sur TOUS les serveurs Linux\nâœ… TrÃ¨s lÃ©ger - Presque pas de ressources\nâœ… Simple - Une ligne = une tÃ¢che\nâœ… Gratuit - DÃ©jÃ  installÃ©",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses",
    "text": "âŒ Faiblesses\nâŒ Pas de dÃ©pendances - MÃªme problÃ¨me que Windows\nâŒ Pas de monitoring - Aucune interface\nâŒ Pas de retry - Si Ã§a Ã©choue, il faut attendre le prochain run\nâŒ Logs manuels - Il faut les gÃ©rer soi-mÃªme",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nâœ… OUI : Serveur Linux, 5-15 scripts indÃ©pendants\nâŒ NON : Scripts avec dÃ©pendances complexes",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn orchestrateur professionnel pour gÃ©rer des workflows complexes, crÃ©Ã© par Airbnb en 2014 et devenu projet Apache en 2016.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    APACHE AIRFLOW                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   \"Airflow is a platform to programmatically author,           â”‚\nâ”‚    schedule, and monitor workflows.\"                            â”‚\nâ”‚                                                                 â”‚\nâ”‚   â€¢ CrÃ©Ã© par Airbnb (2014)                                      â”‚\nâ”‚   â€¢ Apache Top-Level Project (2019)                             â”‚\nâ”‚   â€¢ 30,000+ GitHub stars                                        â”‚\nâ”‚   â€¢ UtilisÃ© par : Airbnb, Netflix, Spotify, Twitter, Adobe...   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nLa grande diffÃ©rence avec Cron :\n# âŒ Avec Cron (problÃ¨me) :\n0 2 * * * python extract.py\n30 2 * * * python transform.py  # Et si extract prend plus de 30min ?\n0 3 * * * python load.py        # Et si transform a plantÃ© ?\n\n# âœ… Avec Airflow (solution) :\nextract &gt;&gt; transform &gt;&gt; load  # transform ATTEND que extract soit terminÃ© !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ—ï¸ Architecture dâ€™Airflow",
    "text": "ğŸ—ï¸ Architecture dâ€™Airflow\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        ARCHITECTURE AIRFLOW                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â”‚   WEB SERVER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    SCHEDULER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  EXECUTOR   â”‚  â”‚\nâ”‚   â”‚   (Flask UI)    â”‚         â”‚  (Orchestrate)  â”‚         â”‚  (Workers)  â”‚  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚            â”‚                           â”‚                         â”‚         â”‚\nâ”‚            â”‚                           â–¼                         â”‚         â”‚\nâ”‚            â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚         â”‚\nâ”‚            â”‚                  â”‚                 â”‚                â”‚         â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    METADATA     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                               â”‚    DATABASE     â”‚                          â”‚\nâ”‚                               â”‚  (PostgreSQL)   â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                        â–²                                   â”‚\nâ”‚                                        â”‚                                   â”‚\nâ”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â”‚    DAG FILES    â”‚                          â”‚\nâ”‚                               â”‚   (Python .py)  â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nComposants clÃ©s\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nDescription\n\n\n\n\nWeb Server\nInterface UI\nDashboard Flask pour visualiser les DAGs\n\n\nScheduler\nPlanification\nDÃ©cide quand exÃ©cuter les tÃ¢ches\n\n\nExecutor\nExÃ©cution\nLance les tÃ¢ches (Local, Celery, K8sâ€¦)\n\n\nMetadata DB\nStockage\nÃ‰tat des DAGs, logs, historique\n\n\nDAG Files\nDÃ©finition\nFichiers Python dÃ©finissant les workflows\n\n\n\n\n\nTypes dâ€™Executors\n\n\n\nExecutor\nUsage\nScalabilitÃ©\n\n\n\n\nSequentialExecutor\nDev/Test\n1 tÃ¢che Ã  la fois\n\n\nLocalExecutor\nPetite prod\nParallÃ¨le sur 1 machine\n\n\nCeleryExecutor\nProduction\nWorkers distribuÃ©s\n\n\nKubernetesExecutor\nCloud\nPod par tÃ¢che",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“š Concepts clÃ©s dâ€™Airflow",
    "text": "ğŸ“š Concepts clÃ©s dâ€™Airflow\n\n1ï¸âƒ£ DAG (Directed Acyclic Graph)\nUn DAG est un graphe de tÃ¢ches sans cycle : les donnÃ©es vont toujours dans une direction.\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚ Extract â”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n          â–¼\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚Transformâ”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n    â–¼           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Load DWâ”‚  â”‚Load S3 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2ï¸âƒ£ Task\nUne Task est une unitÃ© de travail dans un DAG (une Ã©tape).\n\n\n3ï¸âƒ£ Operator\nUn Operator dÃ©finit ce que fait une tÃ¢che.\n\n\n\nOperator\nUsage\n\n\n\n\nPythonOperator\nExÃ©cuter une fonction Python\n\n\nBashOperator\nExÃ©cuter une commande bash\n\n\nEmailOperator\nEnvoyer un email\n\n\nPostgresOperator\nExÃ©cuter du SQL\n\n\nS3ToRedshiftOperator\nCopier S3 â†’ Redshift\n\n\n\n\n\n4ï¸âƒ£ Task Instance\nUne Task Instance = une exÃ©cution spÃ©cifique dâ€™une Task Ã  une date donnÃ©e.\n\n\n5ï¸âƒ£ DAG Run\nUn DAG Run = une exÃ©cution complÃ¨te du DAG.\nDAG: etl_pipeline\nâ”œâ”€â”€ DAG Run 2024-01-15 âœ…\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) âœ…\nâ”‚   â””â”€â”€ load (Task Instance) âœ…\nâ”‚\nâ”œâ”€â”€ DAG Run 2024-01-16 â³\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) â³ running\nâ”‚   â””â”€â”€ load (Task Instance) â¸ï¸ waiting",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”§ Installation locale dâ€™Airflow",
    "text": "ğŸ”§ Installation locale dâ€™Airflow\n# CrÃ©er un environnement virtuel\npython -m venv airflow_venv\nsource airflow_venv/bin/activate  # Linux/Mac\n# ou : airflow_venv\\Scripts\\activate  # Windows\n\n# DÃ©finir le home Airflow\nexport AIRFLOW_HOME=~/airflow\n\n# Installer Airflow (version contrainte pour compatibilitÃ©)\nAIRFLOW_VERSION=2.8.1\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# Initialiser la base de donnÃ©es\nairflow db init\n\n# CrÃ©er un utilisateur admin\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin\n\n# Lancer le webserver (Terminal 1)\nairflow webserver --port 8080\n\n# Lancer le scheduler (Terminal 2)\nairflow scheduler\nğŸ‘‰ AccÃ©der : http://localhost:8080 (login: admin / admin)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“ Ton premier DAG",
    "text": "ğŸ“ Ton premier DAG\nCrÃ©er le fichier ~/airflow/dags/mon_premier_dag.py :\n\n\nCode\n# ~/airflow/dags/mon_premier_dag.py\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\n# Arguments par dÃ©faut pour toutes les tÃ¢ches\ndefault_args = {\n    'owner': 'data_engineer',\n    'depends_on_past': False,\n    'email': ['data-team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DÃ©finir le DAG\ndag = DAG(\n    dag_id='mon_premier_dag',\n    default_args=default_args,\n    description='Mon premier pipeline ETL',\n    schedule_interval='@daily',  # ExÃ©cution quotidienne\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Ne pas exÃ©cuter les runs passÃ©s\n    tags=['etl', 'tutorial'],\n)\n\n# Fonctions Python\ndef extract():\n    \"\"\"Extraire les donnÃ©es\"\"\"\n    print(\"ğŸ“¥ Extraction des donnÃ©es depuis l'API...\")\n    # Simuler extraction\n    data = {'records': 1000, 'source': 'api'}\n    return data  # RetournÃ© via XCom\n\ndef transform(**context):\n    \"\"\"Transformer les donnÃ©es\"\"\"\n    # RÃ©cupÃ©rer les donnÃ©es de extract via XCom\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ğŸ”„ Transformation de {data['records']} enregistrements\")\n    return {'records': data['records'], 'cleaned': True}\n\ndef load(**context):\n    \"\"\"Charger les donnÃ©es\"\"\"\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='transform')\n    print(f\"ğŸ’¾ Chargement de {data['records']} enregistrements nettoyÃ©s\")\n\n# CrÃ©er les tÃ¢ches\ntask_start = BashOperator(\n    task_id='start',\n    bash_command='echo \"ğŸš€ DÃ©marrage du pipeline Ã  $(date)\"',\n    dag=dag,\n)\n\ntask_extract = PythonOperator(\n    task_id='extract',\n    python_callable=extract,\n    dag=dag,\n)\n\ntask_transform = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\ntask_load = PythonOperator(\n    task_id='load',\n    python_callable=load,\n    dag=dag,\n)\n\ntask_end = BashOperator(\n    task_id='end',\n    bash_command='echo \"âœ… Pipeline terminÃ© avec succÃ¨s !\"',\n    dag=dag,\n)\n\n# DÃ©finir les dÃ©pendances\ntask_start &gt;&gt; task_extract &gt;&gt; task_transform &gt;&gt; task_load &gt;&gt; task_end\n\n# Ã‰quivalent Ã  :\n# task_start.set_downstream(task_extract)\n# task_extract.set_downstream(task_transform)\n# etc.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âš™ï¸ ParamÃ¨tres importants du DAG",
    "text": "âš™ï¸ ParamÃ¨tres importants du DAG\n\nSchedule Interval (frÃ©quence dâ€™exÃ©cution)\n\n\n\nPreset\nÃ‰quivalent Cron\nDescription\n\n\n\n\n@once\n-\nUne seule fois\n\n\n@hourly\n0 * * * *\nChaque heure\n\n\n@daily\n0 0 * * *\nChaque jour Ã  minuit\n\n\n@weekly\n0 0 * * 0\nChaque dimanche\n\n\n@monthly\n0 0 1 * *\nLe 1er du mois\n\n\n@yearly\n0 0 1 1 *\nLe 1er janvier\n\n\nNone\n-\nDÃ©clenchÃ© manuellement\n\n\n'0 6 * * 1-5'\nCron\nLun-Ven Ã  6h\n\n\n\n\n\nCatchup\n# catchup=True (dÃ©faut) :\n# Si start_date=2024-01-01 et on est le 2024-01-10,\n# Airflow va exÃ©cuter les 10 DAG Runs manquÃ©s !\n\n# catchup=False :\n# ExÃ©cute seulement Ã  partir de maintenant\n\n\nDefault Args importants\ndefault_args = {\n    'owner': 'data_team',           # PropriÃ©taire\n    'depends_on_past': False,       # DÃ©pend du run prÃ©cÃ©dent ?\n    'email_on_failure': True,       # Email si Ã©chec\n    'retries': 3,                   # Nombre de retry\n    'retry_delay': timedelta(minutes=5),  # DÃ©lai entre retries\n    'execution_timeout': timedelta(hours=1),  # Timeout\n    'sla': timedelta(hours=2),      # SLA (alerte si dÃ©passÃ©)\n}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”§ Les Operators les plus utilisÃ©s",
    "text": "ğŸ”§ Les Operators les plus utilisÃ©s\n\nPythonOperator\nfrom airflow.operators.python import PythonOperator\n\ndef my_function(name, **context):\n    print(f\"Hello {name}!\")\n    print(f\"Execution date: {context['ds']}\")\n    return \"success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    op_kwargs={'name': 'World'},  # Arguments de la fonction\n)\n\n\nBashOperator\nfrom airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Date: {{ ds }}\" && python /scripts/etl.py',\n)\n\n\nPostgresOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',  # Connexion dÃ©finie dans UI\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100)\n        );\n    \"\"\",\n)\n\n\nEmailOperator\nfrom airflow.operators.email import EmailOperator\n\ntask = EmailOperator(\n    task_id='send_report',\n    to='team@company.com',\n    subject='Pipeline Report - {{ ds }}',\n    html_content='&lt;h1&gt;Pipeline completed!&lt;/h1&gt;',\n)\n\n\nEmptyOperator (anciennement DummyOperator)\nfrom airflow.operators.empty import EmptyOperator\n\n# Utile pour crÃ©er des points de jonction\nstart = EmptyOperator(task_id='start')\nend = EmptyOperator(task_id='end')",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“¬ XCom â€” Passer des donnÃ©es entre tÃ¢ches",
    "text": "ğŸ“¬ XCom â€” Passer des donnÃ©es entre tÃ¢ches\nXCom (Cross-Communication) permet de partager des donnÃ©es entre tÃ¢ches.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         XCom          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Task A  â”‚ â”€â”€â”€â”€â”€â”€â”€ data â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Task B  â”‚\nâ”‚  return  â”‚                       â”‚ xcom_pull â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nMÃ©thode 1 : Return (automatique)\n\n\nCode\n# XCom avec return (automatique)\n\ndef extract():\n    data = {'records': 1000, 'status': 'ok'}\n    return data  # Automatiquement stockÃ© dans XCom\n\ndef transform(**context):\n    # RÃ©cupÃ©rer via ti (task instance)\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ReÃ§u: {data}\")\n    return {'records': data['records'], 'transformed': True}\n\n\n\n\nMÃ©thode 2 : Push/Pull explicite\ndef task_a(**context):\n    # Push explicite avec une clÃ©\n    context['ti'].xcom_push(key='my_data', value={'count': 42})\n    context['ti'].xcom_push(key='status', value='success')\n\ndef task_b(**context):\n    # Pull avec la clÃ©\n    data = context['ti'].xcom_pull(task_ids='task_a', key='my_data')\n    status = context['ti'].xcom_pull(task_ids='task_a', key='status')\n\n\nâš ï¸ Limites de XCom\n\n\n\nLimite\nDescription\n\n\n\n\nTaille\nMax ~48KB par dÃ©faut (stockÃ© en DB)\n\n\nSÃ©rialisation\nDoit Ãªtre JSON-sÃ©rialisable\n\n\nPas pour big data\nUtiliser S3/GCS pour gros fichiers\n\n\n\n# âŒ Mauvaise pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')  # 1GB\n    return df.to_dict()  # âŒ Trop gros pour XCom !\n\n# âœ… Bonne pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')\n    path = '/data/output/extract_2024-01-15.parquet'\n    df.to_parquet(path)\n    return path  # âœ… Passer le chemin, pas les donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ TaskFlow API (Airflow 2.0+)",
    "text": "ğŸ¯ TaskFlow API (Airflow 2.0+)\nSyntaxe moderne et plus Pythonic avec des dÃ©corateurs :\n\n\nCode\n# TaskFlow API - Syntaxe moderne (Airflow 2.0+)\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='etl_taskflow',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'taskflow'],\n)\ndef etl_pipeline():\n    \n    @task()\n    def extract():\n        \"\"\"Extraire les donnÃ©es\"\"\"\n        return {'records': 1000, 'source': 'api'}\n    \n    @task()\n    def transform(data: dict):\n        \"\"\"Transformer les donnÃ©es\"\"\"\n        return {\n            'records': data['records'],\n            'cleaned': True\n        }\n    \n    @task()\n    def load(data: dict):\n        \"\"\"Charger les donnÃ©es\"\"\"\n        print(f\"Loading {data['records']} records\")\n    \n    # DÃ©finir le flow - XCom automatique !\n    raw_data = extract()\n    clean_data = transform(raw_data)\n    load(clean_data)\n\n# Instancier le DAG\netl_pipeline()\n\n\n\nAvantages TaskFlow API\n\n\n\nAvantage\nDescription\n\n\n\n\nXCom automatique\nLes retours sont passÃ©s automatiquement\n\n\nCode plus lisible\nRessemble Ã  du Python normal\n\n\nType hints\nSupport des annotations de type\n\n\nMoins de boilerplate\nPas besoin de crÃ©er des Operators",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ‘ï¸ Sensors â€” Attendre une condition",
    "text": "ğŸ‘ï¸ Sensors â€” Attendre une condition\nLes Sensors attendent quâ€™une condition soit remplie avant de continuer.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                              â”‚\nâ”‚   FileSensor          S3KeySensor         HttpSensor         â”‚\nâ”‚   \"Fichier existe?\"   \"Fichier sur S3?\"   \"API disponible?\"  â”‚\nâ”‚                                                              â”‚\nâ”‚        â³                   â³                   â³            â”‚\nâ”‚        â”‚                    â”‚                    â”‚           â”‚\nâ”‚        â–¼                    â–¼                    â–¼           â”‚\nâ”‚       âœ…                   âœ…                   âœ…           â”‚\nâ”‚   Continuer            Continuer            Continuer        â”‚\nâ”‚                                                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nFileSensor\n\n\nCode\n# FileSensor - Attendre qu'un fichier existe\n\nfrom airflow.sensors.filesystem import FileSensor\n\nwait_for_file = FileSensor(\n    task_id='wait_for_file',\n    filepath='/data/input/daily_export.csv',\n    poke_interval=60,      # VÃ©rifier toutes les 60 secondes\n    timeout=3600,          # Timeout aprÃ¨s 1 heure\n    mode='poke',           # poke ou reschedule\n)\n\n# Le pipeline attend le fichier avant de continuer\nwait_for_file &gt;&gt; process_file\n\n\n\n\nAutres Sensors utiles\n# S3KeySensor - Attendre un fichier sur S3\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n\nwait_s3 = S3KeySensor(\n    task_id='wait_for_s3',\n    bucket_name='my-bucket',\n    bucket_key='data/{{ ds }}/export.csv',\n    aws_conn_id='aws_default',\n)\n\n# HttpSensor - Attendre qu'une API rÃ©ponde\nfrom airflow.providers.http.sensors.http import HttpSensor\n\nwait_api = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='api_connection',\n    endpoint='health',\n    response_check=lambda response: response.status_code == 200,\n)\n\n# ExternalTaskSensor - Attendre un autre DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\nwait_other_dag = ExternalTaskSensor(\n    task_id='wait_upstream',\n    external_dag_id='upstream_dag',\n    external_task_id='final_task',\n)\n\n\nMode poke vs reschedule\n\n\n\nMode\nDescription\nRessources\n\n\n\n\npoke\nGarde un worker slot\nConsomme plus\n\n\nreschedule\nLibÃ¨re le slot entre checks\nRecommandÃ©",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”€ Branching â€” Logique conditionnelle",
    "text": "ğŸ”€ Branching â€” Logique conditionnelle\nExÃ©cuter diffÃ©rentes tÃ¢ches selon une condition.\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Check   â”‚\n                    â”‚ Conditionâ”‚\n                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n                         â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚          â”‚          â”‚\n              â–¼          â–¼          â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Path A â”‚ â”‚ Path B â”‚ â”‚ Path C â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n# Branching - ExÃ©cution conditionnelle\n\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndef choose_branch(**context):\n    \"\"\"DÃ©cider quelle branche exÃ©cuter\"\"\"\n    # Exemple : vÃ©rifier le jour de la semaine\n    day = context['ds_nodash']  # Format YYYYMMDD\n    \n    # Logique mÃ©tier\n    if int(day) % 2 == 0:\n        return 'process_even'  # Retourner le task_id Ã  exÃ©cuter\n    else:\n        return 'process_odd'\n\nbranch = BranchPythonOperator(\n    task_id='branch',\n    python_callable=choose_branch,\n)\n\nprocess_even = EmptyOperator(task_id='process_even')\nprocess_odd = EmptyOperator(task_id='process_odd')\nend = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')\n\n# DÃ©finir le flow\nbranch &gt;&gt; [process_even, process_odd] &gt;&gt; end",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸšï¸ Trigger Rules",
    "text": "ğŸšï¸ Trigger Rules\nContrÃ´ler quand une tÃ¢che sâ€™exÃ©cute en fonction du statut des tÃ¢ches parentes.\n\n\n\nTrigger Rule\nExÃ©cute siâ€¦\n\n\n\n\nall_success\nTous les parents ont rÃ©ussi (dÃ©faut)\n\n\nall_failed\nTous les parents ont Ã©chouÃ©\n\n\nall_done\nTous les parents sont terminÃ©s (peu importe le statut)\n\n\none_success\nAu moins un parent a rÃ©ussi\n\n\none_failed\nAu moins un parent a Ã©chouÃ©\n\n\nnone_failed\nAucun parent nâ€™a Ã©chouÃ© (succÃ¨s ou skipped)\n\n\nnone_skipped\nAucun parent nâ€™a Ã©tÃ© skipped\n\n\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# TÃ¢che de notification en cas d'Ã©chec\nnotify_failure = EmailOperator(\n    task_id='notify_failure',\n    to='team@company.com',\n    subject='Pipeline Failed!',\n    html_content='...',\n    trigger_rule=TriggerRule.ONE_FAILED,  # ExÃ©cute si un parent Ã©choue\n)\n\n# TÃ¢che finale qui s'exÃ©cute toujours\ncleanup = BashOperator(\n    task_id='cleanup',\n    bash_command='rm -rf /tmp/data/*',\n    trigger_rule=TriggerRule.ALL_DONE,  # Toujours exÃ©cuter\n)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ” Connections et Variables",
    "text": "ğŸ” Connections et Variables\n\nConnections\nStocker les informations de connexion aux systÃ¨mes externes.\nDans lâ€™UI : Admin â†’ Connections â†’ +\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Add Connection                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  Connection Id:   â”‚ my_postgres                 â”‚           â”‚\nâ”‚  Connection Type: â”‚ Postgres           â–¼        â”‚           â”‚\nâ”‚  Host:            â”‚ localhost                   â”‚           â”‚\nâ”‚  Schema:          â”‚ mydb                        â”‚           â”‚\nâ”‚  Login:           â”‚ admin                       â”‚           â”‚\nâ”‚  Password:        â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                    â”‚           â”‚\nâ”‚  Port:            â”‚ 5432                        â”‚           â”‚\nâ”‚                                                             â”‚\nâ”‚                              [ Save ]                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nUtilisation dans le code :\nfrom airflow.hooks.postgres_hook import PostgresHook\n\ndef query_postgres():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df('SELECT * FROM users')\n    return df\n\n\nVariables\nStocker des configurations rÃ©utilisables.\nDans lâ€™UI : Admin â†’ Variables â†’ +\nfrom airflow.models import Variable\n\n# RÃ©cupÃ©rer une variable\napi_key = Variable.get('API_KEY')\n\n# Variable JSON\nconfig = Variable.get('pipeline_config', deserialize_json=True)\n# config = {'batch_size': 1000, 'env': 'prod'}\n\n# Dans un template Jinja\n# {{ var.value.API_KEY }}\n# {{ var.json.pipeline_config.batch_size }}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Monitoring et Alertes",
    "text": "ğŸ“Š Monitoring et Alertes\n\nInterface Web\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Airflow - DAGs                                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  DAG                    Schedule    Owner    Runs   Recent Tasks            â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€    â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚  â–¶ etl_pipeline         @daily      team     125    âœ…âœ…âœ…âœ…âœ…               â”‚\nâ”‚  â–¶ data_quality_check   @hourly     team     560    âœ…âœ…âœ…âŒâœ…               â”‚\nâ”‚  â–¶ weekly_report        @weekly     team     52     âœ…âœ…âœ…âœ…âœ…               â”‚\nâ”‚  â¸ maintenance          None        admin    3      âœ…âœ…âœ…                   â”‚\nâ”‚                                                                             â”‚\nâ”‚  âœ… Success  âŒ Failed  â³ Running  â¸ Paused                                â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVues disponibles\n\n\n\nVue\nDescription\n\n\n\n\nGrid\nVue matricielle des runs\n\n\nGraph\nGraphe du DAG\n\n\nCalendar\nHistorique par date\n\n\nGantt\nTimeline dâ€™exÃ©cution\n\n\nCode\nCode source du DAG\n\n\n\n\n\nConfigurer les alertes email\n# airflow.cfg\n[smtp]\nsmtp_host = smtp.gmail.com\nsmtp_port = 587\nsmtp_user = airflow@company.com\nsmtp_password = your_password\nsmtp_mail_from = airflow@company.com\n\n# Dans le DAG\ndefault_args = {\n    'email': ['team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n}\n\n\nAlertes Slack\nfrom airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\n\ndef alert_slack_on_failure(context):\n    \"\"\"Callback en cas d'Ã©chec\"\"\"\n    slack_msg = f\"\"\"\n        :red_circle: Task Failed!\n        *DAG*: {context['dag'].dag_id}\n        *Task*: {context['task'].task_id}\n        *Execution Time*: {context['execution_date']}\n    \"\"\"\n    return SlackWebhookOperator(\n        task_id='slack_alert',\n        slack_webhook_conn_id='slack_webhook',\n        message=slack_msg,\n    ).execute(context)\n\n# Utiliser le callback\ndefault_args = {\n    'on_failure_callback': alert_slack_on_failure,\n}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Bonnes pratiques Airflow",
    "text": "âœ… Bonnes pratiques Airflow\n\n1. Structure des DAGs\nairflow/\nâ”œâ”€â”€ dags/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ etl/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ daily_etl.py\nâ”‚   â”‚   â””â”€â”€ weekly_report.py\nâ”‚   â”œâ”€â”€ utils/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ helpers.py\nâ”‚   â””â”€â”€ config/\nâ”‚       â””â”€â”€ settings.py\nâ”œâ”€â”€ plugins/\nâ”‚   â””â”€â”€ custom_operators/\nâ””â”€â”€ tests/\n    â””â”€â”€ test_dags.py\n\n\n2. Idempotence\n# âŒ Non idempotent - accumule des donnÃ©es\ndef bad_load():\n    db.execute(\"INSERT INTO table VALUES (...)\")\n\n# âœ… Idempotent - mÃªme rÃ©sultat si relancÃ©\ndef good_load():\n    db.execute(\"DELETE FROM table WHERE date = '{{ ds }}'\")\n    db.execute(\"INSERT INTO table SELECT ... WHERE date = '{{ ds }}'\")\n\n\n3. AtomicitÃ© des tÃ¢ches\n# âŒ TÃ¢che trop grosse\ndef do_everything():\n    extract()\n    transform()\n    load()\n\n# âœ… TÃ¢ches atomiques\nextract &gt;&gt; transform &gt;&gt; load\n\n\n4. Ne pas mettre de logique dans le DAG\n# âŒ Code exÃ©cutÃ© Ã  chaque parsing\ndata = fetch_from_api()  # AppelÃ© toutes les 30s !\n\n# âœ… Logique dans les tasks\n@task\ndef fetch_data():\n    return fetch_from_api()\n\n\n5. Tester les DAGs\n# tests/test_dags.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dag_bag = DagBag()\n    assert len(dag_bag.import_errors) == 0\n\ndef test_dag_structure():\n    dag_bag = DagBag()\n    dag = dag_bag.get_dag('etl_pipeline')\n    assert dag is not None\n    assert len(dag.tasks) == 5",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces dâ€™Airflow",
    "text": "âœ… Forces dâ€™Airflow\nâœ… Gestion des dÃ©pendances - A &gt;&gt; B = B attend A\nâœ… Retry automatique - RÃ©essaie en cas dâ€™Ã©chec\nâœ… Interface web - Visualisation complÃ¨te\nâœ… Monitoring - Logs centralisÃ©s\nâœ… Alertes - Email/Slack en cas dâ€™Ã©chec\nâœ… Scalable - GÃ¨re 100+ pipelines\nâœ… Extensible - Custom operators, hooks\nâœ… CommunautÃ© - TrÃ¨s active, beaucoup de providers",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses dâ€™Airflow",
    "text": "âŒ Faiblesses dâ€™Airflow\nâŒ Complexe - Courbe dâ€™apprentissage\nâŒ Ressources - Besoin de 4-8 GB RAM\nâŒ Overkill - Pour 1-3 scripts simples\nâŒ Setup - Installation et configuration\nâŒ Pas pour le streaming - Batch only (utiliser Kafka)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand utiliser Airflow ?",
    "text": "Quand utiliser Airflow ?\nâœ… OUI : 10+ pipelines, dÃ©pendances complexes, production\nâŒ NON : 1-5 scripts simples sans dÃ©pendances",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quel outil choisir ?",
    "text": "Quel outil choisir ?\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nJâ€™ai 1-3 scripts sur Windows\nğŸªŸ Task Scheduler\n\n\nJâ€™ai 1-3 scripts sur Linux\nğŸ§ Crontab\n\n\nJâ€™ai 5-10 scripts indÃ©pendants\nğŸ§ Crontab\n\n\nJâ€™ai 10+ scripts avec dÃ©pendances\nğŸŒ¬ï¸ Airflow\n\n\nScript B doit attendre script A\nğŸŒ¬ï¸ Airflow\n\n\nJe veux un dashboard\nğŸŒ¬ï¸ Airflow\n\n\nJe dÃ©bute en automatisation\nğŸªŸ Task Scheduler\n\n\nProduction critique\nğŸŒ¬ï¸ Airflow",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Progression naturelle",
    "text": "Progression naturelle\n1. DÃ©butez avec Task Scheduler ou Cron\n2. Quand vous avez 5+ scripts â†’ Pensez Ã  migrer\n3. Quand vous avez des dÃ©pendances â†’ Migrez vers Airflow",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Points clÃ©s",
    "text": "Points clÃ©s\n\nğŸªŸ Windows Task Scheduler\n\nPour qui : DÃ©butants sur Windows\nForce : TrÃ¨s facile (GUI)\nFaiblesse : Pas de dÃ©pendances\nLimite : 5 scripts max\n\n\n\nğŸ§ Crontab\n\nPour qui : Utilisateurs Linux\nForce : Universel, lÃ©ger\nFaiblesse : Pas de monitoring\nLimite : 15 scripts max\n\n\n\nğŸŒ¬ï¸ Airflow\n\nPour qui : Production, Ã©quipes data\nForce : DÃ©pendances, monitoring, scalable\nFaiblesse : Complexe, ressources\nLimite : Aucune (scalable)\n\n\n\nConcepts Airflow Ã  retenir\n\n\n\nConcept\nDescription\n\n\n\n\nDAG\nGraphe de tÃ¢ches (workflow)\n\n\nTask\nUnitÃ© de travail\n\n\nOperator\nType de tÃ¢che (Python, Bash, SQLâ€¦)\n\n\nXCom\nPassage de donnÃ©es entre tÃ¢ches\n\n\nSensor\nAttendre une condition\n\n\nConnection\nCredentials stockÃ©s\n\n\nVariable\nConfiguration stockÃ©e",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Ressources",
    "text": "Ressources\n\nCrontab : crontab.guru (tester expressions)\nAirflow : airflow.apache.org\nAirflow Tutorial : Documentation officielle",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert orchestration !\n8-9/10 : ğŸŒŸ TrÃ¨s bien !\n6-7/10 : ğŸ’ª Bon dÃ©but !\n&lt; 6/10 : ğŸ“š Relisez le notebook",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nOutils\n\nCrontab Guru â€” Tester et gÃ©nÃ©rer des expressions cron\nApache Airflow â€” Documentation officielle\nAirflow Tutorial\nAstronomer â€” Guides et bonnes pratiques\n\n\n\nAlternatives Ã  Airflow\n\n\n\n\n\n\n\n\nOutil\nDescription\nCas dâ€™usage\n\n\n\n\nPrefect\nOrchestration moderne, Pythonic\nAlternative plus simple Ã  Airflow\n\n\nDagster\nData orchestration avec types\nPipelines ML\n\n\nLuigi\nPar Spotify, simple\nPipelines batch\n\n\nMage\nLow-code, moderne\nPrototypage rapide\n\n\nKestra\nEvent-driven, YAML\nWorkflows dÃ©claratifs\n\n\n\n\n\nCloud managed\n\n\n\nCloud\nService\n\n\n\n\nAWS\nMWAA (Managed Airflow), Step Functions\n\n\nGCP\nCloud Composer (Managed Airflow)\n\n\nAzure\nData Factory, Synapse Pipelines",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ‰ Fin du niveau dÃ©butant !",
    "text": "ğŸ‰ Fin du niveau dÃ©butant !\nTu as terminÃ© le parcours Data Engineering - From Zero to Hero niveau dÃ©butant ! ğŸ‰\n\nğŸ“‹ RÃ©capitulatif des modules\n\n\n\n#\nModule\nCompÃ©tence acquise\n\n\n\n\n01\nIntroduction\nVision du mÃ©tier\n\n\n02\nBash\nLigne de commande\n\n\n03\nGit\nVersioning\n\n\n04\nPython Basics\nProgrammation\n\n\n05\nPython Data Processing\nPandas, visualisation\n\n\n06\nIntro Bases Relationnelles\nConcepts relationnels\n\n\n07\nSQL\nRequÃªtes SQL\n\n\n08\nBig Data & NoSQL\nSystÃ¨mes distribuÃ©s\n\n\n09\nMongoDB\nBase NoSQL document\n\n\n10\nElasticsearch\nRecherche et indexation\n\n\n11\nPySpark\nTraitement distribuÃ©\n\n\n12\nOrchestration\nAirflow, pipelines\n\n\n\n\n\nğŸ Module BONUS disponible\n\n\n\n\n\n\n\n\n#\nModule\nDescription\n\n\n\n\n13\nBONUS FastAPI\nCrÃ©er des APIs REST pour exposer tes donnÃ©es\n\n\n\nğŸ‘‰ Parfait pour exposer les rÃ©sultats de tes pipelines via API !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "title": "â° Orchestration de Pipelines Data",
    "section": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire",
    "text": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire\nTu es maintenant prÃªt pour le niveau intermÃ©diaire qui couvrira :\n\n\n\nModule\nDescription\n\n\n\n\nDocker\nConteneurisation des pipelines\n\n\nData Lakes\nParquet, Delta Lake, Iceberg\n\n\nKafka\nStreaming en temps rÃ©el\n\n\ndbt\nTransformation dans le warehouse\n\n\nData Quality\nGreat Expectations, tests\n\n\nCloud\nAWS / GCP / Azure\n\n\nCI/CD\nGitHub Actions, tests automatisÃ©s\n\n\nKibana\nDashboards et monitoring\n\n\nProjet intÃ©grateur\nPipeline complet end-to-end\n\n\n\n\nğŸš€ Bravo ! Tu as maintenant toutes les bases pour construire des pipelines de donnÃ©es !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente Elasticsearch, le moteur de recherche et dâ€™analytics distribuÃ©.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre lâ€™architecture Elasticsearch (index, shards, replicas)\nâœ… Utiliser Elasticvue pour interagir avec le cluster\nâœ… CrÃ©er des index avec mapping appropriÃ©\nâœ… CrÃ©er des Index Templates pour automatiser les mappings\nâœ… Indexer, rechercher, modifier et supprimer des documents\nâœ… Ã‰crire des requÃªtes de recherche (match, bool, fuzzy, range)\nâœ… RÃ©aliser des agrÃ©gations analytiques",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âš ï¸ Version Elasticsearch",
    "text": "âš ï¸ Version Elasticsearch\nCe cours utilise Elasticsearch 8.x (version recommandÃ©e : 8.12+).\n\n\n\n\n\n\n\nVersion\nNotes\n\n\n\n\n8.x\nâœ… RecommandÃ©e â€” SÃ©curitÃ© par dÃ©faut, nouvelles fonctionnalitÃ©s\n\n\n7.x\nâš ï¸ Encore supportÃ©e mais migration conseillÃ©e\n\n\n6.x et avant\nâŒ ObsolÃ¨te\n\n\n\n\nğŸ’¡ Les exemples de ce cours fonctionnent avec ES 7.x et 8.x. Pour ES 8.x, la sÃ©curitÃ© est activÃ©e par dÃ©faut â€” nous la dÃ©sactivons pour les tests locaux.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 les diffÃ©rents types de bases NoSQL. Elasticsearch est un moteur de recherche (Search Engine), parfois classÃ© Ã  part des bases NoSQL traditionnelles.\n\nPosition dans les types NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Document  â”‚ ClÃ©-Valeurâ”‚  Colonnes â”‚  Graphe   â”‚ Search Engine  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚                â”‚\nâ”‚  MongoDB  â”‚   Redis   â”‚ Cassandra â”‚   Neo4j   â”‚ ELASTICSEARCH  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚     â—„â”€â”€â”€       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment Elasticsearch rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs nÅ“uds)\n\n\nVelocity\nIndexation temps rÃ©el, near real-time search\n\n\nVariety\nDocuments JSON flexibles, analyse full-text\n\n\nVeracity\nScoring de pertinence, recherche approximative\n\n\nValue\nRecherche instantanÃ©e, dashboards Kibana\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nElasticsearch\n\n\n\n\nCAP\nAP (Availability + Partition tolerance) par dÃ©faut\n\n\nBASE\nEventually consistent (cohÃ©rence Ã©ventuelle)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Pourquoi Elasticsearch en Data Engineering ?",
    "text": "ğŸ¯ Pourquoi Elasticsearch en Data Engineering ?\n\n\n\n\n\n\n\nCas dâ€™usage\nDescription\n\n\n\n\nğŸ“Š Logs & Monitoring\nStack ELK (Elasticsearch, Logstash, Kibana)\n\n\nğŸ” Recherche full-text\nMoteur de recherche pour sites web, e-commerce\n\n\nğŸ“ˆ Analytics temps rÃ©el\nDashboards et mÃ©triques en temps rÃ©el\n\n\nğŸ”” Alerting\nDÃ©tection dâ€™anomalies, alertes sur seuils\n\n\n\n\nğŸ’¡ ELK Stack = Elasticsearch + Logstash + Kibana â€” trÃ¨s utilisÃ© en entreprise.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“š 1. Concepts fondamentaux",
    "text": "ğŸ“š 1. Concepts fondamentaux\n\nğŸ“– Vocabulaire\n\n\n\nSQL\nElasticsearch\nDescription\n\n\n\n\nDatabase\nCluster\nEnsemble de nÅ“uds\n\n\nTable\nIndex\nCollection de documents\n\n\nRow\nDocument\nUne entrÃ©e (JSON)\n\n\nColumn\nField\nUn champ du document\n\n\nSchema\nMapping\nStructure des champs\n\n\n\n\n\nğŸ§± Shards & Replicas\nIndex \"clients\"\nâ”œâ”€â”€ Primary Shard 0  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 0\nâ”œâ”€â”€ Primary Shard 1  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 1\nâ””â”€â”€ Primary Shard 2  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 2\n\n\n\nConcept\nRÃ´le\n\n\n\n\nShard\nPartition des donnÃ©es (scalabilitÃ©)\n\n\nReplica\nCopie dâ€™un shard (haute disponibilitÃ©)\n\n\n\n\n\nğŸ“Š Ã‰tat du cluster\n\n\n\nÃ‰tat\nSignification\n\n\n\n\nğŸŸ¢ Green\nTous les shards OK\n\n\nğŸŸ¡ Yellow\nPrimaires OK, replicas non allouÃ©s (1 seul nÅ“ud)\n\n\nğŸ”´ Red\nDonnÃ©es inaccessibles",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âš™ï¸ 2. Installation",
    "text": "âš™ï¸ 2. Installation\n\nğŸ“¦ Ã‰tape 1 : TÃ©lÃ©charger Elasticsearch\nğŸ‘‰ https://www.elastic.co/downloads/elasticsearch\n\nTÃ©lÃ©charger le ZIP pour ton OS\nDÃ©zipper dans un dossier (ex: C:\\elasticsearch)\nModifier config/elasticsearch.yml :\n\n# DÃ©sactiver la sÃ©curitÃ© pour les tests\nxpack.security.enabled: false\n\nLancer Elasticsearch :\n\n# Windows\n.\\bin\\elasticsearch.bat\n\n# macOS / Linux\n./bin/elasticsearch\n\nVÃ©rifier : ouvrir http://localhost:9200 dans le navigateur\n\n\n\n\nğŸ§­ Ã‰tape 2 : Installer Elasticvue\nElasticvue est une interface graphique pour Elasticsearch â€” beaucoup plus simple que les commandes curl !\nOptions dâ€™installation :\n\n\n\nOption\nLien\n\n\n\n\nğŸŒ App Web (recommandÃ©)\nhttps://app.elasticvue.com\n\n\nğŸ§© Extension Chrome\nChrome Web Store\n\n\nğŸ¦Š Extension Firefox\nFirefox Add-ons\n\n\nğŸ’» App Desktop\nelasticvue.com",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ§­ 3. Guide Elasticvue â€” Prise en main",
    "text": "ğŸ§­ 3. Guide Elasticvue â€” Prise en main\n\nğŸ”Œ 3.1 Connexion au cluster\n\nOuvrir Elasticvue\nCliquer sur â€œAdd Clusterâ€\nRemplir :\n\nName : Local (ou ce que tu veux)\nURI : http://localhost:9200\n\nCliquer â€œConnectâ€\n\nâœ… Tu devrais voir le statut du cluster (ğŸŸ¢ Green ou ğŸŸ¡ Yellow)\n\n\n\nğŸ—‚ï¸ 3.2 Interface principale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Elasticvue                                    [Cluster: Local] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚             â”‚                                               â”‚\nâ”‚  ğŸ“Š Home    â”‚   Cluster Health: ğŸŸ¢ Green                    â”‚\nâ”‚             â”‚   Nodes: 1                                    â”‚\nâ”‚  ğŸ“ Indices â”‚   Indices: 3                                  â”‚\nâ”‚             â”‚   Documents: 1,234                            â”‚\nâ”‚  ğŸ” Search  â”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚  ğŸ“ REST    â”‚   â—„â”€â”€ C'est ici qu'on Ã©crit les requÃªtes !   â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚  âš™ï¸ Settingsâ”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“ Onglets importants :\n\n\n\nOnglet\nUsage\n\n\n\n\nIndices\nVoir/crÃ©er/supprimer des index\n\n\nSearch\nRechercher visuellement dans un index\n\n\nREST\nÃ‰crire des requÃªtes (comme Kibana Dev Tools)\n\n\n\n\n\n\nğŸ“ 3.3 Utiliser la console REST\nLâ€™onglet REST permet dâ€™exÃ©cuter des requÃªtes Elasticsearch avec une syntaxe simple.\n\nğŸ“Œ Format des requÃªtes\nMÃ‰THODE /chemin\n{\n  \"corps\": \"de la requÃªte en JSON\"\n}\n\n\nğŸ¯ Exemple pas Ã  pas\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  REST Query                                        [â–¶ Run]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                              â”‚\nâ”‚  GET /clients/_search                                        â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"query\": {                                                â”‚\nâ”‚      \"match\": { \"pays\": \"France\" }                          â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ”‚                                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Response (200 OK)                                           â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"hits\": {                                                 â”‚\nâ”‚      \"total\": { \"value\": 2 },                                â”‚\nâ”‚      \"hits\": [...]                                           â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâŒ¨ï¸ Raccourcis utiles\n\n\n\nRaccourci\nAction\n\n\n\n\nCtrl + Enter\nExÃ©cuter la requÃªte\n\n\nCtrl + /\nCommenter une ligne\n\n\nCtrl + Space\nAutocomplÃ©tion",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ› ï¸ 4. CrÃ©er un index et insÃ©rer des donnÃ©es",
    "text": "ğŸ› ï¸ 4. CrÃ©er un index et insÃ©rer des donnÃ©es\n\nğŸ“Œ 4.1 CrÃ©er lâ€™index clients\nDans lâ€™onglet REST, copier-coller :\nPUT /clients\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"nom\": { \"type\": \"text\" },\n      \"email\": { \"type\": \"keyword\" },\n      \"pays\": { \"type\": \"keyword\" },\n      \"age\": { \"type\": \"integer\" },\n      \"salaire\": { \"type\": \"float\" }\n    }\n  }\n}\nâœ… RÃ©ponse attendue : { \"acknowledged\": true }\n\n\n\nğŸ“ 4.2 Comprendre le mapping\n\n\n\nType\nUsage\nRecherche full-text\nAgrÃ©gation\n\n\n\n\ntext\nTexte analysÃ©\nâœ… Oui\nâŒ Non\n\n\nkeyword\nValeur exacte\nâŒ Non\nâœ… Oui\n\n\ninteger, float\nNombres\nRange âœ…\nâœ… Oui\n\n\ndate\nDates\nRange âœ…\nâœ… Oui\n\n\nboolean\ntrue/false\nâœ…\nâœ… Oui\n\n\n\n\nğŸ’¡ RÃ¨gle : Utilise keyword pour les champs sur lesquels tu veux faire des agrÃ©gations (GROUP BY).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ Index Templates â€” Automatiser les mappings",
    "text": "ğŸ“‹ Index Templates â€” Automatiser les mappings\n\nğŸ¤” Pourquoi utiliser des templates ?\nQuand tu gÃ¨res des logs ou des donnÃ©es temporelles, tu crÃ©es souvent des index par jour ou par mois :\nlogs-2024-01-01\nlogs-2024-01-02\nlogs-2024-01-03\n...\nProblÃ¨me : Sans template, chaque index utilise le mapping dynamique (Elasticsearch devine les types). Câ€™est risquÃ© !\nSolution : CrÃ©er un Index Template qui sâ€™applique automatiquement Ã  tous les index correspondant Ã  un pattern.\n\n\n\nğŸ“Œ CrÃ©er un Index Template\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"priority\": 1,\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": { \"type\": \"date\" },\n        \"level\": { \"type\": \"keyword\" },\n        \"message\": { \"type\": \"text\" },\n        \"service\": { \"type\": \"keyword\" },\n        \"host\": { \"type\": \"keyword\" },\n        \"response_time_ms\": { \"type\": \"integer\" }\n      }\n    }\n  }\n}\nâœ… Maintenant, tout index crÃ©Ã© avec le pattern logs-* aura automatiquement ce mapping !\n\n\n\nğŸ§ª Tester le template\n# CrÃ©er un index qui match le pattern\nPOST /logs-2024-01-15/_doc\n{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"level\": \"ERROR\",\n  \"message\": \"Connection timeout to database\",\n  \"service\": \"api-gateway\",\n  \"host\": \"server-01\",\n  \"response_time_ms\": 5000\n}\n# VÃ©rifier que le mapping a Ã©tÃ© appliquÃ©\nGET /logs-2024-01-15/_mapping\n\n\n\nğŸ“‹ Lister les templates existants\nGET /_index_template\n\n\nğŸ—‘ï¸ Supprimer un template\nDELETE /_index_template/logs_template\n\n\n\nğŸ’¡ Bonnes pratiques Index Templates\n\n\n\n\n\n\n\nPratique\nExplication\n\n\n\n\nToujours dÃ©finir un mapping\nNe pas laisser ES deviner les types\n\n\nUtiliser keyword pour les agrÃ©gations\nlevel, service, host â†’ keyword\n\n\nUtiliser text pour la recherche\nmessage â†’ text\n\n\nDÃ©finir date explicitement\nÃ‰vite les erreurs de parsing\n\n\nNommer clairement\nlogs_template, metrics_template\n\n\nUtiliser priority\nQuand plusieurs templates matchent\n\n\n\n\n\n\nğŸ“ 4.3 InsÃ©rer un document\nPOST /clients/_doc\n{\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@email.com\",\n  \"pays\": \"France\",\n  \"age\": 30,\n  \"salaire\": 55000\n}\n\n\n\nğŸ“ 4.4 InsÃ©rer plusieurs documents (Bulk)\nPOST /_bulk\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Bob Martin\", \"email\": \"bob@email.com\", \"pays\": \"France\", \"age\": 25, \"salaire\": 48000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Charlie Konan\", \"email\": \"charlie@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 35, \"salaire\": 62000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Diana Schmidt\", \"email\": \"diana@email.com\", \"pays\": \"Allemagne\", \"age\": 28, \"salaire\": 51000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Eve Kouassi\", \"email\": \"eve@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 32, \"salaire\": 58000 }\n\nâš ï¸ Attention : En bulk, chaque ligne doit Ãªtre sur une seule ligne (pas de formatage JSON multi-lignes).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ” 5. RequÃªtes de recherche",
    "text": "ğŸ” 5. RequÃªtes de recherche\n\nğŸ“Œ 5.1 Afficher tous les documents\nGET /clients/_search\nâœ… SQL Ã©quivalent : SELECT * FROM clients\n\n\n\nğŸ“Œ 5.2 Filtrer avec match (full-text)\nGET /clients/_search\n{\n  \"query\": {\n    \"match\": { \"nom\": \"Alice\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE nom LIKE '%Alice%'\n\n\n\nğŸ“Œ 5.3 Filtrer avec term (exact match)\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France'\n\nğŸ’¡ Utilise term pour les champs keyword, match pour les champs text.\n\n\n\n\nğŸ“Œ 5.4 Filtrer par plage de valeurs (range)\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"age\": {\n        \"gte\": 25,\n        \"lte\": 35\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE age BETWEEN 25 AND 35\n\n\n\nOpÃ©rateur\nSignification\n\n\n\n\ngt\n&gt; (greater than)\n\n\ngte\n&gt;= (greater than or equal)\n\n\nlt\n&lt; (less than)\n\n\nlte\n&lt;= (less than or equal)\n\n\n\n\n\n\nğŸ“Œ 5.5 Conditions multiples (bool)\nLa requÃªte bool combine plusieurs conditions :\n\n\n\nClause\nComportement\nSQL Ã©quivalent\n\n\n\n\nmust\nToutes les conditions requises\nAND\n\n\nshould\nAu moins une condition\nOR\n\n\nmust_not\nExclure\nNOT / !=\n\n\nfilter\nComme must mais sans score\nWHERE (optimisÃ©)\n\n\n\n\nExemple : Clients franÃ§ais de plus de 25 ans\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"range\": { \"age\": { \"gt\": 25 } } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France' AND age &gt; 25\n\n\n\nExemple : Clients franÃ§ais OU ivoiriens, mais PAS Bob\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"term\": { \"pays\": \"CÃ´te d'Ivoire\" } }\n      ],\n      \"minimum_should_match\": 1,\n      \"must_not\": [\n        { \"match\": { \"nom\": \"Bob\" } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE (pays = 'France' OR pays = 'CÃ´te d''Ivoire') AND nom != 'Bob'\n\n\n\n\nğŸ“Œ 5.6 Recherche floue (fuzzy)\nTrouve des rÃ©sultats mÃªme avec des fautes de frappe :\nGET /clients/_search\n{\n  \"query\": {\n    \"fuzzy\": {\n      \"nom\": {\n        \"value\": \"Alise\",\n        \"fuzziness\": \"AUTO\"\n      }\n    }\n  }\n}\nâœ… Trouve â€œAliceâ€ mÃªme si on tape â€œAliseâ€ !\n\nğŸ’¡ Pas dâ€™Ã©quivalent simple en SQL â€” câ€™est la force dâ€™Elasticsearch.\n\n\n\n\nğŸ“Œ 5.7 Trier et limiter les rÃ©sultats\nGET /clients/_search\n{\n  \"query\": { \"match_all\": {} },\n  \"sort\": [\n    { \"salaire\": \"desc\" }\n  ],\n  \"size\": 3\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients ORDER BY salaire DESC LIMIT 3",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“Š 6. AgrÃ©gations (GROUP BY)",
    "text": "ğŸ“Š 6. AgrÃ©gations (GROUP BY)\n\nğŸ“Œ 6.1 Compter par catÃ©gorie (terms)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT pays, COUNT(*) FROM clients GROUP BY pays\n\nğŸ’¡ size: 0 = ne pas retourner les documents, seulement lâ€™agrÃ©gation.\n\n\n\n\nğŸ“Œ 6.2 MÃ©triques (sum, avg, min, max)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n    \"salaire_max\": { \"max\": { \"field\": \"salaire\" } },\n    \"salaire_min\": { \"min\": { \"field\": \"salaire\" } },\n    \"salaire_total\": { \"sum\": { \"field\": \"salaire\" } }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT AVG(salaire), MAX(salaire), MIN(salaire), SUM(salaire) FROM clients\n\n\n\nğŸ“Œ 6.3 Stats complÃ¨tes en une requÃªte\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"stats_salaire\": {\n      \"stats\": { \"field\": \"salaire\" }\n    }\n  }\n}\nRÃ©sultat : count, min, max, avg, sum en une seule requÃªte !\n\n\n\nğŸ“Œ 6.4 AgrÃ©gations imbriquÃ©es (GROUP BY + mÃ©triques)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n        \"age_moyen\": { \"avg\": { \"field\": \"age\" } }\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, AVG(salaire) AS salaire_moyen, AVG(age) AS age_moyen\nFROM clients\nGROUP BY pays;\n\n\n\nğŸ“Œ 6.5 Filtrer avant dâ€™agrÃ©ger\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"salaire\": { \"gte\": 50000 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, COUNT(*)\nFROM clients\nWHERE salaire &gt;= 50000\nGROUP BY pays;",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âœï¸ 7. Modifier et supprimer",
    "text": "âœï¸ 7. Modifier et supprimer\n\nğŸ“Œ 7.1 Mettre Ã  jour un document (par ID)\nPOST /clients/_update/1\n{\n  \"doc\": {\n    \"salaire\": 60000\n  }\n}\n\n\n\nğŸ“Œ 7.2 Mettre Ã  jour par requÃªte\nPOST /clients/_update_by_query\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.salaire += 1000\"\n  }\n}\nâœ… SQL Ã©quivalent : UPDATE clients SET salaire = salaire + 1000 WHERE pays = 'France'\n\n\n\nğŸ“Œ 7.3 Supprimer un document (par ID)\nDELETE /clients/_doc/1\n\n\n\nğŸ“Œ 7.4 Supprimer par requÃªte\nPOST /clients/_delete_by_query\n{\n  \"query\": {\n    \"range\": { \"age\": { \"lt\": 18 } }\n  }\n}\nâœ… SQL Ã©quivalent : DELETE FROM clients WHERE age &lt; 18\n\n\n\nğŸ“Œ 7.5 Supprimer un index entier\nDELETE /clients\n\nâš ï¸ Attention : IrrÃ©versible !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet Elasticsearch",
    "text": "ğŸ“‹ Cheatsheet Elasticsearch\n\nğŸ”§ Gestion des index\n\n\n\nAction\nRequÃªte\n\n\n\n\nCrÃ©er un index\nPUT /mon_index\n\n\nSupprimer\nDELETE /mon_index\n\n\nLister\nGET /_cat/indices?v\n\n\nVoir mapping\nGET /mon_index/_mapping\n\n\nSantÃ© cluster\nGET /_cluster/health\n\n\n\n\n\nğŸ“ CRUD Documents\n\n\n\nAction\nRequÃªte\n\n\n\n\nInsÃ©rer\nPOST /index/_doc\n\n\nLire (ID)\nGET /index/_doc/1\n\n\nMettre Ã  jour\nPOST /index/_update/1\n\n\nSupprimer\nDELETE /index/_doc/1\n\n\nBulk\nPOST /_bulk\n\n\n\n\n\nğŸ” Types de requÃªtes\n\n\n\nType\nUsage\n\n\n\n\nmatch\nFull-text (champs text)\n\n\nterm\nExact (champs keyword)\n\n\nrange\nPlage (nombres, dates)\n\n\nbool\nCombiner (must, should, must_not)\n\n\nfuzzy\nTolÃ©rant aux fautes\n\n\n\n\n\nğŸ“Š AgrÃ©gations\n\n\n\nType\nSQL Ã©quivalent\n\n\n\n\nterms\nGROUP BY\n\n\nsum, avg, min, max\nFonctions dâ€™agrÃ©gation\n\n\nstats\nToutes les stats\n\n\ncardinality\nCOUNT(DISTINCT)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Exercices pratiques",
    "text": "ğŸ¯ Exercices pratiques\nUtilise lâ€™onglet REST dâ€™Elasticvue pour rÃ©soudre ces exercices.\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nAfficher tous les clients.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nTrouver les clients de CÃ´te dâ€™Ivoire.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"CÃ´te d'Ivoire\" }\n  }\n}\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nTrouver les clients avec un salaire entre 50000 et 60000, triÃ©s par Ã¢ge dÃ©croissant.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"salaire\": { \"gte\": 50000, \"lte\": 60000 }\n    }\n  },\n  \"sort\": [{ \"age\": \"desc\" }]\n}\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nCalculer le nombre de clients par pays.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nCalculer le salaire moyen par pays, seulement pour les clients de plus de 25 ans.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"age\": { \"gt\": 25 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } }\n      }\n    }\n  }\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quelle requÃªte crÃ©e un index ?\n\nPOST /clients\n\nPUT /clients\n\nGET /clients\n\nCREATE /clients\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” PUT /index crÃ©e un nouvel index.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre text et keyword ?\n\nAucune diffÃ©rence\n\ntext est analysÃ© (tokenisÃ©), keyword est stockÃ© tel quel\n\nkeyword est plus rapide\n\ntext est pour les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” text est dÃ©coupÃ© en tokens pour la recherche full-text, keyword reste intact pour les matchs exacts et agrÃ©gations.\n\n\n\n\nâ“ Q3. Quelle requÃªte utiliser pour un GROUP BY ?\n\nmatch\n\nbool\n\nterms (dans aggs)\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Lâ€™agrÃ©gation terms groupe les documents par valeur de champ.\n\n\n\n\nâ“ Q4. Que fait size: 0 dans une requÃªte dâ€™agrÃ©gation ?\n\nSupprime les donnÃ©es\n\nRetourne uniquement lâ€™agrÃ©gation, pas les documents\n\nLimite Ã  0 rÃ©sultat dâ€™agrÃ©gation\n\nErreur\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” size: 0 Ã©vite de retourner les documents, seulement les rÃ©sultats dâ€™agrÃ©gation.\n\n\n\n\nâ“ Q5. Quelle requÃªte permet de chercher â€œAliceâ€ mÃªme si on tape â€œAliseâ€ ?\n\nmatch\n\nterm\n\nfuzzy\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” fuzzy tolÃ¨re les fautes de frappe.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nElasticsearch Documentation\nElasticvue â€” Interface graphique utilisÃ©e dans ce cours\nKibana â€” Visualisation et dashboards\nElastic Cloud â€” Version cloud managÃ©e",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant les bases NoSQL (MongoDB et Elasticsearch) ! Passons au traitement distribuÃ© avec PySpark.\nğŸ‘‰ Module suivant : 11_pyspark_for_data_engineering.ipynb â€” PySpark pour le traitement Big Data\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Elasticsearch pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/tmp.html",
    "href": "notebooks/beginner/tmp.html",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "",
    "text": "Code\n---\n\n## ğŸªœ ProblÃ©matique 7 : Filtrer aprÃ¨s agrÃ©gation â€” `bucket_selector` (Ã©quivalent `HAVING`)\n\n```bash\ncurl -X GET \"http://localhost:9200/clients/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": {\n        \"field\": \"pays.keyword\"\n      },\n      \"aggs\": {\n        \"nb_docs\": {\n          \"value_count\": { \"field\": \"pays.keyword\" }\n        },\n        \"filtre_nb\": {\n          \"bucket_selector\": {\n            \"buckets_path\": { \"total\": \"nb_docs\" },\n            \"script\": \"params.total &gt; 1\"\n          }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT pays, COUNT(*) as nb_clients\nFROM clients\nGROUP BY pays\nHAVING COUNT(*) &gt; 1;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 8 : Champs calculÃ©s â€” `script_fields` (Ã©quivalent `CASE`)\n\n```bash\ncurl -X GET \"http://localhost:9200/clients/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"script_fields\": {\n    \"continent\": {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"if (doc['pays.keyword'].value == 'France') { return 'Europe'; } else { return 'Autre'; }\"\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT nom, pays,\nCASE \n  WHEN pays = 'France' THEN 'Europe'\n  ELSE 'Autre'\nEND AS continent\nFROM clients;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 9 : AgrÃ©gations imbriquÃ©es (Ã©quivalent sous-requÃªtes)\n\n```bash\ncurl -X GET \"http://localhost:9200/ventes/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays.keyword\" },\n      \"aggs\": {\n        \"par_produit\": {\n          \"terms\": { \"field\": \"produit.keyword\" }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT pays, produit, COUNT(*)\nFROM ventes\nGROUP BY pays, produit;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 10 : Fonctions analytiques â€” `top_hits`, `bucket_sort` (Ã©quivalent `RANK()`)\n\n```bash\ncurl -X GET \"http://localhost:9200/ventes/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays.keyword\" },\n      \"aggs\": {\n        \"top_vente\": {\n          \"top_hits\": {\n            \"size\": 1,\n            \"sort\": [ { \"montant\": { \"order\": \"desc\" } } ]\n          }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT *\nFROM (\n  SELECT *, RANK() OVER (PARTITION BY pays ORDER BY montant DESC) as rang\n  FROM ventes\n) t\nWHERE rang = 1;\n```\n\n\n\n\n\nCode\noui mais pour le contenu ; il faut faire dÃ©finition ; rappeler les notions shards et replicas , installations\n\n\n\n\nCode\nbeeruschatgpt_db_user   ------  cH0cBlDbokqCohcB"
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre les commandes Bash essentielles pour manipuler des fichiers, automatiser des tÃ¢ches, et interagir avec ton environnement systÃ¨me â€” des compÃ©tences indispensables pour un Data Engineer !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 01_intro_data_engineering\n\n\nâœ… Requis\nAvoir accÃ¨s Ã  un terminal (Linux, Mac, ou Windows avec WSL/Git Bash)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nNaviguer dans lâ€™arborescence de fichiers\nManipuler des fichiers et dossiers\nFiltrer et rechercher dans des donnÃ©es\nÃ‰crire des scripts Bash pour automatiser des tÃ¢ches\nPlanifier des jobs avec cron",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ§  Câ€™est quoi le langage Bash ?",
    "text": "ğŸ§  Câ€™est quoi le langage Bash ?\nBash (abrÃ©viation de Bourne Again SHell) est un langage de commande et de script utilisÃ© dans la majoritÃ© des systÃ¨mes Unix/Linux (et mÃªme sous Windows via WSL ou Git Bash).\nIl te permet de :\n\nğŸ“‚ Naviguer dans les dossiers\n\nğŸ“„ Manipuler des fichiers et des donnÃ©es\n\nâš™ï¸ Automatiser des tÃ¢ches rÃ©pÃ©titives\n\nğŸ”„ Ã‰crire des scripts shell pour lancer des traitements de donnÃ©es\n\n\n\nğŸ§° Pourquoi câ€™est utile pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nLancer des pipelines ETL\npython etl_pipeline.py && echo \"Success\" \\|\\| echo \"Failed\"\n\n\nÃ‰crire des jobs cron\nExtraction automatique de donnÃ©es chaque nuit Ã  2h\n\n\nManipuler des fichiers\nFusionner 100 fichiers CSV en un seul\n\n\nOrchestrer des outils\nLancer Docker, Spark, ou Airflow depuis un script\n\n\nAnalyser des logs\nTrouver toutes les erreurs dans les logs du jour\n\n\n\n\nğŸ’¡ En bref : le Bash est ton couteau suisse pour parler avec ton ordinateur et piloter lâ€™Ã©cosystÃ¨me data.\n\n\nâ„¹ï¸ Le savais-tu ?\nLe mot Bash signifie â€œBourne Again SHellâ€, un jeu de mots sur :\n\nLe shell Unix original : le Bourne Shell (sh), dÃ©veloppÃ© dans les annÃ©es 1970 par Stephen Bourne\nLâ€™expression anglaise â€œborn againâ€ = renaÃ®tre\n\nBash est donc une nouvelle version amÃ©liorÃ©e du shell Bourne, libre, puissante, et utilisÃ©e par dÃ©faut dans la plupart des systÃ¨mes Unix/Linux modernes.\nğŸ“– Biographie de Stephen R. Bourne sur Wikipedia",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ’» Comment accÃ©der Ã  Bash ?",
    "text": "ğŸ’» Comment accÃ©der Ã  Bash ?\n\n\n\n\n\n\n\nSystÃ¨me\nComment y accÃ©der\n\n\n\n\nğŸ§ Linux\nBash est installÃ© par dÃ©faut. Ouvre un Terminal\n\n\nğŸ macOS\nOuvre Terminal (Applications â†’ Utilitaires â†’ Terminal)\n\n\nğŸªŸ Windows\nInstalle WSL (Windows Subsystem for Linux) ou Git Bash\n\n\n\n\nInstallation de WSL sur Windows\n# Dans PowerShell en administrateur\nwsl --install\nAprÃ¨s redÃ©marrage, tu auras accÃ¨s Ã  un terminal Linux complet !\n\n\nVÃ©rifier ta version de Bash\nbash --version",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“ 1. Navigation & exploration",
    "text": "ğŸ“ 1. Navigation & exploration\nLes commandes de base pour se dÃ©placer dans lâ€™arborescence :\n\n\nCode\n%%bash\n# Affiche le chemin du dossier courant (Print Working Directory)\npwd\n\n# Liste les fichiers du dossier courant\nls\n\n# Liste avec dÃ©tails (permissions, taille, date)\nls -lh\n\n# Liste incluant les fichiers cachÃ©s\nls -la\n\n# Affiche l'arborescence (si installÃ©)\n# tree\n\n# Change de dossier\ncd /tmp\npwd\n\n# Revenir au dossier prÃ©cÃ©dent\ncd -\n\n# Aller au dossier home\ncd ~\n\n\n\nğŸ”‘ Raccourcis de navigation essentiels\n\n\n\nSymbole\nSignification\nExemple\n\n\n\n\n.\nDossier courant\n./script.sh\n\n\n..\nDossier parent\ncd ..\n\n\n~\nDossier home\ncd ~\n\n\n/\nRacine du systÃ¨me\ncd /\n\n\n-\nDossier prÃ©cÃ©dent\ncd -",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“‚ 2. CrÃ©ation et manipulation de fichiers",
    "text": "ğŸ“‚ 2. CrÃ©ation et manipulation de fichiers\nCrÃ©er, copier, dÃ©placer, supprimer :\n\n\nCode\n%%bash\n# CrÃ©er un dossier\nmkdir data\n\n# CrÃ©er un dossier avec ses parents (pas d'erreur si existe)\nmkdir -p data/raw/2024\n\n# CrÃ©er un fichier vide\ntouch data/fichier.csv\n\n# CrÃ©er plusieurs fichiers\ntouch data/file1.csv data/file2.csv data/file3.csv\n\n# Copier un fichier\ncp data/fichier.csv data/fichier_backup.csv\n\n# Copier un dossier entier (rÃ©cursif)\ncp -r data/ data_backup/\n\n# DÃ©placer / Renommer un fichier\nmv data/fichier.csv data/nouveau_nom.csv\n\n# Supprimer un fichier\nrm data/file1.csv\n\n# Supprimer un dossier vide\nrmdir data/raw/2024\n\n# Supprimer un dossier et son contenu (âš ï¸ DANGEREUX)\nrm -r data_backup/",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“„ 3. Lecture de contenu",
    "text": "ğŸ“„ 3. Lecture de contenu\nLire, afficher, compter les lignes :\n\n\nCode\n%%bash\n# CrÃ©ons d'abord un fichier exemple\ncat &lt;&lt; 'EOF' &gt; ventes.csv\ndate,produit,quantite,prix\n2024-01-01,Laptop,5,999.99\n2024-01-02,Souris,20,29.99\n2024-01-03,Clavier,15,79.99\n2024-01-04,Ã‰cran,8,299.99\n2024-01-05,Laptop,3,999.99\n2024-01-06,Souris,25,29.99\n2024-01-07,Casque,12,149.99\nEOF\n\necho \"âœ… Fichier ventes.csv crÃ©Ã©\"\n\n\n\n\nCode\n%%bash\n# Affiche le contenu entier\necho \"=== cat ===\"\ncat ventes.csv\n\necho \"\"\necho \"=== head (3 premiÃ¨res lignes) ===\"\nhead -n 3 ventes.csv\n\necho \"\"\necho \"=== tail (2 derniÃ¨res lignes) ===\"\ntail -n 2 ventes.csv\n\necho \"\"\necho \"=== wc (comptage) ===\"\nwc -l ventes.csv    # Nombre de lignes\nwc -w ventes.csv    # Nombre de mots\nwc -c ventes.csv    # Nombre de caractÃ¨res\n\n\n\nğŸ“– Lire des gros fichiers avec less\nPour les fichiers volumineux, utilise less qui permet de naviguer :\nless gros_fichier.csv\n\n\n\nTouche\nAction\n\n\n\n\nâ†“ ou j\nLigne suivante\n\n\nâ†‘ ou k\nLigne prÃ©cÃ©dente\n\n\nSpace\nPage suivante\n\n\nb\nPage prÃ©cÃ©dente\n\n\n/mot\nRechercher â€œmotâ€\n\n\nn\nOccurrence suivante\n\n\nq\nQuitter",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ” 4. Recherche & filtrage",
    "text": "ğŸ” 4. Recherche & filtrage\nExtraire des informations prÃ©cises â€” essentiel pour un Data Engineer !\n\n\nCode\n%%bash\necho \"=== grep : recherche de motifs ===\"\n\n# Rechercher les lignes contenant \"Laptop\"\necho \"Lignes avec 'Laptop':\"\ngrep \"Laptop\" ventes.csv\n\necho \"\"\n# Recherche insensible Ã  la casse\necho \"Recherche insensible Ã  la casse (-i):\"\ngrep -i \"laptop\" ventes.csv\n\necho \"\"\n# Compter le nombre de correspondances\necho \"Nombre de lignes avec 'Souris':\"\ngrep -c \"Souris\" ventes.csv\n\necho \"\"\n# Afficher les numÃ©ros de ligne\necho \"Avec numÃ©ros de ligne (-n):\"\ngrep -n \"99.99\" ventes.csv\n\necho \"\"\n# Inverser la recherche (lignes qui NE contiennent PAS)\necho \"Lignes SANS 'Laptop' (-v):\"\ngrep -v \"Laptop\" ventes.csv\n\n\n\n\nCode\n%%bash\necho \"=== find : trouver des fichiers ===\"\n\n# CrÃ©er quelques fichiers pour l'exemple\nmkdir -p projet/data projet/scripts\ntouch projet/data/users.csv projet/data/sales.csv projet/data/old.json\ntouch projet/scripts/etl.py projet/scripts/utils.py\n\n# Trouver tous les fichiers .csv\necho \"Fichiers .csv:\"\nfind projet/ -name \"*.csv\"\n\necho \"\"\n# Trouver tous les fichiers .py\necho \"Fichiers .py:\"\nfind projet/ -name \"*.py\"\n\necho \"\"\n# Trouver les fichiers modifiÃ©s dans les derniÃ¨res 24h\necho \"Fichiers modifiÃ©s rÃ©cemment:\"\nfind projet/ -mtime -1 -type f\n\n# Nettoyage\nrm -r projet/\n\n\n\n\nCode\n%%bash\necho \"=== cut : extraire des colonnes ===\"\n\n# Extraire la 2Ã¨me colonne (produit)\necho \"Colonne 'produit':\"\ncut -d',' -f2 ventes.csv\n\necho \"\"\necho \"=== sort : trier ===\"\n# Trier par produit (2Ã¨me colonne)\necho \"TriÃ© par produit:\"\ntail -n +2 ventes.csv | sort -t',' -k2\n\necho \"\"\necho \"=== uniq : valeurs uniques ===\"\n# Liste des produits uniques\necho \"Produits uniques:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq\n\necho \"\"\n# Compter les occurrences\necho \"Comptage par produit:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ”— 5. Pipes & redirections",
    "text": "ğŸ”— 5. Pipes & redirections\nLe pipe (|) est lâ€™outil le plus puissant de Bash : il permet de chaÃ®ner des commandes en envoyant la sortie dâ€™une commande vers lâ€™entrÃ©e de la suivante.\n\n\nCode\n%%bash\necho \"=== Exemples de pipes ===\"\n\n# Trouver les ventes de Laptop et compter\necho \"Nombre de ventes Laptop:\"\ncat ventes.csv | grep \"Laptop\" | wc -l\n\necho \"\"\n# Top 3 des produits les plus vendus\necho \"Top 3 produits (par nombre de lignes):\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c | sort -rn | head -3\n\necho \"\"\n# Pipeline complexe : produits avec prix &gt; 100\necho \"Produits avec prix &gt; 100:\"\ntail -n +2 ventes.csv | awk -F',' '$4 &gt; 100 {print $2, $4}' | sort -u\n\n\n\n\nCode\n%%bash\necho \"=== Redirections ===\"\n\n# Rediriger vers un fichier (Ã©crase)\ngrep \"Laptop\" ventes.csv &gt; laptops.txt\necho \"Contenu de laptops.txt:\"\ncat laptops.txt\n\necho \"\"\n# Ajouter Ã  un fichier (append)\ngrep \"Ã‰cran\" ventes.csv &gt;&gt; laptops.txt\necho \"AprÃ¨s ajout:\"\ncat laptops.txt\n\necho \"\"\n# Rediriger les erreurs\nls fichier_inexistant 2&gt; erreurs.log\necho \"Erreur capturÃ©e:\"\ncat erreurs.log\n\n# Nettoyage\nrm -f laptops.txt erreurs.log\n\n\n\nğŸ“‹ RÃ©capitulatif des redirections\n\n\n\n\n\n\n\n\nSymbole\nDescription\nExemple\n\n\n\n\n&gt;\nRedirige stdout vers fichier (Ã©crase)\necho \"hello\" &gt; file.txt\n\n\n&gt;&gt;\nRedirige stdout vers fichier (ajoute)\necho \"world\" &gt;&gt; file.txt\n\n\n2&gt;\nRedirige stderr vers fichier\ncmd 2&gt; errors.log\n\n\n&&gt;\nRedirige stdout ET stderr\ncmd &&gt; all.log\n\n\n&lt;\nUtilise fichier comme entrÃ©e\nwc -l &lt; file.txt\n\n\n\\|\nPipe : stdout â†’ stdin suivant\ncat file \\| grep mot",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ” 6. Variables et boucles",
    "text": "ğŸ” 6. Variables et boucles\nAutomatiser avec des scripts bash :\n\n\nCode\n%%bash\necho \"=== Variables ===\"\n\n# DÃ©clarer une variable (PAS d'espace autour du =)\nnom=\"Data Engineer\"\nannee=2024\ndossier_data=\"/home/user/data\"\n\n# Utiliser une variable avec $\necho \"Bienvenue $nom !\"\necho \"Nous sommes en $annee\"\n\n# Utiliser ${} pour Ã©viter l'ambiguÃ¯tÃ©\necho \"Fichier: ${dossier_data}/ventes.csv\"\n\necho \"\"\necho \"=== Variables d'environnement ===\"\necho \"Home: $HOME\"\necho \"User: $USER\"\necho \"Shell: $SHELL\"\necho \"Path: $PATH\" | cut -c1-50  # TronquÃ© pour l'affichage\n\n\n\n\nCode\n%%bash\necho \"=== Boucle for ===\"\n\n# CrÃ©er des fichiers de test\nmkdir -p data_test\ntouch data_test/jan.csv data_test/feb.csv data_test/mar.csv\n\n# Boucle sur les fichiers CSV\nfor fichier in data_test/*.csv; do\n    echo \"ğŸ“„ Traitement de: $fichier\"\n    echo \"   Nom: $(basename \"$fichier\")\"\ndone\n\necho \"\"\necho \"=== Boucle avec sÃ©quence ===\"\nfor i in {1..5}; do\n    echo \"ItÃ©ration $i\"\ndone\n\necho \"\"\necho \"=== Boucle while ===\"\ncompteur=1\nwhile [ $compteur -le 3 ]; do\n    echo \"Compteur: $compteur\"\n    ((compteur++))\ndone\n\n# Nettoyage\nrm -r data_test/",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ”€ 7. Conditions (if/else)",
    "text": "ğŸ”€ 7. Conditions (if/else)\nPrendre des dÃ©cisions dans tes scripts :\n\n\nCode\n%%bash\necho \"=== Conditions de base ===\"\n\n# VÃ©rifier si un fichier existe\nif [ -f \"ventes.csv\" ]; then\n    echo \"âœ… Le fichier ventes.csv existe\"\nelse\n    echo \"âŒ Le fichier n'existe pas\"\nfi\n\necho \"\"\n# VÃ©rifier si un dossier existe\nif [ -d \"/tmp\" ]; then\n    echo \"âœ… Le dossier /tmp existe\"\nfi\n\necho \"\"\n# Comparer des nombres\nnb_lignes=$(wc -l &lt; ventes.csv)\necho \"Nombre de lignes: $nb_lignes\"\n\nif [ $nb_lignes -gt 5 ]; then\n    echo \"ğŸ“Š Fichier volumineux (&gt; 5 lignes)\"\nelse\n    echo \"ğŸ“„ Petit fichier\"\nfi\n\n\n\nğŸ“‹ OpÃ©rateurs de test\n\n\n\nTest fichiers\nDescription\n\n\n\n\n-f fichier\nFichier existe\n\n\n-d dossier\nDossier existe\n\n\n-r fichier\nFichier lisible\n\n\n-w fichier\nFichier modifiable\n\n\n-s fichier\nFichier non vide\n\n\n\n\n\n\nTest nombres\nDescription\n\n\n\n\n-eq\nÃ‰gal\n\n\n-ne\nDiffÃ©rent\n\n\n-gt\nPlus grand que\n\n\n-lt\nPlus petit que\n\n\n-ge\nPlus grand ou Ã©gal\n\n\n-le\nPlus petit ou Ã©gal\n\n\n\n\n\n\nTest chaÃ®nes\nDescription\n\n\n\n\n=\nÃ‰gal\n\n\n!=\nDiffÃ©rent\n\n\n-z\nChaÃ®ne vide\n\n\n-n\nChaÃ®ne non vide",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âš™ï¸ 8. CrÃ©er et exÃ©cuter un script Bash",
    "text": "âš™ï¸ 8. CrÃ©er et exÃ©cuter un script Bash\nUn script Bash est simplement un fichier texte contenant des commandes :\n\n\nCode\n%%bash\n# CrÃ©er un script complet\ncat &lt;&lt; 'EOF' &gt; mon_script.sh\n#!/bin/bash\n# Script de traitement de donnÃ©es\n# Auteur: Data Engineer\n# Date: 2024\n\necho \"ğŸš€ DÃ©marrage du script\"\necho \"ğŸ“… Date: $(date)\"\necho \"ğŸ‘¤ Utilisateur: $USER\"\necho \"ğŸ“‚ Dossier: $(pwd)\"\n\n# VÃ©rifier si un argument est passÃ©\nif [ -z \"$1\" ]; then\n    echo \"âš ï¸ Usage: ./mon_script.sh &lt;nom_fichier&gt;\"\n    exit 1\nfi\n\necho \"ğŸ“„ Fichier Ã  traiter: $1\"\necho \"âœ… Script terminÃ©\"\nEOF\n\n# Rendre exÃ©cutable\nchmod +x mon_script.sh\n\n# ExÃ©cuter le script\necho \"=== ExÃ©cution sans argument ===\"\n./mon_script.sh\n\necho \"\"\necho \"=== ExÃ©cution avec argument ===\"\n./mon_script.sh ventes.csv\n\n# Nettoyage\nrm mon_script.sh",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "â° 9. Automatisation avec Cron",
    "text": "â° 9. Automatisation avec Cron\nCron permet de planifier lâ€™exÃ©cution automatique de scripts â€” indispensable pour les pipelines ETL !\n\nğŸ“… Format dâ€™une ligne crontab\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande Ã  exÃ©cuter\n\n\nğŸ”§ Exemples courants pour Data Engineers\n\n\n\nExpression\nDescription\nCas dâ€™usage\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h\nETL nocturne\n\n\n*/15 * * * *\nToutes les 15 minutes\nMonitoring\n\n\n0 0 * * 0\nChaque dimanche Ã  minuit\nRapport hebdomadaire\n\n\n0 9 1 * *\nLe 1er de chaque mois Ã  9h\nRapport mensuel\n\n\n0 */4 * * *\nToutes les 4 heures\nSynchronisation donnÃ©es\n\n\n\n\n\nğŸ’» Commandes cron\n# Ã‰diter la crontab\ncrontab -e\n\n# Lister les jobs planifiÃ©s\ncrontab -l\n\n# Supprimer tous les jobs\ncrontab -r\n\n\nğŸ“ Exemple de crontab pour Data Engineer\n# ETL quotidien Ã  2h du matin\n0 2 * * * /home/user/scripts/etl_pipeline.sh &gt;&gt; /var/log/etl.log 2&gt;&1\n\n# Backup des donnÃ©es chaque dimanche Ã  3h\n0 3 * * 0 /home/user/scripts/backup.sh\n\n# Nettoyage des fichiers temporaires chaque jour Ã  4h\n0 4 * * * find /tmp -mtime +7 -delete\n\nğŸ’¡ Astuce : Utilise crontab.guru pour gÃ©nÃ©rer facilement des expressions cron !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âœ… Exercice pratique",
    "text": "âœ… Exercice pratique\n\nğŸ§  Instructions\n\nCrÃ©e un dossier de travail nommÃ© mon_premier_script\nEntre dans ce dossier\nCrÃ©e un fichier script appelÃ© bonjour.sh\nÃ‰dite ce fichier et Ã©cris un script qui :\n\nAffiche â€œBonjour Data Engineer ğŸ‘‹â€\nAffiche la date du jour\nTe souhaite une bonne session\n\nRends le script exÃ©cutable\nCrÃ©e un sous-dossier nommÃ© data/ et place-y quelques fichiers .csv (mÃªme vides)\nAjoute une Ã©tape dans le script pour :\n\nAfficher tous les fichiers .csv prÃ©sents dans le dossier data/\nPour chaque fichier .csv, afficher son nom avec un message comme :\nğŸ‘‰ â€œFichier trouvÃ© : nom_du_fichier.csv âœ…â€\n\n\nğŸ“Œ Quelle structure utiliser ? (indice : boucle for)\n\n\nâœ… Correction\n\n\nğŸ“¥ Afficher la correction complÃ¨te\n\n#!/bin/bash\n\n# 1. ğŸ“¢ Afficher un message de bienvenue\necho \"Bonjour Data Engineer ğŸ‘‹\"\n\n# 2. ğŸ—“ï¸ Afficher la date du jour\necho \"Date: $(date)\"\n\n# 3. ğŸ’¬ Souhaiter une bonne session\necho \"Bonne session de travail ğŸ’ª\"\n\n# 4. ğŸ“ CrÃ©er le dossier 'data/' s'il n'existe pas\nmkdir -p data\n\n# 5. ğŸ—‚ï¸ CrÃ©er quelques fichiers de test\ntouch data/fichier1.csv data/fichier2.csv data/fichier3.csv\n\n# 6. ğŸ” Lister les fichiers CSV\necho \"\"\necho \"ğŸ” Recherche de fichiers CSV dans ./data...\"\n\nfor fichier in data/*.csv; do\n    if [ -f \"$fichier\" ]; then\n        echo \"Fichier trouvÃ© : $(basename \"$fichier\") âœ…\"\n    fi\ndone\n\n# 7. âœ… Fin du script\necho \"\"\necho \"Traitement terminÃ© âœ…\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“„ Cheatsheet Bash â€“ Commandes essentielles",
    "text": "ğŸ“„ Cheatsheet Bash â€“ Commandes essentielles\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nğŸ“ Navigation\npwd\nAffiche le chemin actuel\n\n\n\ncd dossier/\nSe dÃ©placer dans un dossier\n\n\n\nls -lh\nListe les fichiers avec dÃ©tails\n\n\nğŸ“„ Fichiers\ntouch nom.txt\nCrÃ©er un fichier vide\n\n\n\ncp fichier.txt dossier/\nCopier un fichier\n\n\n\nmv fichier.txt nouveau.txt\nRenommer ou dÃ©placer\n\n\n\nrm fichier.txt\nSupprimer un fichier\n\n\nğŸ“š Dossiers\nmkdir dossier/\nCrÃ©er un dossier\n\n\n\nmkdir -p a/b/c\nCrÃ©er avec parents\n\n\n\nrm -r dossier/\nSupprimer dossier + contenu\n\n\nğŸ“– Lecture\ncat fichier.txt\nAfficher tout le contenu\n\n\n\nhead -n 10 fichier.txt\n10 premiÃ¨res lignes\n\n\n\ntail -n 10 fichier.txt\n10 derniÃ¨res lignes\n\n\n\nwc -l fichier.txt\nCompter les lignes\n\n\nğŸ” Recherche\ngrep \"mot\" fichier.txt\nRechercher un mot\n\n\n\nfind . -name \"*.csv\"\nTrouver des fichiers\n\n\n\ncut -d',' -f1 fichier.csv\nExtraire une colonne\n\n\nğŸ”— Pipes\ncmd1 \\| cmd2\nChaÃ®ner des commandes\n\n\n\ncmd &gt; fichier.txt\nRediriger vers fichier\n\n\n\ncmd &gt;&gt; fichier.txt\nAjouter Ã  un fichier\n\n\nğŸ§  Scripts\nchmod +x script.sh\nRendre exÃ©cutable\n\n\n\n./script.sh\nLancer un script\n\n\nğŸ”„ Boucles\nfor f in *.csv; do ...; done\nBoucle sur fichiers\n\n\nâ° Cron\ncrontab -e\nÃ‰diter les tÃ¢ches planifiÃ©es\n\n\n\ncrontab -l\nLister les tÃ¢ches\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Bash Cheatsheet PDF (fr)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âš ï¸ Erreurs classiques Ã  Ã©viter",
    "text": "âš ï¸ Erreurs classiques Ã  Ã©viter\n\n\n\n\n\n\n\n\nâŒ Erreur\nğŸ’¥ ConsÃ©quence\nâœ… Bonne pratique\n\n\n\n\nrm -rf /\nSupprime TOUT le systÃ¨me !\nToujours vÃ©rifier le chemin avant rm -rf\n\n\n$fichier sans guillemets\nBug si espaces dans le nom\nUtiliser \"$fichier\"\n\n\nsudo sans rÃ©flÃ©chir\nÃ‰crase des fichiers systÃ¨me\nComprendre la commande avant dâ€™utiliser sudo\n\n\nScript non testÃ© en prod\nPerte de donnÃ©es\nToujours tester en sandbox dâ€™abord\n\n\nVAR = valeur (avec espaces)\nErreur de syntaxe\nVAR=valeur (sans espaces)\n\n\nOublier #!/bin/bash\nScript peut mal sâ€™exÃ©cuter\nToujours commencer par le shebang\n\n\n\n\nğŸ§  Conseil : Avant dâ€™exÃ©cuter une commande destructive (rm, mv), utilise echo pour voir ce qui serait affectÃ© :\n# Au lieu de :\nrm -rf data/*.csv\n\n# D'abord tester avec :\necho data/*.csv",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle commande affiche le chemin du dossier courant ?\n\ncd\n\npwd\n\nls\n\npath\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” pwd = Print Working Directory\n\n\n\n\nâ“ Q2. Que fait la commande rm -rf mon_dossier/ ?\n\nRedÃ©marre lâ€™ordinateur\n\nRÃ©organise un fichier\n\nSupprime un dossier et son contenu\n\nReformate le disque\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” -r = rÃ©cursif, -f = force (sans confirmation)\n\n\n\n\nâ“ Q3. Quelle commande affiche les 10 premiÃ¨res lignes dâ€™un fichier ?\n\nhead -n 10\n\ncat -10\n\nstart 10\n\ntop 10\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” head -n 10 fichier.txt\n\n\n\n\nâ“ Q4. Pourquoi Ã©crire \"$fichier\" au lieu de $fichier ?\n\nPour que Bash reconnaisse les fichiers CSV\n\nPour faire du style\n\nPour Ã©viter les bugs avec les noms contenant des espaces\n\nÃ‡a nâ€™a pas dâ€™importance\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les guillemets protÃ¨gent les valeurs contenant des espaces\n\n\n\n\nâ“ Q5. Que signifie le | (pipe) en Bash ?\n\nInterrompre une commande\n\nExÃ©cuter un script\n\nEnvoyer la sortie dâ€™une commande vers lâ€™entrÃ©e dâ€™une autre\n\nCrÃ©er un fichier temporaire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le pipe chaÃ®ne les commandes : cmd1 | cmd2\n\n\n\n\nâ“ Q6. Quelle expression cron exÃ©cute un script tous les jours Ã  2h du matin ?\n\n2 0 * * *\n\n0 2 * * *\n\n* 2 * * *\n\n0 0 2 * *\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Format : minute heure jour mois jour_semaine",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸš€ Mini-projet : Archiver intelligemment des fichiers CSV",
    "text": "ğŸš€ Mini-projet : Archiver intelligemment des fichiers CSV\n\nğŸ¯ Objectif\nCrÃ©er un script Bash rÃ©aliste qui automatise lâ€™archivage de fichiers .csv selon leur anciennetÃ©.\n\n\nğŸ”§ Contexte\nTu travailles dans une Ã©quipe data. Chaque jour, des fichiers .csv sont dÃ©posÃ©s dans un dossier data/.\nTu dois crÃ©er un script qui :\n\nğŸ“¦ RepÃ¨re tous les fichiers .csv modifiÃ©s il y a plus de 7 jours\nğŸ—‚ï¸ Les archive dans un fichier .tar.gz nommÃ© archive_YYYYMMDD.tar.gz\nğŸ§¹ DÃ©place ces fichiers dans un dossier archive/\n\n\n\nğŸ§  Contraintes\n\nLe script doit fonctionner mÃªme si aucun fichier nâ€™est Ã©ligible\nLâ€™archive doit Ãªtre horodatÃ©e automatiquement\nLe dossier archive/ doit Ãªtre crÃ©Ã© sâ€™il nâ€™existe pas\nAjouter du logging pour tracer les actions\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n#!/bin/bash\n#\n# Script: archive_csv.sh\n# Description: Archive les fichiers CSV de plus de 7 jours\n# Auteur: Data Engineer\n#\n\n# Configuration\nDATA_DIR=\"./data\"\nARCHIVE_DIR=\"./archive\"\nDAYS_OLD=7\nDATE_TAG=$(date +%Y%m%d)\nARCHIVE_NAME=\"archive_${DATE_TAG}.tar.gz\"\nLOG_FILE=\"archive.log\"\n\n# Fonction de logging\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog \"ğŸš€ DÃ©marrage du script d'archivage\"\n\n# VÃ©rifier que le dossier source existe\nif [ ! -d \"$DATA_DIR\" ]; then\n    log \"âŒ Erreur: Le dossier $DATA_DIR n'existe pas\"\n    exit 1\nfi\n\n# CrÃ©er le dossier d'archive si nÃ©cessaire\nmkdir -p \"$ARCHIVE_DIR\"\nlog \"ğŸ“ Dossier d'archive: $ARCHIVE_DIR\"\n\n# Trouver les fichiers CSV de plus de 7 jours\nOLD_FILES=$(find \"$DATA_DIR\" -name \"*.csv\" -mtime +$DAYS_OLD -type f)\n\n# VÃ©rifier s'il y a des fichiers Ã  archiver\nif [ -z \"$OLD_FILES\" ]; then\n    log \"â„¹ï¸ Aucun fichier CSV de plus de $DAYS_OLD jours trouvÃ©\"\n    exit 0\nfi\n\n# Compter les fichiers\nNB_FILES=$(echo \"$OLD_FILES\" | wc -l)\nlog \"ğŸ“Š $NB_FILES fichier(s) Ã  archiver\"\n\n# CrÃ©er l'archive\nlog \"ğŸ“¦ CrÃ©ation de l'archive $ARCHIVE_NAME...\"\necho \"$OLD_FILES\" | tar -czvf \"$ARCHIVE_DIR/$ARCHIVE_NAME\" -T -\n\nif [ $? -eq 0 ]; then\n    log \"âœ… Archive crÃ©Ã©e avec succÃ¨s\"\n    \n    # DÃ©placer les fichiers archivÃ©s\n    for file in $OLD_FILES; do\n        mv \"$file\" \"$ARCHIVE_DIR/\"\n        log \"   â†³ DÃ©placÃ©: $(basename \"$file\")\"\n    done\n    \n    log \"ğŸ‰ Archivage terminÃ© avec succÃ¨s\"\nelse\n    log \"âŒ Erreur lors de la crÃ©ation de l'archive\"\n    exit 1\nfi\nPour lâ€™utiliser :\nchmod +x archive_csv.sh\n./archive_csv.sh\nPour lâ€™automatiser avec cron (tous les jours Ã  3h) :\n0 3 * * * /home/user/scripts/archive_csv.sh",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Sites & outils\n\nExplainShell â€” Explique nâ€™importe quelle commande Bash\nShellCheck â€” VÃ©rifie la syntaxe de tes scripts\nCrontab Guru â€” GÃ©nÃ©rateur dâ€™expressions cron\nLinux Command â€” Tutoriel complet\n\n\n\nğŸ“– Documentation\n\nGNU Bash Manual\nAdvanced Bash-Scripting Guide\n\n\n\nğŸ® Pratique\n\nOverTheWire - Bandit â€” Jeu pour apprendre Bash\nCmdchallenge â€” DÃ©fis en ligne de commande",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Bash, passons Ã  un autre outil essentiel : Git !\nğŸ‘‰ Module suivant : 03_git_for_data_engineers.ipynb â€” Versionner ton code et collaborer\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Bash pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  dÃ©ployer et gÃ©rer des charges de travail data sur Kubernetes. Tu dÃ©couvriras les patterns avancÃ©s pour les Jobs ETL, les bases de donnÃ©es, le scaling et le monitoring â€” le tout appliquÃ© au Data Engineering !",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#prÃ©requis",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#prÃ©requis",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 15_kubernetes_fundamentals\n\n\nâœ… Requis\nMaÃ®triser Pod, Deployment, Service, ConfigMap, Secret, PVC\n\n\nâœ… Requis\nSavoir utiliser kubectl\n\n\nğŸ’¡ RecommandÃ©\nCluster K8s local fonctionnel (Docker Desktop ou Minikube)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#objectifs-du-module",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#objectifs-du-module",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre ce quâ€™est un workload data et ses caractÃ©ristiques\nConfigurer des Jobs et CronJobs avancÃ©s pour des ETL robustes\nDÃ©ployer des bases de donnÃ©es avec StatefulSets\nUtiliser Helm pour dÃ©ployer des stacks data\nGÃ©rer le scaling et les ressources pour des workloads gourmands\nMettre en place le monitoring de tes pipelines\nAvoir un aperÃ§u de Spark et Airflow sur Kubernetes",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#cest-quoi-un-workload-data",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#cest-quoi-un-workload-data",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ§  Câ€™est quoi un â€œWorkload Dataâ€ ?",
    "text": "ğŸ§  Câ€™est quoi un â€œWorkload Dataâ€ ?\n\nğŸ“Š Un workload data (charge de travail data) dÃ©signe toute tÃ¢che ou application dÃ©diÃ©e au traitement, transformation, ou dÃ©placement de donnÃ©es.\n\nEn Data Engineering, les workloads typiques incluent :\n\n\n\n\n\n\n\n\nType\nDescription\nExemple concret\n\n\n\n\nBatch ETL\nTraitement planifiÃ© de donnÃ©es\nJob Python qui transforme des CSV chaque nuit\n\n\nIngestion\nChargement de donnÃ©es dans un systÃ¨me\nCSV â†’ PostgreSQL, API â†’ Data Lake\n\n\nTransformation\nCalculs et agrÃ©gations\nJointures, agrÃ©gations, nettoyage\n\n\nProcessing lourd\nCalculs intensifs en ressources\nFeature engineering, ML preprocessing\n\n\nOrchestration\nCoordination de plusieurs tÃ¢ches\nDAG Airflow avec 10 Ã©tapes\n\n\n\n\nğŸ¯ CaractÃ©ristiques des workloads data\n\n\n\n\n\n\n\nCaractÃ©ristique\nExplication\n\n\n\n\nÃ‰phÃ©mÃ¨res\nSâ€™exÃ©cutent puis se terminent (run-to-completion)\n\n\nGourmands\nBesoin de CPU et RAM significatifs\n\n\nI/O intensifs\nLecture/Ã©criture de grandes quantitÃ©s de donnÃ©es\n\n\nPlanifiÃ©s\nSouvent exÃ©cutÃ©s selon un schedule (quotidien, horaire)\n\n\nReproductibles\nDoivent pouvoir Ãªtre relancÃ©s en cas dâ€™Ã©chec\n\n\n\n\n\nğŸ”„ Workloads data vs Applications classiques\n\n\n\nAspect\nApplication web\nWorkload data\n\n\n\n\nDurÃ©e de vie\nContinue (24/7)\nÃ‰phÃ©mÃ¨re (minutes/heures)\n\n\nRessource K8s\nDeployment\nJob / CronJob\n\n\nScaling\nHorizontal (replicas)\nVertical (plus de RAM/CPU)\n\n\nÃ‰tat final\nToujours running\nCompleted ou Failed\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nLe terme â€œworkloadâ€ vient du monde des mainframes IBM des annÃ©es 1960, oÃ¹ il dÃ©signait la quantitÃ© de travail quâ€™une machine devait traiter.\nAujourdâ€™hui, dans le contexte cloud-native et Kubernetes, un workload dÃ©signe toute unitÃ© de travail dÃ©ployÃ©e sur un cluster : une app web, un job batch, un service de streaming, etc.\nLes â€œdata workloadsâ€ sont simplement les workloads spÃ©cialisÃ©s dans le traitement de donnÃ©es !",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#rappels-kubernetes-essentiels",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#rappels-kubernetes-essentiels",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ”‘ 1. Rappels Kubernetes essentiels",
    "text": "ğŸ”‘ 1. Rappels Kubernetes essentiels\nAvant dâ€™aller plus loin, voici un rÃ©capitulatif rapide des concepts K8s vus dans le module prÃ©cÃ©dent :\n\n\n\n\n\n\n\n\nRessource\nRÃ´le\nUsage Data Engineering\n\n\n\n\nPod\nUnitÃ© de base (1+ containers)\nExÃ©cute ton script ETL\n\n\nDeployment\nGÃ¨re des replicas de pods\nApps long-running (API, workers)\n\n\nJob\nTÃ¢che one-shot\nETL ponctuel, migration\n\n\nCronJob\nJob planifiÃ©\nETL quotidien, rapport hebdo\n\n\nService\nExpose des pods\nAccÃ¨s Ã  PostgreSQL, APIs\n\n\nConfigMap\nConfig non sensible\nChemins, URLs, paramÃ¨tres\n\n\nSecret\nConfig sensible\nPasswords, API keys\n\n\nPVC\nStockage persistant\nDonnÃ©es PostgreSQL, fichiers\n\n\nNamespace\nIsolation logique\nUn namespace par projet\n\n\n\n\nğŸ’¡ Si ces concepts ne sont pas clairs, revois le module 15_kubernetes_fundamentals avant de continuer.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#jobs-cronjobs-avancÃ©s",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#jobs-cronjobs-avancÃ©s",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ”„ 2. Jobs & CronJobs avancÃ©s",
    "text": "ğŸ”„ 2. Jobs & CronJobs avancÃ©s\nLes Jobs et CronJobs sont les ressources K8s idÃ©ales pour les workloads data.\n\n2.1 Anatomie complÃ¨te dâ€™un Job\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-advanced-job\nspec:\n  # Nombre d'exÃ©cutions rÃ©ussies requises\n  completions: 1\n  \n  # Nombre de pods en parallÃ¨le\n  parallelism: 1\n  \n  # Nombre de retries avant Ã©chec dÃ©finitif\n  backoffLimit: 3\n  \n  # Timeout global du job (en secondes)\n  activeDeadlineSeconds: 3600  # 1 heure max\n  \n  # Auto-suppression aprÃ¨s completion (cleanup)\n  ttlSecondsAfterFinished: 86400  # SupprimÃ© aprÃ¨s 24h\n  \n  template:\n    spec:\n      containers:\n      - name: etl\n        image: my-etl:1.0\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n      restartPolicy: OnFailure\n\n\nğŸ“Š ParamÃ¨tres clÃ©s expliquÃ©s\n\n\n\nParamÃ¨tre\nDescription\nValeur typique ETL\n\n\n\n\ncompletions\nNombre de succÃ¨s requis\n1 (une seule exÃ©cution)\n\n\nparallelism\nPods simultanÃ©s\n1 Ã  N (selon le use case)\n\n\nbackoffLimit\nRetries avant Ã©chec\n3-5\n\n\nactiveDeadlineSeconds\nTimeout global\n1800-7200 (30min-2h)\n\n\nttlSecondsAfterFinished\nAuto-cleanup\n86400 (24h)\n\n\n\n\n\n2.2 Patterns ETL avec parallelism\nPattern : Traitement de plusieurs fichiers en parallÃ¨le\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-parallel\nspec:\n  completions: 10      # 10 fichiers Ã  traiter\n  parallelism: 3       # 3 pods en parallÃ¨le\n  completionMode: Indexed  # Chaque pod reÃ§oit un index (0-9)\n  template:\n    spec:\n      containers:\n      - name: etl\n        image: my-etl:1.0\n        command: [\"python\", \"etl.py\"]\n        env:\n        - name: FILE_INDEX\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n      restartPolicy: OnFailure\n\n\n2.3 CronJob avancÃ©\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h\n  \n  # Que faire si le job prÃ©cÃ©dent n'est pas terminÃ© ?\n  concurrencyPolicy: Forbid  # Ne pas lancer si le prÃ©cÃ©dent tourne\n  # Autres options : Allow, Replace\n  \n  # Historique conservÃ©\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 2\n  \n  # Deadline pour dÃ©marrer (si le scheduler est en retard)\n  startingDeadlineSeconds: 300  # 5 minutes de tolÃ©rance\n  \n  jobTemplate:\n    spec:\n      backoffLimit: 3\n      activeDeadlineSeconds: 3600\n      template:\n        spec:\n          containers:\n          - name: etl\n            image: my-etl:1.0\n            envFrom:\n            - configMapRef:\n                name: etl-config\n          restartPolicy: OnFailure\n\n\nğŸ“… Expressions cron courantes\n\n\n\nExpression\nSignification\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h00\n\n\n0 */6 * * *\nToutes les 6 heures\n\n\n*/15 * * * *\nToutes les 15 minutes\n\n\n0 0 * * 0\nTous les dimanches Ã  minuit\n\n\n0 8 1 * *\nLe 1er de chaque mois Ã  8h\n\n\n\n\n\n2.4 ParamÃ©trage via ConfigMap\n# etl-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\ndata:\n  INPUT_PATH: \"/data/raw/\"\n  OUTPUT_PATH: \"/data/processed/\"\n  DATE_FORMAT: \"%Y-%m-%d\"\n  BATCH_SIZE: \"10000\"\n  LOG_LEVEL: \"INFO\"\nUtilisation dans le Job :\nspec:\n  containers:\n  - name: etl\n    envFrom:\n    - configMapRef:\n        name: etl-config\n\n\nCode\n%%bash\n# Commandes utiles pour les Jobs\n\necho \"=== Lister les Jobs ===\"\nkubectl get jobs\n\necho \"\"\necho \"=== Lister les CronJobs ===\"\nkubectl get cronjobs\n\necho \"\"\necho \"=== DÃ©clencher manuellement un CronJob ===\"\necho \"kubectl create job test-etl --from=cronjob/etl-daily\"\n\necho \"\"\necho \"=== Voir les logs d'un Job ===\"\necho \"kubectl logs job/etl-job\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#statefulsets-bases-de-donnÃ©es-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#statefulsets-bases-de-donnÃ©es-sur-kubernetes",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ—„ï¸ 3. StatefulSets : Bases de donnÃ©es sur Kubernetes",
    "text": "ğŸ—„ï¸ 3. StatefulSets : Bases de donnÃ©es sur Kubernetes\nLes StatefulSets sont conÃ§us pour les applications stateful (avec Ã©tat) comme les bases de donnÃ©es.\n\n3.1 Deployment vs StatefulSet\n\n\n\n\n\n\n\n\nAspect\nDeployment\nStatefulSet\n\n\n\n\nIdentitÃ© pods\nAlÃ©atoire (pod-xyz123)\nStable (pod-0, pod-1)\n\n\nStockage\nPVC partagÃ© ou Ã©phÃ©mÃ¨re\nPVC par pod\n\n\nOrdre dÃ©marrage\nParallÃ¨le\nSÃ©quentiel (0 â†’ 1 â†’ 2)\n\n\nOrdre arrÃªt\nParallÃ¨le\nInverse (2 â†’ 1 â†’ 0)\n\n\nRÃ©seau\nService ClusterIP\nHeadless Service (DNS par pod)\n\n\nUsage\nApps stateless\nBases de donnÃ©es, caches\n\n\n\n\n\n3.2 Exemple : PostgreSQL avec StatefulSet\n# postgres-statefulset.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\nspec:\n  serviceName: postgres-headless\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: \"de_db\"\n        - name: PGDATA\n          value: \"/var/lib/postgresql/data/pgdata\"\n        volumeMounts:\n        - name: postgres-data\n          mountPath: /var/lib/postgresql/data\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n  # PVC crÃ©Ã© automatiquement pour chaque replica\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 5Gi\n---\n# Headless Service (pas de ClusterIP)\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-headless\nspec:\n  clusterIP: None  # Headless !\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n---\n# Service normal pour l'accÃ¨s\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\nspec:\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n\n\n3.3 Avantages du StatefulSet\n\nIdentitÃ© stable : postgres-0 reste postgres-0 mÃªme aprÃ¨s redÃ©marrage\nPVC persistant : chaque pod a son propre volume\nDNS prÃ©visible : postgres-0.postgres-headless.namespace.svc.cluster.local\n\n\n\nâš ï¸ Recommandation production\n\nEn production, prÃ©fÃ¨re les services managÃ©s : - AWS RDS / Aurora - GCP Cloud SQL - Azure Database\nLes StatefulSets sont parfaits pour dev/test ou des cas spÃ©cifiques oÃ¹ tu veux tout contrÃ´ler. Les services managÃ©s offrent : haute disponibilitÃ©, backups automatiques, patches de sÃ©curitÃ©.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#helm-package-manager-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#helm-package-manager-kubernetes",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ“¦ 4. Helm : Package Manager Kubernetes",
    "text": "ğŸ“¦ 4. Helm : Package Manager Kubernetes\nHelm est le gestionnaire de packages pour Kubernetes â€” comme apt pour Ubuntu ou pip pour Python.\n\n4.1 Pourquoi Helm ?\n\n\n\n\n\n\n\nSans Helm\nAvec Helm\n\n\n\n\n10+ fichiers YAML Ã  gÃ©rer\n1 commande helm install\n\n\nCopier-coller entre environnements\nvalues.yaml pour personnaliser\n\n\nPas de versioning\nRollback facile\n\n\nMise Ã  jour manuelle\nhelm upgrade\n\n\n\n\n\n4.2 Concepts clÃ©s\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nChart\nPackage K8s (templates + values)\nUn package .deb ou .rpm\n\n\nRelease\nInstance dÃ©ployÃ©e dâ€™un chart\nUne installation du package\n\n\nRepository\nMagasin de charts\nUn apt repository\n\n\nValues\nConfiguration personnalisÃ©e\nUn fichier de config\n\n\n\n\n\n4.3 Installation de Helm\n# macOS\nbrew install helm\n\n# Linux\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# VÃ©rifier\nhelm version\n\n\n4.4 Commandes essentielles\n# Ajouter un repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Rechercher un chart\nhelm search repo postgresql\n\n# Voir les valeurs par dÃ©faut\nhelm show values bitnami/postgresql\n\n# Installer un chart\nhelm install my-postgres bitnami/postgresql \\\n  --namespace data \\\n  --create-namespace \\\n  --set auth.postgresPassword=mypassword\n\n# Lister les releases\nhelm list -A\n\n# Mettre Ã  jour\nhelm upgrade my-postgres bitnami/postgresql --set auth.postgresPassword=newpassword\n\n# Rollback\nhelm rollback my-postgres 1\n\n# DÃ©sinstaller\nhelm uninstall my-postgres -n data\n\n\n4.5 Fichier values.yaml personnalisÃ©\n# postgres-values.yaml\nauth:\n  postgresPassword: \"de_password\"\n  database: \"de_db\"\n\nprimary:\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"512Mi\"\n      cpu: \"500m\"\n  persistence:\n    size: 5Gi\n# Installer avec le fichier values\nhelm install my-postgres bitnami/postgresql -f postgres-values.yaml\n\n\n4.6 Charts utiles pour Data Engineering\n\n\n\n\n\n\n\n\nChart\nRepository\nUsage\n\n\n\n\nbitnami/postgresql\nbitnami\nBase de donnÃ©es relationnelle\n\n\nbitnami/redis\nbitnami\nCache, broker de messages\n\n\nminio/minio\nminio\nObject storage S3-compatible\n\n\napache-airflow/airflow\napache-airflow\nOrchestration (module 25)\n\n\nbitnami/spark\nbitnami\nSpark cluster (module 19)\n\n\n\n# Ajouter les repos utiles\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add minio https://charts.min.io/\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n\nCode\n%%bash\n# VÃ©rifier l'installation de Helm\necho \"=== Version Helm ===\"\nhelm version --short 2&gt;/dev/null || echo \"Helm non installÃ©\"\n\necho \"\"\necho \"=== Repositories configurÃ©s ===\"\nhelm repo list 2&gt;/dev/null || echo \"Aucun repo configurÃ©\"\n\necho \"\"\necho \"=== Releases dÃ©ployÃ©es ===\"\nhelm list -A 2&gt;/dev/null || echo \"Aucune release\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#storage-avancÃ©-pour-workloads-data",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#storage-avancÃ©-pour-workloads-data",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ’¾ 5. Storage avancÃ© pour Workloads Data",
    "text": "ğŸ’¾ 5. Storage avancÃ© pour Workloads Data\nLe stockage est critique pour les workloads data. Voici les patterns recommandÃ©s.\n\n5.1 Quel stockage pour quel usage ?\n\n\n\n\n\n\n\n\nUsage\nRessource K8s\nRecommandation\n\n\n\n\nBase de donnÃ©es\nPVC (StatefulSet)\nStorage rapide (SSD)\n\n\nInput data (fichiers)\nPVC ou S3/GCS\nExternalisÃ© si possible\n\n\nOutput data\nPVC ou S3/GCS\nExternalisÃ© pour durabilitÃ©\n\n\nDonnÃ©es temporaires\nemptyDir\nNettoyÃ© Ã  la fin du pod\n\n\nCache local\nemptyDir avec medium: Memory\nRAM disk ultra-rapide\n\n\n\n\n\n5.2 emptyDir : stockage Ã©phÃ©mÃ¨re\nspec:\n  containers:\n  - name: etl\n    volumeMounts:\n    - name: tmp-data\n      mountPath: /tmp/processing\n    - name: cache\n      mountPath: /cache\n  volumes:\n  # Stockage sur disque (Ã©phÃ©mÃ¨re)\n  - name: tmp-data\n    emptyDir: {}\n  # RAM disk (ultra-rapide)\n  - name: cache\n    emptyDir:\n      medium: Memory\n      sizeLimit: 256Mi\n\n\n5.3 MinIO : Object Storage S3-compatible\nMinIO permet dâ€™avoir un storage S3-compatible directement dans ton cluster K8s.\n# Installer MinIO avec Helm\nhelm repo add minio https://charts.min.io/\nhelm install minio minio/minio \\\n  --namespace storage \\\n  --create-namespace \\\n  --set rootUser=admin \\\n  --set rootPassword=password123 \\\n  --set persistence.size=10Gi\nUtilisation dans ton code Python :\nimport boto3\n\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://minio.storage.svc.cluster.local:9000',\n    aws_access_key_id='admin',\n    aws_secret_access_key='password123'\n)\n\n# Upload\ns3.upload_file('data.csv', 'my-bucket', 'raw/data.csv')\n\n# Download\ns3.download_file('my-bucket', 'raw/data.csv', 'local_data.csv')\n\n\n5.4 StorageClass\nLes StorageClasses dÃ©finissent le type de stockage disponible :\n# Voir les StorageClasses disponibles\nkubectl get storageclasses\n# PVC avec StorageClass spÃ©cifique\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: fast-storage\nspec:\n  storageClassName: fast-ssd  # StorageClass spÃ©cifique\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#scaling-gestion-des-ressources",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#scaling-gestion-des-ressources",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ“ˆ 6. Scaling & Gestion des ressources",
    "text": "ğŸ“ˆ 6. Scaling & Gestion des ressources\nLes workloads data sont souvent gourmands en ressources. Voici comment les gÃ©rer.\n\n6.1 Requests vs Limits (rappel approfondi)\n\n\n\nType\nDescription\nConsÃ©quence si dÃ©passÃ©\n\n\n\n\nrequests\nMinimum garanti\nPod non schedulÃ© si insuffisant\n\n\nlimits\nMaximum autorisÃ©\nCPU: throttling / RAM: OOMKilled\n\n\n\nresources:\n  requests:\n    memory: \"512Mi\"   # Garanti\n    cpu: \"500m\"       # 0.5 CPU garanti\n  limits:\n    memory: \"2Gi\"     # Max avant OOMKilled\n    cpu: \"2000m\"      # Max 2 CPUs\n\n\n6.2 QoS Classes\nK8s attribue une classe QoS Ã  chaque pod selon ses ressources :\n\n\n\n\n\n\n\n\nQoS Class\nCondition\nPrioritÃ© dâ€™Ã©viction\n\n\n\n\nGuaranteed\nrequests = limits (pour CPU et RAM)\nDerniers Ã©vincÃ©s\n\n\nBurstable\nrequests &lt; limits\nÃ‰vincÃ©s si nÃ©cessaire\n\n\nBestEffort\nPas de requests/limits\nPremiers Ã©vincÃ©s\n\n\n\n\nğŸ’¡ Pour les workloads data critiques, utilise Guaranteed (requests = limits).\n\n\n\n6.3 Horizontal Pod Autoscaler (HPA)\nLe HPA scale automatiquement le nombre de pods selon les mÃ©triques.\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: etl-worker-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: etl-worker\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n# Voir les HPA\nkubectl get hpa\n\n# DÃ©tails\nkubectl describe hpa etl-worker-hpa\n\n\n6.4 Resource Quotas par namespace\nLimite les ressources consommables par namespace (utile en Ã©quipe) :\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: data-team-quota\n  namespace: data-pipeline\nspec:\n  hard:\n    requests.cpu: \"10\"\n    requests.memory: \"20Gi\"\n    limits.cpu: \"20\"\n    limits.memory: \"40Gi\"\n    pods: \"50\"\n    persistentvolumeclaims: \"10\"\n\n\n6.5 Node Affinity & Taints/Tolerations\nNode Affinity : placer les pods sur des nodes spÃ©cifiques.\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: node-type\n            operator: In\n            values:\n            - high-memory  # Nodes avec beaucoup de RAM\nTaints/Tolerations : rÃ©server des nodes pour certains workloads.\n# Taint un node (le rÃ©server)\nkubectl taint nodes node1 workload=data:NoSchedule\n# Toleration dans le pod\nspec:\n  tolerations:\n  - key: \"workload\"\n    operator: \"Equal\"\n    value: \"data\"\n    effect: \"NoSchedule\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#monitoring-observabilitÃ©",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#monitoring-observabilitÃ©",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ“Š 7. Monitoring & ObservabilitÃ©",
    "text": "ğŸ“Š 7. Monitoring & ObservabilitÃ©\nSurveiller tes workloads data est essentiel pour dÃ©tecter les problÃ¨mes.\n\n7.1 MÃ©triques de base avec kubectl\n# CPU/RAM des pods (nÃ©cessite metrics-server)\nkubectl top pods\nkubectl top pods -n data-pipeline\n\n# CPU/RAM des nodes\nkubectl top nodes\n\n# Events (problÃ¨mes rÃ©cents)\nkubectl get events --sort-by='.lastTimestamp'\nkubectl get events -n data-pipeline\n\n\n7.2 Prometheus + Grafana (aperÃ§u)\nLa stack Prometheus + Grafana est le standard pour le monitoring K8s :\n\n\n\nOutil\nRÃ´le\n\n\n\n\nPrometheus\nCollecte et stocke les mÃ©triques\n\n\nGrafana\nVisualisation et dashboards\n\n\nAlertManager\nAlertes (Slack, email, PagerDuty)\n\n\n\n# Installation rapide via Helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install monitoring prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace\n\n\n7.3 Logs\n# Logs d'un pod\nkubectl logs &lt;pod&gt;\nkubectl logs -f &lt;pod&gt;           # Follow\nkubectl logs -p &lt;pod&gt;           # Previous (aprÃ¨s crash)\nkubectl logs --tail=100 &lt;pod&gt;   # 100 derniÃ¨res lignes\n\n# Logs d'un Job\nkubectl logs job/&lt;job-name&gt;\n\n# Logs de tous les pods d'un label\nkubectl logs -l app=etl --all-containers\n\n\n7.4 Centralisation des logs\nPour les environnements de production, centralise les logs avec :\n\n\n\nSolution\nDescription\n\n\n\n\nELK Stack\nElasticsearch + Logstash + Kibana\n\n\nLoki + Grafana\nSolution lÃ©gÃ¨re (recommandÃ©e)\n\n\nCloud native\nCloudWatch (AWS), Cloud Logging (GCP)\n\n\n\n\n\n7.5 Dashboards utiles pour Data Engineering\n\n\n\nDashboard\nMÃ©triques\n\n\n\n\nK8s Cluster Overview\nCPU, RAM, pods par node\n\n\nJob Success Rate\nJobs succeeded vs failed\n\n\nPod Resource Usage\nConsommation vs requests/limits\n\n\nPVC Usage\nEspace disque utilisÃ©",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#aperÃ§u-spark-airflow-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#aperÃ§u-spark-airflow-sur-kubernetes",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ”® 8. AperÃ§u : Spark & Airflow sur Kubernetes",
    "text": "ğŸ”® 8. AperÃ§u : Spark & Airflow sur Kubernetes\n\nâš ï¸ Note : Cette section est un aperÃ§u pour te donner une vision dâ€™ensemble. - Spark sera dÃ©taillÃ© dans les modules 19-22 - Airflow sera dÃ©taillÃ© dans le module 25\n\n\n8.1 Spark on Kubernetes (aperÃ§u)\nPourquoi Spark sur K8s ? - Pas besoin de cluster YARN ou Mesos dÃ©diÃ© - Ã‰lasticitÃ© native (pods crÃ©Ã©s Ã  la demande) - Parfait pour jobs ETL batch Ã©phÃ©mÃ¨res - IntÃ©gration cloud-native\nArchitecture simplifiÃ©e :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  spark-submit   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Driver Pod    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Executor Pod   â”‚ x N\nâ”‚   (coordonne)   â”‚         â”‚  (traitement)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemple de commande (aperÃ§u) :\n# Tu verras Ã§a en dÃ©tail dans le module 19\nspark-submit \\\n  --master k8s://https://&lt;K8S_API&gt; \\\n  --deploy-mode cluster \\\n  --conf spark.kubernetes.container.image=spark:3.5 \\\n  --conf spark.executor.instances=3 \\\n  local:///opt/spark/jobs/etl.py\n\n\n\n8.2 Airflow on Kubernetes (aperÃ§u)\nPourquoi Airflow sur K8s ? - ScalabilitÃ© des workers - Isolation parfaite des tÃ¢ches - KubernetesExecutor : 1 tÃ¢che = 1 pod\nArchitectures possibles :\n\n\n\nExecutor\nDescription\nUsage\n\n\n\n\nLocalExecutor\nTout dans 1 pod\nDev/test\n\n\nCeleryExecutor\nWorkers via Redis\nProduction classique\n\n\nKubernetesExecutor\n1 pod par tÃ¢che\nProduction K8s native\n\n\n\nDÃ©ploiement via Helm (aperÃ§u) :\n# Tu verras Ã§a en dÃ©tail dans le module 25\nhelm repo add apache-airflow https://airflow.apache.org\nhelm install airflow apache-airflow/airflow \\\n  --namespace airflow \\\n  --set executor=KubernetesExecutor\nKubernetesPodOperator (aperÃ§u) :\n# ExÃ©cuter une tÃ¢che dans un pod K8s dÃ©diÃ©\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\netl_task = KubernetesPodOperator(\n    task_id=\"etl_task\",\n    namespace=\"data-pipeline\",\n    image=\"my-etl:1.0\",\n    cmds=[\"python\", \"etl.py\"],\n    name=\"etl-pod\",\n)\n\nğŸ’¡ AprÃ¨s avoir terminÃ© les modules Spark (19-22) et Airflow (25), reviens sur cette section pour mieux comprendre lâ€™intÃ©gration K8s !",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "âš ï¸ 9. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "âš ï¸ 9. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nCause\nSolution\n\n\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter limits.memory\n\n\nDeadlineExceeded\nJob trop long\nAugmenter activeDeadlineSeconds\n\n\nPending (Job)\nPas de ressources disponibles\nVÃ©rifier quotas, rÃ©duire requests\n\n\nCrashLoopBackOff\nApp plante au dÃ©marrage\nkubectl logs &lt;pod&gt;\n\n\nImagePullBackOff\nImage/registry incorrects\nVÃ©rifier image, imagePullSecrets\n\n\nPVC Pending\nPas de PV disponible\nVÃ©rifier StorageClass\n\n\nJob jamais nettoyÃ©\nPas de TTL\nAjouter ttlSecondsAfterFinished\n\n\n\n\n\nâœ… Bonnes pratiques pour workloads data\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nToujours dÃ©finir resources\nÃ‰vite OOM et problÃ¨mes de scheduling\n\n\nTTL sur les Jobs\nNettoyage automatique\n\n\nLogs externalisÃ©s\nNe pas dÃ©pendre des logs K8s\n\n\nConfigMaps pour paramÃ¨tres\nPas de hardcoding\n\n\nSecrets pour credentials\nSÃ©curitÃ©\n\n\nLabels systÃ©matiques\nFiltrage et monitoring\n\n\nNamespace par projet\nIsolation, quotas\n\n\nHealthchecks\nK8s sait si lâ€™app est prÃªte\n\n\n\n\n\nğŸ·ï¸ Labels recommandÃ©s pour workloads data\nmetadata:\n  labels:\n    app: etl-pipeline\n    component: transform    # extract, transform, load\n    env: production\n    team: data-engineering\n    version: \"1.2.0\"\n    schedule: daily         # Pour les CronJobs",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#quiz-de-fin-de-module",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quâ€™est-ce quâ€™un â€œworkload dataâ€ ?\n\nUne application web qui affiche des donnÃ©es\n\nUne charge de travail dÃ©diÃ©e au traitement, transformation ou dÃ©placement de donnÃ©es\n\nUn dashboard de visualisation\n\nUne base de donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Un workload data est une tÃ¢che liÃ©e au traitement de donnÃ©es : ETL, transformations, ingestion, etc.\n\n\n\n\nâ“ Q2. Quel paramÃ¨tre permet de nettoyer automatiquement un Job terminÃ© aprÃ¨s 24h ?\n\nbackoffLimit: 24\n\nttlSecondsAfterFinished: 86400\n\nactiveDeadlineSeconds: 86400\n\ncleanupAfter: 24h\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ttlSecondsAfterFinished: 86400 supprime automatiquement le Job 24h aprÃ¨s sa completion.\n\n\n\n\nâ“ Q3. Quelle est la diffÃ©rence principale entre Deployment et StatefulSet ?\n\nDeployment est plus rÃ©cent\n\nStatefulSet garantit une identitÃ© stable et un stockage persistant par pod\n\nDeployment ne supporte pas les volumes\n\nStatefulSet est uniquement pour les bases NoSQL\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” StatefulSet donne une identitÃ© stable (pod-0, pod-1) et un PVC par pod, idÃ©al pour les bases de donnÃ©es.\n\n\n\n\nâ“ Q4. Quâ€™est-ce que Helm ?\n\nUn outil de monitoring\n\nUn package manager pour Kubernetes\n\nUn orchestrateur de containers\n\nUn systÃ¨me de logs\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Helm est le package manager de K8s, permettant dâ€™installer des applications complexes avec une seule commande.\n\n\n\n\nâ“ Q5. Que signifie lâ€™erreur OOMKilled ?\n\nLe pod a Ã©tÃ© tuÃ© par lâ€™administrateur\n\nLâ€™image Docker est corrompue\n\nLe container a dÃ©passÃ© sa limite de mÃ©moire\n\nLe rÃ©seau est indisponible\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” OOMKilled (Out Of Memory Killed) signifie que le container a dÃ©passÃ© limits.memory et a Ã©tÃ© tuÃ© par K8s.\n\n\n\n\nâ“ Q6. Quel executor Airflow crÃ©e un pod K8s par tÃ¢che ?\n\nLocalExecutor\n\nCeleryExecutor\n\nKubernetesExecutor\n\nSequentialExecutor\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le KubernetesExecutor lance chaque tÃ¢che Airflow dans un pod K8s dÃ©diÃ©.\n\n\n\n\nâ“ Q7. Quelle QoS class offre la meilleure protection contre lâ€™Ã©viction ?\n\nBestEffort\n\nBurstable\n\nGuaranteed\n\nProtected\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les pods Guaranteed (requests = limits) sont les derniers Ã  Ãªtre Ã©vincÃ©s en cas de pression sur les ressources.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#mini-projet-pipeline-etl-python-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#mini-projet-pipeline-etl-python-sur-kubernetes",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸš€ Mini-projet : Pipeline ETL Python sur Kubernetes",
    "text": "ğŸš€ Mini-projet : Pipeline ETL Python sur Kubernetes\n\nğŸ¯ Objectif\nDÃ©ployer un pipeline ETL batch complet sur Kubernetes, sans Spark (Python/pandas), avec : - MinIO : stockage des fichiers source (S3-compatible) - PostgreSQL : destination des donnÃ©es - CronJob : ETL planifiÃ© quotidiennement\n\n\nğŸ—ï¸ Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      MinIO      â”‚â”€â”€â”€â”€â–¶â”‚   CronJob ETL   â”‚â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚\nâ”‚  (S3 / input)   â”‚     â”‚    (Python)     â”‚     â”‚    (output)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                       â”‚                       â”‚\n        â””â”€â”€â”€â”€â”€ Helm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€ Manifests â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“ Structure du projet\nk8s-etl-project/\nâ”œâ”€â”€ helm-values/\nâ”‚   â”œâ”€â”€ minio-values.yaml\nâ”‚   â””â”€â”€ postgres-values.yaml\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ etl-configmap.yaml\nâ”‚   â”œâ”€â”€ etl-secret.yaml\nâ”‚   â””â”€â”€ etl-cronjob.yaml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sales.csv\nâ””â”€â”€ README.md\n\n\nğŸ§  Ã‰tapes\n\nCrÃ©er le namespace data-pipeline\nDÃ©ployer MinIO via Helm\nDÃ©ployer PostgreSQL via Helm\nUploader les donnÃ©es dans MinIO\nBuild & push lâ€™image ETL\nDÃ©ployer le CronJob\nTester manuellement\nVÃ©rifier les donnÃ©es dans PostgreSQL\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. manifests/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: data-pipeline\n  labels:\n    team: data-engineering\n    project: etl-demo\n2. helm-values/minio-values.yaml\nrootUser: admin\nrootPassword: minio123456\npersistence:\n  size: 5Gi\nresources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"100m\"\n3. helm-values/postgres-values.yaml\nauth:\n  postgresPassword: \"postgres123\"\n  username: \"de_user\"\n  password: \"de_password\"\n  database: \"de_db\"\nprimary:\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"250m\"\n  persistence:\n    size: 2Gi\n4. manifests/etl-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\n  namespace: data-pipeline\ndata:\n  MINIO_ENDPOINT: \"minio.data-pipeline.svc.cluster.local:9000\"\n  MINIO_BUCKET: \"raw-data\"\n  MINIO_FILE: \"sales.csv\"\n  DB_HOST: \"postgres-postgresql.data-pipeline.svc.cluster.local\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"de_db\"\n  DB_USER: \"de_user\"\n5. manifests/etl-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: etl-secret\n  namespace: data-pipeline\ntype: Opaque\nstringData:\n  MINIO_ACCESS_KEY: \"admin\"\n  MINIO_SECRET_KEY: \"minio123456\"\n  DB_PASSWORD: \"de_password\"\n6. manifests/etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\n  namespace: data-pipeline\n  labels:\n    app: etl-pipeline\n    schedule: daily\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 2\n  jobTemplate:\n    spec:\n      backoffLimit: 3\n      activeDeadlineSeconds: 1800\n      ttlSecondsAfterFinished: 86400\n      template:\n        metadata:\n          labels:\n            app: etl-job\n        spec:\n          containers:\n          - name: etl\n            image: my-etl:1.0  # Remplacer par ton image\n            envFrom:\n            - configMapRef:\n                name: etl-config\n            - secretRef:\n                name: etl-secret\n            resources:\n              requests:\n                memory: \"256Mi\"\n                cpu: \"200m\"\n              limits:\n                memory: \"512Mi\"\n                cpu: \"500m\"\n          restartPolicy: OnFailure\n7. etl/requirements.txt\npandas==2.1.4\nboto3==1.34.0\npsycopg2-binary==2.9.9\nsqlalchemy==2.0.25\n8. etl/Dockerfile\nFROM python:3.11-slim\n\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY etl.py .\n\nCMD [\"python\", \"etl.py\"]\n9. etl/etl.py\nimport os\nimport pandas as pd\nimport boto3\nfrom sqlalchemy import create_engine\nfrom io import BytesIO\n\ndef main():\n    print(\"ğŸš€ DÃ©marrage ETL...\")\n    \n    # Config MinIO\n    minio_endpoint = os.environ['MINIO_ENDPOINT']\n    minio_access = os.environ['MINIO_ACCESS_KEY']\n    minio_secret = os.environ['MINIO_SECRET_KEY']\n    bucket = os.environ['MINIO_BUCKET']\n    file_key = os.environ['MINIO_FILE']\n    \n    # Config PostgreSQL\n    db_host = os.environ['DB_HOST']\n    db_port = os.environ['DB_PORT']\n    db_name = os.environ['DB_NAME']\n    db_user = os.environ['DB_USER']\n    db_pass = os.environ['DB_PASSWORD']\n    \n    # EXTRACT : Lire depuis MinIO\n    print(f\"ğŸ“¥ Lecture depuis MinIO: {bucket}/{file_key}\")\n    s3 = boto3.client(\n        's3',\n        endpoint_url=f'http://{minio_endpoint}',\n        aws_access_key_id=minio_access,\n        aws_secret_access_key=minio_secret\n    )\n    \n    response = s3.get_object(Bucket=bucket, Key=file_key)\n    df = pd.read_csv(BytesIO(response['Body'].read()))\n    print(f\"   {len(df)} lignes lues\")\n    \n    # TRANSFORM\n    print(\"ğŸ”„ Transformation...\")\n    df['total'] = df['quantity'] * df['price']\n    df['loaded_at'] = pd.Timestamp.now()\n    \n    # LOAD : Ã‰crire dans PostgreSQL\n    print(f\"ğŸ“¤ Chargement dans PostgreSQL...\")\n    engine = create_engine(\n        f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n    )\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© avec succÃ¨s !\")\n    print(df.head())\n\nif __name__ == \"__main__\":\n    main()\n10. Commandes de dÃ©ploiement\n# 1. CrÃ©er le namespace\nkubectl apply -f manifests/namespace.yaml\n\n# 2. Installer MinIO\nhelm repo add minio https://charts.min.io/\nhelm install minio minio/minio \\\n  -n data-pipeline \\\n  -f helm-values/minio-values.yaml\n\n# 3. Installer PostgreSQL\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install postgres bitnami/postgresql \\\n  -n data-pipeline \\\n  -f helm-values/postgres-values.yaml\n\n# 4. Attendre que tout soit prÃªt\nkubectl get pods -n data-pipeline -w\n\n# 5. Upload data dans MinIO (via port-forward)\nkubectl port-forward svc/minio -n data-pipeline 9000:9000 &\n# Puis utiliser mc (MinIO client) ou l'UI\n\n# 6. Build & push image ETL\ncd etl\ndocker build -t my-etl:1.0 .\n# docker push my-registry/my-etl:1.0\n\n# 7. DÃ©ployer les manifests\nkubectl apply -f manifests/etl-configmap.yaml\nkubectl apply -f manifests/etl-secret.yaml\nkubectl apply -f manifests/etl-cronjob.yaml\n\n# 8. Tester manuellement\nkubectl create job test-etl --from=cronjob/etl-daily -n data-pipeline\n\n# 9. Voir les logs\nkubectl logs -f job/test-etl -n data-pipeline\n\n# 10. VÃ©rifier dans PostgreSQL\nkubectl exec -it postgres-postgresql-0 -n data-pipeline -- \\\n  psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#ressources-pour-aller-plus-loin",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nKubernetes Jobs â€” Documentation Jobs\nKubernetes CronJobs â€” Documentation CronJobs\nStatefulSets â€” Applications stateful\nHelm Docs â€” Documentation Helm\n\n\n\nğŸ“¦ Charts Helm utiles\n\nArtifact Hub â€” Recherche de charts Helm\nBitnami Charts â€” Charts de qualitÃ© production\n\n\n\nğŸ”§ Outils\n\nk9s â€” Terminal UI pour Kubernetes\nLens â€” IDE Kubernetes\nMinIO Client (mc) â€” CLI pour MinIO",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#prochaine-Ã©tape",
    "title": "â˜¸ï¸ Kubernetes pour Workloads Data",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les workloads data sur Kubernetes, passons au traitement de donnÃ©es haute performance avec Python !\nğŸ‘‰ Module suivant : 17_polars_for_data_engineering.ipynb â€” Polars : le DataFrame ultra-rapide\nTu vas apprendre : - Pourquoi Polars est plus rapide que Pandas - API lazy vs eager - Optimisations automatiques - Migration depuis Pandas\n\n\nğŸ“ Note importante\nAprÃ¨s avoir terminÃ© les modules suivants, tu pourras revenir sur ce module avec une meilleure comprÃ©hension :\n\n\n\nModule\nContenu\nLien avec ce module\n\n\n\n\n19-22\nSpark\nSpark on Kubernetes (section 8.1)\n\n\n25\nAirflow Advanced\nAirflow on Kubernetes (section 8.2)\n\n\n23-24\nKafka & Streaming\nWorkloads streaming sur K8s\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Kubernetes pour Workloads Data.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜¸ï¸ Kubernetes pour Workloads Data"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser Spark SQL en profondeur. Tu dÃ©couvriras les Window Functions, les agrÃ©gations avancÃ©es, le reshaping de donnÃ©es, et comment optimiser tes requÃªtes SQL.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#prÃ©requis",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#prÃ©requis",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 19 : PySpark Advanced\n\n\nâœ… Requis\nConnaissances SQL de base\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec des requÃªtes analytiques",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#objectifs-du-module",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#objectifs-du-module",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nMaÃ®triser les Window Functions (ranking, lag/lead, frames)\nUtiliser PIVOT/UNPIVOT pour reshaper les donnÃ©es\nAppliquer GROUPING SETS, CUBE, ROLLUP pour des agrÃ©gations multidimensionnelles\nManipuler les donnÃ©es semi-structurÃ©es avec EXPLODE\nStructurer des requÃªtes complexes avec CTEs\nOptimiser les requÃªtes SQL (hints, statistiques)\nConstruire un datamart analytique complet\n\n\n\nCode\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\n\n# CrÃ©er une SparkSession\nspark = SparkSession.builder \\\n    .appName(\"Spark SQL Deep Dive\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\nprint(f\"âœ… Spark {spark.version} initialisÃ©\")\nprint(f\"ğŸ” Spark UI : {spark.sparkContext.uiWebUrl}\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#sql-dans-spark-rappels-fondamentaux",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#sql-dans-spark-rappels-fondamentaux",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“š 1. SQL dans Spark : Rappels & Fondamentaux",
    "text": "ğŸ“š 1. SQL dans Spark : Rappels & Fondamentaux\n\nâš¡ Rappel rapide â€” Les dÃ©tails sur Catalyst et lâ€™architecture sont dans le module 19.\n\n\n1.1 CrÃ©er et utiliser des vues\n\n\nCode\n# CrÃ©er des donnÃ©es de test\nsales_data = [\n    (1, \"Electronics\", \"North\", 1200, \"2024-01-15\"),\n    (2, \"Electronics\", \"South\", 800, \"2024-01-16\"),\n    (3, \"Clothing\", \"North\", 450, \"2024-01-15\"),\n    (4, \"Clothing\", \"South\", 650, \"2024-01-17\"),\n    (5, \"Electronics\", \"North\", 950, \"2024-01-18\"),\n    (6, \"Food\", \"South\", 320, \"2024-01-15\"),\n]\n\nsales_df = spark.createDataFrame(sales_data, \n    [\"id\", \"category\", \"region\", \"amount\", \"date\"])\n\n# CrÃ©er une vue temporaire\nsales_df.createOrReplaceTempView(\"sales\")\n\n# Utiliser SQL\nresult = spark.sql(\"\"\"\n    SELECT category, SUM(amount) as total_sales\n    FROM sales\n    WHERE amount &gt; 500\n    GROUP BY category\n    ORDER BY total_sales DESC\n\"\"\")\n\nresult.show()\n\n\n\n\n1.2 DataFrame API vs SQL\n\n\n\nAspect\nDataFrame API\nSQL\n\n\n\n\nTypage\nCompile-time\nRuntime\n\n\nLisibilitÃ©\nCode Python\nFamilier aux analystes\n\n\nRÃ©utilisabilitÃ©\nFonctions Python\nCTEs, Vues\n\n\nPerformance\nIdentique\nIdentique\n\n\nDebug\nStack trace Python\nErreurs SQL\n\n\nIDE Support\nAutocomplÃ©tion\nVariable\n\n\n\n\nğŸ’¡ RÃ¨gle : Utilise ce qui est le plus lisible pour ton Ã©quipe. Les deux sont optimisÃ©s par Catalyst.\n\n\n\nCode\n# Ã‰quivalence DataFrame API vs SQL\n\n# === DataFrame API ===\nresult_df = sales_df \\\n    .filter(col(\"amount\") &gt; 500) \\\n    .groupBy(\"category\") \\\n    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n    .orderBy(desc(\"total_sales\"))\n\n# === SQL ===\nresult_sql = spark.sql(\"\"\"\n    SELECT category, SUM(amount) as total_sales\n    FROM sales\n    WHERE amount &gt; 500\n    GROUP BY category\n    ORDER BY total_sales DESC\n\"\"\")\n\nprint(\"DataFrame API:\")\nresult_df.show()\n\nprint(\"SQL:\")\nresult_sql.show()\n\n# Les plans sont identiques !\nprint(\"\\nâœ… Les deux approches gÃ©nÃ¨rent le mÃªme plan d'exÃ©cution\")\n\n\n\n\n1.3 Lire un plan dâ€™exÃ©cution SQL\n\n\nCode\n# CrÃ©er des donnÃ©es pour dÃ©montrer les joins\nproducts_data = [\n    (1, \"Laptop\", \"Electronics\"),\n    (2, \"T-Shirt\", \"Clothing\"),\n    (3, \"Apple\", \"Food\"),\n]\nproducts_df = spark.createDataFrame(products_data, [\"product_id\", \"name\", \"category\"])\nproducts_df.createOrReplaceTempView(\"products\")\n\n# Voir le plan d'exÃ©cution\nspark.sql(\"\"\"\n    EXPLAIN FORMATTED\n    SELECT s.*, p.name\n    FROM sales s\n    JOIN products p ON s.category = p.category\n    WHERE s.amount &gt; 500\n\"\"\").show(truncate=False)\n\n\nCe quâ€™il faut repÃ©rer dans le plan :\n\n\n\nÃ‰lÃ©ment\nSignification\nBon/Mauvais\n\n\n\n\nBroadcastHashJoin\nPetite table broadcastÃ©e\nâœ… Bon\n\n\nSortMergeJoin\nShuffle des deux tables\nâš ï¸ CoÃ»teux\n\n\nPushedFilters\nFiltre appliquÃ© Ã  la source\nâœ… Bon\n\n\nExchange\nShuffle (redistribution)\nâš ï¸ Ã€ surveiller",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#window-functions-le-cÅ“ur-analytique",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#window-functions-le-cÅ“ur-analytique",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸªŸ 2. Window Functions â€” Le cÅ“ur analytique",
    "text": "ğŸªŸ 2. Window Functions â€” Le cÅ“ur analytique\n\nğŸ”¥ Section la plus importante du module. Les Window Functions sont essentielles pour lâ€™analytics.\n\n\n2.1 Syntaxe et concepts\nFUNCTION() OVER (\n  PARTITION BY column1, column2   -- Grouper les donnÃ©es\n  ORDER BY column3                -- Ordonner dans chaque groupe\n  ROWS BETWEEN start AND end      -- DÃ©finir la fenÃªtre de calcul\n)\nğŸ–¼ï¸ Comment fonctionne une Window Function :\n\nDonnÃ©es originales          PARTITION BY category    ORDER BY date       Calcul sur la fenÃªtre\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ cat â”‚ date â”‚ amtâ”‚         â”‚ Electronics     â”‚     â”‚ 01-15 â”‚ 1200   â”‚  â”‚ ROW_NUMBER = 1  â”‚\nâ”‚ Elecâ”‚ 01-15â”‚1200â”‚   â†’     â”‚ â”œâ”€ 01-15 â”‚ 1200â”‚  â†’  â”‚ 01-16 â”‚  800   â”‚  â”‚ ROW_NUMBER = 2  â”‚\nâ”‚ Elecâ”‚ 01-16â”‚ 800â”‚         â”‚ â”œâ”€ 01-16 â”‚  800â”‚     â”‚ 01-18 â”‚  950   â”‚  â”‚ ROW_NUMBER = 3  â”‚\nâ”‚ Elecâ”‚ 01-18â”‚ 950â”‚         â”‚ â””â”€ 01-18 â”‚  950â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚ Clthâ”‚ 01-15â”‚ 450â”‚         â”‚                 â”‚\nâ”‚ Clthâ”‚ 01-17â”‚ 650â”‚         â”‚ Clothing        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”œâ”€ 01-15 â”‚  450â”‚\n                            â”‚ â””â”€ 01-17 â”‚  650â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n# CrÃ©er des donnÃ©es plus riches pour les window functions\norders_data = [\n    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n    (6, 103, \"2024-01-10\", 400.0, \"Premium\"),\n    (7, 103, \"2024-01-25\", 400.0, \"Premium\"),  # Ex-aequo intentionnel\n    (8, 103, \"2024-02-10\", 350.0, \"Premium\"),\n    (9, 104, \"2024-01-03\", 100.0, \"Standard\"),\n    (10, 104, \"2024-02-15\", 120.0, \"Standard\"),\n]\n\norders_df = spark.createDataFrame(orders_data,\n    [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\norders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n\norders_df.createOrReplaceTempView(\"orders\")\norders_df.show()\n\n\n\n\n2.2 Fonctions de Ranking\n\n\n\nFonction\nDescription\nGÃ¨re les ex-aequo\n\n\n\n\nROW_NUMBER()\nNumÃ©ro unique sÃ©quentiel\nNon (arbitraire)\n\n\nRANK()\nRang avec gaps aprÃ¨s ex-aequo\nOui (1,1,3)\n\n\nDENSE_RANK()\nRang sans gaps\nOui (1,1,2)\n\n\nNTILE(n)\nDivise en n groupes Ã©gaux\n-\n\n\nPERCENT_RANK()\nRang en percentile (0-1)\nOui\n\n\nCUME_DIST()\nDistribution cumulative\nOui\n\n\n\n\n\nCode\n# DÃ©monstration des fonctions de ranking\nranking_result = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as row_num,\n        RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rank,\n        DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as dense_rank,\n        NTILE(2) OVER (PARTITION BY customer_id ORDER BY amount DESC) as ntile_2\n    FROM orders\n    WHERE customer_id = 103\n    ORDER BY customer_id, amount DESC\n\"\"\")\n\nprint(\"ğŸ“Š Comparaison ROW_NUMBER vs RANK vs DENSE_RANK (customer 103 avec ex-aequo):\")\nranking_result.show()\n\n\n\n\nCode\n# Cas d'usage : Top 2 commandes par client\ntop_orders = spark.sql(\"\"\"\n    SELECT * FROM (\n        SELECT \n            customer_id,\n            order_date,\n            amount,\n            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rn\n        FROM orders\n    )\n    WHERE rn &lt;= 2\n    ORDER BY customer_id, rn\n\"\"\")\n\nprint(\"ğŸ† Top 2 commandes par client:\")\ntop_orders.show()\n\n\n\n\n2.3 Fonctions de dÃ©calage (LAG/LEAD)\n\n\n\nFonction\nDescription\n\n\n\n\nLAG(col, n, default)\nValeur n lignes AVANT\n\n\nLEAD(col, n, default)\nValeur n lignes APRÃˆS\n\n\nFIRST_VALUE(col)\nPremiÃ¨re valeur de la fenÃªtre\n\n\nLAST_VALUE(col)\nDerniÃ¨re valeur de la fenÃªtre\n\n\nNTH_VALUE(col, n)\nN-iÃ¨me valeur de la fenÃªtre\n\n\n\n\n\nCode\n# Cas d'usage : DÃ©lai entre commandes\norder_gaps = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n        LEAD(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date,\n        DATEDIFF(\n            order_date, \n            LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date)\n        ) as days_since_last_order\n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“… Analyse du dÃ©lai entre commandes:\")\norder_gaps.show()\n\n\n\n\nCode\n# FIRST_VALUE et LAST_VALUE\nfirst_last = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        FIRST_VALUE(order_date) OVER (\n            PARTITION BY customer_id ORDER BY order_date\n        ) as first_order_date,\n        FIRST_VALUE(amount) OVER (\n            PARTITION BY customer_id ORDER BY order_date\n        ) as first_order_amount\n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“Š FIRST_VALUE - Date et montant de la premiÃ¨re commande:\")\nfirst_last.show()\n\n\n\n\n2.4 AgrÃ©gations dans une fenÃªtre\n\n\nCode\n# Running total, Running average, % du total\nwindow_agg = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        \n        -- Somme cumulative\n        SUM(amount) OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as running_total,\n        \n        -- Total du client (pour calculer %)\n        SUM(amount) OVER (PARTITION BY customer_id) as customer_total,\n        \n        -- % de chaque commande\n        ROUND(amount / SUM(amount) OVER (PARTITION BY customer_id) * 100, 1) as pct_of_customer_total\n        \n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“Š AgrÃ©gations fenÃªtrÃ©es:\")\nwindow_agg.show()\n\n\n\n\n2.5 Window Frames (âš ï¸ PiÃ¨ges Spark)\nROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW  -- Du dÃ©but jusqu'Ã  maintenant\nROWS BETWEEN 6 PRECEDING AND CURRENT ROW          -- 7 derniÃ¨res lignes (rolling)\nROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING  -- De maintenant jusqu'Ã  la fin\nROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING          -- 3 lignes (prÃ©cÃ©dente, courante, suivante)\n\n\n\n\n\n\n\n\nClause\nBasÃ©e sur\nComportement\n\n\n\n\nROWS\nPosition physique\nPrÃ©visible, recommandÃ©\n\n\nRANGE\nValeur logique\nâš ï¸ Peut inclure plus de lignes si doublons\n\n\n\n\nğŸ’¡ En cas de doute, utilise ROWS (plus prÃ©visible)\n\n\n\nCode\n# Rolling average (moyenne mobile)\n# CrÃ©er plus de donnÃ©es pour dÃ©montrer\ndaily_sales = spark.createDataFrame([\n    (\"2024-01-01\", 100), (\"2024-01-02\", 150), (\"2024-01-03\", 120),\n    (\"2024-01-04\", 180), (\"2024-01-05\", 90), (\"2024-01-06\", 200),\n    (\"2024-01-07\", 170), (\"2024-01-08\", 140), (\"2024-01-09\", 160),\n    (\"2024-01-10\", 190),\n], [\"date\", \"sales\"])\ndaily_sales = daily_sales.withColumn(\"date\", to_date(col(\"date\")))\ndaily_sales.createOrReplaceTempView(\"daily_sales\")\n\n# Moyenne mobile sur 3 jours\nrolling = spark.sql(\"\"\"\n    SELECT \n        date,\n        sales,\n        ROUND(AVG(sales) OVER (\n            ORDER BY date\n            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n        ), 1) as rolling_avg_3d,\n        SUM(sales) OVER (\n            ORDER BY date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as cumulative_sum\n    FROM daily_sales\n\"\"\")\n\nprint(\"ğŸ“ˆ Moyenne mobile 3 jours et somme cumulative:\")\nrolling.show()",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#pivot-unpivot-reshaping-des-donnÃ©es",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#pivot-unpivot-reshaping-des-donnÃ©es",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ”„ 3. PIVOT & UNPIVOT â€” Reshaping des donnÃ©es",
    "text": "ğŸ”„ 3. PIVOT & UNPIVOT â€” Reshaping des donnÃ©es\n\n3.1 PIVOT : lignes â†’ colonnes\nTransforme les valeurs dâ€™une colonne en colonnes distinctes.\n\n\nCode\n# DonnÃ©es mensuelles\nmonthly_data = spark.createDataFrame([\n    (\"Alice\", \"Jan\", 1000), (\"Alice\", \"Feb\", 1200), (\"Alice\", \"Mar\", 1100),\n    (\"Bob\", \"Jan\", 800), (\"Bob\", \"Feb\", 900), (\"Bob\", \"Mar\", 950),\n    (\"Charlie\", \"Jan\", 1500), (\"Charlie\", \"Feb\", 1400), (\"Charlie\", \"Mar\", 1600),\n], [\"salesperson\", \"month\", \"revenue\"])\n\nmonthly_data.createOrReplaceTempView(\"monthly_sales\")\n\nprint(\"DonnÃ©es originales (format long):\")\nmonthly_data.show()\n\n# PIVOT : transformer les mois en colonnes\npivoted = spark.sql(\"\"\"\n    SELECT * FROM monthly_sales\n    PIVOT (\n        SUM(revenue)\n        FOR month IN ('Jan', 'Feb', 'Mar')\n    )\n\"\"\")\n\nprint(\"AprÃ¨s PIVOT (format large):\")\npivoted.show()\n\n\n\n\nCode\n# PIVOT avec DataFrame API\npivoted_df = monthly_data.groupBy(\"salesperson\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).sum(\"revenue\")\n\nprint(\"PIVOT avec DataFrame API:\")\npivoted_df.show()\n\n\n\n\n3.2 UNPIVOT : colonnes â†’ lignes\nLâ€™opÃ©ration inverse de PIVOT.\n\n\nCode\n# CrÃ©er une vue du DataFrame pivotÃ©\npivoted_df.createOrReplaceTempView(\"pivoted_sales\")\n\n# UNPIVOT (Spark 3.4+)\n# Pour les versions antÃ©rieures, utiliser stack()\nunpivoted = spark.sql(\"\"\"\n    SELECT \n        salesperson,\n        stack(3, \n            'Jan', Jan, \n            'Feb', Feb, \n            'Mar', Mar\n        ) as (month, revenue)\n    FROM pivoted_sales\n\"\"\")\n\nprint(\"AprÃ¨s UNPIVOT (retour au format long):\")\nunpivoted.show()",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#agrÃ©gations-avancÃ©es-grouping-sets-cube-rollup",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#agrÃ©gations-avancÃ©es-grouping-sets-cube-rollup",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“Š 4. AgrÃ©gations AvancÃ©es â€” GROUPING SETS, CUBE, ROLLUP",
    "text": "ğŸ“Š 4. AgrÃ©gations AvancÃ©es â€” GROUPING SETS, CUBE, ROLLUP\nCes fonctions permettent de calculer plusieurs niveaux dâ€™agrÃ©gation en une seule requÃªte.\n\n4.1 GROUPING SETS : agrÃ©gations multiples\n\n\nCode\n# GROUPING SETS : plusieurs agrÃ©gations en une requÃªte\ngrouping_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales,\n        COUNT(*) as num_transactions\n    FROM sales\n    GROUP BY GROUPING SETS (\n        (category, region),  -- Par catÃ©gorie ET rÃ©gion\n        (category),          -- Par catÃ©gorie seulement\n        (region),            -- Par rÃ©gion seulement\n        ()                   -- Total global\n    )\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š GROUPING SETS - AgrÃ©gations multiples:\")\ngrouping_result.show()\n\n\n\n\n4.2 ROLLUP : hiÃ©rarchie dâ€™agrÃ©gations\nROLLUP(a, b, c) = GROUPING SETS ((a,b,c), (a,b), (a), ())\n\n\nCode\n# ROLLUP : hiÃ©rarchie (catÃ©gorie â†’ rÃ©gion â†’ total)\nrollup_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales\n    FROM sales\n    GROUP BY ROLLUP (category, region)\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š ROLLUP - HiÃ©rarchie d'agrÃ©gations:\")\nrollup_result.show()\n\n\n\n\n4.3 CUBE : toutes les combinaisons\nCUBE(a, b) = GROUPING SETS ((a,b), (a), (b), ())\n\n\nCode\n# CUBE : toutes les combinaisons possibles\ncube_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales\n    FROM sales\n    GROUP BY CUBE (category, region)\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š CUBE - Toutes les combinaisons:\")\ncube_result.show()\n\n\n\n\nCode\n# GROUPING() : identifier les sous-totaux\ngrouping_with_flags = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales,\n        GROUPING(category) as is_category_subtotal,\n        GROUPING(region) as is_region_subtotal,\n        CASE \n            WHEN GROUPING(category) = 1 AND GROUPING(region) = 1 THEN 'GRAND TOTAL'\n            WHEN GROUPING(category) = 1 THEN 'Region Subtotal'\n            WHEN GROUPING(region) = 1 THEN 'Category Subtotal'\n            ELSE 'Detail'\n        END as row_type\n    FROM sales\n    GROUP BY CUBE (category, region)\n    ORDER BY GROUPING(category), GROUPING(region), category, region\n\"\"\")\n\nprint(\"ğŸ“Š GROUPING() pour identifier les sous-totaux:\")\ngrouping_with_flags.show()",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#donnÃ©es-semi-structurÃ©es-explode-json",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#donnÃ©es-semi-structurÃ©es-explode-json",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ§© 5. DonnÃ©es Semi-StructurÃ©es â€” EXPLODE & JSON",
    "text": "ğŸ§© 5. DonnÃ©es Semi-StructurÃ©es â€” EXPLODE & JSON\n\n5.1 EXPLODE : Ã©clater arrays et maps\n\n\nCode\n# CrÃ©er des donnÃ©es avec arrays\ncustomers_with_tags = spark.createDataFrame([\n    (1, \"Alice\", [\"premium\", \"loyal\", \"newsletter\"]),\n    (2, \"Bob\", [\"new\", \"newsletter\"]),\n    (3, \"Charlie\", [\"premium\", \"vip\"]),\n], [\"id\", \"name\", \"tags\"])\n\ncustomers_with_tags.createOrReplaceTempView(\"customers_tags\")\n\nprint(\"DonnÃ©es avec arrays:\")\ncustomers_with_tags.show(truncate=False)\n\n# EXPLODE : une ligne par tag\nexploded = spark.sql(\"\"\"\n    SELECT id, name, tag\n    FROM customers_tags\n    LATERAL VIEW EXPLODE(tags) t AS tag\n\"\"\")\n\nprint(\"AprÃ¨s EXPLODE:\")\nexploded.show()\n\n\n\n\nCode\n# POSEXPLODE : avec position\nposexploded = spark.sql(\"\"\"\n    SELECT id, name, pos, tag\n    FROM customers_tags\n    LATERAL VIEW POSEXPLODE(tags) t AS pos, tag\n\"\"\")\n\nprint(\"POSEXPLODE (avec index):\")\nposexploded.show()\n\n\n\n\nCode\n# EXPLODE avec Map\ncustomers_with_attrs = spark.createDataFrame([\n    (1, \"Alice\", {\"city\": \"Paris\", \"country\": \"France\"}),\n    (2, \"Bob\", {\"city\": \"London\", \"country\": \"UK\", \"postal\": \"SW1\"}),\n], [\"id\", \"name\", \"attributes\"])\n\ncustomers_with_attrs.createOrReplaceTempView(\"customers_attrs\")\n\nprint(\"DonnÃ©es avec Map:\")\ncustomers_with_attrs.show(truncate=False)\n\n# EXPLODE sur Map â†’ (key, value)\nexploded_map = spark.sql(\"\"\"\n    SELECT id, name, key, value\n    FROM customers_attrs\n    LATERAL VIEW EXPLODE(attributes) t AS key, value\n\"\"\")\n\nprint(\"EXPLODE sur Map:\")\nexploded_map.show()\n\n\n\n\n5.2 AccÃ¨s JSON\n\n\nCode\n# DonnÃ©es JSON\nevents = spark.createDataFrame([\n    (1, '{\"user\": \"alice\", \"action\": \"click\", \"details\": {\"page\": \"home\", \"duration\": 5}}'),\n    (2, '{\"user\": \"bob\", \"action\": \"purchase\", \"details\": {\"page\": \"cart\", \"amount\": 99.99}}'),\n], [\"id\", \"json_data\"])\n\nevents.createOrReplaceTempView(\"events\")\n\n# Extraire des champs JSON\njson_extracted = spark.sql(\"\"\"\n    SELECT \n        id,\n        get_json_object(json_data, '$.user') as user,\n        get_json_object(json_data, '$.action') as action,\n        get_json_object(json_data, '$.details.page') as page\n    FROM events\n\"\"\")\n\nprint(\"Extraction JSON avec get_json_object:\")\njson_extracted.show()\n\n\n\n\nCode\n# Parser JSON complet avec schema\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\ndetails_schema = StructType([\n    StructField(\"page\", StringType()),\n    StructField(\"duration\", DoubleType()),\n    StructField(\"amount\", DoubleType())\n])\n\nevent_schema = StructType([\n    StructField(\"user\", StringType()),\n    StructField(\"action\", StringType()),\n    StructField(\"details\", details_schema)\n])\n\nparsed = events.withColumn(\"parsed\", from_json(col(\"json_data\"), event_schema))\nparsed.select(\"id\", \"parsed.user\", \"parsed.action\", \"parsed.details.page\").show()",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#ctes-subqueries-structurer-ses-requÃªtes",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#ctes-subqueries-structurer-ses-requÃªtes",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“ 6. CTEs & Subqueries â€” Structurer ses requÃªtes",
    "text": "ğŸ“ 6. CTEs & Subqueries â€” Structurer ses requÃªtes\n\n6.1 Common Table Expressions (CTEs)\n\n\nCode\n# CTE : structurer une requÃªte complexe\ncte_result = spark.sql(\"\"\"\n    WITH \n    -- Ã‰tape 1 : AgrÃ©gation par client\n    customer_stats AS (\n        SELECT \n            customer_id,\n            COUNT(*) as num_orders,\n            SUM(amount) as total_spent,\n            AVG(amount) as avg_order\n        FROM orders\n        GROUP BY customer_id\n    ),\n    \n    -- Ã‰tape 2 : Moyenne globale\n    global_avg AS (\n        SELECT AVG(total_spent) as avg_total_spent\n        FROM customer_stats\n    )\n    \n    -- RequÃªte finale : clients au-dessus de la moyenne\n    SELECT \n        c.*,\n        g.avg_total_spent,\n        CASE WHEN c.total_spent &gt; g.avg_total_spent THEN 'Above Average' ELSE 'Below Average' END as status\n    FROM customer_stats c\n    CROSS JOIN global_avg g\n    ORDER BY total_spent DESC\n\"\"\")\n\nprint(\"ğŸ“Š Analyse client avec CTEs:\")\ncte_result.show()\n\n\n\n\n6.2 CTEs vs Subqueries\n\n\n\n\n\n\n\n\nAspect\nCTE\nSubquery\n\n\n\n\nLisibilitÃ©\nâœ… Excellent\nâš ï¸ Peut Ãªtre confus\n\n\nRÃ©utilisabilitÃ©\nâœ… Peut Ãªtre rÃ©fÃ©rencÃ© plusieurs fois\nâŒ RÃ©pÃ©tition nÃ©cessaire\n\n\nPerformance\nâš ï¸ Pas toujours optimisÃ©\nâš ï¸ Variable\n\n\n\n\nâš ï¸ Mythe : Les CTEs ne sont PAS toujours plus rapides. Catalyst les traite comme des subqueries inline.\n\n\n\nCode\n# âŒ Subquery corrÃ©lÃ©e (potentiellement lent)\n# SELECT * FROM orders o\n# WHERE amount &gt; (SELECT AVG(amount) FROM orders WHERE customer_id = o.customer_id)\n\n# âœ… Window function (plus efficace)\nefficient_query = spark.sql(\"\"\"\n    SELECT * FROM (\n        SELECT \n            *,\n            AVG(amount) OVER (PARTITION BY customer_id) as avg_customer_amount\n        FROM orders\n    )\n    WHERE amount &gt; avg_customer_amount\n\"\"\")\n\nprint(\"âœ… Commandes au-dessus de la moyenne du client (window function):\")\nefficient_query.show()",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#optimisation-sql-dans-spark",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#optimisation-sql-dans-spark",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "âš¡ 7. Optimisation SQL dans Spark",
    "text": "âš¡ 7. Optimisation SQL dans Spark\n\n7.1 Join Hints\n\n\nCode\n# CrÃ©er des tables pour dÃ©montrer les hints\nspark.sql(\"DROP TABLE IF EXISTS dim_segment\")\nsegments = spark.createDataFrame([\n    (\"Premium\", 1.2), (\"Standard\", 1.0)\n], [\"segment\", \"multiplier\"])\nsegments.createOrReplaceTempView(\"dim_segment\")\n\n# BROADCAST hint\nbroadcast_join = spark.sql(\"\"\"\n    SELECT /*+ BROADCAST(dim_segment) */ \n        o.*,\n        s.multiplier,\n        o.amount * s.multiplier as adjusted_amount\n    FROM orders o\n    JOIN dim_segment s ON o.segment = s.segment\n\"\"\")\n\nprint(\"Plan avec BROADCAST hint:\")\nbroadcast_join.explain()\nprint(\"\\nRÃ©sultat:\")\nbroadcast_join.show(5)\n\n\nHints disponibles :\n\n\n\n\n\n\n\n\nHint\nUsage\nQuand lâ€™utiliser\n\n\n\n\n/*+ BROADCAST(table) */\nForce broadcast\nPetite table (&lt; 100 MB)\n\n\n/*+ MERGE(t1, t2) */\nForce sort-merge join\nGrandes tables triÃ©es\n\n\n/*+ SHUFFLE_HASH(t1) */\nForce shuffle hash\nTables moyennes\n\n\n/*+ COALESCE(n) */\nRÃ©duit les partitions\nAvant Ã©criture\n\n\n\n\n\nCode\n# Collecter les statistiques (amÃ©liore l'optimiseur)\n# Note : fonctionne sur des tables persistÃ©es, pas des vues temporaires\n\nprint(\"ğŸ’¡ Pour collecter des statistiques sur une table persistÃ©e :\")\nprint(\"\"\"\n-- Statistiques de base\nANALYZE TABLE my_table COMPUTE STATISTICS\n\n-- Statistiques sur colonnes spÃ©cifiques\nANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1, col2\n\n-- Voir les statistiques\nDESCRIBE EXTENDED my_table\n\"\"\")\n\n\n\n\n7.2 Anti-patterns SQL\n\n\n\n\n\n\n\n\nAnti-pattern\nProblÃ¨me\nSolution\n\n\n\n\nSELECT *\nLit toutes les colonnes\nSELECT col1, col2\n\n\nORDER BY sans LIMIT\nTri global coÃ»teux\nAjouter LIMIT\n\n\nUDF dans WHERE\nPas de pushdown\nExpression native\n\n\nCOUNT(DISTINCT) haute cardinalitÃ©\nTrÃ¨s lent\nAPPROX_COUNT_DISTINCT\n\n\nNOT IN avec NULL\nRÃ©sultats inattendus\nNOT EXISTS ou LEFT JOIN\n\n\n\n\n\nCode\n# Approximation vs COUNT DISTINCT\ncomparison = spark.sql(\"\"\"\n    SELECT \n        COUNT(DISTINCT customer_id) as exact_count,\n        APPROX_COUNT_DISTINCT(customer_id) as approx_count\n    FROM orders\n\"\"\")\n\nprint(\"COUNT DISTINCT vs APPROX_COUNT_DISTINCT:\")\ncomparison.show()\nprint(\"ğŸ’¡ APPROX_COUNT_DISTINCT est ~10x plus rapide sur de grandes tables\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#mini-projet-datamart-customer-analytics",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#mini-projet-datamart-customer-analytics",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸš€ Mini-Projet : Datamart Customer Analytics",
    "text": "ğŸš€ Mini-Projet : Datamart Customer Analytics\n\nğŸ¯ Objectif\nConstruire un datamart analytique client complet en utilisant toutes les techniques apprises.\n\n\nMÃ©triques Ã  calculer\n\nDate premiÃ¨re commande (FIRST_VALUE)\nNombre de commandes (COUNT OVER)\nRevenu total client (SUM OVER)\nDÃ©lai moyen entre commandes (LAG + AVG)\nRang du client par segment (RANK)\n% du revenu du segment (SUM OVER partition)\n\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Orders    â”‚     â”‚  Segments   â”‚\nâ”‚   (fact)    â”‚     â”‚   (dim)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚                   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  CTE: enriched    â”‚\n       â”‚  - joins          â”‚\n       â”‚  - window funcs   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  CTE: customer    â”‚\n       â”‚      stats        â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚    Datamart       â”‚\n       â”‚   [Parquet]       â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\nimport os\nimport shutil\n\n# CrÃ©er des donnÃ©es plus riches\norders_extended = spark.createDataFrame([\n    # Customer 101 - Premium, 3 commandes\n    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n    # Customer 102 - Standard, 4 commandes\n    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n    (6, 102, \"2024-02-10\", 275.0, \"Standard\"),\n    (7, 102, \"2024-03-01\", 320.0, \"Standard\"),\n    # Customer 103 - Premium, 2 commandes\n    (8, 103, \"2024-01-10\", 400.0, \"Premium\"),\n    (9, 103, \"2024-02-15\", 450.0, \"Premium\"),\n    # Customer 104 - Standard, 3 commandes\n    (10, 104, \"2024-01-03\", 100.0, \"Standard\"),\n    (11, 104, \"2024-01-25\", 120.0, \"Standard\"),\n    (12, 104, \"2024-02-20\", 90.0, \"Standard\"),\n    # Customer 105 - Premium, 1 commande\n    (13, 105, \"2024-02-01\", 500.0, \"Premium\"),\n], [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\n\norders_extended = orders_extended.withColumn(\"order_date\", to_date(col(\"order_date\")))\norders_extended.createOrReplaceTempView(\"orders_ext\")\n\nprint(\"ğŸ“¦ DonnÃ©es source:\")\norders_extended.show()\n\n\n\n\nCode\n# Construction du Datamart avec CTEs et Window Functions\ndatamart = spark.sql(\"\"\"\n    WITH \n    -- Ã‰tape 1 : Enrichir les commandes avec mÃ©triques temporelles\n    orders_enriched AS (\n        SELECT \n            customer_id,\n            segment,\n            order_date,\n            amount,\n            \n            -- NumÃ©ro de commande du client\n            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_number,\n            \n            -- Date de la commande prÃ©cÃ©dente\n            LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n            \n            -- DÃ©lai depuis la derniÃ¨re commande\n            DATEDIFF(\n                order_date,\n                LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date)\n            ) as days_since_last\n        FROM orders_ext\n    ),\n    \n    -- Ã‰tape 2 : AgrÃ©ger par client\n    customer_stats AS (\n        SELECT \n            customer_id,\n            segment,\n            \n            -- PremiÃ¨re commande\n            MIN(order_date) as first_order_date,\n            \n            -- DerniÃ¨re commande\n            MAX(order_date) as last_order_date,\n            \n            -- Nombre de commandes\n            COUNT(*) as total_orders,\n            \n            -- Revenus\n            SUM(amount) as total_revenue,\n            AVG(amount) as avg_order_value,\n            \n            -- DÃ©lai moyen entre commandes (exclut NULL de la premiÃ¨re commande)\n            AVG(days_since_last) as avg_days_between_orders\n        FROM orders_enriched\n        GROUP BY customer_id, segment\n    ),\n    \n    -- Ã‰tape 3 : Ajouter des mÃ©triques de segment\n    customer_with_segment_stats AS (\n        SELECT \n            *,\n            \n            -- Total du segment\n            SUM(total_revenue) OVER (PARTITION BY segment) as segment_total_revenue,\n            \n            -- % du revenu du segment\n            ROUND(total_revenue / SUM(total_revenue) OVER (PARTITION BY segment) * 100, 2) as pct_of_segment,\n            \n            -- Rang dans le segment\n            RANK() OVER (PARTITION BY segment ORDER BY total_revenue DESC) as rank_in_segment,\n            \n            -- Nombre de clients dans le segment\n            COUNT(*) OVER (PARTITION BY segment) as customers_in_segment\n        FROM customer_stats\n    )\n    \n    -- RÃ©sultat final\n    SELECT \n        customer_id,\n        segment,\n        first_order_date,\n        last_order_date,\n        total_orders,\n        ROUND(total_revenue, 2) as total_revenue,\n        ROUND(avg_order_value, 2) as avg_order_value,\n        ROUND(avg_days_between_orders, 1) as avg_days_between_orders,\n        rank_in_segment,\n        pct_of_segment,\n        customers_in_segment\n    FROM customer_with_segment_stats\n    ORDER BY segment, rank_in_segment\n\"\"\")\n\nprint(\"ğŸ“Š DATAMART CUSTOMER ANALYTICS:\")\ndatamart.show(truncate=False)\n\n\n\n\nCode\n# Exporter le datamart\noutput_path = \"/tmp/customer_datamart\"\nif os.path.exists(output_path):\n    shutil.rmtree(output_path)\n\ndatamart.write.partitionBy(\"segment\").parquet(output_path)\n\nprint(f\"âœ… Datamart exportÃ© : {output_path}\")\nprint(f\"ğŸ“ PartitionnÃ© par segment\")\n\n# VÃ©rifier la structure\nfor root, dirs, files in os.walk(output_path):\n    level = root.replace(output_path, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n\n\n\n\nCode\n# RÃ©sumÃ© du mini-projet\nprint(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘           ğŸ“Š RÃ‰SUMÃ‰ DU DATAMART                              â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                              â•‘\nâ•‘  Techniques utilisÃ©es :                                      â•‘\nâ•‘  âœ… CTEs pour structurer la transformation                   â•‘\nâ•‘  âœ… ROW_NUMBER() pour numÃ©roter les commandes                â•‘\nâ•‘  âœ… LAG() pour calculer le dÃ©lai entre commandes             â•‘\nâ•‘  âœ… RANK() pour classer les clients par segment              â•‘\nâ•‘  âœ… SUM() OVER pour calculer les totaux de segment           â•‘\nâ•‘  âœ… Partitionnement Parquet par segment                      â•‘\nâ•‘                                                              â•‘\nâ•‘  MÃ©triques calculÃ©es :                                       â•‘\nâ•‘  â€¢ Date premiÃ¨re/derniÃ¨re commande                           â•‘\nâ•‘  â€¢ Nombre total de commandes                                 â•‘\nâ•‘  â€¢ Revenu total et moyen                                     â•‘\nâ•‘  â€¢ DÃ©lai moyen entre commandes                               â•‘\nâ•‘  â€¢ Rang dans le segment                                      â•‘\nâ•‘  â€¢ % du revenu du segment                                    â•‘\nâ•‘                                                              â•‘\nâ•‘  ğŸ’¡ Ce type de transformation sera automatisÃ©                â•‘\nâ•‘     avec dbt dans le module 26                               â•‘\nâ•‘                                                              â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#quiz-de-fin-de-module",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre ROW_NUMBER() et RANK() ?\n\nROW_NUMBER() est plus rapide\n\nRANK() gÃ¨re les ex-aequo avec des gaps, ROW_NUMBER() donne des numÃ©ros uniques\n\nROW_NUMBER() nÃ©cessite ORDER BY, pas RANK()\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” RANK() donne le mÃªme rang aux ex-aequo (1,1,3), ROW_NUMBER() donne toujours des numÃ©ros uniques (1,2,3).\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre ROWS et RANGE dans une window frame ?\n\nROWS est plus rapide\n\nRANGE supporte plus de fonctions\n\nROWS se base sur la position physique, RANGE sur la valeur logique\n\nRANGE ne fonctionne quâ€™avec des dates\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” ROWS compte les lignes physiquement, RANGE peut inclure plusieurs lignes si elles ont la mÃªme valeur.\n\n\n\n\nâ“ Q3. Que fait la fonction GROUPING() ?\n\nGroupe les donnÃ©es\n\nIdentifie si une colonne est agrÃ©gÃ©e (sous-total)\n\nCompte le nombre de groupes\n\nTrie les groupes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” GROUPING() retourne 1 si la colonne est agrÃ©gÃ©e (NULL dans un sous-total), 0 sinon.\n\n\n\n\nâ“ Q4. Comment forcer un broadcast join en SQL Spark ?\n\nFORCE BROADCAST\n\n/*+ BROADCAST(table) */\n\nBROADCAST JOIN\n\nSET spark.broadcast = true\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” On utilise le hint SQL /*+ BROADCAST(table) */ aprÃ¨s SELECT.\n\n\n\n\nâ“ Q5. PIVOT transformeâ€¦ ?\n\nColonnes en lignes\n\nLignes en colonnes\n\nJSON en colonnes\n\nArrays en lignes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” PIVOT transforme les valeurs distinctes dâ€™une colonne en colonnes sÃ©parÃ©es.\n\n\n\n\nâ“ Q6. EXPLODE est utilisÃ© pourâ€¦ ?\n\nCompresser les donnÃ©es\n\nSupprimer les doublons\n\nTransformer un array en plusieurs lignes\n\nJoindre des tables\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” EXPLODE Ã©clate un array (ou map) en crÃ©ant une ligne pour chaque Ã©lÃ©ment.\n\n\n\n\nâ“ Q7. Les CTEs sont-ils toujours plus performants que les subqueries ?\n\nOui, toujours\n\nNon, Catalyst les traite de maniÃ¨re similaire\n\nOui, car ils sont matÃ©rialisÃ©s\n\nNon, ils sont toujours plus lents\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Câ€™est un mythe ! Catalyst inline les CTEs comme des subqueries. Lâ€™avantage est la lisibilitÃ©.\n\n\n\n\nâ“ Q8. Quel hint utiliser pour Ã©viter un shuffle lors dâ€™un join avec une petite table ?\n\n/*+ MERGE */\n\n/*+ SHUFFLE_HASH */\n\n/*+ BROADCAST */\n\n/*+ COALESCE */\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” BROADCAST envoie la petite table Ã  tous les executors, Ã©vitant le shuffle.\n\n\n\n\nâ“ Q9. UNBOUNDED PRECEDING signifieâ€¦ ?\n\nLa ligne prÃ©cÃ©dente\n\nToutes les lignes depuis le dÃ©but de la partition\n\nLa premiÃ¨re ligne de la table\n\nAucune limite de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” UNBOUNDED PRECEDING inclut toutes les lignes depuis le dÃ©but de la partition jusquâ€™Ã  la position actuelle.\n\n\n\n\nâ“ Q10. Comment collecter des statistiques sur une table pour amÃ©liorer lâ€™optimiseur ?\n\nCOMPUTE STATS table\n\nANALYZE TABLE table COMPUTE STATISTICS\n\nCOLLECT STATISTICS table\n\nDESCRIBE STATISTICS table\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ANALYZE TABLE table COMPUTE STATISTICS collecte les statistiques pour lâ€™optimiseur Catalyst.",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark SQL Guide\nWindow Functions\nSQL Syntax\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Window Functions\nAdvanced SQL Optimization",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#prochaine-Ã©tape",
    "title": "ğŸ” Spark SQL Deep Dive & Optimisation",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Spark SQL, passons aux architectures Lakehouse modernes !\nğŸ‘‰ Module suivant : 21_lakehouse_delta_iceberg.ipynb â€” Lakehouse avec Delta Lake & Iceberg\nTu vas apprendre : - Delta Lake : ACID, Time Travel, Versioning - Apache Iceberg : Table format moderne - Schema Evolution : GÃ©rer les changements de schÃ©ma - Optimisations : Compaction, Z-Ordering, Vacuum\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\n\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nWindow Functions\nROW_NUMBER, RANK, LAG/LEAD, FIRST_VALUE, frames\n\n\nPIVOT/UNPIVOT\nReshaping des donnÃ©es\n\n\nGROUPING SETS\nCUBE, ROLLUP, agrÃ©gations multidimensionnelles\n\n\nEXPLODE\nDonnÃ©es semi-structurÃ©es, JSON\n\n\nCTEs\nStructurer les requÃªtes complexes\n\n\nOptimisation\nHints, statistiques, anti-patterns\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Spark SQL Deep Dive.\n\n\nCode\n# Nettoyage\nspark.stop()\nprint(\"âœ… SparkSession arrÃªtÃ©e\")\n\n# Nettoyage des fichiers temporaires (optionnel)\n# import shutil\n# if os.path.exists(\"/tmp/customer_datamart\"):\n#     shutil.rmtree(\"/tmp/customer_datamart\")\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "IntermÃ©diaire",
      "ğŸ” Spark SQL Deep Dive & Optimisation"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html",
    "href": "notebooks/intermediate/18_high_performance_python.html",
    "title": "âš¡ High Performance Python",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  accÃ©lÃ©rer considÃ©rablement tes pipelines Python. Tu dÃ©couvriras comment contourner les limitations du GIL, parallÃ©liser tes traitements, et gÃ©rer des fichiers plus grands que ta RAM !",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#prÃ©requis",
    "href": "notebooks/intermediate/18_high_performance_python.html#prÃ©requis",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nMaÃ®triser Python (fonctions, classes)\n\n\nâœ… Requis\nAvoir suivi le module 17_polars\n\n\nğŸ’¡ RecommandÃ©\nConnaÃ®tre Pandas",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#objectifs-du-module",
    "href": "notebooks/intermediate/18_high_performance_python.html#objectifs-du-module",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre le GIL et ses implications sur la performance\nProfiler ton code pour identifier les vrais goulots dâ€™Ã©tranglement\nUtiliser concurrent.futures pour parallÃ©liser simplement\nMaÃ®triser asyncio pour lâ€™I/O massivement parallÃ¨le\nExploiter Dask pour traiter des fichiers plus grands que la RAM\nChoisir le bon outil selon ton problÃ¨me",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#le-gil-comprendre-la-limitation-fondamentale",
    "href": "notebooks/intermediate/18_high_performance_python.html#le-gil-comprendre-la-limitation-fondamentale",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ”’ 1. Le GIL : Comprendre la limitation fondamentale",
    "text": "ğŸ”’ 1. Le GIL : Comprendre la limitation fondamentale\n\nğŸ§  Cette section est essentielle pour comprendre pourquoi certaines techniques fonctionnent et dâ€™autres non.\n\n\n1.1 Quâ€™est-ce que le GIL ?\nLe Global Interpreter Lock (GIL) est un verrou qui empÃªche Python dâ€™exÃ©cuter plusieurs threads Python simultanÃ©ment.\nAVEC LE GIL (Python standard)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nThread 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\nThread 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps\n          \nâ†’ Les threads s'exÃ©cutent en alternance, pas en parallÃ¨le !\n\n\nSANS GIL (multiprocessing)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nProcess 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nProcess 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps\n           \nâ†’ Vraie exÃ©cution parallÃ¨le sur plusieurs CPUs !\n\n\n1.2 CPU-bound vs I/O-bound\n\n\n\n\n\n\n\n\n\nType\nExemple\nGIL bloquant ?\nSolution\n\n\n\n\nCPU-bound\nCalculs, transformations, ETL\nâœ… Oui\nmultiprocessing, ProcessPoolExecutor\n\n\nI/O-bound\nAPI, fichiers, base de donnÃ©es\nâŒ Non\nthreading, asyncio\n\n\n\nPourquoi le GIL ne bloque pas lâ€™I/O ?\nQuand Python attend une rÃ©ponse rÃ©seau ou disque, il relÃ¢che le GIL. Un autre thread peut donc sâ€™exÃ©cuter pendant ce temps dâ€™attente.\n\nâ„¹ï¸ Le savais-tu ?\nLe GIL a Ã©tÃ© introduit dans Python pour simplifier la gestion de la mÃ©moire. Il rend Python thread-safe par dÃ©faut, mais au prix de la performance multi-thread.\nDes projets comme nogil (Python 3.13+) et subinterpreters travaillent Ã  supprimer ou contourner cette limitation.\nEn attendant, les Data Engineers utilisent multiprocessing pour contourner le GIL !\n\n\n\nCode\nimport time\nimport threading\nfrom multiprocessing import Process\n\ndef cpu_intensive(n):\n    \"\"\"TÃ¢che CPU-bound : calcul intensif\"\"\"\n    total = 0\n    for i in range(n):\n        total += i ** 2\n    return total\n\nN = 5_000_000\n\n# ============ SÃ‰QUENTIEL ============\nstart = time.time()\ncpu_intensive(N)\ncpu_intensive(N)\nseq_time = time.time() - start\nprint(f\"â±ï¸ SÃ©quentiel : {seq_time:.2f}s\")\n\n# ============ THREADING (bloquÃ© par GIL) ============\nstart = time.time()\nt1 = threading.Thread(target=cpu_intensive, args=(N,))\nt2 = threading.Thread(target=cpu_intensive, args=(N,))\nt1.start(); t2.start()\nt1.join(); t2.join()\nthread_time = time.time() - start\nprint(f\"ğŸ§µ Threading : {thread_time:.2f}s (GIL bloque!)\")\n\n# ============ MULTIPROCESSING (contourne GIL) ============\nstart = time.time()\np1 = Process(target=cpu_intensive, args=(N,))\np2 = Process(target=cpu_intensive, args=(N,))\np1.start(); p2.start()\np1.join(); p2.join()\nproc_time = time.time() - start\nprint(f\"ğŸš€ Multiprocessing : {proc_time:.2f}s (vraie parallÃ©lisation!)\")\n\nprint(f\"\\nğŸ“Š Speedup multiprocessing vs sÃ©quentiel : {seq_time/proc_time:.1f}x\")",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#identifier-les-goulots-profiling",
    "href": "notebooks/intermediate/18_high_performance_python.html#identifier-les-goulots-profiling",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ” 2. Identifier les goulots : Profiling",
    "text": "ğŸ” 2. Identifier les goulots : Profiling\n\nâš ï¸ â€œPremature optimization is the root of all evilâ€ â€” Donald Knuth\nAvant dâ€™optimiser, il faut mesurer pour identifier le vrai problÃ¨me.\n\n\n2.1 Outils de profiling\n\n\n\nOutil\nUsage\nComment lâ€™utiliser\n\n\n\n\n%%time\nTemps dâ€™une cellule\nJupyter magic\n\n\n%%timeit\nTemps moyen (plusieurs runs)\nJupyter magic\n\n\ncProfile\nProfiling par fonction\npython -m cProfile script.py\n\n\nline_profiler\nProfiling ligne par ligne\n@profile decorator\n\n\nmemory_profiler\nUsage RAM\n@profile + mprof run\n\n\n\n\n\nCode\n# %%time - mesure le temps d'exÃ©cution d'une cellule\nimport time\n\ndef slow_function():\n    total = 0\n    for i in range(1_000_000):\n        total += i\n    return total\n\n%time result = slow_function()\n\n\n\n\nCode\n# %%timeit - moyenne sur plusieurs exÃ©cutions\ndef fast_function():\n    return sum(range(1_000_000))\n\n%timeit fast_function()\n\n\n\n\nCode\nimport cProfile\nimport pstats\nfrom io import StringIO\n\ndef main_pipeline():\n    \"\"\"Pipeline simulÃ© avec plusieurs Ã©tapes\"\"\"\n    data = list(range(100_000))\n    \n    # Ã‰tape 1 : transformation\n    transformed = [x ** 2 for x in data]\n    \n    # Ã‰tape 2 : filtrage\n    filtered = [x for x in transformed if x % 2 == 0]\n    \n    # Ã‰tape 3 : agrÃ©gation\n    result = sum(filtered)\n    \n    return result\n\n# Profiler le code\nprofiler = cProfile.Profile()\nprofiler.enable()\n\nresult = main_pipeline()\n\nprofiler.disable()\n\n# Afficher les rÃ©sultats\nstream = StringIO()\nstats = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')\nstats.print_stats(10)\nprint(stream.getvalue())",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#stratÃ©gies-de-performance-vue-densemble",
    "href": "notebooks/intermediate/18_high_performance_python.html#stratÃ©gies-de-performance-vue-densemble",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ—ºï¸ 3. StratÃ©gies de performance : Vue dâ€™ensemble",
    "text": "ğŸ—ºï¸ 3. StratÃ©gies de performance : Vue dâ€™ensemble\n\n\n\n\n\n\n\n\nBesoin\nSolution\nQuand lâ€™utiliser\n\n\n\n\nMulti-CPU (CPU-bound)\nProcessPoolExecutor\nETL lourd, calculs\n\n\nI/O parallÃ¨le (simple)\nThreadPoolExecutor\n&lt; 20 requÃªtes/fichiers\n\n\nI/O parallÃ¨le (massif)\nasyncio\n100+ requÃªtes API\n\n\nGros fichiers (&gt; RAM)\nPolars streaming, Dask\n10-100+ Go\n\n\nParallÃ©lisation simple\njoblib\nBoucles, ML\n\n\n\n\nğŸŒ³ Arbre de dÃ©cision\nTon problÃ¨me est...\nâ”‚\nâ”œâ”€â–¶ CPU-bound (calculs, transformations) ?\nâ”‚   â”œâ”€â–¶ Simple/boucle â†’ joblib\nâ”‚   â””â”€â–¶ Complexe/chunks â†’ ProcessPoolExecutor\nâ”‚\nâ”œâ”€â–¶ I/O-bound (API, fichiers, DB) ?\nâ”‚   â”œâ”€â–¶ &lt; 20 requÃªtes â†’ ThreadPoolExecutor\nâ”‚   â””â”€â–¶ 100+ requÃªtes â†’ asyncio\nâ”‚\nâ””â”€â–¶ Gros fichiers (&gt; RAM) ?\n    â”œâ”€â–¶ Single file, Polars-like â†’ Polars streaming\n    â”œâ”€â–¶ Multi-files, Pandas-like â†’ Dask\n    â””â”€â–¶ Cluster distribuÃ© â†’ Spark (module 19)",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#concurrent.futures-lapi-moderne",
    "href": "notebooks/intermediate/18_high_performance_python.html#concurrent.futures-lapi-moderne",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ¯ 4. concurrent.futures â€” Lâ€™API moderne",
    "text": "ğŸ¯ 4. concurrent.futures â€” Lâ€™API moderne\n\nâœ… Plus simple que multiprocessing et threading bruts âœ… Interface unifiÃ©e pour threads et processes\n\n\n4.1 ThreadPoolExecutor (I/O-bound)\n\n\nCode\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n\ndef simulate_io_task(task_id):\n    \"\"\"Simule une tÃ¢che I/O (ex: requÃªte API)\"\"\"\n    time.sleep(0.5)  # Simule latence rÃ©seau\n    return f\"Task {task_id} completed\"\n\ntasks = list(range(10))\n\n# ============ SÃ‰QUENTIEL ============\nstart = time.time()\nresults_seq = [simulate_io_task(t) for t in tasks]\nprint(f\"â±ï¸ SÃ©quentiel : {time.time() - start:.2f}s\")\n\n# ============ THREADPOOL ============\nstart = time.time()\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    results_parallel = list(executor.map(simulate_io_task, tasks))\nprint(f\"ğŸš€ ThreadPool : {time.time() - start:.2f}s\")\n\nprint(f\"\\nğŸ“Š Speedup : {5.0 / (time.time() - start + 0.01):.1f}x environ\")\n\n\n\n\n4.2 ProcessPoolExecutor (CPU-bound)\n\n\nCode\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\nimport os\n\ndef cpu_task(n):\n    \"\"\"TÃ¢che CPU-intensive\"\"\"\n    return sum(i ** 2 for i in range(n))\n\n# DonnÃ©es Ã  traiter\ndata_chunks = [1_000_000] * 8  # 8 chunks\n\n# ============ SÃ‰QUENTIEL ============\nstart = time.time()\nresults_seq = [cpu_task(chunk) for chunk in data_chunks]\nseq_time = time.time() - start\nprint(f\"â±ï¸ SÃ©quentiel : {seq_time:.2f}s\")\n\n# ============ PROCESSPOOL ============\nstart = time.time()\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    results_parallel = list(executor.map(cpu_task, data_chunks))\nproc_time = time.time() - start\nprint(f\"ğŸš€ ProcessPool : {proc_time:.2f}s\")\n\nprint(f\"\\nğŸ“Š Speedup : {seq_time/proc_time:.1f}x\")\nprint(f\"ğŸ’» CPUs disponibles : {os.cpu_count()}\")\n\n\n\n\n4.3 Gestion avancÃ©e : submit() et as_completed()\n\n\nCode\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport random\n\ndef fetch_data(task_id):\n    \"\"\"Simule une requÃªte avec temps variable\"\"\"\n    delay = random.uniform(0.1, 1.0)\n    time.sleep(delay)\n    return {\"task_id\": task_id, \"delay\": delay}\n\n# Utiliser submit() pour plus de contrÃ´le\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    # Soumettre les tÃ¢ches\n    futures = {executor.submit(fetch_data, i): i for i in range(5)}\n    \n    # Traiter les rÃ©sultats au fur et Ã  mesure qu'ils arrivent\n    for future in as_completed(futures):\n        task_id = futures[future]\n        try:\n            result = future.result(timeout=5)  # Timeout de 5s\n            print(f\"âœ… Task {task_id} terminÃ©e en {result['delay']:.2f}s\")\n        except Exception as e:\n            print(f\"âŒ Task {task_id} a Ã©chouÃ© : {e}\")\n\n\n\n\n4.4 Quand utiliser quoi ?\n\n\n\n\n\n\n\n\n\nExecutor\nGIL contournÃ© ?\nUsage\nExemple\n\n\n\n\nThreadPoolExecutor\nâŒ Non\nI/O : API, fichiers\nTÃ©lÃ©charger 50 fichiers\n\n\nProcessPoolExecutor\nâœ… Oui\nCPU : calculs, ETL\nTransformer 8 chunks de donnÃ©es",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#multiprocessing-contrÃ´le-avancÃ©",
    "href": "notebooks/intermediate/18_high_performance_python.html#multiprocessing-contrÃ´le-avancÃ©",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ”§ 5. multiprocessing â€” ContrÃ´le avancÃ©",
    "text": "ğŸ”§ 5. multiprocessing â€” ContrÃ´le avancÃ©\nPour les cas oÃ¹ concurrent.futures ne suffit pas.\n\n5.1 Pool avec map\n\n\nCode\nfrom multiprocessing import Pool, cpu_count\nimport numpy as np\n\ndef process_chunk(chunk):\n    \"\"\"Traite un chunk de donnÃ©es\"\"\"\n    return np.sum(chunk ** 2)\n\n# CrÃ©er des donnÃ©es\nbig_array = np.random.rand(1_000_000)\n\n# Partitionner en chunks\nn_workers = cpu_count()\nchunks = np.array_split(big_array, n_workers)\n\nprint(f\"ğŸ’» Nombre de workers : {n_workers}\")\nprint(f\"ğŸ“¦ Taille des chunks : {[len(c) for c in chunks]}\")\n\n# Traitement parallÃ¨le\nwith Pool(processes=n_workers) as pool:\n    results = pool.map(process_chunk, chunks)\n\ntotal = sum(results)\nprint(f\"\\nâœ… RÃ©sultat total : {total:.2f}\")\n\n\n\n\n5.2 starmap pour plusieurs arguments\n\n\nCode\nfrom multiprocessing import Pool\n\ndef process_with_params(data, multiplier, offset):\n    \"\"\"Fonction avec plusieurs paramÃ¨tres\"\"\"\n    return sum(data) * multiplier + offset\n\n# PrÃ©parer les arguments\nargs_list = [\n    ([1, 2, 3], 2, 10),\n    ([4, 5, 6], 3, 20),\n    ([7, 8, 9], 4, 30),\n]\n\nwith Pool(3) as pool:\n    results = pool.starmap(process_with_params, args_list)\n\nprint(\"RÃ©sultats:\", results)\n\n\n\n\n5.3 Limites et prÃ©cautions\n\n\n\nâš ï¸ Limite\nExplication\n\n\n\n\nOverhead\nCrÃ©er des process prend du temps (~100ms)\n\n\nSÃ©rialisation\nLes donnÃ©es sont copiÃ©es (pickle)\n\n\nif __name__ == \"__main__\"\nObligatoire sur Windows\n\n\nMÃ©moire\nChaque process a sa propre mÃ©moire\n\n\n\n# âš ï¸ Toujours protÃ©ger avec if __name__ == \"__main__\"\nif __name__ == \"__main__\":\n    with Pool(4) as pool:\n        results = pool.map(my_func, data)",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#asyncio-io-massivement-parallÃ¨le",
    "href": "notebooks/intermediate/18_high_performance_python.html#asyncio-io-massivement-parallÃ¨le",
    "title": "âš¡ High Performance Python",
    "section": "ğŸŒ 6. asyncio â€” I/O massivement parallÃ¨le",
    "text": "ğŸŒ 6. asyncio â€” I/O massivement parallÃ¨le\n\nâœ… IdÃ©al pour : ingestion depuis API, requÃªtes DB/S3 massives âŒ Pas pour : calculs CPU-intensive\n\n\n6.1 Concepts : async/await\n\n\nCode\nimport asyncio\nimport time\n\nasync def fetch_data(task_id):\n    \"\"\"Simule une requÃªte API asynchrone\"\"\"\n    print(f\"ğŸš€ DÃ©but tÃ¢che {task_id}\")\n    await asyncio.sleep(1)  # Simule latence rÃ©seau (non-bloquant !)\n    print(f\"âœ… Fin tÃ¢che {task_id}\")\n    return f\"result_{task_id}\"\n\nasync def main():\n    # Lancer 5 tÃ¢ches en parallÃ¨le\n    tasks = [fetch_data(i) for i in range(5)]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# ExÃ©cuter\nstart = time.time()\nresults = await main()  # Dans Jupyter, pas besoin de asyncio.run()\nprint(f\"\\nâ±ï¸ Temps total : {time.time() - start:.2f}s (au lieu de 5s sÃ©quentiel)\")\nprint(f\"ğŸ“Š RÃ©sultats : {results}\")\n\n\n\n\n6.2 Exemple rÃ©el avec aiohttp\n\n\nCode\n# Installation : pip install aiohttp\nimport asyncio\n\n# Simulons aiohttp pour l'exemple (sans vraies requÃªtes)\nasync def fetch_url(session, url):\n    \"\"\"Simule une requÃªte HTTP\"\"\"\n    await asyncio.sleep(0.1)  # Simule latence\n    return {\"url\": url, \"status\": 200}\n\nasync def fetch_all_urls(urls):\n    \"\"\"Fetch toutes les URLs en parallÃ¨le\"\"\"\n    session = None  # En vrai : async with aiohttp.ClientSession() as session:\n    \n    tasks = [fetch_url(session, url) for url in urls]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n\n# Simuler 20 URLs\nurls = [f\"https://api.example.com/data/{i}\" for i in range(20)]\n\nstart = time.time()\nresults = await fetch_all_urls(urls)\nprint(f\"â±ï¸ 20 requÃªtes en {time.time() - start:.2f}s\")\nprint(f\"âœ… SuccÃ¨s : {len([r for r in results if isinstance(r, dict)])}\")\n\n\n\n\n6.3 Semaphore : limiter les connexions simultanÃ©es\n\n\nCode\nimport asyncio\nimport time\n\n# Limiter Ã  5 connexions simultanÃ©es\nsemaphore = asyncio.Semaphore(5)\n\nasync def fetch_limited(task_id):\n    \"\"\"Fetch avec limite de concurrence\"\"\"\n    async with semaphore:  # Attend si dÃ©jÃ  5 en cours\n        print(f\"ğŸš€ TÃ¢che {task_id} dÃ©marre\")\n        await asyncio.sleep(0.5)\n        print(f\"âœ… TÃ¢che {task_id} terminÃ©e\")\n        return task_id\n\nasync def main():\n    tasks = [fetch_limited(i) for i in range(15)]\n    return await asyncio.gather(*tasks)\n\nstart = time.time()\nresults = await main()\nprint(f\"\\nâ±ï¸ 15 tÃ¢ches (max 5 simultanÃ©es) en {time.time() - start:.2f}s\")\n\n\n\n\n6.4 Quand NE PAS utiliser asyncio\n\n\n\nSituation\nasyncio efficace ?\nAlternative\n\n\n\n\n100+ appels API\nâœ… Oui\n-\n\n\nLecture S3/DB massives\nâœ… Oui\n-\n\n\nCalculs CPU\nâŒ Non\nProcessPoolExecutor\n\n\n5 requÃªtes simples\nâŒ Overkill\nThreadPoolExecutor\n\n\nCode synchrone existant\nâŒ Refactoring lourd\nThreadPoolExecutor",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#dask-pandas-distribuÃ©",
    "href": "notebooks/intermediate/18_high_performance_python.html#dask-pandas-distribuÃ©",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ“Š 7. Dask â€” Pandas distribuÃ©",
    "text": "ğŸ“Š 7. Dask â€” Pandas distribuÃ©\n\nğŸ”¥ Le plus utile avant Spark pour traiter des fichiers plus grands que la RAM.\n\n\n7.1 Pourquoi Dask ?\n\n\n\nAvantage\nDescription\n\n\n\n\nAPI Pandas-like\nMigration facile\n\n\nLazy evaluation\nOptimisation automatique\n\n\nParallÃ©lisme\nUtilise tous les CPUs\n\n\nOut-of-core\nFichiers &gt; RAM\n\n\nScalable\nOptionnellement sur cluster\n\n\n\n\n\nCode\n# Installation : pip install dask[complete]\nimport dask.dataframe as dd\nimport pandas as pd\nimport os\n\n# CrÃ©er des fichiers de test\nos.makedirs(\"data/dask_demo\", exist_ok=True)\n\nfor i in range(5):\n    df = pd.DataFrame({\n        \"id\": range(i * 10000, (i + 1) * 10000),\n        \"category\": [f\"cat_{j % 5}\" for j in range(10000)],\n        \"amount\": [float(j % 1000) for j in range(10000)]\n    })\n    df.to_csv(f\"data/dask_demo/file_{i}.csv\", index=False)\n\nprint(\"âœ… 5 fichiers CSV crÃ©Ã©s (50,000 lignes au total)\")\n\n\n\n\nCode\nimport dask.dataframe as dd\nimport time\n\n# Lire TOUS les fichiers avec glob pattern (lazy !)\nddf = dd.read_csv(\"data/dask_demo/*.csv\")\n\nprint(\"Type:\", type(ddf))\nprint(f\"Partitions: {ddf.npartitions}\")\nprint(\"\\nâš ï¸ Les donnÃ©es ne sont PAS encore chargÃ©es !\")\nprint(ddf)  # Affiche le schÃ©ma, pas les donnÃ©es\n\n\n\n\nCode\n# Pipeline Dask (lazy)\nresult = (\n    ddf\n    .filter(ddf.amount &gt; 100)  # Filtrage\n    .assign(amount_doubled=ddf.amount * 2)  # Transformation\n    .groupby(\"category\")  # AgrÃ©gation\n    .amount.sum()\n)\n\nprint(\"Pipeline dÃ©fini (lazy) :\")\nprint(result)\n\n# ExÃ©cuter avec compute()\nstart = time.time()\nfinal_result = result.compute()\nprint(f\"\\nâ±ï¸ ExÃ©cution : {time.time() - start:.2f}s\")\nprint(\"\\nRÃ©sultat :\")\nprint(final_result)\n\n\n\n\n7.2 Dask vs Polars vs Spark\n\n\n\nAspect\nPolars\nDask\nSpark\n\n\n\n\nSingle machine\nâ­â­â­\nâ­â­\nâ­\n\n\nCluster\nâŒ\nâ­â­\nâ­â­â­\n\n\nAPI\nPropre\nPandas-like\nPropre\n\n\nSetup\nSimple\nSimple\nComplexe\n\n\nVitesse (single node)\nâ­â­â­\nâ­â­\nâ­\n\n\nÃ‰cosystÃ¨me\nNouveau\nMature\nTrÃ¨s mature\n\n\n\n\n\n7.3 Quand utiliser Dask ?\n\nâœ… Fichiers &gt; RAM mais &lt; 100 Go\nâœ… Tu connais dÃ©jÃ  Pandas\nâœ… Pas besoin dâ€™un cluster Spark\nâœ… Traitement multi-fichiers\nâŒ Si single file &lt; 10 Go â†’ utilise Polars\nâŒ Si &gt; 100 Go ou cluster â†’ utilise Spark",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#joblib-parallÃ©lisation-simple",
    "href": "notebooks/intermediate/18_high_performance_python.html#joblib-parallÃ©lisation-simple",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ¨ 8. joblib â€” ParallÃ©lisation simple",
    "text": "ğŸ¨ 8. joblib â€” ParallÃ©lisation simple\n\nâœ… Ultra-simple â€” parfait pour parallÃ©liser une boucle rapidement âœ… TrÃ¨s utilisÃ© en Data Science (sklearn lâ€™utilise en interne)\n\n\n\nCode\n# Installation : pip install joblib\nfrom joblib import Parallel, delayed\nimport time\n\ndef expensive_computation(x):\n    \"\"\"Calcul coÃ»teux\"\"\"\n    time.sleep(0.1)  # Simule un calcul\n    return x ** 2\n\ndata = list(range(20))\n\n# ============ SÃ‰QUENTIEL ============\nstart = time.time()\nresults_seq = [expensive_computation(x) for x in data]\nprint(f\"â±ï¸ SÃ©quentiel : {time.time() - start:.2f}s\")\n\n# ============ JOBLIB ============\nstart = time.time()\nresults_parallel = Parallel(n_jobs=-1)(  # -1 = tous les CPUs\n    delayed(expensive_computation)(x) for x in data\n)\nprint(f\"ğŸš€ Joblib : {time.time() - start:.2f}s\")\n\n\n\n\nCode\nfrom joblib import Parallel, delayed\n\ndef io_task(x):\n    time.sleep(0.1)\n    return x\n\n# Options utiles\nresults = Parallel(\n    n_jobs=4,              # Nombre de workers\n    backend=\"threading\",   # \"threading\" pour I/O, \"loky\" (dÃ©faut) pour CPU\n    verbose=1              # Affiche la progression\n)(delayed(io_task)(x) for x in range(10))\n\nprint(f\"\\nâœ… RÃ©sultats : {results}\")",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#choisir-la-bonne-technologie",
    "href": "notebooks/intermediate/18_high_performance_python.html#choisir-la-bonne-technologie",
    "title": "âš¡ High Performance Python",
    "section": "ğŸŒ³ 9. Choisir la bonne technologie",
    "text": "ğŸŒ³ 9. Choisir la bonne technologie\n\nRÃ©capitulatif\n\n\n\nOutil\nType\nGIL contournÃ©\nComplexitÃ©\nUse case\n\n\n\n\nThreadPoolExecutor\nI/O\nâŒ Non\nâ­\n&lt; 20 requÃªtes/fichiers\n\n\nProcessPoolExecutor\nCPU\nâœ… Oui\nâ­â­\nETL, calculs\n\n\nasyncio\nI/O\nâŒ Non\nâ­â­â­\n100+ requÃªtes API\n\n\njoblib\nCPU/I/O\nâœ… (loky)\nâ­\nBoucles simples, ML\n\n\nDask\nBig Data\nâœ… Oui\nâ­â­\nFichiers &gt; RAM\n\n\n\n\n\nğŸ–¼ï¸ Arbre de dÃ©cision visuel\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Quel problÃ¨me? â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚CPU-boundâ”‚        â”‚I/O-boundâ”‚        â”‚Fichiers &gt;RAMâ”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                  â”‚                    â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n Simple?  Complex?  &lt;20 req?  100+ req?  &lt;100Go?  &gt;100Go?\n    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n joblib   Process   Thread    asyncio    Dask     Spark\n          Pool      Pool                        (mod 19)",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "href": "notebooks/intermediate/18_high_performance_python.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "title": "âš¡ High Performance Python",
    "section": "âš ï¸ 10. Bonnes pratiques & Erreurs frÃ©quentes",
    "text": "âš ï¸ 10. Bonnes pratiques & Erreurs frÃ©quentes\n\nâŒ Erreurs frÃ©quentes\n\n\n\nErreur\nPourquoi câ€™est faux\nSolution\n\n\n\n\nThreading pour CPU\nGIL bloque\nProcessPoolExecutor\n\n\nAsync pour calculs\nPas de gain\nProcessPoolExecutor\n\n\nPandas sur 50 Go\nCrash RAM\nDask ou Polars streaming\n\n\n100 workers pour 10 tÃ¢ches\nOverhead inutile\nAdapter au workload\n\n\nPas de profiling\nOptimise au hasard\nToujours profiler dâ€™abord\n\n\nOublier if __name__\nCrash sur Windows\nToujours protÃ©ger\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nProfiler dâ€™abord\nIdentifier le vrai goulot\n\n\nÃ‰crire en Parquet\nI/O plus rapide\n\n\nPartitionner intelligemment\nÃ‰vite surcharge mÃ©moire\n\n\nTester avec peu de workers\nPuis augmenter progressivement\n\n\nif __name__ == \"__main__\"\nObligatoire pour multiprocessing\n\n\nUtiliser n_jobs=-1\nUtilise tous les CPUs disponibles",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/18_high_performance_python.html#quiz-de-fin-de-module",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quâ€™est-ce que le GIL empÃªche en Python ?\n\nLâ€™exÃ©cution de code Python\n\nLâ€™exÃ©cution simultanÃ©e de plusieurs threads Python\n\nLâ€™utilisation de la mÃ©moire\n\nLa lecture de fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le GIL (Global Interpreter Lock) empÃªche lâ€™exÃ©cution simultanÃ©e de plusieurs threads Python, les forÃ§ant Ã  sâ€™exÃ©cuter en alternance.\n\n\n\n\nâ“ Q2. Pour un traitement CPU-intensive, quel outil utiliser ?\n\nThreadPoolExecutor\n\nProcessPoolExecutor\n\nasyncio\n\nTous sont Ã©quivalents\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ProcessPoolExecutor contourne le GIL en utilisant des processus sÃ©parÃ©s, permettant une vraie parallÃ©lisation CPU.\n\n\n\n\nâ“ Q3. Quand utiliser asyncio ?\n\nPour des calculs mathÃ©matiques complexes\n\nPour tÃ©lÃ©charger 100+ fichiers depuis une API\n\nPour trier un gros tableau\n\nPour compresser des fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” asyncio est idÃ©al pour lâ€™I/O massivement parallÃ¨le (requÃªtes API, tÃ©lÃ©chargements). Les autres sont CPU-bound.\n\n\n\n\nâ“ Q4. Que fait ddf.compute() dans Dask ?\n\nDÃ©finit le pipeline\n\nAffiche le schÃ©ma\n\nDÃ©clenche lâ€™exÃ©cution et retourne un DataFrame Pandas\n\nSauvegarde les donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” .compute() dÃ©clenche lâ€™exÃ©cution du pipeline lazy et retourne le rÃ©sultat sous forme de DataFrame Pandas.\n\n\n\n\nâ“ Q5. Que signifie n_jobs=-1 dans joblib ?\n\nDÃ©sactive le parallÃ©lisme\n\nUtilise un seul CPU\n\nUtilise tous les CPUs disponibles\n\nErreur de configuration\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” n_jobs=-1 indique Ã  joblib dâ€™utiliser tous les CPUs disponibles sur la machine.\n\n\n\n\nâ“ Q6. Pourquoi ThreadPoolExecutor ne accÃ©lÃ¨re pas les calculs CPU ?\n\nParce quâ€™il est mal implÃ©mentÃ©\n\nParce que le GIL force lâ€™exÃ©cution sÃ©quentielle des threads Python\n\nParce quâ€™il utilise trop de mÃ©moire\n\nParce quâ€™il est obsolÃ¨te\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le GIL empÃªche les threads Python de sâ€™exÃ©cuter en parallÃ¨le. Pour du CPU-bound, il faut utiliser des processus.\n\n\n\n\nâ“ Q7. Pour traiter un fichier de 50 Go avec une API Pandas-like, quel outil choisir ?\n\nPandas\n\nPolars\n\nDask\n\nasyncio\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Dask permet de traiter des fichiers plus grands que la RAM avec une API similaire Ã  Pandas.\n\n\n\n\nâ“ Q8. Quelle est la premiÃ¨re Ã©tape avant dâ€™optimiser du code ?\n\nAjouter du multiprocessing\n\nRÃ©Ã©crire en Rust\n\nProfiler pour identifier le goulot dâ€™Ã©tranglement\n\nUtiliser asyncio\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” â€œPremature optimization is the root of all evilâ€. Il faut dâ€™abord mesurer pour savoir oÃ¹ optimiser.",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#mini-projet-pipeline-haute-performance",
    "href": "notebooks/intermediate/18_high_performance_python.html#mini-projet-pipeline-haute-performance",
    "title": "âš¡ High Performance Python",
    "section": "ğŸš€ Mini-projet : Pipeline haute performance",
    "text": "ğŸš€ Mini-projet : Pipeline haute performance\n\nğŸ¯ Objectif\nCombiner plusieurs techniques pour crÃ©er un pipeline performant : - ProcessPoolExecutor pour transformation CPU-intensive - Dask pour agrÃ©gation - Export Parquet\n\n\nğŸ—ï¸ Architecture\ndata/raw/*.csv\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ProcessPoolExecutor â”‚  Transformation parallÃ¨le (CPU)\nâ”‚  - Nettoyage        â”‚\nâ”‚  - Feature eng.     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\ndata/intermediate/*.csv\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Dask DataFrame    â”‚  AgrÃ©gation (multi-fichiers)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\ndata/processed/result.parquet\n\n\nCode\n# Setup : crÃ©er les donnÃ©es de test\nimport pandas as pd\nimport numpy as np\nimport os\n\nos.makedirs(\"data/raw\", exist_ok=True)\nos.makedirs(\"data/intermediate\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\n\n# CrÃ©er 10 fichiers CSV (simule des logs)\nnp.random.seed(42)\ncategories = [\"web\", \"api\", \"db\", \"cache\", \"auth\"]\nstatuses = [\"success\", \"error\", \"timeout\"]\n\nfor i in range(10):\n    n_rows = 10000\n    df = pd.DataFrame({\n        \"timestamp\": pd.date_range(\"2024-01-01\", periods=n_rows, freq=\"s\"),\n        \"category\": np.random.choice(categories, n_rows),\n        \"status\": np.random.choice(statuses, n_rows, p=[0.8, 0.15, 0.05]),\n        \"response_time_ms\": np.random.exponential(100, n_rows),\n        \"bytes_sent\": np.random.randint(100, 10000, n_rows)\n    })\n    df.to_csv(f\"data/raw/logs_{i:02d}.csv\", index=False)\n\nprint(\"âœ… 10 fichiers CSV crÃ©Ã©s (100,000 lignes au total)\")\n\n\n\n\nCode\nfrom concurrent.futures import ProcessPoolExecutor\nimport pandas as pd\nimport numpy as np\nimport glob\nimport time\n\ndef transform_file(filepath):\n    \"\"\"\n    Transformation CPU-intensive d'un fichier.\n    - Nettoyage\n    - Feature engineering\n    - Export intermÃ©diaire\n    \"\"\"\n    # Lire\n    df = pd.read_csv(filepath)\n    \n    # Nettoyage : filtrer les timeouts extrÃªmes\n    df = df[df[\"response_time_ms\"] &lt; 10000]\n    \n    # Feature engineering (CPU-intensive)\n    df[\"response_time_log\"] = np.log1p(df[\"response_time_ms\"])\n    df[\"is_error\"] = (df[\"status\"] != \"success\").astype(int)\n    df[\"throughput\"] = df[\"bytes_sent\"] / (df[\"response_time_ms\"] + 1)\n    \n    # Extraction date\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n    \n    # Export intermÃ©diaire\n    output_path = filepath.replace(\"raw\", \"intermediate\")\n    df.to_csv(output_path, index=False)\n    \n    return output_path\n\n# Liste des fichiers\ninput_files = sorted(glob.glob(\"data/raw/*.csv\"))\nprint(f\"ğŸ“ {len(input_files)} fichiers Ã  traiter\")\n\n# ============ TRANSFORMATION PARALLÃˆLE ============\nstart = time.time()\n\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    output_files = list(executor.map(transform_file, input_files))\n\nprint(f\"\\nâ±ï¸ Transformation : {time.time() - start:.2f}s\")\nprint(f\"âœ… {len(output_files)} fichiers transformÃ©s\")\n\n\n\n\nCode\nimport dask.dataframe as dd\nimport time\n\n# ============ AGRÃ‰GATION AVEC DASK ============\nstart = time.time()\n\n# Lire tous les fichiers intermÃ©diaires (lazy)\nddf = dd.read_csv(\"data/intermediate/*.csv\")\n\n# Pipeline d'agrÃ©gation\nresult = (\n    ddf\n    .groupby([\"category\", \"status\", \"hour\"])\n    .agg({\n        \"response_time_ms\": [\"mean\", \"max\", \"count\"],\n        \"bytes_sent\": \"sum\",\n        \"is_error\": \"sum\",\n        \"throughput\": \"mean\"\n    })\n    .compute()  # ExÃ©cution\n)\n\n# Aplatir les colonnes multi-index\nresult.columns = ['_'.join(col).strip() for col in result.columns.values]\nresult = result.reset_index()\n\nprint(f\"â±ï¸ AgrÃ©gation Dask : {time.time() - start:.2f}s\")\nprint(f\"\\nğŸ“Š RÃ©sultat : {len(result)} lignes\")\nprint(result.head(10))\n\n\n\n\nCode\n# ============ EXPORT PARQUET ============\nresult.to_parquet(\"data/processed/aggregated_logs.parquet\", index=False)\nprint(\"âœ… RÃ©sultat exportÃ© : data/processed/aggregated_logs.parquet\")\n\n# VÃ©rification\nimport os\nsize_bytes = os.path.getsize(\"data/processed/aggregated_logs.parquet\")\nprint(f\"ğŸ“¦ Taille : {size_bytes / 1024:.1f} KB\")\n\n\n\n\nCode\n# ============ RÃ‰SUMÃ‰ DU PIPELINE ============\nprint(\"\\n\" + \"=\"*50)\nprint(\"ğŸ“Š RÃ‰SUMÃ‰ DU PIPELINE HAUTE PERFORMANCE\")\nprint(\"=\"*50)\nprint(f\"\"\"  \n1ï¸âƒ£ Input : 10 fichiers CSV (100K lignes)\n2ï¸âƒ£ Transformation : ProcessPoolExecutor (4 workers)\n   - Nettoyage\n   - Feature engineering\n3ï¸âƒ£ AgrÃ©gation : Dask DataFrame\n   - GroupBy multi-colonnes\n   - Aggregations multiples\n4ï¸âƒ£ Output : Parquet ({size_bytes / 1024:.1f} KB)\n\"\"\")",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/18_high_performance_python.html#ressources-pour-aller-plus-loin",
    "title": "âš¡ High Performance Python",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nconcurrent.futures â€” Doc Python\nasyncio â€” Doc Python\nDask Documentation â€” Guide complet\njoblib â€” ParallÃ©lisation simple\n\n\n\nğŸ“– Articles & Tutoriels\n\nReal Python - Async IO â€” Tutoriel complet\nSpeed Up Your Python Code â€” Guide concurrence\n\n\n\nğŸ”§ Outils de profiling\n\npy-spy â€” Sampling profiler\nScalene â€” CPU + mÃ©moire + GPU profiler",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/18_high_performance_python.html#prochaine-Ã©tape",
    "title": "âš¡ High Performance Python",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les techniques de performance en Python, passons au traitement distribuÃ© Ã  grande Ã©chelle avec Spark !\nğŸ‘‰ Module suivant : 19_pyspark_advanced.ipynb â€” PySpark AvancÃ©\nTu vas apprendre : - Architecture Spark (Driver, Executors) - RDDs et DataFrames Spark - Optimisations (partitioning, caching) - Spark sur Kubernetes\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nOutil\nType\nQuand lâ€™utiliser\n\n\n\n\nThreadPoolExecutor\nI/O\n&lt; 20 requÃªtes/fichiers\n\n\nProcessPoolExecutor\nCPU\nCalculs, transformations\n\n\nasyncio\nI/O massif\n100+ requÃªtes API\n\n\njoblib\nSimple\nParallÃ©liser une boucle\n\n\nDask\nBig Data\nFichiers &gt; RAM, API Pandas\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module High Performance Python.\n\n\nCode\n# Nettoyage des fichiers temporaires (optionnel)\nimport shutil\nimport os\n\n# DÃ©commenter pour nettoyer\n# for folder in [\"data/raw\", \"data/intermediate\", \"data/processed\", \"data/dask_demo\"]:\n#     if os.path.exists(folder):\n#         shutil.rmtree(folder)\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "IntermÃ©diaire",
      "âš¡ High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le Cloud Computing et lâ€™Object Storage, les fondations de tout Data Lake moderne.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#prÃ©requis",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#prÃ©requis",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\n\nNiveau\nModule\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 14\nDocker Fundamentals\n\n\nâœ… Requis\nModule 19\nPySpark Advanced\n\n\nâœ… Requis\nModule 21\nSpark on Kubernetes\n\n\nğŸ’¡ RecommandÃ©\nModule 08\nBig Data Introduction (Medallion Architecture)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#objectifs-du-module",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#objectifs-du-module",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les modÃ¨les Cloud (IaaS, PaaS, SaaS)\nConnaÃ®tre les services Data Engineering sur AWS, GCP, Azure\nMaÃ®triser les concepts de lâ€™Object Storage (buckets, keys, prefixes)\nLire/Ã©crire sur S3, Azure Blob, GCS avec Python et Spark\nDÃ©ployer MinIO pour pratiquer localement\nOptimiser les performances (formats, partitionnement, small files)\nGÃ©rer la sÃ©curitÃ© et les coÃ»ts",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-pourquoi-le-cloud-pour-le-data-engineering",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-pourquoi-le-cloud-pour-le-data-engineering",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ“š 1. Introduction â€” Pourquoi le Cloud pour le Data Engineering ?",
    "text": "ğŸ“š 1. Introduction â€” Pourquoi le Cloud pour le Data Engineering ?\n\n1.1 Lâ€™Ã©volution du Data Engineering\n2000s                    2010s                    2020s+\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  On-Premise â”‚         â”‚   Hadoop    â”‚         â”‚    Cloud    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ RDBMS â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚ HDFS  â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚  S3   â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  CoÃ»teux    â”‚         â”‚  Complexe   â”‚         â”‚  Scalable   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Avantages du Cloud pour le Data Engineering\n\n\n\nAvantage\nDescription\n\n\n\n\nScalabilitÃ©\nStockage et compute illimitÃ©s Ã  la demande\n\n\nCoÃ»t\nPay-as-you-go, pas dâ€™investissement initial\n\n\nManaged Services\nMoins dâ€™ops, plus de focus sur les donnÃ©es\n\n\nSÃ©paration compute/storage\nScaler indÃ©pendamment\n\n\nDurabilitÃ©\n99.999999999% (11 nines) pour S3\n\n\nGlobal\nDonnÃ©es accessibles partout",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-au-cloud-computing",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-au-cloud-computing",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "â˜ï¸ 2. Introduction au Cloud Computing",
    "text": "â˜ï¸ 2. Introduction au Cloud Computing\n\n2.1 Câ€™est quoi le Cloud ?\nLe Cloud = des serveurs, du stockage et des services accessibles via Internet, gÃ©rÃ©s par un provider.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        LE CLOUD                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚  â”‚ Compute â”‚  â”‚ Storage â”‚  â”‚ Network â”‚  â”‚  ....   â”‚       â”‚\nâ”‚  â”‚  (VMs)  â”‚  â”‚ (Disks) â”‚  â”‚  (VPC)  â”‚  â”‚         â”‚       â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\nâ”‚                                                             â”‚\nâ”‚  Tu ne gÃ¨res pas le hardware, juste les services            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â–²\n                              â”‚ Internet\n                              â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   Ton application  â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 Les 3 modÃ¨les de service\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RESPONSABILITÃ‰                              â”‚\nâ”‚                                                                â”‚\nâ”‚   On-Premise      IaaS           PaaS           SaaS          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â—€â”€â”€ Provider â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚  Data    â”‚  â”‚  Data    â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Runtime  â”‚  â”‚ Runtime  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚   OS     â”‚  â”‚   OS     â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Virtual  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Servers  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Storage  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Network  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                                â”‚\nâ”‚  â–ˆ = GÃ©rÃ© par le Cloud Provider                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\n\nModÃ¨le\nDescription\nExemples\nTu gÃ¨res\n\n\n\n\nIaaS\nInfrastructure as a Service\nEC2, VMs, VPC\nOS, Runtime, App\n\n\nPaaS\nPlatform as a Service\nRDS, Cloud SQL, EKS\nApp, Data\n\n\nSaaS\nSoftware as a Service\nSnowflake, Databricks\nRien (juste utiliser)\n\n\n\n\n\n2.3 Les 3 grands Cloud Providers\n\n\n\n\n\n\n\n\n\nProvider\nPart de marchÃ©\nForces\nFaiblesse\n\n\n\n\nAWS\n~32%\nLeader, plus de services, maturitÃ©\nComplexitÃ©, coÃ»ts\n\n\nAzure\n~23%\nIntÃ©gration Microsoft, entreprises\nUX parfois confuse\n\n\nGCP\n~10%\nBigQuery, ML/AI, Kubernetes\nMoins de services\n\n\n\n\n\n2.4 RÃ©gions & Zones de disponibilitÃ©\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     AWS Global                              â”‚\nâ”‚                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚  â”‚  Region:        â”‚    â”‚  Region:        â”‚                â”‚\nâ”‚  â”‚  eu-west-1      â”‚    â”‚  us-east-1      â”‚                â”‚\nâ”‚  â”‚  (Ireland)      â”‚    â”‚  (N. Virginia)  â”‚                â”‚\nâ”‚  â”‚                 â”‚    â”‚                 â”‚                â”‚\nâ”‚  â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚    â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚                â”‚\nâ”‚  â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚    â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚                â”‚\nâ”‚  â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚    â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚                â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                             â”‚\nâ”‚  AZ = Availability Zone = Data Center isolÃ©                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRegion : Zone gÃ©ographique (Paris, Dublin, N. Virginia)\nAvailability Zone : Data center isolÃ© dans une rÃ©gion\nLatence : Choisir la rÃ©gion proche des utilisateurs\nCompliance : GDPR â†’ donnÃ©es en Europe\n\n\n\nğŸ‹ï¸ Exercice 1 : Identifier IaaS / PaaS / SaaS\nClasse ces services dans la bonne catÃ©gorie :\n\n\n\nService\nIaaS / PaaS / SaaS ?\n\n\n\n\nAmazon EC2\n?\n\n\nGoogle BigQuery\n?\n\n\nSnowflake\n?\n\n\nAzure Kubernetes Service\n?\n\n\nAmazon S3\n?\n\n\nDatabricks\n?\n\n\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\n\n\n\n\n\n\n\nService\nCatÃ©gorie\nExplication\n\n\n\n\nAmazon EC2\nIaaS\nTu gÃ¨res lâ€™OS et tout ce quâ€™il y a dessus\n\n\nGoogle BigQuery\nPaaS/SaaS\nServerless, tu gÃ¨res juste les requÃªtes\n\n\nSnowflake\nSaaS\nEntiÃ¨rement managÃ©\n\n\nAzure Kubernetes Service\nPaaS\nK8s managÃ©, tu gÃ¨res les workloads\n\n\nAmazon S3\nPaaS\nStockage managÃ©, tu gÃ¨res les donnÃ©es\n\n\nDatabricks\nPaaS/SaaS\nSpark managÃ©",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#services-cloud-pour-le-data-engineering",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#services-cloud-pour-le-data-engineering",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ› ï¸ 3. Services Cloud pour le Data Engineering",
    "text": "ğŸ› ï¸ 3. Services Cloud pour le Data Engineering\n\n3.1 Tableau comparatif complet\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nAWS\nGCP\nAzure\n\n\n\n\nObject Storage\nS3\nGCS\nBlob Storage\n\n\nData Warehouse\nRedshift\nBigQuery\nSynapse Analytics\n\n\nETL Serverless\nGlue\nDataflow\nData Factory\n\n\nQuery Engine\nAthena\nBigQuery\nSynapse Serverless\n\n\nOrchestration\nMWAA (Airflow)\nComposer (Airflow)\nData Factory\n\n\nStreaming\nKinesis\nPub/Sub + Dataflow\nEvent Hubs\n\n\nCatalog\nGlue Catalog\nData Catalog\nPurview\n\n\nKubernetes\nEKS\nGKE\nAKS\n\n\nServerless Compute\nLambda\nCloud Functions\nFunctions\n\n\nNoSQL\nDynamoDB\nFirestore/Bigtable\nCosmosDB\n\n\nMessage Queue\nSQS/SNS\nPub/Sub\nService Bus\n\n\n\n\n\n3.2 Focus sur les services Data Engineering\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  ARCHITECTURE DATA CLOUD                        â”‚\nâ”‚                                                                 â”‚\nâ”‚  Sources              Ingestion           Storage               â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚ API â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Kinesis â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   S3    â”‚           â”‚\nâ”‚  â”‚ DB  â”‚             â”‚ Pub/Sub â”‚         â”‚  GCS    â”‚           â”‚\nâ”‚  â”‚ Filesâ”‚             â”‚ EventHubâ”‚         â”‚  Blob   â”‚           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\nâ”‚                                               â”‚                 â”‚\nâ”‚                                               â–¼                 â”‚\nâ”‚  Processing                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Glue   â”‚           â”‚\nâ”‚  â”‚                                       â”‚Dataflow â”‚           â”‚\nâ”‚  â”‚                                       â”‚  ADF    â”‚           â”‚\nâ”‚  â”‚                                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\nâ”‚  â”‚                                            â”‚                 â”‚\nâ”‚  â”‚                                            â–¼                 â”‚\nâ”‚  â”‚  Serving                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚Redshift â”‚           â”‚\nâ”‚  â””â”€â”€â”‚ Athena  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚BigQuery â”‚           â”‚\nâ”‚     â”‚ Synapse â”‚                          â”‚ Synapse â”‚           â”‚\nâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#storage-models-block-vs-file-vs-object",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#storage-models-block-vs-file-vs-object",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ’¾ 4. Storage Models : Block vs File vs Object",
    "text": "ğŸ’¾ 4. Storage Models : Block vs File vs Object\n\n4.1 Comparaison dÃ©taillÃ©e\n\n\n\nCritÃ¨re\nBlock Storage\nFile Storage\nObject Storage\n\n\n\n\nStructure\nBlocs bruts\nHiÃ©rarchie fichiers\nClÃ©-valeur plat\n\n\nAccÃ¨s\nBas niveau (disque)\nPOSIX (NFS, SMB)\nAPI HTTP (REST)\n\n\nMetadata\nMinimales\nBasiques\nRiches, custom\n\n\nScalabilitÃ©\nLimitÃ©e (TB)\nLimitÃ©e (TB)\nIllimitÃ©e (PB+)\n\n\nPerformance\nTrÃ¨s haute\nMoyenne\nVariable\n\n\nCoÃ»t\nÃ‰levÃ©\nMoyen\nFaible\n\n\nUse case\nBases de donnÃ©es\nPartage fichiers\nData Lakes\n\n\nExemples\nEBS, Azure Disk\nEFS, Azure Files\nS3, GCS, Blob\n\n\n\n\n\n4.2 Pourquoi Object Storage pour les Data Lakes ?\nâœ… ScalabilitÃ© illimitÃ©e      âœ… CoÃ»t faible\nâœ… DurabilitÃ© 11 nines        âœ… API HTTP standard\nâœ… SÃ©paration compute/storage âœ… Metadata riches\nâœ… Formats natifs (Parquet)   âœ… Multi-cloud possible\n\n\n4.3 SÃ©paration Compute / Storage\nAVANT (Hadoop/HDFS)                    APRÃˆS (Cloud/Object Storage)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Node 1         â”‚               â”‚    Compute (Spark)   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚  â”‚Computeâ”‚ â”‚Data â”‚  â”‚               â”‚    â”‚  Executor â”‚     â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚               â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚      CouplÃ©s !      â”‚               â”‚          â”‚           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                 â”‚ Network\n                                                 â–¼\n                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                      â”‚   Storage (S3)      â”‚\n                                      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n                                      â”‚    â”‚  Data   â”‚      â”‚\n                                      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n                                      â”‚   Scale sÃ©parÃ©ment  â”‚\n                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ‹ï¸ Exercice 2 : Choisir le bon type de storage\nQuel type de storage pour chaque use case ?\n\n\n\nUse case\nBlock / File / Object ?\n\n\n\n\nBase de donnÃ©es PostgreSQL\n?\n\n\nData Lake avec fichiers Parquet\n?\n\n\nPartage de documents entre Ã©quipes\n?\n\n\nLogs dâ€™application (PB de donnÃ©es)\n?\n\n\nVM avec OS Windows\n?\n\n\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\n\n\nUse case\nType\nRaison\n\n\n\n\nPostgreSQL\nBlock\nIOPS Ã©levÃ©s, accÃ¨s bas niveau\n\n\nData Lake Parquet\nObject\nScalable, coÃ»t faible\n\n\nPartage documents\nFile\nAccÃ¨s POSIX, permissions\n\n\nLogs application\nObject\nVolume Ã©norme, coÃ»t faible\n\n\nVM Windows\nBlock\nDisque systÃ¨me",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#object-storage-concepts-fondamentaux",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#object-storage-concepts-fondamentaux",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸª£ 5. Object Storage â€” Concepts fondamentaux",
    "text": "ğŸª£ 5. Object Storage â€” Concepts fondamentaux\n\n5.1 Buckets, Keys, Prefixes\ns3://my-bucket/bronze/2024/01/transactions.parquet\n     â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       Bucket           Prefix            Object Key\n\nâš ï¸ IMPORTANT : Les prÃ©fixes NE SONT PAS des dossiers !\n   C'est juste une convention de nommage (clÃ©-valeur plat)\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nBucket\nConteneur racine, nom unique global\nmy-company-datalake\n\n\nKey\nIdentifiant unique de lâ€™objet\nbronze/2024/01/data.parquet\n\n\nPrefix\nâ€œFaux dossierâ€, filtre de listing\nbronze/2024/\n\n\nObject\nLe fichier + ses metadata\nParquet, CSV, JSONâ€¦\n\n\n\n\n\n5.2 Metadata & Tags\nChaque objet peut avoir des metadata custom :\n\n\nCode\n# Exemple de metadata sur un objet S3\nmetadata_example = {\n    # Metadata systÃ¨me (automatiques)\n    \"Content-Type\": \"application/octet-stream\",\n    \"Content-Length\": 1048576,\n    \"Last-Modified\": \"2024-01-15T10:30:00Z\",\n    \"ETag\": \"d41d8cd98f00b204e9800998ecf8427e\",\n    \n    # Metadata custom (x-amz-meta-*)\n    \"x-amz-meta-source\": \"kafka-topic-orders\",\n    \"x-amz-meta-pipeline\": \"etl-daily\",\n    \"x-amz-meta-schema-version\": \"2.1\",\n}\n\n# Tags (pour billing, governance)\ntags = {\n    \"Environment\": \"production\",\n    \"Team\": \"data-engineering\",\n    \"CostCenter\": \"DE-001\",\n}\n\nprint(\"Metadata et Tags permettent de :\")\nprint(\"- Tracer l'origine des donnÃ©es\")\nprint(\"- Filtrer pour la gouvernance\")\nprint(\"- Allouer les coÃ»ts par Ã©quipe\")\n\n\n\n\n5.3 Versioning & Lifecycle Policies\nVersioning : Garder plusieurs versions dâ€™un mÃªme objet\ns3://bucket/data.csv\n   â”‚\n   â”œâ”€â”€ Version 1 (2024-01-01) â† Ancienne\n   â”œâ”€â”€ Version 2 (2024-01-15) â† Ancienne\n   â””â”€â”€ Version 3 (2024-02-01) â† Current\nLifecycle Policies : Automatiser la gestion du stockage\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    30 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   90 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Standard  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Infrequent  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   Glacier   â”‚\nâ”‚   $0.023/GB â”‚                â”‚   $0.0125/GBâ”‚               â”‚  $0.004/GB  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     Hot                           Cool                          Archive\n\n\n5.4 Classes de stockage\n\n\n\nClasse\nUsage\nLatence\nCoÃ»t stockage\n\n\n\n\nStandard\nAccÃ¨s frÃ©quent\nms\n$0.023/GB\n\n\nIA (Infrequent Access)\nAccÃ¨s rare\nms\n$0.0125/GB\n\n\nGlacier\nArchivage\nminutes-heures\n$0.004/GB\n\n\nGlacier Deep Archive\nCompliance\nheures\n$0.00099/GB\n\n\n\n\n\n5.5 Protocoles & Drivers\n\nğŸ”¥ Section essentielle â€” Source de confusion frÃ©quente !\n\n\n\n\n\n\n\n\n\n\n\nProtocole\nCloud\nUsage\nDriver/SDK\nExemple\n\n\n\n\ns3://\nAWS\nCLI, Python\nboto3, aws cli\ns3://bucket/key\n\n\ns3a://\nAWS\nSpark/Hadoop\nhadoop-aws\ns3a://bucket/key\n\n\nabfs://\nAzure\nSpark (legacy)\nhadoop-azure\nabfs://container@account.dfs.core.windows.net/\n\n\nabfss://\nAzure\nSpark (TLS)\nhadoop-azure\nabfss://container@account.dfs.core.windows.net/\n\n\ngs://\nGCP\nSpark, CLI\ngcs-connector\ngs://bucket/key\n\n\nhttps://\nTous\nDirect HTTP\nrequests\nSigned URLs\n\n\n\nâš ï¸ ATTENTION :\n\ns3://  â‰   s3a://\n\n- s3://  â†’ AWS CLI, boto3 (haut niveau)\n- s3a:// â†’ Hadoop/Spark (bas niveau, optimisÃ© Big Data)\n\nDans Spark, TOUJOURS utiliser s3a://, abfss://, ou gs://\n\n\n5.6 Metadata Catalogs â€” Transition vers le module 23\nLâ€™Object Storage stocke des fichiers bruts. Mais pour faire du SQL, on a besoin de : - SchÃ©ma des tables (colonnes, types) - Localisation des partitions - Statistiques pour lâ€™optimiseur\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    METADATA CATALOG                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚  Table: sales                                           â”‚   â”‚\nâ”‚  â”‚  Location: s3://bucket/silver/sales/                    â”‚   â”‚\nâ”‚  â”‚  Schema: id INT, amount DOUBLE, date DATE               â”‚   â”‚\nâ”‚  â”‚  Partitions: date=2024-01-01, date=2024-01-02, ...      â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n                                â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    OBJECT STORAGE (S3)                          â”‚\nâ”‚  s3://bucket/silver/sales/date=2024-01-01/part-00000.parquet   â”‚\nâ”‚  s3://bucket/silver/sales/date=2024-01-02/part-00000.parquet   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nSolutions Catalog :\n\n\n\nSolution\nType\nUtilisÃ© par\n\n\n\n\nHive Metastore\nOpen-source\nSpark, Presto, Trino\n\n\nAWS Glue Catalog\nManaged\nAthena, Glue, EMR\n\n\nGCP Data Catalog\nManaged\nBigQuery, Dataproc\n\n\nAzure Purview\nManaged\nSynapse, Databricks\n\n\n\n\nğŸ’¡ Preview Module 23 : Delta Lake et Iceberg intÃ¨grent leur propre Transaction Log directement dans lâ€™Object Storage. Plus besoin de catalogue externe !\n\n\n\n5.7 Layout Data Lake â€” Rappel (rÃ©f module 08)\ns3://my-datalake/\nâ”‚\nâ”œâ”€â”€ bronze/                    â† Raw, immutable, source of truth\nâ”‚   â”œâ”€â”€ orders/\nâ”‚   â”‚   â””â”€â”€ 2024/01/01/\nâ”‚   â”‚       â””â”€â”€ orders_raw.json\nâ”‚   â””â”€â”€ customers/\nâ”‚       â””â”€â”€ customers_full.csv\nâ”‚\nâ”œâ”€â”€ silver/                    â† Cleaned, validated, deduplicated\nâ”‚   â”œâ”€â”€ orders/\nâ”‚   â”‚   â””â”€â”€ date=2024-01-01/\nâ”‚   â”‚       â””â”€â”€ part-00000.parquet\nâ”‚   â””â”€â”€ customers/\nâ”‚       â””â”€â”€ part-00000.parquet\nâ”‚\nâ””â”€â”€ gold/                      â† Aggregated, business-ready\n    â”œâ”€â”€ daily_sales/\n    â”‚   â””â”€â”€ part-00000.parquet\n    â””â”€â”€ customer_360/\n        â””â”€â”€ part-00000.parquet\n\n\nğŸ‹ï¸ Exercice 3 : Calculer le coÃ»t de stockage\nScÃ©nario : Tu as un Data Lake avec : - Bronze : 500 GB (accÃ¨s rare) - Silver : 200 GB (accÃ¨s frÃ©quent) - Gold : 50 GB (accÃ¨s trÃ¨s frÃ©quent)\nCalcule le coÃ»t mensuel optimal sur S3.\n\n\nğŸ’¡ Voir la solution\n\n\n\n\nLayer\nTaille\nClasse\nPrix/GB\nCoÃ»t\n\n\n\n\nBronze\n500 GB\nS3 IA\n$0.0125\n$6.25\n\n\nSilver\n200 GB\nStandard\n$0.023\n$4.60\n\n\nGold\n50 GB\nStandard\n$0.023\n$1.15\n\n\nTotal\n750 GB\n\n\n$12.00/mois\n\n\n\nSans optimisation (tout en Standard) : 750 Ã— $0.023 = $17.25/mois\nÃ‰conomie : 30% !",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#aws-s3-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#aws-s3-deep-dive",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸŸ  6. AWS S3 â€” Deep Dive",
    "text": "ğŸŸ  6. AWS S3 â€” Deep Dive\n\n6.1 Concepts & Classes de stockage\n\n\n\nClasse\nDurabilitÃ©\nDisponibilitÃ©\nMin storage\nRetrieval\n\n\n\n\nStandard\n11 nines\n99.99%\n-\nImmÃ©diat\n\n\nIntelligent-Tiering\n11 nines\n99.9%\n-\nImmÃ©diat\n\n\nStandard-IA\n11 nines\n99.9%\n30 jours\nImmÃ©diat\n\n\nGlacier Instant\n11 nines\n99.9%\n90 jours\nms\n\n\nGlacier Flexible\n11 nines\n99.99%\n90 jours\n1-12h\n\n\nGlacier Deep Archive\n11 nines\n99.99%\n180 jours\n12-48h\n\n\n\n\n\n6.2 OpÃ©rations CLI\n\n\nCode\ns3_cli_commands = \"\"\"\n# Lister les buckets\naws s3 ls\n\n# Lister le contenu d'un bucket\naws s3 ls s3://my-bucket/bronze/\n\n# Copier un fichier local vers S3\naws s3 cp data.csv s3://my-bucket/bronze/data.csv\n\n# Copier un fichier S3 vers local\naws s3 cp s3://my-bucket/bronze/data.csv ./data.csv\n\n# Synchroniser un dossier\naws s3 sync ./local-folder/ s3://my-bucket/bronze/\n\n# Supprimer un fichier\naws s3 rm s3://my-bucket/bronze/old-data.csv\n\n# Supprimer rÃ©cursivement\naws s3 rm s3://my-bucket/temp/ --recursive\n\"\"\"\nprint(s3_cli_commands)\n\n\n\n\n6.3 Python avec boto3\n\n\nCode\n# Installation : pip install boto3\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n# CrÃ©er un client S3\ns3 = boto3.client('s3')\n\n# --- Upload ---\ndef upload_file(file_path, bucket, key):\n    \"\"\"Upload un fichier vers S3.\"\"\"\n    s3.upload_file(file_path, bucket, key)\n    print(f\"âœ… Uploaded {file_path} to s3://{bucket}/{key}\")\n\n# --- Download ---\ndef download_file(bucket, key, file_path):\n    \"\"\"Download un fichier depuis S3.\"\"\"\n    s3.download_file(bucket, key, file_path)\n    print(f\"âœ… Downloaded s3://{bucket}/{key} to {file_path}\")\n\n# --- List objects ---\ndef list_objects(bucket, prefix=\"\"):\n    \"\"\"Liste les objets dans un bucket.\"\"\"\n    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    if 'Contents' in response:\n        for obj in response['Contents']:\n            print(f\"  {obj['Key']} ({obj['Size']} bytes)\")\n    return response.get('Contents', [])\n\n# --- Presigned URL ---\ndef generate_presigned_url(bucket, key, expiration=3600):\n    \"\"\"GÃ©nÃ¨re une URL temporaire pour accÃ¨s direct.\"\"\"\n    url = s3.generate_presigned_url(\n        'get_object',\n        Params={'Bucket': bucket, 'Key': key},\n        ExpiresIn=expiration\n    )\n    return url\n\n# Exemple d'utilisation (commentÃ© car pas de credentials)\n# upload_file('data.csv', 'my-bucket', 'bronze/data.csv')\n# list_objects('my-bucket', 'bronze/')\nprint(\"ğŸ“ Fonctions boto3 dÃ©finies (upload, download, list, presigned URL)\")\n\n\n\n\n6.4 S3 avec Spark (s3a://)\n\n\nCode\n# Configuration Spark pour S3\nspark_s3_config = \"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"S3 Access\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n    .getOrCreate()\n\n# Lire depuis S3\ndf = spark.read.parquet(\"s3a://my-bucket/silver/sales/\")\n\n# Ã‰crire vers S3\ndf.write.mode(\"overwrite\").parquet(\"s3a://my-bucket/gold/aggregates/\")\n\"\"\"\n\n# Optimisations S3A\ns3a_optimizations = \"\"\"\n# Performance optimizations\nspark.hadoop.fs.s3a.connection.maximum=200\nspark.hadoop.fs.s3a.threads.max=64\nspark.hadoop.fs.s3a.fast.upload=true\nspark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer\nspark.hadoop.fs.s3a.multipart.size=104857600  # 100MB\n\"\"\"\nprint(spark_s3_config)\nprint(\"\\n--- Optimisations S3A ---\")\nprint(s3a_optimizations)\n\n\n\n\n6.5 Authentification & IAM\n\n\n\n\n\n\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nAccess Keys\nâš ï¸ Faible\nDev local\nâŒ Non\n\n\nInstance Profile\nâœ… Haute\nEC2\nâœ… Oui\n\n\nIRSA (IAM Roles for Service Accounts)\nâœ… Haute\nEKS/K8s\nâœ… Oui\n\n\nAssumeRole\nâœ… Haute\nCross-account\nâœ… Oui\n\n\n\nğŸ” Best Practice : JAMAIS de credentials dans le code !\n\nUtiliser :\n- Variables d'environnement\n- Instance Profiles (EC2)\n- IRSA (Kubernetes) â† Module 21\n- AWS Secrets Manager\n\n\nğŸ‹ï¸ Exercice 4 : Upload/Download avec boto3\nÃ‰cris un script Python qui : 1. CrÃ©e un fichier CSV local avec 3 lignes 2. Lâ€™upload vers S3 (ou MinIO) 3. Liste les fichiers du bucket 4. TÃ©lÃ©charge le fichier sous un autre nom\n\n\nğŸ’¡ Voir la solution\n\nimport boto3\nimport csv\n\n# 1. CrÃ©er un fichier CSV\nwith open('test_data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['id', 'name', 'amount'])\n    writer.writerow([1, 'Alice', 100])\n    writer.writerow([2, 'Bob', 200])\n    writer.writerow([3, 'Charlie', 150])\n\n# 2. Upload\ns3 = boto3.client('s3')\ns3.upload_file('test_data.csv', 'my-bucket', 'bronze/test_data.csv')\n\n# 3. List\nresponse = s3.list_objects_v2(Bucket='my-bucket', Prefix='bronze/')\nfor obj in response.get('Contents', []):\n    print(obj['Key'])\n\n# 4. Download\ns3.download_file('my-bucket', 'bronze/test_data.csv', 'downloaded_data.csv')",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#azure-blob-storage-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#azure-blob-storage-deep-dive",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ”µ 7. Azure Blob Storage â€” Deep Dive",
    "text": "ğŸ”µ 7. Azure Blob Storage â€” Deep Dive\n\n7.1 Concepts\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   AZURE STORAGE ACCOUNT                     â”‚\nâ”‚                   (mystorageaccount)                        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚  â”‚   Container:    â”‚  â”‚   Container:    â”‚                  â”‚\nâ”‚  â”‚   bronze        â”‚  â”‚   silver        â”‚                  â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚\nâ”‚  â”‚  â”‚ data.csv  â”‚  â”‚  â”‚  â”‚ data.parq â”‚  â”‚                  â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nConcept Azure\nÃ‰quivalent S3\n\n\n\n\nStorage Account\n(pas dâ€™Ã©quivalent, niveau compte)\n\n\nContainer\nBucket\n\n\nBlob\nObject\n\n\nADLS Gen2\nS3 + Glue Catalog intÃ©grÃ©\n\n\n\n\n\n7.2 Access Tiers\n\n\n\nTier\nUsage\nCoÃ»t stockage\nCoÃ»t accÃ¨s\n\n\n\n\nHot\nFrÃ©quent\nÃ‰levÃ©\nFaible\n\n\nCool\nRare (30+ jours)\nMoyen\nMoyen\n\n\nArchive\nArchivage (180+ jours)\nTrÃ¨s faible\nÃ‰levÃ©\n\n\n\n\n\nCode\n# Installation : pip install azure-storage-blob\n\nazure_blob_example = \"\"\"\nfrom azure.storage.blob import BlobServiceClient, BlobClient\n\n# Connection string (dev only !)\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\"\n\n# CrÃ©er le client\nblob_service = BlobServiceClient.from_connection_string(connection_string)\n\n# Upload\nblob_client = blob_service.get_blob_client(container=\"bronze\", blob=\"data.csv\")\nwith open(\"data.csv\", \"rb\") as f:\n    blob_client.upload_blob(f, overwrite=True)\n\n# Download\nwith open(\"downloaded.csv\", \"wb\") as f:\n    f.write(blob_client.download_blob().readall())\n\n# List blobs\ncontainer_client = blob_service.get_container_client(\"bronze\")\nfor blob in container_client.list_blobs():\n    print(blob.name)\n\"\"\"\nprint(azure_blob_example)\n\n\n\n\n7.3 Azure Blob avec Spark (abfss://)\n\n\nCode\nspark_azure_config = \"\"\"\n# Configuration Spark pour Azure Blob (ADLS Gen2)\nstorage_account = \"mystorageaccount\"\ncontainer = \"bronze\"\n\n# MÃ©thode 1 : Access Key (dev only)\nspark.conf.set(\n    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n    \"YOUR_ACCESS_KEY\"\n)\n\n# MÃ©thode 2 : Service Principal (production)\nspark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", \"CLIENT_ID\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", \"CLIENT_SECRET\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n               \"https://login.microsoftonline.com/TENANT_ID/oauth2/token\")\n\n# Lire\ndf = spark.read.parquet(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/data/\")\n\"\"\"\nprint(spark_azure_config)\n\n\n\n\n7.4 Authentification Azure\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nAccess Keys\nâš ï¸ Faible\nDev\nâŒ Non\n\n\nSAS Token\nâš ï¸ Moyenne\nTemporaire, externe\nâš ï¸ LimitÃ©\n\n\nService Principal\nâœ… Haute\nApps, CI/CD\nâœ… Oui\n\n\nManaged Identity\nâœ… TrÃ¨s haute\nVMs, AKS\nâœ… Oui\n\n\n\n\n\nğŸ‹ï¸ Exercice 5 : GÃ©nÃ©rer un SAS Token\nUn SAS Token permet de donner un accÃ¨s temporaire Ã  un blob.\n\n\nğŸ’¡ Voir la solution\n\nfrom azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\nfrom datetime import datetime, timedelta\n\naccount_name = \"mystorageaccount\"\naccount_key = \"YOUR_KEY\"\ncontainer = \"bronze\"\nblob_name = \"data.csv\"\n\n# GÃ©nÃ©rer SAS Token (valide 1 heure)\nsas_token = generate_blob_sas(\n    account_name=account_name,\n    container_name=container,\n    blob_name=blob_name,\n    account_key=account_key,\n    permission=BlobSasPermissions(read=True),\n    expiry=datetime.utcnow() + timedelta(hours=1)\n)\n\n# URL complÃ¨te\nurl = f\"https://{account_name}.blob.core.windows.net/{container}/{blob_name}?{sas_token}\"\nprint(url)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#google-cloud-storage-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#google-cloud-storage-deep-dive",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸŸ¡ 8. Google Cloud Storage â€” Deep Dive",
    "text": "ğŸŸ¡ 8. Google Cloud Storage â€” Deep Dive\n\n8.1 Concepts\n\n\n\nClasse\nUsage\nSLA\nCoÃ»t\n\n\n\n\nStandard\nFrÃ©quent\n99.99%\n$0.020/GB\n\n\nNearline\n1x/mois\n99.9%\n$0.010/GB\n\n\nColdline\n1x/trimestre\n99.9%\n$0.004/GB\n\n\nArchive\n1x/an\n99.9%\n$0.0012/GB\n\n\n\n\n\nCode\ngcs_cli_commands = \"\"\"\n# gsutil - CLI pour GCS\n\n# Lister les buckets\ngsutil ls\n\n# Lister le contenu d'un bucket\ngsutil ls gs://my-bucket/bronze/\n\n# Copier\ngsutil cp data.csv gs://my-bucket/bronze/\ngsutil cp gs://my-bucket/bronze/data.csv ./\n\n# Synchroniser (comme rsync)\ngsutil rsync -r ./local/ gs://my-bucket/bronze/\n\n# Copie parallÃ¨le (gros fichiers)\ngsutil -m cp -r ./data/ gs://my-bucket/bronze/\n\"\"\"\nprint(gcs_cli_commands)\n\n\n\n\nCode\n# Installation : pip install google-cloud-storage\n\ngcs_python_example = \"\"\"\nfrom google.cloud import storage\n\n# CrÃ©er le client (utilise GOOGLE_APPLICATION_CREDENTIALS)\nclient = storage.Client()\n\n# AccÃ©der au bucket\nbucket = client.bucket(\"my-bucket\")\n\n# Upload\nblob = bucket.blob(\"bronze/data.csv\")\nblob.upload_from_filename(\"data.csv\")\n\n# Download\nblob.download_to_filename(\"downloaded.csv\")\n\n# List blobs\nblobs = bucket.list_blobs(prefix=\"bronze/\")\nfor blob in blobs:\n    print(blob.name)\n\n# Signed URL (temporaire)\nurl = blob.generate_signed_url(expiration=3600)  # 1 heure\n\"\"\"\nprint(gcs_python_example)\n\n\n\n\n8.3 GCS avec Spark (gs://)\n\n\nCode\nspark_gcs_config = \"\"\"\n# Configuration Spark pour GCS\n\n# Option 1 : Service Account Key (dev)\nspark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\nspark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/keyfile.json\")\n\n# Option 2 : Application Default Credentials (GKE, Cloud Functions)\n# Pas de config nÃ©cessaire si ADC est configurÃ©\n\n# Lire depuis GCS\ndf = spark.read.parquet(\"gs://my-bucket/silver/data/\")\n\n# Ã‰crire vers GCS\ndf.write.mode(\"overwrite\").parquet(\"gs://my-bucket/gold/aggregates/\")\n\"\"\"\nprint(spark_gcs_config)\n\n\n\n\n8.4 Authentification GCP\n\n\n\n\n\n\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nService Account Key\nâš ï¸ Moyenne\nDev, CI/CD\nâš ï¸ Avec prÃ©caution\n\n\nWorkload Identity\nâœ… Haute\nGKE\nâœ… Oui\n\n\nADC (Application Default Credentials)\nâœ… Haute\nCloud Functions, Cloud Run\nâœ… Oui\n\n\n\n\n\nğŸ‹ï¸ Exercice 6 : Lister et tÃ©lÃ©charger depuis GCS\nÃ‰cris un script qui liste tous les fichiers .parquet dans un bucket et tÃ©lÃ©charge le premier.\n\n\nğŸ’¡ Voir la solution\n\nfrom google.cloud import storage\n\nclient = storage.Client()\nbucket = client.bucket(\"my-bucket\")\n\n# Lister les fichiers .parquet\nparquet_files = []\nfor blob in bucket.list_blobs(prefix=\"silver/\"):\n    if blob.name.endswith('.parquet'):\n        parquet_files.append(blob)\n        print(f\"Found: {blob.name}\")\n\n# TÃ©lÃ©charger le premier\nif parquet_files:\n    first_file = parquet_files[0]\n    first_file.download_to_filename(\"downloaded.parquet\")\n    print(f\"Downloaded: {first_file.name}\")",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#minio-object-storage-local",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#minio-object-storage-local",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸŸ¢ 9. MinIO â€” Object Storage Local",
    "text": "ğŸŸ¢ 9. MinIO â€” Object Storage Local\n\n9.1 Pourquoi MinIO ?\n\n\n\nAvantage\nDescription\n\n\n\n\n100% S3 compatible\nMÃªme API, mÃªme code boto3\n\n\nGratuit\nOpen-source, pas de compte cloud\n\n\nLocal\nParfait pour dev/test\n\n\nLÃ©ger\nDocker, un seul binaire\n\n\nProduction-ready\nUtilisÃ© aussi en production (on-premise)\n\n\n\nğŸ’¡ Le code Ã©crit pour MinIO fonctionne sur S3 sans modification !\n   Il suffit de changer l'endpoint.\n\n\n9.2 Installation avec Docker\n\n\nCode\n%%writefile /tmp/minio/docker-compose.yaml\nversion: '3.8'\n\nservices:\n  minio:\n    image: minio/minio:latest\n    container_name: minio\n    ports:\n      - \"9000:9000\"   # API S3\n      - \"9001:9001\"   # Console Web\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio-data:/data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\nvolumes:\n  minio-data:\n\n\n\n\nCode\nminio_commands = \"\"\"\n# DÃ©marrer MinIO\ndocker-compose up -d\n\n# AccÃ©der Ã  la console web\n# http://localhost:9001\n# Login: minioadmin / minioadmin\n\n# Installer mc (MinIO Client)\n# Linux\nwget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\nsudo mv mc /usr/local/bin/\n\n# Mac\nbrew install minio/stable/mc\n\n# Configurer mc\nmc alias set myminio http://localhost:9000 minioadmin minioadmin\n\n# CrÃ©er des buckets\nmc mb myminio/bronze\nmc mb myminio/silver\nmc mb myminio/gold\n\n# Lister\nmc ls myminio/\n\n# Upload\nmc cp data.csv myminio/bronze/\n\"\"\"\nprint(minio_commands)\n\n\n\n\n9.4 Python avec MinIO (boto3)\n\n\nCode\n# Le mÃªme code boto3 fonctionne avec MinIO !\n# Il suffit de spÃ©cifier endpoint_url\n\nminio_boto3_example = \"\"\"\nimport boto3\nfrom botocore.client import Config\n\n# Configuration pour MinIO\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://localhost:9000',  # â† La seule diffÃ©rence !\n    aws_access_key_id='minioadmin',\n    aws_secret_access_key='minioadmin',\n    config=Config(signature_version='s3v4')\n)\n\n# CrÃ©er un bucket\ns3.create_bucket(Bucket='bronze')\n\n# Upload (identique Ã  S3)\ns3.upload_file('data.csv', 'bronze', 'raw/data.csv')\n\n# List (identique Ã  S3)\nresponse = s3.list_objects_v2(Bucket='bronze')\nfor obj in response.get('Contents', []):\n    print(obj['Key'])\n\n# Download (identique Ã  S3)\ns3.download_file('bronze', 'raw/data.csv', 'downloaded.csv')\n\"\"\"\nprint(minio_boto3_example)\nprint(\"\\nğŸ’¡ Ce code fonctionne sur S3 en enlevant juste endpoint_url !\")\n\n\n\n\n9.5 Spark avec MinIO\n\n\nCode\nspark_minio_config = \"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"MinIO Access\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\n# Lire depuis MinIO (mÃªme syntaxe que S3 !)\ndf = spark.read.csv(\"s3a://bronze/raw/data.csv\", header=True)\n\n# Ã‰crire vers MinIO\ndf.write.mode(\"overwrite\").parquet(\"s3a://silver/clean/data/\")\n\"\"\"\nprint(spark_minio_config)\n\n\n\n\nğŸ‹ï¸ Exercice 7 : DÃ©ployer MinIO et crÃ©er un bucket\n\nLance MinIO avec Docker\nAccÃ¨de Ã  la console (http://localhost:9001)\nCrÃ©e les buckets bronze, silver, gold\nUpload un fichier via la console\n\n# Solution rapide\ndocker run -d --name minio \\\n  -p 9000:9000 -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#performance-optimisation",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#performance-optimisation",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "âš¡ 10. Performance & Optimisation",
    "text": "âš¡ 10. Performance & Optimisation\n\n10.1 Le problÃ¨me des petits fichiers\nâŒ MAUVAIS : 10,000 fichiers Ã— 1 MB = 10 GB\n   - 10,000 requÃªtes API (LIST + GET)\n   - Overhead Ã©norme pour Spark\n   - Temps de lecture : minutes\n\nâœ… BON : 100 fichiers Ã— 100 MB = 10 GB\n   - 100 requÃªtes API\n   - ParallÃ©lisme optimal\n   - Temps de lecture : secondes\nTaille idÃ©ale : 100 MB - 1 GB par fichier\nSolutions : - df.coalesce(n) ou df.repartition(n) avant Ã©criture - Compaction pÃ©riodique - Delta Lake / Iceberg (Module 23) = compaction automatique\n\n\nCode\n# Ã‰viter les petits fichiers avec Spark\nsmall_files_solution = \"\"\"\n# Lecture de beaucoup de petits fichiers\ndf = spark.read.parquet(\"s3a://bronze/data/\")  # 10,000 fichiers\n\n# âŒ Ã‰criture directe = mÃªme nombre de fichiers\n# df.write.parquet(\"s3a://silver/data/\")\n\n# âœ… Repartitionner avant d'Ã©crire\ndf.repartition(100) \\  # 100 fichiers de ~100 MB\n  .write.mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/data/\")\n\n# âœ… Ou coalesce (moins de shuffle)\ndf.coalesce(100) \\\n  .write.mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/data/\")\n\"\"\"\nprint(small_files_solution)\n\n\n\n\n10.2 Partitionnement\ns3://bucket/silver/sales/\nâ”œâ”€â”€ year=2024/\nâ”‚   â”œâ”€â”€ month=01/\nâ”‚   â”‚   â”œâ”€â”€ day=01/\nâ”‚   â”‚   â”‚   â””â”€â”€ part-00000.parquet\nâ”‚   â”‚   â””â”€â”€ day=02/\nâ”‚   â”‚       â””â”€â”€ part-00000.parquet\nâ”‚   â””â”€â”€ month=02/\nâ”‚       â””â”€â”€ ...\nâ””â”€â”€ year=2023/\n    â””â”€â”€ ...\nAvantages : - Partition pruning (Spark ne lit que les partitions nÃ©cessaires) - RequÃªtes plus rapides\nâš ï¸ Attention au over-partitioning : - Trop de partitions = trop de petits fichiers - RÃ¨gle : max 10,000 partitions\n\n\nCode\n# Partitionnement avec Spark\npartitioning_example = \"\"\"\n# Ã‰criture partitionnÃ©e\ndf.write \\\n  .partitionBy(\"year\", \"month\") \\\n  .mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/sales/\")\n\n# Lecture avec partition pruning\ndf = spark.read.parquet(\"s3a://silver/sales/\")\n\n# Cette requÃªte ne lit QUE year=2024/month=01\ndf.filter(\"year = 2024 AND month = 1\").show()\n\"\"\"\nprint(partitioning_example)\n\n\n\n\n10.3 Formats de fichiers\n\n\n\nFormat\nType\nCompression\nLecture colonnes\nUse case\n\n\n\n\nCSV\nRow\nNon\nâŒ\nÃ‰change, debug\n\n\nJSON\nRow\nNon\nâŒ\nAPIs, logs\n\n\nAvro\nRow\nOui\nâŒ\nStreaming, Kafka\n\n\nParquet\nColumnar\nOui\nâœ…\nAnalytics, Data Lake\n\n\nORC\nColumnar\nOui\nâœ…\nHive, analytics\n\n\n\nğŸ’¡ Pour le Data Engineering : PARQUET est le standard\n   - Compression excellente (snappy, zstd)\n   - Lecture par colonnes\n   - Predicate pushdown\n   - Schema intÃ©grÃ©\n\n\nğŸ‹ï¸ Exercice 8 : Comparer Parquet vs CSV\n\n\nCode\n# Exercice : Comparer la taille et le temps de lecture\n\nformat_comparison = \"\"\"\nimport time\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Format Comparison\").getOrCreate()\n\n# CrÃ©er un DataFrame de test (1M lignes)\ndf = spark.range(1000000).toDF(\"id\") \\\n    .withColumn(\"name\", lit(\"test_name\")) \\\n    .withColumn(\"amount\", rand() * 1000)\n\n# Ã‰crire en CSV\nstart = time.time()\ndf.write.mode(\"overwrite\").csv(\"s3a://test/csv/\")\ncsv_write_time = time.time() - start\n\n# Ã‰crire en Parquet\nstart = time.time()\ndf.write.mode(\"overwrite\").parquet(\"s3a://test/parquet/\")\nparquet_write_time = time.time() - start\n\n# Lire CSV\nstart = time.time()\ndf_csv = spark.read.csv(\"s3a://test/csv/\").count()\ncsv_read_time = time.time() - start\n\n# Lire Parquet\nstart = time.time()\ndf_parquet = spark.read.parquet(\"s3a://test/parquet/\").count()\nparquet_read_time = time.time() - start\n\nprint(f\"CSV Write: {csv_write_time:.2f}s, Read: {csv_read_time:.2f}s\")\nprint(f\"Parquet Write: {parquet_write_time:.2f}s, Read: {parquet_read_time:.2f}s\")\n\"\"\"\nprint(format_comparison)\nprint(\"\\nğŸ’¡ RÃ©sultat attendu : Parquet 5-10x plus rapide et 5-10x plus petit\")\n\n\n\n\nğŸ‹ï¸ Exercice 9 : Impact de la taille des fichiers\nObjectif : Mesurer lâ€™impact du nombre de fichiers sur les performances.\n# ScÃ©nario A : 1000 petits fichiers\ndf.repartition(1000).write.parquet(\"s3a://test/small-files/\")\n\n# ScÃ©nario B : 10 gros fichiers\ndf.repartition(10).write.parquet(\"s3a://test/large-files/\")\n\n# Mesurer le temps de lecture pour chaque\nRÃ©sultat attendu : ScÃ©nario B sera beaucoup plus rapide.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#sÃ©curitÃ©-gouvernance",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#sÃ©curitÃ©-gouvernance",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ”’ 11. SÃ©curitÃ© & Gouvernance",
    "text": "ğŸ”’ 11. SÃ©curitÃ© & Gouvernance\n\n11.1 Encryption\n\n\n\nType\nDescription\nQui gÃ¨re la clÃ© ?\n\n\n\n\nSSE-S3\nEncryption cÃ´tÃ© serveur, clÃ© AWS\nAWS\n\n\nSSE-KMS\nEncryption avec AWS KMS\nAWS (tu choisis la clÃ©)\n\n\nSSE-C\nEncryption avec clÃ© client\nToi\n\n\nClient-side\nEncryption avant upload\nToi\n\n\n\nğŸ’¡ Best Practice : SSE-KMS pour la plupart des cas\n   - Rotation automatique des clÃ©s\n   - Audit dans CloudTrail\n   - ContrÃ´le d'accÃ¨s fin\n\n\n11.2 Access Control\n\n\nCode\n# Exemple de bucket policy S3\nbucket_policy_example = \"\"\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowDataTeamRead\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789:role/DataEngineerRole\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-datalake\",\n                \"arn:aws:s3:::my-datalake/*\"\n            ]\n        },\n        {\n            \"Sid\": \"DenyPublicAccess\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::my-datalake/*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:SecureTransport\": \"false\"\n                }\n            }\n        }\n    ]\n}\n\"\"\"\nprint(bucket_policy_example)\n\n\n\n\n11.3 Audit & Compliance\n\n\n\nService\nCloud\nCe quâ€™il trace\n\n\n\n\nS3 Access Logs\nAWS\nQui accÃ¨de Ã  quoi\n\n\nCloudTrail\nAWS\nToutes les API calls\n\n\nActivity Logs\nAzure\nOpÃ©rations sur Blob\n\n\nAudit Logs\nGCP\nAccÃ¨s aux ressources",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#coÃ»ts-optimisation-financiÃ¨re",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#coÃ»ts-optimisation-financiÃ¨re",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ’° 12. CoÃ»ts & Optimisation financiÃ¨re",
    "text": "ğŸ’° 12. CoÃ»ts & Optimisation financiÃ¨re\n\n12.1 Composantes du coÃ»t\n\n\n\nComposante\nDescription\nExemple S3\n\n\n\n\nStockage\nGB/mois\n$0.023/GB (Standard)\n\n\nPUT/POST\nÃ‰critures\n$0.005 / 1000 requÃªtes\n\n\nGET\nLectures\n$0.0004 / 1000 requÃªtes\n\n\nLIST\nListing\n$0.005 / 1000 requÃªtes\n\n\nEgress\nSortie du cloud\n$0.09/GB (vers Internet)\n\n\n\n\n\n12.2 CoÃ»ts cachÃ©s\nâš ï¸ Attention aux coÃ»ts cachÃ©s :\n\n1. LISTING frÃ©quent\n   - Spark fait un LIST avant chaque lecture\n   - 1M de fichiers = 1000 LIST calls = $5\n\n2. EGRESS\n   - DonnÃ©es sortant du cloud = coÃ»teux\n   - Cross-region = $0.02/GB\n   - Vers Internet = $0.09/GB\n\n3. Small files\n   - Plus de requÃªtes API\n   - Plus de listing\n\n\n12.3 Lifecycle Policies\n\n\nCode\n# Exemple de lifecycle policy S3\nlifecycle_policy = \"\"\"\n{\n    \"Rules\": [\n        {\n            \"ID\": \"ArchiveBronzeData\",\n            \"Status\": \"Enabled\",\n            \"Filter\": {\n                \"Prefix\": \"bronze/\"\n            },\n            \"Transitions\": [\n                {\n                    \"Days\": 30,\n                    \"StorageClass\": \"STANDARD_IA\"\n                },\n                {\n                    \"Days\": 90,\n                    \"StorageClass\": \"GLACIER\"\n                }\n            ],\n            \"Expiration\": {\n                \"Days\": 365\n            }\n        }\n    ]\n}\n\"\"\"\nprint(lifecycle_policy)\n\n\n\n\nğŸ‹ï¸ Exercice 10 : Estimer le coÃ»t mensuel dâ€™un Data Lake\nScÃ©nario : - Bronze : 1 TB (accÃ¨s rare) - Silver : 500 GB (accÃ¨s frÃ©quent) - Gold : 100 GB (accÃ¨s trÃ¨s frÃ©quent) - 100,000 requÃªtes GET/jour - 10,000 requÃªtes PUT/jour - 50 GB egress/mois\nCalcule le coÃ»t mensuel sur AWS S3.\n\n\nğŸ’¡ Voir la solution\n\n\n\n\nÃ‰lÃ©ment\nCalcul\nCoÃ»t\n\n\n\n\nBronze (S3 IA)\n1000 GB Ã— $0.0125\n$12.50\n\n\nSilver (Standard)\n500 GB Ã— $0.023\n$11.50\n\n\nGold (Standard)\n100 GB Ã— $0.023\n$2.30\n\n\nGET requests\n100K Ã— 30 / 1000 Ã— $0.0004\n$1.20\n\n\nPUT requests\n10K Ã— 30 / 1000 Ã— $0.005\n$1.50\n\n\nEgress\n50 GB Ã— $0.09\n$4.50\n\n\nTotal\n\n~$33.50/mois",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#mini-projet-data-lake-avec-minio",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#mini-projet-data-lake-avec-minio",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸš€ 13. Mini-Projet : Data Lake avec MinIO",
    "text": "ğŸš€ 13. Mini-Projet : Data Lake avec MinIO\n\nğŸ¯ Objectif\nConstruire un Data Lake local complet avec MinIO.\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       MinIO (Docker)                            â”‚\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚   bronze/   â”‚    â”‚   silver/   â”‚    â”‚    gold/    â”‚        â”‚\nâ”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\nâ”‚  â”‚  raw.csv    â”‚â”€â”€â”€â–¶â”‚ clean.parq  â”‚â”€â”€â”€â–¶â”‚  agg.parq   â”‚        â”‚\nâ”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚        â–²                  â–²                  â–²                  â”‚\nâ”‚        â”‚                  â”‚                  â”‚                  â”‚\nâ”‚     Upload            Transform          Aggregate              â”‚\nâ”‚    (boto3)           (PySpark)         (Spark SQL)             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nÃ‰tapes du projet\n\n\nCode\n# Ã‰tape 1 : DÃ©marrer MinIO\nprint(\"\"\"\n# Terminal 1 : DÃ©marrer MinIO\ndocker run -d --name minio \\\n  -p 9000:9000 -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"\n\n# VÃ©rifier que MinIO tourne\ncurl http://localhost:9000/minio/health/live\n\"\"\")\n\n\n\n\nCode\n# Ã‰tape 2 : CrÃ©er les buckets et uploader les donnÃ©es\n\nstep2_code = \"\"\"\nimport boto3\nfrom botocore.client import Config\nimport csv\n\n# Connexion Ã  MinIO\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://localhost:9000',\n    aws_access_key_id='minioadmin',\n    aws_secret_access_key='minioadmin',\n    config=Config(signature_version='s3v4')\n)\n\n# CrÃ©er les buckets\nfor bucket in ['bronze', 'silver', 'gold']:\n    try:\n        s3.create_bucket(Bucket=bucket)\n        print(f\"âœ… Bucket '{bucket}' crÃ©Ã©\")\n    except Exception as e:\n        print(f\"âš ï¸ Bucket '{bucket}' existe dÃ©jÃ \")\n\n# CrÃ©er des donnÃ©es de test\nwith open('sales_data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['date', 'product', 'category', 'amount', 'quantity'])\n    writer.writerow(['2024-01-01', 'Laptop', 'Electronics', 1200, 1])\n    writer.writerow(['2024-01-01', 'Mouse', 'Electronics', 25, 3])\n    writer.writerow(['2024-01-02', 'Desk', 'Furniture', 350, 1])\n    writer.writerow(['2024-01-02', 'Chair', 'Furniture', 150, 2])\n    writer.writerow(['2024-01-03', 'Laptop', 'Electronics', 1200, 2])\n    writer.writerow(['2024-01-03', 'Monitor', 'Electronics', 400, 1])\n\n# Upload vers bronze\ns3.upload_file('sales_data.csv', 'bronze', 'raw/sales_data.csv')\nprint(\"âœ… DonnÃ©es uploadÃ©es vers bronze/raw/sales_data.csv\")\n\"\"\"\nprint(step2_code)\n\n\n\n\nCode\n# Ã‰tape 3 : Transformer avec PySpark (Bronze â†’ Silver)\n\nstep3_code = \"\"\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, upper\n\n# CrÃ©er SparkSession avec config MinIO\nspark = SparkSession.builder \\\n    .appName(\"Bronze to Silver\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\n# Lire depuis Bronze\ndf_bronze = spark.read.csv(\n    \"s3a://bronze/raw/sales_data.csv\",\n    header=True,\n    inferSchema=True\n)\n\nprint(\"ğŸ“¥ DonnÃ©es Bronze:\")\ndf_bronze.show()\n\n# Transformer\ndf_silver = df_bronze \\\n    .withColumn(\"date\", to_date(col(\"date\"))) \\\n    .withColumn(\"category\", upper(col(\"category\"))) \\\n    .withColumn(\"total\", col(\"amount\") * col(\"quantity\")) \\\n    .dropDuplicates()\n\nprint(\"ğŸ”„ DonnÃ©es transformÃ©es:\")\ndf_silver.show()\n\n# Ã‰crire vers Silver (Parquet partitionnÃ©)\ndf_silver.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"category\") \\\n    .parquet(\"s3a://silver/sales/\")\n\nprint(\"âœ… DonnÃ©es Ã©crites vers silver/sales/\")\n\"\"\"\nprint(step3_code)\n\n\n\n\nCode\n# Ã‰tape 4 : AgrÃ©ger avec Spark SQL (Silver â†’ Gold)\n\nstep4_code = \"\"\"\n# Lire depuis Silver\ndf_silver = spark.read.parquet(\"s3a://silver/sales/\")\n\n# CrÃ©er une vue temporaire\ndf_silver.createOrReplaceTempView(\"sales\")\n\n# AgrÃ©gation avec Spark SQL\ndf_gold = spark.sql(\\\"\\\"\\\"\n    SELECT \n        category,\n        COUNT(*) as num_transactions,\n        SUM(quantity) as total_quantity,\n        SUM(total) as total_revenue,\n        AVG(total) as avg_transaction\n    FROM sales\n    GROUP BY category\n    ORDER BY total_revenue DESC\n\\\"\\\"\\\")\n\nprint(\"ğŸ“Š AgrÃ©gations Gold:\")\ndf_gold.show()\n\n# Ã‰crire vers Gold\ndf_gold.coalesce(1) \\\n    .write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"s3a://gold/category_summary/\")\n\nprint(\"âœ… DonnÃ©es Ã©crites vers gold/category_summary/\")\n\"\"\"\nprint(step4_code)\n\n\n\n\nCode\n# Ã‰tape 5 : VÃ©rifier les rÃ©sultats\n\nstep5_code = \"\"\"\n# Lister les fichiers crÃ©Ã©s\nimport subprocess\n\n# Avec mc CLI\nprint(\"ğŸ“ Contenu de bronze/:\")\n!mc ls myminio/bronze/ --recursive\n\nprint(\"\\nğŸ“ Contenu de silver/:\")\n!mc ls myminio/silver/ --recursive\n\nprint(\"\\nğŸ“ Contenu de gold/:\")\n!mc ls myminio/gold/ --recursive\n\n# Ou avec boto3\nfor bucket in ['bronze', 'silver', 'gold']:\n    print(f\"\\nğŸ“ {bucket}/\")\n    response = s3.list_objects_v2(Bucket=bucket)\n    for obj in response.get('Contents', []):\n        print(f\"   {obj['Key']} ({obj['Size']} bytes)\")\n\"\"\"\nprint(step5_code)",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#quiz-de-fin-de-module",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre un prefix et un dossier dans S3 ?\n\nAucune diffÃ©rence\n\nUn prefix est un vrai dossier crÃ©Ã© par S3\n\nUn prefix est juste une convention de nommage, S3 est un key-value store plat\n\nUn dossier peut contenir des sous-dossiers, pas un prefix\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” S3 est un key-value store plat. Les â€œ/â€ dans les clÃ©s sont juste des caractÃ¨res comme les autres.\n\n\n\n\nâ“ Q2. Quel protocole utiliser pour lire S3 avec Spark ?\n\ns3://\n\ns3a://\n\nhttps://\n\nhdfs://\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” s3a:// est le protocole Hadoop optimisÃ© pour Spark. s3:// est pour AWS CLI/boto3.\n\n\n\n\nâ“ Q3. Pourquoi les petits fichiers sont-ils un problÃ¨me ?\n\nIls prennent plus de place\n\nIls gÃ©nÃ¨rent trop de requÃªtes API et dâ€™overhead\n\nIls ne sont pas supportÃ©s par Parquet\n\nIls ne peuvent pas Ãªtre partitionnÃ©s\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Chaque fichier = une requÃªte API. 10,000 fichiers = 10,000 GET requests = lent et coÃ»teux.\n\n\n\n\nâ“ Q4. Quelle est la diffÃ©rence entre SAS Token et Managed Identity sur Azure ?\n\nSAS Token est plus sÃ©curisÃ©\n\nManaged Identity est temporaire, SAS est permanent\n\nSAS Token est temporaire et partageable, Managed Identity est liÃ©e Ã  une ressource Azure\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” SAS Token = URL temporaire partageable. Managed Identity = identitÃ© attachÃ©e Ã  une VM/AKS, plus sÃ©curisÃ©.\n\n\n\n\nâ“ Q5. Pourquoi MinIO est-il compatible avec S3 ?\n\nCâ€™est un produit AWS\n\nIl implÃ©mente la mÃªme API REST que S3\n\nIl utilise les mÃªmes serveurs\n\nIl copie les donnÃ©es depuis S3\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” MinIO implÃ©mente lâ€™API S3 (REST). Le mÃªme code boto3 fonctionne avec les deux.\n\n\n\n\nâ“ Q6. Quelle classe de stockage pour des donnÃ©es rarement lues ?\n\nS3 Standard\n\nS3 Intelligent-Tiering\n\nS3 Glacier\n\nS3 One Zone-IA\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Glacier pour lâ€™archivage (donnÃ©es rarement lues). Intelligent-Tiering si le pattern dâ€™accÃ¨s est imprÃ©visible.\n\n\n\n\nâ“ Q7. Quel est lâ€™avantage du partitionnement dans un Data Lake ?\n\nLes fichiers sont plus petits\n\nSpark peut ignorer les partitions non pertinentes (partition pruning)\n\nLe stockage coÃ»te moins cher\n\nLes donnÃ©es sont automatiquement compressÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Partition pruning = Spark lit uniquement les partitions qui matchent le filtre.\n\n\n\n\nâ“ Q8. Comment authentifier Spark on K8s vers S3 en production ?\n\nAccess Keys dans le code\n\nVariables dâ€™environnement\n\nIAM Roles for Service Accounts (IRSA)\n\nFichier de config local\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” IRSA permet dâ€™associer un IAM Role Ã  un ServiceAccount K8s. Pas de credentials dans le code.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Metadata Catalog (Glue, Hive Metastore) ?\n\nUn systÃ¨me de stockage\n\nUn registre des schÃ©mas et partitions des tables\n\nUn outil de requÃªtage SQL\n\nUn systÃ¨me de cache\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le catalog stocke les mÃ©tadonnÃ©es : schÃ©ma, localisation, partitions, stats. Lâ€™Object Storage ne stocke que les fichiers bruts.\n\n\n\n\nâ“ Q10. Quel format de fichier est recommandÃ© pour un Data Lake analytique ?\n\nCSV\n\nJSON\n\nParquet\n\nXML\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Parquet est columnar, compressÃ©, avec schema intÃ©grÃ©. IdÃ©al pour lâ€™analytics.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#ressources-pour-aller-plus-loin",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nAWS S3 Documentation\nAzure Blob Storage\nGoogle Cloud Storage\nMinIO Documentation\n\n\n\nğŸ“– Articles & Tutoriels\n\nSpark + S3 Best Practices\nData Lake Architecture",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#prochaine-Ã©tape",
    "title": "â˜ï¸ Cloud & Object Storage for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises lâ€™Object Storage, passons aux Table Formats pour transformer ton Data Lake en Lakehouse !\nğŸ‘‰ Module suivant : 23_table_formats_delta_iceberg.ipynb â€” Delta Lake & Apache Iceberg\nTu vas apprendre : - Delta Lake : ACID, Time Travel, Schema Evolution - Apache Iceberg : Table format open-source - Transaction Log : Comment Ã§a remplace le Metastore - Optimisations : Compaction, Z-Ordering, Vacuum\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nCloud Computing\nIaaS, PaaS, SaaS, rÃ©gions\n\n\nServices Cloud DE\nS3, Glue, BigQuery, Synapseâ€¦\n\n\nStorage Models\nBlock vs File vs Object\n\n\nObject Storage\nBuckets, keys, prefixes, protocols\n\n\nAWS S3\nboto3, s3a://, IAM\n\n\nAzure Blob\nSDK, abfss://, SAS, Managed Identity\n\n\nGCS\ngsutil, gs://, Workload Identity\n\n\nMinIO\nObject Storage local S3-compatible\n\n\nPerformance\nSmall files, partitioning, formats\n\n\nSÃ©curitÃ©\nEncryption, bucket policies\n\n\nCoÃ»ts\nClasses de stockage, lifecycle\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Cloud & Object Storage.",
    "crumbs": [
      "IntermÃ©diaire",
      "â˜ï¸ Cloud & Object Storage for Data Engineers"
    ]
  }
]