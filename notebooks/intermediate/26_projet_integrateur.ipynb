{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet IntÃ©grateur : Pipeline E-commerce Olist\n",
    "\n",
    "## SynthÃ¨se de Toutes les CompÃ©tences Data Engineering\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Tu viens de rejoindre l'Ã©quipe **Data Engineering** d'**Olist**, la plus grande plateforme e-commerce du BrÃ©sil. Olist connecte des petits commerÃ§ants aux grandes marketplaces comme Amazon, Mercado Libre, etc.\n",
    "\n",
    "Actuellement, les donnÃ©es sont stockÃ©es dans des **fichiers CSV** et analysÃ©es manuellement par l'Ã©quipe BI. Ton manager te confie une mission critique :\n",
    "\n",
    "> **\"Nous avons besoin d'une architecture Lakehouse moderne. Tu dois construire un pipeline complet qui ingÃ¨re nos donnÃ©es en temps rÃ©el, les transforme, et les met Ã  disposition pour les dashboards analytiques.\"**\n",
    "\n",
    "---\n",
    "\n",
    "## Ta Mission\n",
    "\n",
    "Construire un **Data Pipeline complet** de bout en bout :\n",
    "\n",
    "```\n",
    "CSV (Kaggle) â†’ Kafka â†’ Spark SSS â†’ Delta Lake (Bronze/Silver) â†’ dbt (Gold) â†’ Dashboard-ready\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**Brazilian E-Commerce Public Dataset by Olist**\n",
    "\n",
    "ğŸ”— **Kaggle** : https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "Ce dataset contient **~100 000 commandes rÃ©elles** (anonymisÃ©es) passÃ©es sur Olist entre 2016 et 2018.\n",
    "\n",
    "### Tables disponibles\n",
    "\n",
    "| Fichier | Description | Lignes |\n",
    "|---------|-------------|--------|\n",
    "| `olist_orders_dataset.csv` | Commandes | ~100K |\n",
    "| `olist_order_items_dataset.csv` | Lignes de commande | ~113K |\n",
    "| `olist_customers_dataset.csv` | Clients | ~100K |\n",
    "| `olist_products_dataset.csv` | Produits | ~33K |\n",
    "| `olist_sellers_dataset.csv` | Vendeurs | ~3K |\n",
    "| `olist_order_payments_dataset.csv` | Paiements | ~104K |\n",
    "| `olist_order_reviews_dataset.csv` | Avis clients | ~100K |\n",
    "| `olist_geolocation_dataset.csv` | GÃ©olocalisation | ~1M |\n",
    "| `product_category_name_translation.csv` | Traduction catÃ©gories | ~71 |\n",
    "\n",
    "### SchÃ©ma relationnel\n",
    "\n",
    "```\n",
    "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                              â”‚    customers     â”‚\n",
    "                              â”‚  customer_id (PK)â”‚\n",
    "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                       â”‚\n",
    "                                       â”‚ 1:N\n",
    "                                       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1:N     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     N:1     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   sellers    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     orders       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   payments   â”‚\n",
    "â”‚ seller_id(PK)â”‚            â”‚  order_id (PK)   â”‚             â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  customer_id(FK) â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â–²                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                             â”‚\n",
    "       â”‚ N:1                         â”‚ 1:N\n",
    "       â”‚                             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    N:1     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     1:N     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   products   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   order_items    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   reviews    â”‚\n",
    "â”‚product_id(PK)â”‚            â”‚ order_id (FK)    â”‚             â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ product_id (FK)  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚ seller_id (FK)   â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Architecture Cible\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         PIPELINE E-COMMERCE OLIST                               â”‚\n",
    "â”‚                                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚   CSV Files  â”‚    â”‚    KAFKA    â”‚    â”‚   BRONZE    â”‚    â”‚   SILVER    â”‚    â”‚\n",
    "â”‚  â”‚   (Kaggle)   â”‚â”€â”€â”€â–¶â”‚   Topics    â”‚â”€â”€â”€â–¶â”‚  (Delta)    â”‚â”€â”€â”€â–¶â”‚  (Delta)    â”‚    â”‚\n",
    "â”‚  â”‚              â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ orders     â”‚    â”‚ â€¢ raw_ordersâ”‚    â”‚  Append     â”‚    â”‚ MERGE INTO  â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ items      â”‚    â”‚ â€¢ raw_items â”‚    â”‚  Raw data   â”‚    â”‚ Deduplicatedâ”‚    â”‚\n",
    "â”‚  â”‚ â€¢ customers  â”‚    â”‚ â€¢ raw_custs â”‚    â”‚  Partitionedâ”‚    â”‚ Enriched    â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ products   â”‚    â”‚ â€¢ raw_prods â”‚    â”‚             â”‚    â”‚ Validated   â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ sellers    â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚         â”‚                   â”‚                  â”‚                  â”‚           â”‚\n",
    "â”‚         â”‚            Spark SSS          Spark SSS          Spark SSS          â”‚\n",
    "â”‚         â”‚                                                  + foreachBatch     â”‚\n",
    "â”‚         â”‚                                                                     â”‚\n",
    "â”‚         â–¼                                                        â”‚            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â–¼            â”‚\n",
    "â”‚  â”‚  Producers   â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚  â”‚  (Python)    â”‚                                    â”‚      GOLD       â”‚     â”‚\n",
    "â”‚  â”‚              â”‚                                    â”‚     (dbt)       â”‚     â”‚\n",
    "â”‚  â”‚ Simulate     â”‚                                    â”‚                 â”‚     â”‚\n",
    "â”‚  â”‚ streaming    â”‚                                    â”‚ â€¢ daily_sales   â”‚     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚ â€¢ seller_perf   â”‚     â”‚\n",
    "â”‚                                                      â”‚ â€¢ customer_rfm  â”‚     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â€¢ product_stats â”‚     â”‚\n",
    "â”‚  â”‚           ORCHESTRATION (Airflow)           â”‚    â”‚ â€¢ delivery_perf â”‚     â”‚\n",
    "â”‚  â”‚                                             â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚  â”‚  [Producers] â†’ [Bronze] â†’ [Silver] â†’ [dbt] â†’ [GE Validation]            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â”‚                                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\n",
    "â”‚  â”‚        DATA QUALITY (Great Expectations)    â”‚                            â”‚\n",
    "â”‚  â”‚  â€¢ Bronze: schema, completeness             â”‚                            â”‚\n",
    "â”‚  â”‚  â€¢ Silver: business rules, uniqueness       â”‚                            â”‚\n",
    "â”‚  â”‚  â€¢ Gold: consistency, freshness             â”‚                            â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Livrables Attendus\n",
    "\n",
    "Tu dois produire les livrables suivants :\n",
    "\n",
    "```\n",
    "olist_pipeline/\n",
    "â”‚\n",
    "â”œâ”€â”€ docker-compose.yml           # 1. Infrastructure complÃ¨te\n",
    "â”‚\n",
    "â”œâ”€â”€ producers/                   # 2. Producteurs Kafka\n",
    "â”‚   â”œâ”€â”€ orders_producer.py\n",
    "â”‚   â”œâ”€â”€ items_producer.py\n",
    "â”‚   â”œâ”€â”€ customers_producer.py\n",
    "â”‚   â”œâ”€â”€ products_producer.py\n",
    "â”‚   â””â”€â”€ sellers_producer.py\n",
    "â”‚\n",
    "â”œâ”€â”€ spark_jobs/                  # 3 & 4. Jobs Spark\n",
    "â”‚   â”œâ”€â”€ bronze/\n",
    "â”‚   â”‚   â”œâ”€â”€ ingest_orders.py\n",
    "â”‚   â”‚   â”œâ”€â”€ ingest_items.py\n",
    "â”‚   â”‚   â””â”€â”€ ingest_customers.py\n",
    "â”‚   â””â”€â”€ silver/\n",
    "â”‚       â”œâ”€â”€ silver_orders.py     # Avec MERGE INTO\n",
    "â”‚       â”œâ”€â”€ silver_customers.py\n",
    "â”‚       â””â”€â”€ silver_order_items.py\n",
    "â”‚\n",
    "â”œâ”€â”€ dbt_olist/                   # 5. Projet dbt\n",
    "â”‚   â”œâ”€â”€ dbt_project.yml\n",
    "â”‚   â”œâ”€â”€ models/\n",
    "â”‚   â”‚   â”œâ”€â”€ staging/\n",
    "â”‚   â”‚   â”œâ”€â”€ intermediate/\n",
    "â”‚   â”‚   â””â”€â”€ gold/\n",
    "â”‚   â””â”€â”€ tests/\n",
    "â”‚\n",
    "â”œâ”€â”€ great_expectations/          # 6. Data Quality\n",
    "â”‚   â””â”€â”€ expectations/\n",
    "â”‚\n",
    "â”œâ”€â”€ dags/                        # 7. Airflow DAGs\n",
    "â”‚   â””â”€â”€ olist_pipeline_dag.py\n",
    "â”‚\n",
    "â””â”€â”€ README.md                    # 8. Documentation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tables Gold Attendues\n",
    "\n",
    "Tu dois crÃ©er **5 models Gold** dans dbt :\n",
    "\n",
    "### 1. `gold_daily_sales`\n",
    "Chiffre d'affaires quotidien.\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| order_date | DATE | Date de commande |\n",
    "| total_orders | INT | Nombre de commandes |\n",
    "| total_revenue | DECIMAL | CA total |\n",
    "| avg_order_value | DECIMAL | Panier moyen |\n",
    "| total_items | INT | Nombre d'articles vendus |\n",
    "\n",
    "### 2. `gold_seller_performance`\n",
    "MÃ©triques par vendeur.\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| seller_id | STRING | ID vendeur |\n",
    "| seller_city | STRING | Ville |\n",
    "| total_orders | INT | Commandes traitÃ©es |\n",
    "| total_revenue | DECIMAL | CA gÃ©nÃ©rÃ© |\n",
    "| avg_review_score | DECIMAL | Note moyenne |\n",
    "| avg_delivery_days | DECIMAL | DÃ©lai moyen livraison |\n",
    "\n",
    "### 3. `gold_customer_rfm`\n",
    "Segmentation RFM (Recency, Frequency, Monetary).\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| customer_unique_id | STRING | ID client unique |\n",
    "| recency_days | INT | Jours depuis derniÃ¨re commande |\n",
    "| frequency | INT | Nombre de commandes |\n",
    "| monetary | DECIMAL | Total dÃ©pensÃ© |\n",
    "| rfm_segment | STRING | Segment (Champions, At Risk, etc.) |\n",
    "\n",
    "### 4. `gold_product_analytics`\n",
    "Performance des produits.\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| product_category | STRING | CatÃ©gorie (EN) |\n",
    "| total_sold | INT | QuantitÃ© vendue |\n",
    "| total_revenue | DECIMAL | CA |\n",
    "| avg_price | DECIMAL | Prix moyen |\n",
    "| avg_review_score | DECIMAL | Note moyenne |\n",
    "\n",
    "### 5. `gold_delivery_performance`\n",
    "Performance des livraisons.\n",
    "\n",
    "| Colonne | Type | Description |\n",
    "|---------|------|-------------|\n",
    "| seller_state | STRING | Ã‰tat du vendeur |\n",
    "| customer_state | STRING | Ã‰tat du client |\n",
    "| total_deliveries | INT | Nombre de livraisons |\n",
    "| avg_delivery_days | DECIMAL | DÃ©lai moyen |\n",
    "| on_time_rate | DECIMAL | % livrÃ© Ã  temps |\n",
    "| late_rate | DECIMAL | % en retard |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SpÃ©cifications Techniques\n",
    "\n",
    "### 1. Infrastructure (Docker Compose)\n",
    "\n",
    "Services requis :\n",
    "\n",
    "| Service | Image | Ports | RÃ´le |\n",
    "|---------|-------|-------|------|\n",
    "| **zookeeper** | confluentinc/cp-zookeeper:7.5.0 | 2181 | Coordination Kafka |\n",
    "| **kafka** | confluentinc/cp-kafka:7.5.0 | 9092, 29092 | Message broker |\n",
    "| **schema-registry** | confluentinc/cp-schema-registry:7.5.0 | 8081 | Gestion schÃ©mas |\n",
    "| **minio** | minio/minio | 9000, 9001 | S3 local (Delta Lake) |\n",
    "| **spark-master** | bitnami/spark:3.5 | 8080, 7077 | Spark Master |\n",
    "| **spark-worker** | bitnami/spark:3.5 | 8081 | Spark Worker |\n",
    "| **postgres** | postgres:15 | 5432 | MÃ©tadonnÃ©es Airflow |\n",
    "| **airflow-webserver** | apache/airflow:2.8.0 | 8082 | UI Airflow |\n",
    "| **airflow-scheduler** | apache/airflow:2.8.0 | - | Scheduler |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Producteurs Kafka\n",
    "\n",
    "Chaque producteur doit :\n",
    "\n",
    "- Lire le fichier CSV correspondant\n",
    "- Envoyer les lignes une par une vers Kafka (simuler du streaming)\n",
    "- Ajouter un dÃ©lai alÃ©atoire (50-200ms) entre les messages\n",
    "- **Simuler du late data** : 5% des messages avec un timestamp dÃ©calÃ© de 1-5 minutes\n",
    "- **Simuler des doublons** : 2% des messages envoyÃ©s 2 fois\n",
    "\n",
    "**Topics Kafka :**\n",
    "- `raw_orders`\n",
    "- `raw_order_items`\n",
    "- `raw_customers`\n",
    "- `raw_products`\n",
    "- `raw_sellers`\n",
    "\n",
    "**Format des messages :** JSON\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"order_id\": \"e481f51cbdc54678b7cc49136f2d6af7\",\n",
    "  \"customer_id\": \"9ef432eb6251297304e76186b10a928d\",\n",
    "  \"order_status\": \"delivered\",\n",
    "  \"order_purchase_timestamp\": \"2017-10-02 10:56:33\",\n",
    "  \"_ingestion_timestamp\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Couche Bronze (Spark SSS â†’ Delta)\n",
    "\n",
    "**Mode : Append** (donnÃ©es brutes, pas de transformation)\n",
    "\n",
    "Chaque job Bronze doit :\n",
    "\n",
    "- Lire depuis le topic Kafka correspondant\n",
    "- Parser le JSON\n",
    "- Ajouter une colonne `_bronze_ingested_at` (timestamp d'ingestion)\n",
    "- Ã‰crire en **append** dans Delta Lake\n",
    "- Partitionner par **date d'ingestion** (`_ingestion_date`)\n",
    "- Configurer le **checkpointing**\n",
    "\n",
    "**Chemins Delta :**\n",
    "```\n",
    "s3a://lakehouse/bronze/orders/\n",
    "s3a://lakehouse/bronze/order_items/\n",
    "s3a://lakehouse/bronze/customers/\n",
    "s3a://lakehouse/bronze/products/\n",
    "s3a://lakehouse/bronze/sellers/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Couche Silver (Spark SSS + MERGE INTO) â­\n",
    "\n",
    "**Mode : foreachBatch + MERGE INTO** (upserts, dÃ©duplication)\n",
    "\n",
    "C'est l'Ã©tape **clÃ©** du projet. Chaque job Silver doit :\n",
    "\n",
    "1. **Lire** depuis Bronze (streaming ou batch)\n",
    "2. **DÃ©dupliquer** sur la clÃ© primaire (garder le plus rÃ©cent)\n",
    "3. **Valider** les donnÃ©es (filtrer les invalides)\n",
    "4. **Enrichir** si nÃ©cessaire (jointures)\n",
    "5. **MERGE INTO** Delta Lake Silver\n",
    "\n",
    "**Pattern Ã  utiliser :**\n",
    "\n",
    "```python\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    # 1. DÃ©duplication\n",
    "    deduped = batch_df.dropDuplicates([\"order_id\"])\n",
    "    \n",
    "    # 2. MERGE INTO\n",
    "    delta_table = DeltaTable.forPath(spark, \"s3a://lakehouse/silver/orders\")\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        deduped.alias(\"source\"),\n",
    "        \"target.order_id = source.order_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"source._bronze_ingested_at > target._bronze_ingested_at\",\n",
    "        set={...}\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "\n",
    "bronze_stream.writeStream \\\n",
    "    .foreachBatch(upsert_to_silver) \\\n",
    "    .option(\"checkpointLocation\", \"/checkpoints/silver_orders\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Tables Silver :**\n",
    "\n",
    "| Table | ClÃ© primaire | Enrichissement |\n",
    "|-------|--------------|----------------|\n",
    "| `silver_orders` | order_id | + customer info |\n",
    "| `silver_order_items` | order_id + product_id + seller_id | + product info |\n",
    "| `silver_customers` | customer_id | DÃ©dup sur customer_unique_id |\n",
    "| `silver_products` | product_id | + category translation |\n",
    "| `silver_sellers` | seller_id | - |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Couche Gold (dbt)\n",
    "\n",
    "**Structure du projet dbt :**\n",
    "\n",
    "```\n",
    "dbt_olist/\n",
    "â”œâ”€â”€ dbt_project.yml\n",
    "â”œâ”€â”€ packages.yml                 # dbt-utils, dbt-expectations\n",
    "â”‚\n",
    "â”œâ”€â”€ models/\n",
    "â”‚   â”œâ”€â”€ staging/                 # Vues sur Silver\n",
    "â”‚   â”‚   â”œâ”€â”€ _sources.yml\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_orders.sql\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_order_items.sql\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_customers.sql\n",
    "â”‚   â”‚   â”œâ”€â”€ stg_products.sql\n",
    "â”‚   â”‚   â””â”€â”€ stg_sellers.sql\n",
    "â”‚   â”‚\n",
    "â”‚   â”œâ”€â”€ intermediate/            # Transformations mÃ©tier\n",
    "â”‚   â”‚   â”œâ”€â”€ int_orders_enriched.sql\n",
    "â”‚   â”‚   â””â”€â”€ int_order_items_enriched.sql\n",
    "â”‚   â”‚\n",
    "â”‚   â””â”€â”€ gold/                    # Tables analytiques\n",
    "â”‚       â”œâ”€â”€ _gold__models.yml    # Tests + docs\n",
    "â”‚       â”œâ”€â”€ gold_daily_sales.sql\n",
    "â”‚       â”œâ”€â”€ gold_seller_performance.sql\n",
    "â”‚       â”œâ”€â”€ gold_customer_rfm.sql\n",
    "â”‚       â”œâ”€â”€ gold_product_analytics.sql\n",
    "â”‚       â””â”€â”€ gold_delivery_performance.sql\n",
    "â”‚\n",
    "â”œâ”€â”€ macros/\n",
    "â”‚   â””â”€â”€ rfm_segment.sql          # Macro pour segmentation RFM\n",
    "â”‚\n",
    "â””â”€â”€ tests/\n",
    "    â””â”€â”€ assert_positive_revenue.sql\n",
    "```\n",
    "\n",
    "**MatÃ©rialisations :**\n",
    "- `staging/` : **view**\n",
    "- `intermediate/` : **ephemeral** ou **view**\n",
    "- `gold/` : **incremental** (avec `unique_key`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data Quality (Great Expectations)\n",
    "\n",
    "CrÃ©er des suites d'expectations pour chaque couche :\n",
    "\n",
    "**Suite Bronze :**\n",
    "- Schema validation (colonnes prÃ©sentes)\n",
    "- `expect_column_values_to_not_be_null` sur les IDs\n",
    "\n",
    "**Suite Silver :**\n",
    "- `expect_column_values_to_be_unique` sur les clÃ©s primaires\n",
    "- `expect_column_values_to_be_between` sur les montants (0 - 100000)\n",
    "- `expect_column_values_to_be_in_set` sur les statuts\n",
    "\n",
    "**Suite Gold :**\n",
    "- `expect_column_values_to_be_between` sur les mÃ©triques\n",
    "- `expect_table_row_count_to_be_between` (freshness check)\n",
    "- Tests de cohÃ©rence (total Gold = total Silver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Orchestration (Airflow)\n",
    "\n",
    "**DAG principal : `olist_pipeline`**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ check_freshness â”‚  VÃ©rifier que les donnÃ©es arrivent\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ bronze_to_silverâ”‚  Spark job (MERGE INTO)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    dbt_run      â”‚  dbt run --select gold\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   dbt_test      â”‚  dbt test\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ge_validate    â”‚  Great Expectations checkpoint\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
    "    â–¼         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚notify â”‚ â”‚notify â”‚\n",
    "â”‚successâ”‚ â”‚failureâ”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Configuration :**\n",
    "- Schedule : `0 6 * * *` (tous les jours Ã  6h)\n",
    "- Retries : 2\n",
    "- Alertes : Email ou Slack on_failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¡ Hints & Ressources\n",
    "\n",
    "### Rappels des patterns clÃ©s\n",
    "\n",
    "| Module | Pattern | OÃ¹ l'utiliser |\n",
    "|--------|---------|---------------|\n",
    "| 24 | `readStream.format(\"kafka\")` | Bronze ingestion |\n",
    "| 24 | `foreachBatch` | Silver MERGE |\n",
    "| 23 | `DeltaTable.forPath().merge()` | Silver MERGE |\n",
    "| 23 | `whenMatchedUpdate / whenNotMatchedInsert` | Silver MERGE |\n",
    "| 25 | `{{ ref('...') }}` | dbt models |\n",
    "| 25 | `{{ config(materialized='incremental') }}` | Gold models |\n",
    "| 25 | `is_incremental()` | Gold models |\n",
    "| 22 | `BashOperator` | Airflow tasks |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1 : Structure du producteur Kafka\n",
    "\n",
    "producer_template = '''\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "df = pd.read_csv('data/olist_orders_dataset.csv')\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    message = row.to_dict()\n",
    "    \n",
    "    # Ajouter timestamp d'ingestion\n",
    "    ingestion_ts = datetime.now()\n",
    "    \n",
    "    # Simuler late data (5%)\n",
    "    if random.random() < 0.05:\n",
    "        ingestion_ts -= timedelta(minutes=random.randint(1, 5))\n",
    "    \n",
    "    message['_ingestion_timestamp'] = ingestion_ts.isoformat()\n",
    "    \n",
    "    producer.send('raw_orders', value=message)\n",
    "    \n",
    "    # Simuler doublons (2%)\n",
    "    if random.random() < 0.02:\n",
    "        producer.send('raw_orders', value=message)\n",
    "    \n",
    "    # DÃ©lai alÃ©atoire\n",
    "    time.sleep(random.uniform(0.05, 0.2))\n",
    "\n",
    "producer.flush()\n",
    "'''\n",
    "\n",
    "print(producer_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 2 : Pattern MERGE INTO pour Silver\n",
    "\n",
    "merge_pattern = '''\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "def upsert_orders_to_silver(batch_df, batch_id):\n",
    "    \"\"\"Upsert orders vers Silver avec dÃ©duplication.\"\"\"\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    # 1. DÃ©duplication (garder le plus rÃ©cent)\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import row_number\n",
    "    \n",
    "    window = Window.partitionBy(\"order_id\").orderBy(col(\"_ingestion_timestamp\").desc())\n",
    "    deduped = batch_df.withColumn(\"_row_num\", row_number().over(window)) \\\n",
    "                      .filter(col(\"_row_num\") == 1) \\\n",
    "                      .drop(\"_row_num\")\n",
    "    \n",
    "    # 2. Ajouter timestamp Silver\n",
    "    enriched = deduped.withColumn(\"_silver_updated_at\", current_timestamp())\n",
    "    \n",
    "    # 3. MERGE INTO\n",
    "    silver_path = \"s3a://lakehouse/silver/orders\"\n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "        delta_table = DeltaTable.forPath(spark, silver_path)\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            enriched.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"source._ingestion_timestamp > target._ingestion_timestamp\",\n",
    "            set={\n",
    "                \"order_status\": \"source.order_status\",\n",
    "                \"order_delivered_customer_date\": \"source.order_delivered_customer_date\",\n",
    "                \"_ingestion_timestamp\": \"source._ingestion_timestamp\",\n",
    "                \"_silver_updated_at\": \"source._silver_updated_at\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        # PremiÃ¨re exÃ©cution : crÃ©er la table\n",
    "        enriched.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n",
    "    \n",
    "    print(f\"Batch {batch_id}: {enriched.count()} records merged to Silver\")\n",
    "'''\n",
    "\n",
    "print(merge_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 3 : Model dbt incremental\n",
    "\n",
    "dbt_incremental = '''\n",
    "-- models/gold/gold_daily_sales.sql\n",
    "\n",
    "{{ config(\n",
    "    materialized='incremental',\n",
    "    unique_key='order_date',\n",
    "    incremental_strategy='merge'\n",
    ") }}\n",
    "\n",
    "WITH orders AS (\n",
    "    SELECT * FROM {{ ref('int_orders_enriched') }}\n",
    "    {% if is_incremental() %}\n",
    "    WHERE DATE(order_purchase_timestamp) >= (\n",
    "        SELECT MAX(order_date) - INTERVAL 2 DAY FROM {{ this }}\n",
    "    )\n",
    "    {% endif %}\n",
    "),\n",
    "\n",
    "daily_agg AS (\n",
    "    SELECT\n",
    "        DATE(order_purchase_timestamp) AS order_date,\n",
    "        COUNT(DISTINCT order_id) AS total_orders,\n",
    "        SUM(total_amount) AS total_revenue,\n",
    "        AVG(total_amount) AS avg_order_value,\n",
    "        SUM(total_items) AS total_items\n",
    "    FROM orders\n",
    "    WHERE order_status = 'delivered'\n",
    "    GROUP BY DATE(order_purchase_timestamp)\n",
    ")\n",
    "\n",
    "SELECT * FROM daily_agg\n",
    "'''\n",
    "\n",
    "print(dbt_incremental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 4 : Macro RFM pour dbt\n",
    "\n",
    "rfm_macro = '''\n",
    "-- macros/rfm_segment.sql\n",
    "\n",
    "{% macro rfm_segment(recency, frequency, monetary) %}\n",
    "    CASE\n",
    "        -- Champions : RÃ©cent, FrÃ©quent, Gros dÃ©pensier\n",
    "        WHEN {{ recency }} <= 30 AND {{ frequency }} >= 3 AND {{ monetary }} >= 500 THEN 'Champions'\n",
    "        \n",
    "        -- Loyal Customers : FrÃ©quent\n",
    "        WHEN {{ frequency }} >= 3 THEN 'Loyal Customers'\n",
    "        \n",
    "        -- Potential Loyalists : RÃ©cent, pas encore frÃ©quent\n",
    "        WHEN {{ recency }} <= 30 AND {{ frequency }} < 3 THEN 'Potential Loyalists'\n",
    "        \n",
    "        -- At Risk : Pas rÃ©cent mais Ã©tait frÃ©quent\n",
    "        WHEN {{ recency }} > 90 AND {{ frequency }} >= 2 THEN 'At Risk'\n",
    "        \n",
    "        -- Hibernating : Pas rÃ©cent, peu frÃ©quent\n",
    "        WHEN {{ recency }} > 90 THEN 'Hibernating'\n",
    "        \n",
    "        -- Others\n",
    "        ELSE 'Others'\n",
    "    END\n",
    "{% endmacro %}\n",
    "\n",
    "-- Utilisation dans gold_customer_rfm.sql :\n",
    "-- SELECT\n",
    "--     customer_unique_id,\n",
    "--     recency_days,\n",
    "--     frequency,\n",
    "--     monetary,\n",
    "--     {{ rfm_segment('recency_days', 'frequency', 'monetary') }} AS rfm_segment\n",
    "-- FROM rfm_base\n",
    "'''\n",
    "\n",
    "print(rfm_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 5 : DAG Airflow\n",
    "\n",
    "airflow_dag = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'email_on_failure': True,\n",
    "    'email': ['data-team@olist.com'],\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id='olist_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline E-commerce Olist',\n",
    "    schedule_interval='0 6 * * *',\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    tags=['olist', 'lakehouse'],\n",
    ") as dag:\n",
    "    \n",
    "    # 1. VÃ©rifier la fraÃ®cheur des sources\n",
    "    check_freshness = BashOperator(\n",
    "        task_id='check_freshness',\n",
    "        bash_command='cd /opt/dbt && dbt source freshness',\n",
    "    )\n",
    "    \n",
    "    # 2. Spark : Bronze â†’ Silver\n",
    "    bronze_to_silver = BashOperator(\n",
    "        task_id='bronze_to_silver',\n",
    "        bash_command='spark-submit /opt/spark_jobs/silver/run_all_silver.py',\n",
    "    )\n",
    "    \n",
    "    # 3. dbt run\n",
    "    dbt_run = BashOperator(\n",
    "        task_id='dbt_run',\n",
    "        bash_command='cd /opt/dbt && dbt run --select gold',\n",
    "    )\n",
    "    \n",
    "    # 4. dbt test\n",
    "    dbt_test = BashOperator(\n",
    "        task_id='dbt_test',\n",
    "        bash_command='cd /opt/dbt && dbt test --select gold',\n",
    "    )\n",
    "    \n",
    "    # 5. Great Expectations\n",
    "    ge_validate = BashOperator(\n",
    "        task_id='ge_validate',\n",
    "        bash_command='great_expectations checkpoint run gold_checkpoint',\n",
    "    )\n",
    "    \n",
    "    # 6. Notification succÃ¨s\n",
    "    notify_success = BashOperator(\n",
    "        task_id='notify_success',\n",
    "        bash_command='echo \"Pipeline completed successfully!\"',\n",
    "        trigger_rule='all_success',\n",
    "    )\n",
    "    \n",
    "    # DÃ©pendances\n",
    "    check_freshness >> bronze_to_silver >> dbt_run >> dbt_test >> ge_validate >> notify_success\n",
    "'''\n",
    "\n",
    "print(airflow_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CritÃ¨res d'Ã‰valuation\n",
    "\n",
    "| CritÃ¨re | Points | DÃ©tail |\n",
    "|---------|--------|--------|\n",
    "| **Infrastructure** | /10 | Docker Compose fonctionne, tous les services up |\n",
    "| **Producteurs Kafka** | /10 | 5 producteurs, late data + doublons simulÃ©s |\n",
    "| **Bronze (Append)** | /10 | DonnÃ©es ingÃ©rÃ©es, partitionnÃ©es, checkpointing |\n",
    "| **Silver (MERGE INTO)** | /20 | â­ Pattern foreachBatch + MERGE correct, dÃ©dup |\n",
    "| **Gold (dbt)** | /20 | 5 models, incremental, ref() correct |\n",
    "| **Tests dbt** | /10 | Tests passent, couverture suffisante |\n",
    "| **Great Expectations** | /10 | Suites crÃ©Ã©es, checkpoint fonctionne |\n",
    "| **Airflow DAG** | /5 | DAG fonctionne, dÃ©pendances correctes |\n",
    "| **Documentation** | /5 | README clair, schÃ©mas, instructions |\n",
    "| **TOTAL** | **/100** | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CompÃ©tences ValidÃ©es\n",
    "\n",
    "En complÃ©tant ce projet, tu valides les compÃ©tences suivantes :\n",
    "\n",
    "| Module | CompÃ©tence | AppliquÃ©e dans | âœ… |\n",
    "|--------|------------|----------------|----|\n",
    "| 14-16 | Python, environnements | Producteurs Kafka | â˜ |\n",
    "| 17 | SQL | Transformations dbt | â˜ |\n",
    "| 18-20 | PySpark DataFrame | Jobs Spark | â˜ |\n",
    "| 22 | Airflow | Orchestration DAG | â˜ |\n",
    "| 23 | Delta Lake, MERGE INTO | Bronze â†’ Silver | â˜ |\n",
    "| 24 | Kafka, Spark SSS, foreachBatch | Ingestion streaming | â˜ |\n",
    "| 25 | dbt, Great Expectations | Gold + QualitÃ© | â˜ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions Possibles (Bonus)\n",
    "\n",
    "Si tu as terminÃ© le projet principal, voici des extensions pour aller plus loin :\n",
    "\n",
    "| Extension | Description | DifficultÃ© |\n",
    "|-----------|-------------|------------|\n",
    "| **Monitoring** | Ajouter Prometheus + Grafana pour monitorer le pipeline | â­â­ |\n",
    "| **Kubernetes** | DÃ©ployer sur K8s avec Spark Operator | â­â­â­ |\n",
    "| **ML Pipeline** | Ajouter un modÃ¨le de prÃ©diction (churn, LTV) | â­â­ |\n",
    "| **CDC** | Utiliser Debezium pour capturer les changes | â­â­ |\n",
    "| **Data Catalog** | IntÃ©grer DataHub ou Amundsen | â­â­â­ |\n",
    "| **Streamlit** | CrÃ©er un dashboard interactif | â­ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Solution ComplÃ¨te\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“‚ Cliquer pour voir la solution complÃ¨te</summary>\n",
    "\n",
    "### docker-compose.yml\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  # KAFKA\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    container_name: zookeeper\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    container_name: kafka\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "      - \"29092:29092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n",
    "\n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.5.0\n",
    "    container_name: schema-registry\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092\n",
    "\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  # STORAGE (MinIO = S3 local)\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  minio:\n",
    "    image: minio/minio\n",
    "    container_name: minio\n",
    "    ports:\n",
    "      - \"9000:9000\"\n",
    "      - \"9001:9001\"\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minioadmin\n",
    "      MINIO_ROOT_PASSWORD: minioadmin\n",
    "    command: server /data --console-address \":9001\"\n",
    "    volumes:\n",
    "      - minio_data:/data\n",
    "\n",
    "  # CrÃ©er le bucket au dÃ©marrage\n",
    "  minio-setup:\n",
    "    image: minio/mc\n",
    "    depends_on:\n",
    "      - minio\n",
    "    entrypoint: >\n",
    "      /bin/sh -c \"\n",
    "      sleep 5;\n",
    "      mc alias set myminio http://minio:9000 minioadmin minioadmin;\n",
    "      mc mb myminio/lakehouse --ignore-existing;\n",
    "      exit 0;\n",
    "      \"\n",
    "\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  # SPARK\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  spark-master:\n",
    "    image: bitnami/spark:3.5\n",
    "    container_name: spark-master\n",
    "    environment:\n",
    "      - SPARK_MODE=master\n",
    "      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n",
    "      - SPARK_RPC_ENCRYPTION_ENABLED=no\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "      - \"7077:7077\"\n",
    "    volumes:\n",
    "      - ./spark_jobs:/opt/spark_jobs\n",
    "      - ./data:/opt/data\n",
    "\n",
    "  spark-worker:\n",
    "    image: bitnami/spark:3.5\n",
    "    container_name: spark-worker\n",
    "    depends_on:\n",
    "      - spark-master\n",
    "    environment:\n",
    "      - SPARK_MODE=worker\n",
    "      - SPARK_MASTER_URL=spark://spark-master:7077\n",
    "      - SPARK_WORKER_MEMORY=2G\n",
    "      - SPARK_WORKER_CORES=2\n",
    "    volumes:\n",
    "      - ./spark_jobs:/opt/spark_jobs\n",
    "      - ./data:/opt/data\n",
    "\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  # AIRFLOW\n",
    "  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  postgres:\n",
    "    image: postgres:15\n",
    "    container_name: postgres\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "\n",
    "  airflow-webserver:\n",
    "    image: apache/airflow:2.8.0\n",
    "    container_name: airflow-webserver\n",
    "    depends_on:\n",
    "      - postgres\n",
    "    environment:\n",
    "      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n",
    "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "      AIRFLOW__CORE__FERNET_KEY: ''\n",
    "      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n",
    "    ports:\n",
    "      - \"8082:8080\"\n",
    "    volumes:\n",
    "      - ./dags:/opt/airflow/dags\n",
    "      - ./dbt_olist:/opt/dbt\n",
    "    command: bash -c \"airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && airflow webserver\"\n",
    "\n",
    "  airflow-scheduler:\n",
    "    image: apache/airflow:2.8.0\n",
    "    container_name: airflow-scheduler\n",
    "    depends_on:\n",
    "      - airflow-webserver\n",
    "    environment:\n",
    "      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n",
    "      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "    volumes:\n",
    "      - ./dags:/opt/airflow/dags\n",
    "      - ./dbt_olist:/opt/dbt\n",
    "    command: airflow scheduler\n",
    "\n",
    "volumes:\n",
    "  minio_data:\n",
    "  postgres_data:\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### producers/orders_producer.py\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']\n",
    "TOPIC = 'raw_orders'\n",
    "CSV_PATH = 'data/olist_orders_dataset.csv'\n",
    "\n",
    "# Producteur Kafka\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    "    value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8'),\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None\n",
    ")\n",
    "\n",
    "# Lire le CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df)} orders\")\n",
    "\n",
    "# Envoyer les messages\n",
    "for idx, row in df.iterrows():\n",
    "    message = row.to_dict()\n",
    "    \n",
    "    # Timestamp d'ingestion\n",
    "    ingestion_ts = datetime.now()\n",
    "    \n",
    "    # Simuler late data (5%)\n",
    "    if random.random() < 0.05:\n",
    "        ingestion_ts -= timedelta(minutes=random.randint(1, 5))\n",
    "    \n",
    "    message['_ingestion_timestamp'] = ingestion_ts.isoformat()\n",
    "    \n",
    "    # Envoyer\n",
    "    producer.send(\n",
    "        topic=TOPIC,\n",
    "        key=message['order_id'],\n",
    "        value=message\n",
    "    )\n",
    "    \n",
    "    # Simuler doublons (2%)\n",
    "    if random.random() < 0.02:\n",
    "        producer.send(topic=TOPIC, key=message['order_id'], value=message)\n",
    "    \n",
    "    # Log progress\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Sent {idx}/{len(df)} messages\")\n",
    "    \n",
    "    # DÃ©lai alÃ©atoire (simuler streaming)\n",
    "    time.sleep(random.uniform(0.05, 0.2))\n",
    "\n",
    "producer.flush()\n",
    "print(f\"Done! Sent {len(df)} messages to {TOPIC}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### spark_jobs/bronze/ingest_orders.py\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, current_timestamp, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bronze - Ingest Orders\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"io.delta:delta-spark_2.12:3.1.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Schema des orders\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"order_status\", StringType()),\n",
    "    StructField(\"order_purchase_timestamp\", StringType()),\n",
    "    StructField(\"order_approved_at\", StringType()),\n",
    "    StructField(\"order_delivered_carrier_date\", StringType()),\n",
    "    StructField(\"order_delivered_customer_date\", StringType()),\n",
    "    StructField(\"order_estimated_delivery_date\", StringType()),\n",
    "    StructField(\"_ingestion_timestamp\", StringType())\n",
    "])\n",
    "\n",
    "# Lire depuis Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "    .option(\"subscribe\", \"raw_orders\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parser le JSON\n",
    "parsed_df = kafka_df \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(from_json(col(\"json_value\"), order_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"_bronze_ingested_at\", current_timestamp()) \\\n",
    "    .withColumn(\"_ingestion_date\", to_date(col(\"_ingestion_timestamp\")))\n",
    "\n",
    "# Ã‰crire en Bronze (Append)\n",
    "query = parsed_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://lakehouse/checkpoints/bronze_orders\") \\\n",
    "    .option(\"path\", \"s3a://lakehouse/bronze/orders\") \\\n",
    "    .partitionBy(\"_ingestion_date\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### spark_jobs/silver/silver_orders.py\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Spark Session (mÃªme config que Bronze)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Silver - Orders MERGE\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"io.delta:delta-spark_2.12:3.1.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "BRONZE_PATH = \"s3a://lakehouse/bronze/orders\"\n",
    "SILVER_PATH = \"s3a://lakehouse/silver/orders\"\n",
    "CHECKPOINT_PATH = \"s3a://lakehouse/checkpoints/silver_orders\"\n",
    "\n",
    "def upsert_to_silver(batch_df, batch_id):\n",
    "    \"\"\"Upsert vers Silver avec dÃ©duplication.\"\"\"\n",
    "    \n",
    "    if batch_df.count() == 0:\n",
    "        print(f\"Batch {batch_id}: No data\")\n",
    "        return\n",
    "    \n",
    "    # 1. DÃ©duplication (garder le plus rÃ©cent par order_id)\n",
    "    window = Window.partitionBy(\"order_id\").orderBy(col(\"_bronze_ingested_at\").desc())\n",
    "    deduped = batch_df \\\n",
    "        .withColumn(\"_row_num\", row_number().over(window)) \\\n",
    "        .filter(col(\"_row_num\") == 1) \\\n",
    "        .drop(\"_row_num\")\n",
    "    \n",
    "    # 2. Ajouter timestamp Silver\n",
    "    enriched = deduped.withColumn(\"_silver_updated_at\", current_timestamp())\n",
    "    \n",
    "    # 3. MERGE INTO\n",
    "    if DeltaTable.isDeltaTable(spark, SILVER_PATH):\n",
    "        delta_table = DeltaTable.forPath(spark, SILVER_PATH)\n",
    "        \n",
    "        delta_table.alias(\"target\").merge(\n",
    "            enriched.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"source._bronze_ingested_at > target._bronze_ingested_at\",\n",
    "            set={\n",
    "                \"order_status\": \"source.order_status\",\n",
    "                \"order_delivered_carrier_date\": \"source.order_delivered_carrier_date\",\n",
    "                \"order_delivered_customer_date\": \"source.order_delivered_customer_date\",\n",
    "                \"_bronze_ingested_at\": \"source._bronze_ingested_at\",\n",
    "                \"_silver_updated_at\": \"source._silver_updated_at\"\n",
    "            }\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        print(f\"Batch {batch_id}: MERGED {enriched.count()} records\")\n",
    "    else:\n",
    "        # PremiÃ¨re exÃ©cution\n",
    "        enriched.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n",
    "        print(f\"Batch {batch_id}: CREATED table with {enriched.count()} records\")\n",
    "\n",
    "# Lire Bronze en streaming\n",
    "bronze_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .load(BRONZE_PATH)\n",
    "\n",
    "# Ã‰crire avec foreachBatch\n",
    "query = bronze_stream.writeStream \\\n",
    "    .foreachBatch(upsert_to_silver) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### dbt_olist/models/gold/gold_daily_sales.sql\n",
    "\n",
    "```sql\n",
    "{{ config(\n",
    "    materialized='incremental',\n",
    "    unique_key='order_date',\n",
    "    incremental_strategy='merge'\n",
    ") }}\n",
    "\n",
    "WITH orders AS (\n",
    "    SELECT\n",
    "        o.order_id,\n",
    "        o.order_status,\n",
    "        DATE(o.order_purchase_timestamp) AS order_date,\n",
    "        oi.price,\n",
    "        oi.freight_value\n",
    "    FROM {{ ref('stg_orders') }} o\n",
    "    JOIN {{ ref('stg_order_items') }} oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    {% if is_incremental() %}\n",
    "    AND DATE(o.order_purchase_timestamp) >= (\n",
    "        SELECT MAX(order_date) - INTERVAL 2 DAY FROM {{ this }}\n",
    "    )\n",
    "    {% endif %}\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    order_date,\n",
    "    COUNT(DISTINCT order_id) AS total_orders,\n",
    "    SUM(price + freight_value) AS total_revenue,\n",
    "    AVG(price + freight_value) AS avg_order_value,\n",
    "    COUNT(*) AS total_items\n",
    "FROM orders\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### dbt_olist/models/gold/gold_customer_rfm.sql\n",
    "\n",
    "```sql\n",
    "{{ config(materialized='table') }}\n",
    "\n",
    "WITH customer_orders AS (\n",
    "    SELECT\n",
    "        c.customer_unique_id,\n",
    "        o.order_id,\n",
    "        o.order_purchase_timestamp,\n",
    "        oi.price + oi.freight_value AS order_value\n",
    "    FROM {{ ref('stg_customers') }} c\n",
    "    JOIN {{ ref('stg_orders') }} o ON c.customer_id = o.customer_id\n",
    "    JOIN {{ ref('stg_order_items') }} oi ON o.order_id = oi.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "),\n",
    "\n",
    "rfm_base AS (\n",
    "    SELECT\n",
    "        customer_unique_id,\n",
    "        DATEDIFF(day, MAX(order_purchase_timestamp), CURRENT_DATE) AS recency_days,\n",
    "        COUNT(DISTINCT order_id) AS frequency,\n",
    "        SUM(order_value) AS monetary\n",
    "    FROM customer_orders\n",
    "    GROUP BY customer_unique_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    customer_unique_id,\n",
    "    recency_days,\n",
    "    frequency,\n",
    "    ROUND(monetary, 2) AS monetary,\n",
    "    {{ rfm_segment('recency_days', 'frequency', 'monetary') }} AS rfm_segment\n",
    "FROM rfm_base\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### dbt_olist/models/gold/_gold__models.yml\n",
    "\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: gold_daily_sales\n",
    "    description: \"Chiffre d'affaires quotidien\"\n",
    "    columns:\n",
    "      - name: order_date\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: total_revenue\n",
    "        tests:\n",
    "          - not_null\n",
    "          - dbt_expectations.expect_column_values_to_be_between:\n",
    "              min_value: 0\n",
    "\n",
    "  - name: gold_customer_rfm\n",
    "    description: \"Segmentation RFM des clients\"\n",
    "    columns:\n",
    "      - name: customer_unique_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "      - name: rfm_segment\n",
    "        tests:\n",
    "          - accepted_values:\n",
    "              values: ['Champions', 'Loyal Customers', 'Potential Loyalists', 'At Risk', 'Hibernating', 'Others']\n",
    "\n",
    "  - name: gold_seller_performance\n",
    "    description: \"Performance des vendeurs\"\n",
    "    columns:\n",
    "      - name: seller_id\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "\n",
    "  - name: gold_product_analytics\n",
    "    description: \"Analyse des produits par catÃ©gorie\"\n",
    "    columns:\n",
    "      - name: product_category\n",
    "        tests:\n",
    "          - unique\n",
    "          - not_null\n",
    "\n",
    "  - name: gold_delivery_performance\n",
    "    description: \"Performance des livraisons\"\n",
    "    columns:\n",
    "      - name: on_time_rate\n",
    "        tests:\n",
    "          - dbt_expectations.expect_column_values_to_be_between:\n",
    "              min_value: 0\n",
    "              max_value: 1\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ FÃ©licitations !\n",
    "\n",
    "En complÃ©tant ce projet, tu as construit un **pipeline de donnÃ©es complet** utilisable en production.\n",
    "\n",
    "Tu maÃ®trises maintenant :\n",
    "- âœ… L'ingestion temps rÃ©el avec **Kafka**\n",
    "- âœ… Le traitement streaming avec **Spark Structured Streaming**\n",
    "- âœ… L'architecture **Lakehouse** avec **Delta Lake**\n",
    "- âœ… Le pattern **MERGE INTO** pour les upserts\n",
    "- âœ… La modÃ©lisation analytique avec **dbt**\n",
    "- âœ… La validation de qualitÃ© avec **Great Expectations**\n",
    "- âœ… L'orchestration avec **Airflow**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "- [Dataset Olist sur Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "- [Delta Lake Documentation](https://docs.delta.io/)\n",
    "- [dbt Documentation](https://docs.getdbt.com/)\n",
    "- [Great Expectations Documentation](https://docs.greatexpectations.io/)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Et Maintenant ?\n",
    "\n",
    "Ce projet constitue une **base solide** pour ton portfolio. Tu peux :\n",
    "\n",
    "1. **Le dÃ©ployer sur le cloud** (AWS, GCP, Azure)\n",
    "2. **Ajouter du monitoring** (Prometheus + Grafana)\n",
    "3. **IntÃ©grer du ML** (prÃ©diction de churn, recommandations)\n",
    "4. **Le prÃ©senter en entretien** comme preuve de tes compÃ©tences\n",
    "\n",
    "Bonne chance pour la suite de ton parcours Data Engineering ! ğŸ“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}