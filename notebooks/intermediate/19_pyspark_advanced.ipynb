{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# PySpark Advanced\n",
    "\n",
    "Bienvenue dans ce module avancÃ© oÃ¹ tu vas apprendre Ã  **optimiser Spark comme un expert**. Tu dÃ©couvriras l'architecture interne, les techniques d'optimisation, et comment diagnostiquer et rÃ©soudre les problÃ¨mes de performance.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Module 11 : PySpark for Data Engineering (bases Spark) |\n",
    "| âœ… Requis | Module 18 : High Performance Python |\n",
    "| ğŸ’¡ RecommandÃ© | ExpÃ©rience avec des datasets > 1 Go |\n",
    "\n",
    "## Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre l'**architecture interne** de Spark (Catalyst, Tungsten)\n",
    "- ExÃ©cuter Spark en **production** avec `spark-submit`\n",
    "- **Optimiser** les partitions, shuffles et joins\n",
    "- **Diagnostiquer** un job lent avec Spark UI\n",
    "- RÃ©duire le temps d'exÃ©cution de **80-90%**\n",
    "\n",
    "### Objectif concret\n",
    "\n",
    "> **Transformer un pipeline de 20 minutes en 2 minutes.**\n",
    "\n",
    "C'est ce qui distingue un Data Engineer junior d'un senior sur Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rappels",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Rappels Spark Essentiels\n",
    "\n",
    "> ğŸ’¡ **Si tu as suivi le module 11 (PySpark for Data Engineering)**, cette section est un rappel rapide.\n",
    "> \n",
    "> **Sinon**, commence par ce module avant de continuer â€” les concepts de base sont indispensables.\n",
    "\n",
    "### 1.1 SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_session",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# CrÃ©er une SparkSession (point d'entrÃ©e unique)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Advanced\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"App name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rappels_concepts",
   "metadata": {},
   "source": [
    "### 1.2 DataFrame vs RDD\n",
    "\n",
    "| Aspect | RDD | DataFrame |\n",
    "|--------|-----|----------|\n",
    "| **API** | Bas niveau | Haut niveau |\n",
    "| **Optimisation** | Manuelle | Catalyst (automatique) |\n",
    "| **Performance** | Baseline | 10-100x plus rapide |\n",
    "| **Usage** | Legacy, cas spÃ©ciaux | **Standard** |\n",
    "\n",
    "ğŸ‘‰ **RÃ¨gle : Toujours utiliser DataFrame/Dataset, jamais RDD** (sauf cas trÃ¨s spÃ©cifiques).\n",
    "\n",
    "### 1.3 Transformations vs Actions\n",
    "\n",
    "| Type | Exemples | ExÃ©cution |\n",
    "|------|----------|----------|\n",
    "| **Transformation** | `filter`, `select`, `join`, `groupBy` | Lazy (diffÃ©rÃ©e) |\n",
    "| **Action** | `count`, `collect`, `write`, `show` | ImmÃ©diate |\n",
    "\n",
    "### 1.4 Lazy Evaluation\n",
    "\n",
    "```text\n",
    "df.filter(...)     # Rien ne s'exÃ©cute\n",
    "  .select(...)     # Rien ne s'exÃ©cute\n",
    "  .groupBy(...)    # Rien ne s'exÃ©cute\n",
    "  .count()         # MAINTENANT tout s'exÃ©cute !\n",
    "```\n",
    "\n",
    "**Avantage** : Spark peut optimiser l'ensemble du pipeline avant exÃ©cution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Architecture Interne â€” Ce que les dÃ©butants ne savent pas\n",
    "\n",
    "> La vraie maÃ®trise de Spark commence ici. Comprendre l'architecture interne te permet de **prÃ©dire** et **rÃ©soudre** les problÃ¨mes de performance.\n",
    "\n",
    "### 2.1 Le cycle de vie d'un Job Spark\n",
    "\n",
    "```text\n",
    "Code Python/SQL\n",
    "      â”‚\n",
    "      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    Logical Plan     â”‚  Arbre d'opÃ©rations (ce que tu veux faire)\n",
    "â”‚    (non optimisÃ©)   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Catalyst       â”‚  ğŸ§  Optimiseur de requÃªtes\n",
    "â”‚      Optimizer      â”‚  - Predicate pushdown\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Projection pruning\n",
    "           â”‚             - Join reordering\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Physical Plan     â”‚  Comment exÃ©cuter (stratÃ©gie)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        DAG          â”‚  Directed Acyclic Graph\n",
    "â”‚   (Stages + Tasks)  â”‚  - Stages = unitÃ©s de travail\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Tasks = exÃ©cution par partition\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Tungsten       â”‚  âš¡ ExÃ©cution optimisÃ©e\n",
    "â”‚       Engine        â”‚  - Code generation\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Off-heap memory\n",
    "```\n",
    "\n",
    "### 2.2 Catalyst Optimizer â€” L'arme secrÃ¨te\n",
    "\n",
    "| Optimisation | Description | Gain |\n",
    "|--------------|-------------|------|\n",
    "| **Predicate pushdown** | Filtre appliquÃ© Ã  la source (Parquet, DB) | I/O rÃ©duit drastiquement |\n",
    "| **Projection pruning** | Colonnes inutiles non lues | I/O rÃ©duit |\n",
    "| **Constant folding** | Calculs constants prÃ©-calculÃ©s | CPU rÃ©duit |\n",
    "| **Join reordering** | Ordre optimal des joins | Shuffle rÃ©duit |\n",
    "\n",
    "### 2.3 Tungsten Engine\n",
    "\n",
    "- **Off-heap memory** : stockage hors JVM â†’ Ã©vite le Garbage Collector\n",
    "- **Whole-stage code generation** : gÃ©nÃ¨re du bytecode optimisÃ© Ã  la volÃ©e\n",
    "- **Vectorized execution** : traitement par batch (comme Polars !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain_plan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# CrÃ©er des donnÃ©es de test\n",
    "data = [(i, f\"cat_{i % 5}\", float(i * 10)) for i in range(1000)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\", \"amount\"])\n",
    "\n",
    "# Pipeline avec transformations\n",
    "result = (\n",
    "    df\n",
    "    .filter(col(\"amount\") > 100)\n",
    "    .select(\"category\", \"amount\")\n",
    "    .groupBy(\"category\")\n",
    "    .agg(spark_sum(\"amount\").alias(\"total\"))\n",
    ")\n",
    "\n",
    "# Voir le plan d'exÃ©cution\n",
    "print(\"=\" * 50)\n",
    "print(\"PLAN D'EXÃ‰CUTION (explain)\")\n",
    "print(\"=\" * 50)\n",
    "result.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain_extended",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan complet avec toutes les Ã©tapes\n",
    "print(\"PLAN COMPLET (Parsed â†’ Analyzed â†’ Optimized â†’ Physical)\")\n",
    "print(\"=\" * 60)\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark_submit",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. spark-submit â€” ExÃ©cuter Spark comme un pro\n",
    "\n",
    "> **CompÃ©tence indispensable en entreprise.** TrÃ¨s peu de formations l'enseignent correctement.\n",
    "\n",
    "### 3.1 Pourquoi spark-submit ?\n",
    "\n",
    "| Contexte | Outil | Usage |\n",
    "|----------|-------|-------|\n",
    "| Exploration, dÃ©veloppement | Notebooks (Jupyter, Databricks) | Dev, prototypage |\n",
    "| **Production, CI/CD, scheduling** | **spark-submit** | DÃ©ploiement rÃ©el |\n",
    "\n",
    "> ğŸ’¡ **En entreprise**, `spark-submit` est rarement lancÃ© manuellement. Il est appelÃ© par :\n",
    "> - **Airflow** (orchestration) â†’ Module 25\n",
    "> - **CI/CD pipelines** (GitLab CI, GitHub Actions)\n",
    "> - **Schedulers** (cron, Kubernetes CronJobs)\n",
    "\n",
    "### 3.2 Deploy Modes\n",
    "\n",
    "```text\n",
    "CLIENT MODE                          CLUSTER MODE\n",
    "â•â•â•â•â•â•â•â•â•â•â•                          â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Client    â”‚                      â”‚   Client    â”‚\n",
    "â”‚   Machine   â”‚                      â”‚   Machine   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚             â”‚\n",
    "â”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici       â”‚  (submit)   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n",
    "       â”‚                                    â”‚\n",
    "       â–¼                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Cluster   â”‚                      â”‚   Cluster   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚Exec 1 â”‚  â”‚                      â”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚Exec 2 â”‚  â”‚                      â”‚  â”‚Exec 1 â”‚  â”‚\n",
    "â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\n",
    "â”‚  â”‚Exec 3 â”‚  â”‚                      â”‚  â”‚Exec 2 â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Usage: Debug, interactif            Usage: Production\n",
    "```\n",
    "\n",
    "| Mode | Driver tourne sur | Usage |\n",
    "|------|-------------------|-------|\n",
    "| `client` | Machine qui soumet | Debug, logs visibles |\n",
    "| `cluster` | Worker du cluster | **Production** |\n",
    "\n",
    "### 3.3 Resource Managers (Masters)\n",
    "\n",
    "| Master | Commande | Usage |\n",
    "|--------|----------|-------|\n",
    "| Local | `--master local[*]` | Dev/test (tous les cores) |\n",
    "| Local (N cores) | `--master local[4]` | Dev/test (4 cores) |\n",
    "| Standalone | `--master spark://host:7077` | Cluster Spark simple |\n",
    "| YARN | `--master yarn` | Clusters Hadoop |\n",
    "| Kubernetes | `--master k8s://https://...` | Cloud native |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark_submit_syntax",
   "metadata": {},
   "source": [
    "### 3.4 Syntaxe complÃ¨te spark-submit\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "  # === Resource Manager ===\n",
    "  --master local[4] \\\n",
    "  --deploy-mode client \\\n",
    "  \n",
    "  # === Ressources ===\n",
    "  --driver-memory 4g \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 10 \\\n",
    "  \n",
    "  # === Configuration Spark ===\n",
    "  --conf spark.sql.shuffle.partitions=200 \\\n",
    "  --conf spark.sql.adaptive.enabled=true \\\n",
    "  --conf spark.executor.memoryOverhead=2g \\\n",
    "  \n",
    "  # === DÃ©pendances ===\n",
    "  --packages io.delta:delta-spark_2.12:3.2.0 \\\n",
    "  --jars /path/to/postgres-42.7.jar \\\n",
    "  --py-files utils.zip \\\n",
    "  \n",
    "  # === Application ===\n",
    "  main.py \\\n",
    "  \n",
    "  # === Arguments application ===\n",
    "  --date 2024-01-01 \\\n",
    "  --env prod\n",
    "```\n",
    "\n",
    "### 3.5 Structure projet production\n",
    "\n",
    "```text\n",
    "spark_project/\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e\n",
    "â”‚   â”œâ”€â”€ etl/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ extract.py\n",
    "â”‚   â”‚   â”œâ”€â”€ transform.py\n",
    "â”‚   â”‚   â””â”€â”€ load.py\n",
    "â”‚   â””â”€â”€ utils/\n",
    "â”‚       â”œâ”€â”€ __init__.py\n",
    "â”‚       â”œâ”€â”€ config.py\n",
    "â”‚       â””â”€â”€ logger.py\n",
    "â”œâ”€â”€ config/\n",
    "â”‚   â”œâ”€â”€ dev.yaml\n",
    "â”‚   â””â”€â”€ prod.yaml\n",
    "â”œâ”€â”€ jars/\n",
    "â”‚   â””â”€â”€ postgres-42.7.jar\n",
    "â”œâ”€â”€ scripts/\n",
    "â”‚   â”œâ”€â”€ run_local.sh\n",
    "â”‚   â””â”€â”€ run_cluster.sh\n",
    "â”œâ”€â”€ tests/\n",
    "â”‚   â””â”€â”€ test_transform.py\n",
    "â”œâ”€â”€ requirements.txt\n",
    "â”œâ”€â”€ setup.py                 # Pour crÃ©er .whl\n",
    "â””â”€â”€ README.md\n",
    "```\n",
    "\n",
    "### 3.6 Packaging pour production\n",
    "\n",
    "```bash\n",
    "# âŒ MAUVAIS : liste de fichiers (difficile Ã  maintenir)\n",
    "--py-files utils.py,config.py,helpers.py\n",
    "\n",
    "# âœ… MIEUX : Package .zip\n",
    "cd src/ && zip -r ../app.zip . && cd ..\n",
    "spark-submit --py-files app.zip main.py\n",
    "\n",
    "# âœ… MEILLEUR : Wheel (.whl) - le plus propre\n",
    "pip wheel . -w dist/\n",
    "spark-submit --py-files dist/myproject-1.0.0-py3-none-any.whl main.py\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **En production**, prÃ©fÃ¨re `.zip` ou `.whl` pour un dÃ©ploiement propre et versionnÃ©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "argparse_pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern : arguments en ligne de commande (main.py)\n",
    "\n",
    "# === Exemple de main.py pour spark-submit ===\n",
    "example_main = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Point d'entrÃ©e du job Spark.\"\"\"\n",
    "\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"ETL Pipeline\")\n",
    "    parser.add_argument(\"--date\", required=True, help=\"Date de traitement (YYYY-MM-DD)\")\n",
    "    parser.add_argument(\"--env\", default=\"dev\", choices=[\"dev\", \"prod\"])\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Chemin input\")\n",
    "    parser.add_argument(\"--output\", required=True, help=\"Chemin output\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(f\"ETL-{args.env}-{args.date}\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Charger les donnÃ©es\n",
    "    df = spark.read.parquet(f\"{args.input}/date={args.date}\")\n",
    "    \n",
    "    # Transformations...\n",
    "    result = df.filter(df.amount > 0)\n",
    "    \n",
    "    # Ã‰crire\n",
    "    result.write.mode(\"overwrite\").parquet(f\"{args.output}/date={args.date}\")\n",
    "    \n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "print(example_main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partitioning",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Partitionnement & Shuffle â€” LA source de lenteur\n",
    "\n",
    "> **80% des problÃ¨mes de performance Spark viennent du shuffle et du partitionnement.**\n",
    "\n",
    "### 4.1 Qu'est-ce qu'un Shuffle ?\n",
    "\n",
    "Le shuffle est la **redistribution des donnÃ©es entre partitions**. Il est nÃ©cessaire pour :\n",
    "\n",
    "- `groupBy`, `reduceByKey`\n",
    "- `join` (sauf broadcast)\n",
    "- `distinct`, `repartition`\n",
    "- `orderBy` (tri global)\n",
    "\n",
    "```text\n",
    "SHUFFLE = GOULOT D'Ã‰TRANGLEMENT\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Stage 1                              Stage 2\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Partitionâ”‚                          â”‚Partitionâ”‚\n",
    "â”‚    1    â”‚â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¶â”‚   1'    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\n",
    "â”‚    2    â”‚       â”‚  SHUFFLE â”‚       â”‚   2'    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ (rÃ©seau) â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\n",
    "â”‚    3    â”‚                          â”‚   3'    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "CoÃ»t du Shuffle :\n",
    "1. SÃ©rialisation des donnÃ©es\n",
    "2. Ã‰criture sur disque (shuffle write)\n",
    "3. Transfert rÃ©seau\n",
    "4. Lecture depuis disque (shuffle read)\n",
    "5. DÃ©sÃ©rialisation\n",
    "```\n",
    "\n",
    "### 4.2 repartition vs coalesce\n",
    "\n",
    "| MÃ©thode | Shuffle | Usage |\n",
    "|---------|---------|-------|\n",
    "| `repartition(n)` | âœ… **Toujours** | Augmenter partitions, rÃ©Ã©quilibrer |\n",
    "| `coalesce(n)` | âŒ Non (si rÃ©duction) | RÃ©duire partitions avant write |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partitions_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "# CrÃ©er un DataFrame\n",
    "df = spark.range(0, 1000000)\n",
    "print(f\"Partitions initiales : {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition = SHUFFLE (redistribue les donnÃ©es)\n",
    "df_repart = df.repartition(10)\n",
    "print(f\"AprÃ¨s repartition(10) : {df_repart.rdd.getNumPartitions()}\")\n",
    "\n",
    "# coalesce = PAS DE SHUFFLE (combine les partitions existantes)\n",
    "df_coal = df_repart.coalesce(3)\n",
    "print(f\"AprÃ¨s coalesce(3) : {df_coal.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Voir la distribution des donnÃ©es par partition\n",
    "print(\"\\nDistribution aprÃ¨s repartition(10):\")\n",
    "df_repart.groupBy(spark_partition_id().alias(\"partition\")).count().orderBy(\"partition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partition_size",
   "metadata": {},
   "source": [
    "### 4.3 Taille optimale des partitions\n",
    "\n",
    "| Taille partition | ProblÃ¨me |\n",
    "|------------------|----------|\n",
    "| **Trop petit** (< 10 MB) | Overhead de scheduling, trop de tÃ¢ches |\n",
    "| **Optimal** (128-256 MB) | âœ… Sweet spot |\n",
    "| **Trop grand** (> 1 GB) | OOM, mauvaise parallÃ©lisation |\n",
    "\n",
    "**Formule :**\n",
    "```\n",
    "num_partitions = data_size_mb / target_partition_size_mb\n",
    "\n",
    "Exemple : 50 GB de donnÃ©es\n",
    "num_partitions = 50000 MB / 200 MB = 250 partitions\n",
    "```\n",
    "\n",
    "### 4.4 Data Skew â€” Le tueur de performance\n",
    "\n",
    "```text\n",
    "Ã‰QUILIBRÃ‰ (bon)                     SKEWED (problÃ¨me !)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚            â”‚100Kâ”‚â”‚100Kâ”‚â”‚100Kâ”‚â”‚        10M        â”‚\n",
    "â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Temps: â–ˆâ–ˆâ–ˆâ–ˆ                         Temps: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "       4 tÃ¢ches parallÃ¨les                 â†‘\n",
    "       = temps minimal                     Un seul executor bloquÃ© !\n",
    "                                           Les autres attendent...\n",
    "```\n",
    "\n",
    "**Techniques anti-skew :**\n",
    "\n",
    "1. **Broadcast join** : si une table est petite (< 100 MB)\n",
    "2. **Salting** : ajouter une clÃ© alÃ©atoire pour distribuer\n",
    "3. **AQE** (Adaptive Query Execution) : Spark 3.0+ gÃ¨re automatiquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqe_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activer AQE (recommandÃ© Spark 3.0+)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# VÃ©rifier la configuration\n",
    "print(\"AQE enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"Skew Join enabled:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caching",
   "metadata": {},
   "source": [
    "### 4.5 Caching & Persistence\n",
    "\n",
    "**Quand utiliser le cache ?**\n",
    "\n",
    "- DataFrame rÃ©utilisÃ© dans **plusieurs actions**\n",
    "- Calcul **coÃ»teux** (join, aggregation) rÃ©utilisÃ©\n",
    "- **ItÃ©rations** (ML training)\n",
    "\n",
    "**cache() vs persist()**\n",
    "\n",
    "```python\n",
    "# cache() = persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df.cache()\n",
    "\n",
    "# persist() = contrÃ´le fin du niveau de stockage\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "```\n",
    "\n",
    "| Niveau | RAM | Disque | SÃ©rialisÃ© | Usage |\n",
    "|--------|-----|--------|-----------|-------|\n",
    "| `MEMORY_ONLY` | âœ… | âŒ | âŒ | Petit DF, RAM suffisante |\n",
    "| `MEMORY_AND_DISK` | âœ… | âœ… | âŒ | **DÃ©faut (cache())** |\n",
    "| `MEMORY_ONLY_SER` | âœ… | âŒ | âœ… | Ã‰conomie RAM |\n",
    "| `DISK_ONLY` | âŒ | âœ… | âœ… | TrÃ¨s gros DF |\n",
    "| `OFF_HEAP` | âœ… | âŒ | âœ… | Ã‰viter GC Java |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cache_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "# CrÃ©er un DataFrame avec calculs\n",
    "df = spark.range(0, 5000000).withColumn(\"squared\", col(\"id\") ** 2)\n",
    "\n",
    "# Sans cache : chaque action recalcule tout\n",
    "start = time.time()\n",
    "count1 = df.filter(col(\"squared\") > 1000000).count()\n",
    "count2 = df.filter(col(\"squared\") < 100).count()\n",
    "print(f\"Sans cache : {time.time() - start:.2f}s\")\n",
    "\n",
    "# Avec cache : calcul une seule fois\n",
    "df_cached = df.cache()\n",
    "\n",
    "start = time.time()\n",
    "count1 = df_cached.filter(col(\"squared\") > 1000000).count()\n",
    "count2 = df_cached.filter(col(\"squared\") < 100).count()\n",
    "print(f\"Avec cache : {time.time() - start:.2f}s\")\n",
    "\n",
    "# âš ï¸ IMPORTANT : libÃ©rer la mÃ©moire !\n",
    "df_cached.unpersist()\n",
    "print(\"\\nâœ… Cache libÃ©rÃ© avec unpersist()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cache_warnings",
   "metadata": {},
   "source": [
    "**âš ï¸ Quand NE PAS cacher :**\n",
    "\n",
    "| Situation | Pourquoi |\n",
    "|-----------|----------|\n",
    "| DF utilisÃ© une seule fois | Gaspillage mÃ©moire |\n",
    "| DF trÃ¨s volumineux (> RAM) | DÃ©bordement disque lent |\n",
    "| Avant un shuffle | Inutile, donnÃ©es redistribuÃ©es |\n",
    "| Pipeline simple et rapide | Overhead du cache > gain |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joins",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Joins AvancÃ©s â€” La compÃ©tence qui change tout\n",
    "\n",
    "> Un excellent Data Engineer sait **optimiser ses joins**. C'est souvent lÃ  que se gagne (ou se perd) le plus de temps.\n",
    "\n",
    "### 5.1 Types de Joins internes Spark\n",
    "\n",
    "| Type | Quand Spark l'utilise | Performance |\n",
    "|------|----------------------|-------------|\n",
    "| **Broadcast Hash Join** | Petite table (< seuil) | â­â­â­ **Meilleur** |\n",
    "| **Sort Merge Join** | Grandes tables | â­â­ Standard |\n",
    "| **Shuffle Hash Join** | Tables moyennes | â­ Ã‰viter si possible |\n",
    "\n",
    "### 5.2 Broadcast Join â€” Ton meilleur ami\n",
    "\n",
    "```text\n",
    "SORT MERGE JOIN (shuffle)           BROADCAST JOIN (pas de shuffle)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "   Big Table      Small Table          Big Table      Small Table\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚  P1  â”‚       â”‚  P1  â”‚             â”‚  P1  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n",
    "   â”‚  P2  â”‚       â”‚  P2  â”‚             â”‚  P2  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ COPY â”‚\n",
    "   â”‚  P3  â”‚       â”‚  P3  â”‚             â”‚  P3  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n",
    "   â””â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”¬â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚   SHUFFLE    â”‚                    â”‚\n",
    "      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               Broadcast to\n",
    "             â”‚                       all executors\n",
    "         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”                   (pas de shuffle !)\n",
    "         â”‚ JOIN  â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadcast_join",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Grande table (10M lignes simulÃ©es)\n",
    "big_df = spark.range(0, 1000000).withColumn(\"category_id\", (col(\"id\") % 100).cast(\"int\"))\n",
    "\n",
    "# Petite table (100 lignes)\n",
    "small_df = spark.createDataFrame(\n",
    "    [(i, f\"Category {i}\") for i in range(100)],\n",
    "    [\"category_id\", \"category_name\"]\n",
    ")\n",
    "\n",
    "# âŒ SANS broadcast hint (Spark peut ou non broadcaster)\n",
    "result_no_hint = big_df.join(small_df, \"category_id\")\n",
    "print(\"Sans broadcast hint:\")\n",
    "result_no_hint.explain()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# âœ… AVEC broadcast hint (force le broadcast)\n",
    "result_broadcast = big_df.join(broadcast(small_df), \"category_id\")\n",
    "print(\"Avec broadcast():\")\n",
    "result_broadcast.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadcast_config",
   "metadata": {},
   "source": [
    "### 5.3 Configurer le seuil de broadcast\n",
    "\n",
    "```python\n",
    "# DÃ©faut : 10 MB\n",
    "# Si une table < 10 MB â†’ broadcast automatique\n",
    "\n",
    "# Augmenter pour broadcaster des tables plus grandes\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)  # 100 MB\n",
    "\n",
    "# DÃ©sactiver le broadcast automatique (forcer shuffle)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "```\n",
    "\n",
    "### 5.4 Join Hints (Spark 3.0+)\n",
    "\n",
    "```python\n",
    "# DataFrame API\n",
    "big_df.join(small_df.hint(\"broadcast\"), \"key\")\n",
    "\n",
    "# SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(small_table) */ *\n",
    "    FROM big_table\n",
    "    JOIN small_table ON big_table.key = small_table.key\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### 5.5 Anti-pattern : Join sur clÃ© skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skew_join",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration pour gÃ©rer le skew automatiquement (AQE)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
    "\n",
    "print(\"AQE Skew Join activÃ©\")\n",
    "print(\"Spark va automatiquement dÃ©tecter et gÃ©rer les partitions skewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "io",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Lecture/Ã‰criture OptimisÃ©es\n",
    "\n",
    "### 6.1 Formats : Parquet toujours !\n",
    "\n",
    "| Format | Lecture | Ã‰criture | Compression | Predicate Pushdown |\n",
    "|--------|---------|----------|-------------|-------------------|\n",
    "| CSV | ğŸ¢ Lent | ğŸ¢ Lent | âŒ | âŒ |\n",
    "| JSON | ğŸ¢ Lent | ğŸ¢ Lent | âŒ | âŒ |\n",
    "| **Parquet** | ğŸš€ Rapide | ğŸš€ Rapide | âœ… | âœ… |\n",
    "| ORC | ğŸš€ Rapide | ğŸš€ Rapide | âœ… | âœ… |\n",
    "| **Delta** | ğŸš€ Rapide | ğŸš€ Rapide | âœ… | âœ… + ACID |\n",
    "\n",
    "> ğŸ”­ **Les formats modernes** (Delta, Iceberg, Hudi) seront approfondis dans le **module 21 (Lakehouse)** :\n",
    "\n",
    "> - Transactions ACID\n",
    "> - Time Travel\n",
    "> - Vacuum, Compaction\n",
    "> - Z-Ordering\n",
    "\n",
    "### 6.2 Predicate Pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "predicate_pushdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# CrÃ©er des donnÃ©es de test\n",
    "test_data = spark.range(0, 100000).withColumn(\"category\", (col(\"id\") % 10).cast(\"string\"))\n",
    "\n",
    "# Sauvegarder en Parquet\n",
    "output_path = \"/tmp/test_parquet\"\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "test_data.write.parquet(output_path)\n",
    "\n",
    "# Lecture avec filtre â†’ predicate pushdown\n",
    "df_filtered = spark.read.parquet(output_path).filter(col(\"id\") < 100)\n",
    "\n",
    "print(\"Plan d'exÃ©cution avec Predicate Pushdown:\")\n",
    "df_filtered.explain()\n",
    "# Observe \"PushedFilters\" dans le plan !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partitioning_disk",
   "metadata": {},
   "source": [
    "### 6.3 Partitionnement sur disque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partition_by",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth, lit\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# CrÃ©er des donnÃ©es avec dates\n",
    "dates = [(datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90))).strftime(\"%Y-%m-%d\") \n",
    "         for _ in range(10000)]\n",
    "data = [(i, dates[i % len(dates)], float(random.randint(10, 1000))) \n",
    "        for i in range(10000)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"date\", \"amount\"])\n",
    "df = df.withColumn(\"date\", col(\"date\").cast(\"date\"))\n",
    "df = df.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n",
    "\n",
    "# Ã‰criture partitionnÃ©e\n",
    "output_partitioned = \"/tmp/partitioned_data\"\n",
    "if os.path.exists(output_partitioned):\n",
    "    shutil.rmtree(output_partitioned)\n",
    "\n",
    "df.write.partitionBy(\"year\", \"month\").parquet(output_partitioned)\n",
    "\n",
    "print(\"Structure crÃ©Ã©e:\")\n",
    "for root, dirs, files in os.walk(output_partitioned):\n",
    "    level = root.replace(output_partitioned, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    if level < 2:  # Limiter la profondeur\n",
    "        for d in sorted(dirs)[:3]:\n",
    "            print(f\"{indent}  {d}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema_explicit",
   "metadata": {},
   "source": [
    "### 6.4 SchÃ©mas explicites\n",
    "\n",
    "**Pourquoi dÃ©finir un schÃ©ma ?**\n",
    "\n",
    "- Ã‰vite l'**infÃ©rence** (coÃ»teuse sur gros fichiers)\n",
    "- **Garantit** les types attendus\n",
    "- DÃ©tecte les erreurs **tÃ´t**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DecimalType, DateType\n",
    "\n",
    "# DÃ©finir un schÃ©ma explicite\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), nullable=False),\n",
    "    StructField(\"customer_name\", StringType(), nullable=True),\n",
    "    StructField(\"amount\", DecimalType(10, 2), nullable=True),  # PrÃ©cision pour montants !\n",
    "    StructField(\"transaction_date\", DateType(), nullable=True)\n",
    "])\n",
    "\n",
    "print(\"SchÃ©ma dÃ©fini:\")\n",
    "print(schema.simpleString())\n",
    "\n",
    "# Utilisation\n",
    "# df = spark.read.schema(schema).parquet(\"data/transactions/\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Conseil : Utiliser DecimalType pour les montants financiers\")\n",
    "print(\"   Ã‰vite les erreurs d'arrondi des float/double\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "udfs",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. UDFs : Le piÃ¨ge de performance\n",
    "\n",
    "### 7.1 Pourquoi les Python UDFs sont toxiques\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                 â”‚   SÃ©rialisation    â”‚                 â”‚\n",
    "â”‚       JVM       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚     Python      â”‚\n",
    "â”‚     (Spark)     â”‚                    â”‚   (UDF lente)   â”‚\n",
    "â”‚                 â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   DÃ©sÃ©rialisation  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                            â”‚\n",
    "                            â”‚\n",
    "                    TRÃˆS COÃ›TEUX !\n",
    "                    (par ligne)\n",
    "```\n",
    "\n",
    "### 7.2 HiÃ©rarchie de performance\n",
    "\n",
    "| Type | Performance | Quand l'utiliser |\n",
    "|------|-------------|------------------|\n",
    "| **Expressions Spark natives** | â­â­â­â­â­ | **Toujours si possible** |\n",
    "| **Pandas UDF (vectorized)** | â­â­â­ | Si besoin Python |\n",
    "| **Python UDF** | â­ | Dernier recours |\n",
    "\n",
    "### 7.3 Remplacer UDF par expressions natives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udf_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, when, col\n",
    "from pyspark.sql.types import StringType\n",
    "import time\n",
    "\n",
    "# CrÃ©er des donnÃ©es de test\n",
    "df = spark.range(0, 500000).withColumn(\"amount\", (col(\"id\") % 2000).cast(\"double\"))\n",
    "\n",
    "# âŒ MAUVAIS : Python UDF\n",
    "@udf(StringType())\n",
    "def categorize_udf(amount):\n",
    "    if amount > 1000:\n",
    "        return \"high\"\n",
    "    elif amount > 100:\n",
    "        return \"medium\"\n",
    "    return \"low\"\n",
    "\n",
    "start = time.time()\n",
    "result_udf = df.withColumn(\"category\", categorize_udf(col(\"amount\")))\n",
    "result_udf.write.mode(\"overwrite\").format(\"noop\").save()  # Force l'exÃ©cution\n",
    "udf_time = time.time() - start\n",
    "print(f\"âŒ Python UDF : {udf_time:.2f}s\")\n",
    "\n",
    "# âœ… BON : Expression Spark native\n",
    "start = time.time()\n",
    "result_native = df.withColumn(\"category\",\n",
    "    when(col(\"amount\") > 1000, \"high\")\n",
    "    .when(col(\"amount\") > 100, \"medium\")\n",
    "    .otherwise(\"low\")\n",
    ")\n",
    "result_native.write.mode(\"overwrite\").format(\"noop\").save()\n",
    "native_time = time.time() - start\n",
    "print(f\"âœ… Expression native : {native_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speedup : {udf_time/native_time:.1f}x plus rapide avec expression native !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pandas_udf",
   "metadata": {},
   "source": [
    "### 7.4 Pandas UDF (si Python nÃ©cessaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pandas_udf_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pandas UDF = vectorisÃ© (traite des Series, pas des scalaires)\n",
    "@pandas_udf(\"double\")\n",
    "def log_transform(s: pd.Series) -> pd.Series:\n",
    "    return np.log1p(s)\n",
    "\n",
    "# Test\n",
    "df = spark.range(0, 100000).withColumn(\"value\", col(\"id\").cast(\"double\"))\n",
    "\n",
    "start = time.time()\n",
    "result = df.withColumn(\"log_value\", log_transform(col(\"value\")))\n",
    "result.write.mode(\"overwrite\").format(\"noop\").save()\n",
    "print(f\"Pandas UDF : {time.time() - start:.2f}s\")\n",
    "\n",
    "result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_tuning",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Configuration & Tuning\n",
    "\n",
    "### 8.1 ParamÃ¨tres essentiels\n",
    "\n",
    "| ParamÃ¨tre | DÃ©faut | Recommandation |\n",
    "|-----------|--------|----------------|\n",
    "| `spark.sql.shuffle.partitions` | 200 | Adapter Ã  la taille des donnÃ©es |\n",
    "| `spark.default.parallelism` | Selon cluster | 2-3x num_cores |\n",
    "| `spark.sql.autoBroadcastJoinThreshold` | 10MB | 50-100MB |\n",
    "| `spark.executor.memory` | 1g | 4-16g selon cluster |\n",
    "| `spark.driver.memory` | 1g | 2-8g |\n",
    "| `spark.executor.memoryOverhead` | 10% | 15-20% pour PySpark |\n",
    "\n",
    "### 8.2 Dimensionnement : Executors vs Cores\n",
    "\n",
    "```text\n",
    "âŒ MAUVAIS : 2 executors Ã— 10 cores chacun\n",
    "   - GC Java doit gÃ©rer Ã©norme heap (~50 GB)\n",
    "   - Si 1 executor crash â†’ 50% de perte\n",
    "   - ParallÃ©lisme moins granulaire\n",
    "\n",
    "âœ… BON : 10 executors Ã— 4 cores chacun  \n",
    "   - GC plus efficace (heap ~10 GB)\n",
    "   - Meilleure isolation des erreurs\n",
    "   - ParallÃ©lisme plus granulaire\n",
    "```\n",
    "\n",
    "**RÃ¨gles de dimensionnement :**\n",
    "\n",
    "```text\n",
    "executor_cores = 4-5 max (sweet spot pour GC)\n",
    "executor_memory = 4-16g (selon donnÃ©es)\n",
    "num_executors = (total_cores / executor_cores) - 1\n",
    "\n",
    "# RÃ©server pour le Driver et l'OS\n",
    "driver_memory = 2-8g\n",
    "memoryOverhead = 15-20% pour PySpark (sÃ©rialisation Python)\n",
    "```\n",
    "\n",
    "**Exemple concret** (cluster 100 cores, 400 GB RAM) :\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --executor-cores 4 \\\n",
    "  --executor-memory 12g \\\n",
    "  --num-executors 20 \\\n",
    "  --driver-memory 4g \\\n",
    "  --conf spark.executor.memoryOverhead=2g \\\n",
    "  main.py\n",
    "```\n",
    "\n",
    "### 8.3 Adaptive Query Execution (AQE)\n",
    "\n",
    "L'AQE (Spark 3.0+) rend certaines optimisations manuelles **obsolÃ¨tes** :\n",
    "\n",
    "| Avant AQE (manuel) | Avec AQE (automatique) |\n",
    "|--------------------|------------------------|\n",
    "| `coalesce(n)` aprÃ¨s filter | Auto-coalesce des partitions |\n",
    "| Calculer shuffle.partitions | Auto-optimize partitions |\n",
    "| DÃ©tecter skew manuellement | Auto-skew handling |\n",
    "| Broadcast threshold fixe | Runtime broadcast decisions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aqe_full_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration AQE complÃ¨te (recommandÃ©e)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
    "\n",
    "# Avec AQE, tu peux laisser shuffle.partitions Ã©levÃ©\n",
    "# Spark optimisera automatiquement\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n",
    "\n",
    "print(\"âœ… Configuration AQE optimale :\")\n",
    "print(f\"   adaptive.enabled = {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"   coalescePartitions = {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\n",
    "print(f\"   skewJoin = {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\n",
    "print(f\"   shuffle.partitions = {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "print(\"\\nğŸ’¡ Avec AQE, Spark ajuste automatiquement le nombre de partitions !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spark_ui",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Spark UI & Diagnostic\n",
    "\n",
    "> **Savoir lire Spark UI = savoir debugger.** C'est la compÃ©tence qui fait la diffÃ©rence.\n",
    "\n",
    "### 9.1 AccÃ©der Ã  Spark UI\n",
    "\n",
    "- **Local** : http://localhost:4040\n",
    "- **Cluster** : via le Resource Manager (YARN, K8s dashboard)\n",
    "- **Databricks** : intÃ©grÃ© dans l'interface\n",
    "\n",
    "### 9.2 Les onglets importants\n",
    "\n",
    "| Onglet | Information |\n",
    "|--------|-------------|\n",
    "| **Jobs** | Vue d'ensemble des jobs, durÃ©e |\n",
    "| **Stages** | DÃ©tail des stages, shuffle read/write |\n",
    "| **Storage** | DataFrames en cache |\n",
    "| **Environment** | Configuration Spark |\n",
    "| **Executors** | Ressources, GC, mÃ©moire |\n",
    "| **SQL** | Plans d'exÃ©cution des requÃªtes |\n",
    "\n",
    "### 9.3 MÃ©triques Ã  surveiller\n",
    "\n",
    "| MÃ©trique | Signification | ğŸš¨ ProblÃ¨me si... |\n",
    "|----------|---------------|-------------------|\n",
    "| **Shuffle Read/Write** | DonnÃ©es Ã©changÃ©es | TrÃ¨s Ã©levÃ© (> 10 GB) |\n",
    "| **Spill (Memory/Disk)** | DÃ©bordement mÃ©moire | > 0 |\n",
    "| **Task Duration** | Temps par tÃ¢che | TrÃ¨s variable (skew !) |\n",
    "| **GC Time** | Garbage Collection | > 10% du temps total |\n",
    "| **Input/Output** | DonnÃ©es lues/Ã©crites | Beaucoup plus que prÃ©vu |\n",
    "\n",
    "### 9.4 Patterns de problÃ¨mes\n",
    "\n",
    "| SymptÃ´me dans Spark UI | Diagnostic | Solution |\n",
    "|------------------------|------------|----------|\n",
    "| 1 tÃ¢che 10x plus longue | **Data skew** | Salting, broadcast, AQE |\n",
    "| Shuffle > 50 GB | Join non optimisÃ© | Broadcast join |\n",
    "| Spill to disk | MÃ©moire insuffisante | Plus de RAM, moins de partitions |\n",
    "| GC Time > 20% | Trop d'objets Java | Tungsten, plus de memoryOverhead |\n",
    "| Beaucoup de petites tÃ¢ches | Trop de partitions | Coalesce, AQE |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_ui_info",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL de Spark UI pour cette session\n",
    "print(f\"ğŸ” Spark UI disponible sur : {spark.sparkContext.uiWebUrl}\")\n",
    "print(\"\\nğŸ“Š Pour voir les mÃ©triques d'un job, lance une action puis consulte l'UI\")\n",
    "\n",
    "# Exemple : dÃ©clencher un job pour voir dans l'UI\n",
    "df = spark.range(0, 1000000)\n",
    "result = df.groupBy((col(\"id\") % 100).alias(\"group\")).count()\n",
    "result.collect()  # Action qui dÃ©clenche le job\n",
    "\n",
    "print(\"\\nâœ… Job exÃ©cutÃ© - consulte Spark UI pour voir les stages et mÃ©triques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Bonnes pratiques & Anti-patterns\n",
    "\n",
    "### âŒ Anti-patterns (Ã  Ã©viter absolument)\n",
    "\n",
    "| Anti-pattern | Pourquoi c'est mal | Solution |\n",
    "|--------------|-------------------|----------|\n",
    "| `collect()` sur 100M lignes | OOM Driver garanti | `write()` vers fichier |\n",
    "| CSV en production | Lent, pas de schema | **Parquet/Delta** |\n",
    "| UDF Python partout | 10-100x plus lent | Expressions natives |\n",
    "| `shuffle.partitions=200` toujours | Pas adaptÃ© aux donnÃ©es | Ajuster ou AQE |\n",
    "| Pas de `cache()` sur DF rÃ©utilisÃ© | Recalcul inutile | `cache()` + `unpersist()` |\n",
    "| Ignorer Spark UI | Debug Ã  l'aveugle | **Toujours vÃ©rifier** |\n",
    "| Join sans broadcast | Shuffle Ã©norme | `broadcast()` sur petites tables |\n",
    "| `repartition()` avant `write()` | Shuffle inutile | `coalesce()` pour rÃ©duire |\n",
    "\n",
    "### âœ… Bonnes pratiques\n",
    "\n",
    "| Pratique | BÃ©nÃ©fice |\n",
    "|----------|----------|\n",
    "| **Toujours Parquet** | I/O optimisÃ©, predicate pushdown |\n",
    "| **Broadcast petites tables** | Ã‰vite shuffle |\n",
    "| **AQE activÃ©** | Optimisation runtime |\n",
    "| **Partitionner par date** | Partition pruning |\n",
    "| **Ã‰viter UDFs** | Performance native |\n",
    "| **Monitorer Spark UI** | Debug efficace |\n",
    "| **Cache + unpersist** | Ã‰vite recalculs |\n",
    "| **Schema explicite** | Ã‰vite infÃ©rence |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini-Projet : Optimisation d'un pipeline\n",
    "\n",
    "### Objectif\n",
    "RÃ©duire un pipeline de **20 minutes Ã  < 3 minutes** en appliquant les techniques apprises.\n",
    "\n",
    "### ScÃ©nario : E-commerce Analytics\n",
    "\n",
    "| Table | Lignes | Format initial |\n",
    "|-------|--------|----------------|\n",
    "| Transactions | 5M | Parquet |\n",
    "| Produits | 10K | CSV |\n",
    "| Clients | 500K | Parquet |\n",
    "\n",
    "### Architecture cible\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Transactions  â”‚    â”‚    Produits    â”‚    â”‚    Clients     â”‚\n",
    "â”‚    (5M rows)   â”‚    â”‚   (10K rows)   â”‚    â”‚   (500K rows)  â”‚\n",
    "â”‚   [Parquet]    â”‚    â”‚ [CSVâ†’Parquet]  â”‚    â”‚   [Parquet]    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                     â”‚ broadcast           â”‚\n",
    "        â”‚                     â–¼                     â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â”‚\n",
    "                              â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚      Join + Aggregation       â”‚\n",
    "                 â”‚   - Broadcast products        â”‚\n",
    "                 â”‚   - AQE enabled               â”‚\n",
    "                 â”‚   - Native expressions        â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                 â”‚\n",
    "                                 â–¼\n",
    "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                 â”‚      Partitioned Output       â”‚\n",
    "                 â”‚   partitionBy(\"year\",\"month\") â”‚\n",
    "                 â”‚         [Parquet]             â”‚\n",
    "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Setup : crÃ©er les donnÃ©es de test\n",
    "print(\"ğŸ“¦ CrÃ©ation des donnÃ©es de test...\")\n",
    "\n",
    "# Configuration optimale\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "\n",
    "# Nettoyer les rÃ©pertoires\n",
    "for path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \"/tmp/output\"]:\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "\n",
    "# 1. Transactions (5M lignes)\n",
    "transactions = spark.range(0, 500000).select(\n",
    "    col(\"id\").alias(\"transaction_id\"),\n",
    "    (col(\"id\") % 10000).alias(\"product_id\"),\n",
    "    (col(\"id\") % 50000).alias(\"customer_id\"),\n",
    "    (rand() * 1000).alias(\"amount\"),\n",
    "    date_add(lit(\"2024-01-01\"), (rand() * 90).cast(\"int\")).alias(\"date\")\n",
    ")\n",
    "transactions.write.parquet(\"/tmp/transactions\")\n",
    "print(f\"Transactions : {transactions.count()} lignes\")\n",
    "\n",
    "# 2. Produits (10K lignes) - CSV intentionnellement\n",
    "products_data = [(i, f\"Product_{i}\", f\"Category_{i % 50}\", float(10 + i % 100)) \n",
    "                 for i in range(10000)]\n",
    "products = spark.createDataFrame(products_data, \n",
    "    [\"product_id\", \"product_name\", \"category\", \"base_price\"])\n",
    "products.write.mode(\"overwrite\").option(\"header\", True).csv(\"/tmp/products\")\n",
    "print(f\"Produits : {products.count()} lignes (CSV)\")\n",
    "\n",
    "# 3. Clients (500K lignes)\n",
    "customers = spark.range(0, 50000).select(\n",
    "    col(\"id\").alias(\"customer_id\"),\n",
    "    concat(lit(\"Customer_\"), col(\"id\")).alias(\"customer_name\"),\n",
    "    (col(\"id\") % 5).alias(\"segment\")\n",
    ")\n",
    "customers.write.parquet(\"/tmp/customers\")\n",
    "print(f\"Clients : {customers.count()} lignes\")\n",
    "\n",
    "print(\"\\n DonnÃ©es crÃ©Ã©es avec succÃ¨s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ VERSION NON OPTIMISÃ‰E (baseline)\n",
    "print(\"=\"*50)\n",
    "print(\"âŒ PIPELINE NON OPTIMISÃ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# DÃ©sactiver AQE pour le baseline\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Lecture\n",
    "transactions_df = spark.read.parquet(\"/tmp/transactions\")\n",
    "products_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/products\")  # âŒ InfÃ©rence\n",
    "customers_df = spark.read.parquet(\"/tmp/customers\")\n",
    "\n",
    "# UDF non optimisÃ©\n",
    "@udf(StringType())\n",
    "def get_amount_category(amount):\n",
    "    if amount > 500: return \"high\"\n",
    "    elif amount > 100: return \"medium\"\n",
    "    return \"low\"\n",
    "\n",
    "# Joins sans broadcast\n",
    "result = transactions_df \\\n",
    "    .join(products_df, \"product_id\") \\\n",
    "    .join(customers_df, \"customer_id\") \\\n",
    "    .withColumn(\"amount_category\", get_amount_category(col(\"amount\"))) \\\n",
    "    .groupBy(\"category\", \"segment\", \"amount_category\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        sum(\"amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "# Ã‰criture non partitionnÃ©e\n",
    "if os.path.exists(\"/tmp/output\"):\n",
    "    shutil.rmtree(\"/tmp/output\")\n",
    "result.write.parquet(\"/tmp/output\")\n",
    "\n",
    "baseline_time = time.time() - start\n",
    "print(f\"\\nâ±ï¸ Temps baseline : {baseline_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… VERSION OPTIMISÃ‰E\n",
    "print(\"=\"*50)\n",
    "print(\"âœ… PIPELINE OPTIMISÃ‰\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Activer AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# 1. Lecture avec schema explicite pour CSV\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType()),\n",
    "    StructField(\"product_name\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"base_price\", DoubleType())\n",
    "])\n",
    "\n",
    "transactions_df = spark.read.parquet(\"/tmp/transactions\")\n",
    "products_df = spark.read.option(\"header\", True).schema(products_schema).csv(\"/tmp/products\")  # âœ… Schema explicite\n",
    "customers_df = spark.read.parquet(\"/tmp/customers\")\n",
    "\n",
    "# 2. Broadcast les petites tables\n",
    "products_df = broadcast(products_df)\n",
    "\n",
    "# 3. Expression native au lieu de UDF\n",
    "amount_category_expr = when(col(\"amount\") > 500, \"high\") \\\n",
    "    .when(col(\"amount\") > 100, \"medium\") \\\n",
    "    .otherwise(\"low\")\n",
    "\n",
    "# 4. Pipeline optimisÃ©\n",
    "result_optimized = transactions_df \\\n",
    "    .join(products_df, \"product_id\") \\\n",
    "    .join(customers_df, \"customer_id\") \\\n",
    "    .withColumn(\"amount_category\", amount_category_expr) \\\n",
    "    .withColumn(\"year\", year(\"date\")) \\\n",
    "    .withColumn(\"month\", month(\"date\")) \\\n",
    "    .groupBy(\"category\", \"segment\", \"amount_category\", \"year\", \"month\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"num_transactions\"),\n",
    "        sum(\"amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "# 5. Ã‰criture partitionnÃ©e\n",
    "if os.path.exists(\"/tmp/output_optimized\"):\n",
    "    shutil.rmtree(\"/tmp/output_optimized\")\n",
    "result_optimized.write.partitionBy(\"year\", \"month\").parquet(\"/tmp/output_optimized\")\n",
    "\n",
    "optimized_time = time.time() - start\n",
    "print(f\"\\nâ±ï¸ Temps optimisÃ© : {optimized_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© des optimisations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š RÃ‰SUMÃ‰ DES OPTIMISATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "speedup = baseline_time / optimized_time if optimized_time > 0 else 0\n",
    "reduction = (1 - optimized_time / baseline_time) * 100 if baseline_time > 0 else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "â±ï¸ Temps baseline    : {baseline_time:.2f}s\n",
    "â±ï¸ Temps optimisÃ©    : {optimized_time:.2f}s\n",
    "ğŸš€ Speedup           : {speedup:.1f}x\n",
    "ğŸ“‰ RÃ©duction         : {reduction:.0f}%\n",
    "\n",
    "Optimisations appliquÃ©es :\n",
    "  âœ… AQE activÃ© (Adaptive Query Execution)\n",
    "  âœ… Schema explicite pour CSV (pas d'infÃ©rence)\n",
    "  âœ… Broadcast join pour products (10K lignes)\n",
    "  âœ… Expression native au lieu de UDF Python\n",
    "  âœ… Ã‰criture partitionnÃ©e par year/month\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quiz de fin de module\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Quel composant de Spark optimise automatiquement le plan de requÃªte ?\n",
    "a) Tungsten  \n",
    "b) Catalyst  \n",
    "c) Driver  \n",
    "d) Executor\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Catalyst est l'optimiseur de requÃªtes qui applique predicate pushdown, projection pruning, et join reordering.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Quelle opÃ©ration cause un shuffle ?\n",
    "a) `filter()`  \n",
    "b) `select()`  \n",
    "c) `groupBy()`  \n",
    "d) `withColumn()`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” `groupBy()` nÃ©cessite un shuffle pour regrouper les donnÃ©es par clÃ©. Les autres sont des transformations narrow.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Quelle mÃ©thode utiliser pour rÃ©duire le nombre de partitions SANS shuffle ?\n",
    "a) `repartition()`  \n",
    "b) `coalesce()`  \n",
    "c) `partitionBy()`  \n",
    "d) `bucketBy()`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `coalesce()` combine les partitions existantes sans shuffle (si rÃ©duction). `repartition()` cause toujours un shuffle.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Quelle est la taille optimale d'une partition Spark ?\n",
    "a) 1-10 MB  \n",
    "b) 128-256 MB  \n",
    "c) 1-2 GB  \n",
    "d) 10-50 GB\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” 128-256 MB est le sweet spot. Trop petit = overhead, trop grand = OOM et mauvaise parallÃ©lisation.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Pour joindre une table de 10 GB avec une table de 50 MB, quelle stratÃ©gie utiliser ?\n",
    "a) Sort Merge Join  \n",
    "b) Shuffle Hash Join  \n",
    "c) Broadcast Join  \n",
    "d) Nested Loop Join\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Broadcast Join envoie la petite table (50 MB) Ã  tous les executors, Ã©vitant le shuffle de la grande table.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Pourquoi les Python UDFs sont-ils lents ?\n",
    "a) Python est un langage interprÃ©tÃ©  \n",
    "b) SÃ©rialisation JVM â†” Python pour chaque ligne  \n",
    "c) Le GIL de Python  \n",
    "d) Manque de mÃ©moire\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Chaque ligne nÃ©cessite une sÃ©rialisation JVMâ†’Python et dÃ©sÃ©rialisation Pythonâ†’JVM, ce qui est trÃ¨s coÃ»teux.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Que signifie \"Spill to disk\" dans Spark UI ?\n",
    "a) Les donnÃ©es sont Ã©crites en Parquet  \n",
    "b) La mÃ©moire est insuffisante, donnÃ©es Ã©crites sur disque  \n",
    "c) Le cache est activÃ©  \n",
    "d) Le shuffle est terminÃ©\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Spill indique que la mÃ©moire est insuffisante et les donnÃ©es dÃ©bordent sur le disque, ce qui ralentit l'exÃ©cution.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Quel deploy mode utiliser en production ?\n",
    "a) `client`  \n",
    "b) `cluster`  \n",
    "c) `local`  \n",
    "d) `standalone`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” En mode `cluster`, le Driver tourne sur un worker du cluster, ce qui est plus robuste pour la production.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q9. Que fait l'AQE (Adaptive Query Execution) ?\n",
    "a) Compile le code Python  \n",
    "b) Optimise le plan d'exÃ©cution au runtime  \n",
    "c) Compresse les donnÃ©es  \n",
    "d) GÃ¨re l'authentification\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” AQE optimise dynamiquement le plan d'exÃ©cution pendant l'exÃ©cution (coalesce, skew handling, broadcast).\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q10. Combien de cores par executor est recommandÃ© ?\n",
    "a) 1 core  \n",
    "b) 4-5 cores  \n",
    "c) 10-15 cores  \n",
    "d) Tous les cores disponibles\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” 4-5 cores est le sweet spot. Plus de cores = heap plus grand = GC moins efficace.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "- [Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)\n",
    "- [Tuning Spark](https://spark.apache.org/docs/latest/tuning.html)\n",
    "\n",
    "### ğŸ“– Articles & Tutoriels\n",
    "- [Databricks - Spark Performance Tuning](https://docs.databricks.com/en/optimizations/index.html)\n",
    "- [Understanding Spark Shuffle](https://medium.com/swlh/revealing-apache-spark-shuffling-magic-b2cb1e75a1f8)\n",
    "\n",
    "### ğŸ”§ Outils\n",
    "- [Spark UI](http://localhost:4040) â€” Diagnostic local\n",
    "- [spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html) â€” Guide officiel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises l'optimisation Spark, passons aux **fonctionnalitÃ©s SQL avancÃ©es** !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `20_spark_sql_deep_dive`** â€” Spark SQL Deep Dive\n",
    "\n",
    "Tu vas apprendre :\n",
    "\n",
    "- **Window functions** avancÃ©es\n",
    "- **CTEs** et subqueries\n",
    "- Optimisation SQL\n",
    "- Spark SQL vs DataFrame API\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ **Note** : Spark Streaming sera couvert dans le **module 24** Kafka.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ RÃ©capitulatif de ce module\n",
    "\n",
    "| Concept | Ce que tu as appris |\n",
    "|---------|--------------------|\n",
    "| **Architecture** | Catalyst, Tungsten, DAG |\n",
    "| **spark-submit** | Deploy modes, packaging, structure projet |\n",
    "| **Partitionnement** | Shuffle, repartition vs coalesce, skew |\n",
    "| **Caching** | cache() vs persist(), storage levels |\n",
    "| **Joins** | Broadcast, Sort Merge, hints |\n",
    "| **I/O** | Parquet, partitionnement disque, schemas |\n",
    "| **UDFs** | Ã‰viter Python UDF, expressions natives |\n",
    "| **Tuning** | AQE, executors/cores, configuration |\n",
    "| **Diagnostic** | Spark UI, mÃ©triques |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module PySpark Advanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage\n",
    "spark.stop()\n",
    "print(\"âœ… SparkSession arrÃªtÃ©e\")\n",
    "\n",
    "# Nettoyage des fichiers temporaires (optionnel)\n",
    "# import shutil\n",
    "# for path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \n",
    "#              \"/tmp/output\", \"/tmp/output_optimized\", \"/tmp/test_parquet\", \"/tmp/partitioned_data\"]:\n",
    "#     if os.path.exists(path):\n",
    "#         shutil.rmtree(path)\n",
    "# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}