{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Formats : Delta Lake & Apache Iceberg\n",
    "\n",
    "## Du Data Lake au Data Lakehouse\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas transformer un Data Lake en **Data Lakehouse** â€” combinant flexibilitÃ© et ACID.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Module | CompÃ©tence | Pourquoi ? |\n",
    "|--------|------------|------------|\n",
    "| âœ… 19 | PySpark Advanced | DataFrame API |\n",
    "| âœ… 20 | Spark SQL | SQL, Catalyst |\n",
    "| âœ… 22 | Cloud Storage | MinIO, s3a:// |\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "- Comprendre pourquoi **Parquet seul ne suffit pas**\n",
    "- MaÃ®triser **ACID** pour les Data Lakes\n",
    "- Utiliser **Delta Lake** : MERGE, Time Travel, Schema Evolution\n",
    "- Comprendre **Iceberg** : Hidden Partitioning\n",
    "- Optimiser avec **OPTIMIZE, Z-ORDER, VACUUM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/diakite-data/data-engineering-bootcamp/blob/main/notebooks/intermediate/23_table_formats_delta_iceberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "> ğŸ’¡ **Conseil** : Cliquez sur le badge ci-dessus pour exÃ©cuter ce notebook directement dans Google Colab (aucune installation requise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction â€” Pourquoi les Table Formats ?\n",
    "\n",
    "### 1.1 L'Ã©volution du stockage\n",
    "\n",
    "```\n",
    "2000s: DATA WAREHOUSE     2010s: DATA LAKE        2020s: LAKEHOUSE\n",
    "â€¢ Oracle, Teradata        â€¢ Hadoop, S3+Parquet    â€¢ Delta, Iceberg\n",
    "â€¢ ACID âœ…                 â€¢ Flexible              â€¢ ACID âœ…\n",
    "â€¢ CoÃ»teux ğŸ’°              â€¢ Pas ACID âŒ           â€¢ Flexible\n",
    "â€¢ Rigide                  â€¢ Cheap ğŸ’µ              â€¢ Cheap ğŸ’µ\n",
    "```\n",
    "\n",
    "### 1.2 Le problÃ¨me avec Parquet seul\n",
    "\n",
    "| ScÃ©nario | ProblÃ¨me |\n",
    "|----------|----------|\n",
    "| Deux jobs Ã©crivent en mÃªme temps | ğŸ’¥ DonnÃ©es corrompues |\n",
    "| UPDATE 100 lignes sur 10M | ğŸ˜° RÃ©Ã©crire toute la partition |\n",
    "| Job Ã©choue Ã  mi-chemin | ğŸ—‘ï¸ Fichiers partiels |\n",
    "| Voir donnÃ©es d'il y a 3 jours | âŒ Impossible |\n",
    "| SchÃ©ma source change | ğŸ’” Erreurs de lecture |\n",
    "\n",
    "**Les Table Formats rÃ©solvent TOUS ces problÃ¨mes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 C'est quoi un Data Lakehouse ?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  DATA LAKEHOUSE                     â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚   DATA LAKE          +       DATA WAREHOUSE         â”‚\n",
    "â”‚   â€¢ Stockage cheap           â€¢ ACID                 â”‚\n",
    "â”‚   â€¢ Formats ouverts          â€¢ Schema enforce       â”‚\n",
    "â”‚   â€¢ FlexibilitÃ©              â€¢ SQL performant       â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚   = Le meilleur des deux mondes !                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "Les Table Formats ajoutent une **couche de mÃ©tadonnÃ©es** (Transaction Log) au-dessus des fichiers Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1 : Identifier les limites de Parquet\n",
    "\n",
    "| ScÃ©nario | ProblÃ¨me ? |\n",
    "|----------|------------|\n",
    "| Job A lit pendant que Job B Ã©crit | ? |\n",
    "| UPDATE 100 lignes sur 10M | ? |\n",
    "| Suppression accidentelle d'une partition | ? |\n",
    "\n",
    "<details><summary>ğŸ’¡ RÃ©ponses</summary>\n",
    "\n",
    "- **Dirty Read** : donnÃ©es partielles/incohÃ©rentes\n",
    "- **RÃ©Ã©criture totale** : toute la partition\n",
    "- **Pas de rollback** : donnÃ©es perdues\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Les Limites du Data Lake Classique\n",
    "\n",
    "### 2.1 Pas de transactions ACID\n",
    "\n",
    "| PropriÃ©tÃ© | Signification | Parquet seul ? |\n",
    "|-----------|---------------|----------------|\n",
    "| **A**tomicity | Tout-ou-rien | âŒ Fichiers partiels |\n",
    "| **C**onsistency | Contraintes respectÃ©es | âŒ Pas de validation |\n",
    "| **I**solation | OpÃ©rations isolÃ©es | âŒ Dirty reads |\n",
    "| **D**urability | DonnÃ©es persistantes | âœ… Sur S3 |\n",
    "\n",
    "### 2.2 Mutations coÃ»teuses\n",
    "\n",
    "UPDATE 1 ligne = lire 10M lignes â†’ modifier â†’ rÃ©Ã©crire 10M lignes = **minutes**\n",
    "\n",
    "### 2.3 Small Files Problem\n",
    "\n",
    "Streaming 5min/jour = 288 fichiers/jour Ã— 365 = **100K+ fichiers/an**\n",
    "\n",
    "### 2.4 SchÃ©ma rigide\n",
    "\n",
    "Pas de schema enforcement natif. Changer partitionnement = rÃ©Ã©criture totale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Introduction aux Table Formats\n",
    "\n",
    "### 3.1 Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚           QUERY ENGINE (Spark, Trino)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         TABLE FORMAT LAYER                  â”‚\n",
    "â”‚      (Delta Lake / Iceberg / Hudi)          â”‚\n",
    "â”‚  â€¢ Transaction Log                          â”‚\n",
    "â”‚  â€¢ ACID, Time Travel, Schema Evolution      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚     FILE FORMAT (Parquet) sur STORAGE (S3)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 3.2 Le Transaction Log\n",
    "\n",
    "```\n",
    "Version 0 : ADD file_001.parquet\n",
    "Version 1 : ADD file_002.parquet\n",
    "Version 2 : ADD file_003.parquet, REMOVE file_001.parquet\n",
    "\n",
    "Ã‰tat actuel = { file_002, file_003 }\n",
    "Ã‰tat v1     = { file_001, file_002 }  â† Time Travel !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparaison Delta vs Iceberg vs Hudi\n",
    "\n",
    "| CritÃ¨re | Delta Lake | Iceberg | Hudi |\n",
    "|---------|------------|---------|------|\n",
    "| **CrÃ©ateur** | Databricks | Netflix | Uber |\n",
    "| **Moteurs** | Spark +++ | Multi-engine | Spark, Flink |\n",
    "| **ACID** | âœ… | âœ… | âœ… |\n",
    "| **Time Travel** | âœ…âœ… | âœ…âœ… | âœ… |\n",
    "| **Schema Evolution** | âœ… | âœ…âœ… (best) | âœ… |\n",
    "| **Hidden Partitioning** | âŒ | âœ… (unique!) | âŒ |\n",
    "| **Z-Ordering** | âœ… | âœ… | âŒ |\n",
    "| **Catalog requis** | Optionnel | **Obligatoire** | Obligatoire |\n",
    "| **CDC natif** | Change Data Feed | âŒ | âœ…âœ… (best) |\n",
    "\n",
    "### 3.4 Lequel choisir ?\n",
    "\n",
    "- ğŸ¯ **Delta Lake** : Spark, Databricks, simplicitÃ©, dÃ©butants\n",
    "- ğŸ¯ **Iceberg** : Multi-engine, Hidden Partitioning, AWS Athena\n",
    "- ğŸ¯ **Hudi** : CDC massif, near-real-time, AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Delta Lake â€” Deep Dive\n",
    "\n",
    "### 4.1 Architecture : _delta_log/\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "â”œâ”€â”€ _delta_log/\n",
    "â”‚   â”œâ”€â”€ 00000000000000000000.json  â† Version 0\n",
    "â”‚   â”œâ”€â”€ 00000000000000000001.json  â† Version 1\n",
    "â”‚   â”œâ”€â”€ 00000000000000000010.checkpoint.parquet\n",
    "â”‚   â””â”€â”€ _last_checkpoint\n",
    "â”œâ”€â”€ part-00000-abc.parquet\n",
    "â””â”€â”€ part-00001-def.parquet\n",
    "```\n",
    "\n",
    "Chaque JSON contient :\n",
    "\n",
    "- `add` : nouveau fichier ajoutÃ©\n",
    "- `remove` : fichier retirÃ©\n",
    "- `commitInfo` : mÃ©tadonnÃ©es de l'opÃ©ration\n",
    "- `stats` : min/max pour Data Skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Spark avec Delta Lake\n",
    "\n",
    "config = '''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake Demo\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Pour MinIO\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "'''\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une table Delta\n",
    "\n",
    "create_examples = '''\n",
    "# MÃ‰THODE 1 : Depuis DataFrame\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Alice\", 1200.0, \"2024-01-15\"),\n",
    "    (2, \"Bob\", 350.0, \"2024-01-15\"),\n",
    "], [\"id\", \"customer\", \"amount\", \"date\"])\n",
    "\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://silver/sales/\")\n",
    "\n",
    "# MÃ‰THODE 2 : SQL\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE sales (id INT, customer STRING, amount DOUBLE, date DATE)\n",
    "    USING DELTA\n",
    "    LOCATION 's3a://silver/sales/'\n",
    "\"\"\")\n",
    "\n",
    "# MÃ‰THODE 3 : Convertir Parquet existant (SANS copie !)\n",
    "spark.sql(\"CONVERT TO DELTA parquet.`s3a://bronze/old_table/`\")\n",
    "'''\n",
    "print(create_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2 : CrÃ©er une table Delta\n",
    "\n",
    "```python\n",
    "# 1. CrÃ©er DataFrame\n",
    "data = [(1, \"Laptop\", 1200.0), (2, \"Mouse\", 25.0)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"product\", \"price\"])\n",
    "\n",
    "# 2. Sauvegarder en Delta\n",
    "# TODO: df.write.format(\"delta\")...\n",
    "\n",
    "# 3. Lister _delta_log/\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/products\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 MERGE INTO : CDC et Upsert\n",
    "\n",
    "```\n",
    "TARGET               SOURCE CDC             APRÃˆS MERGE\n",
    "â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ id â”‚ name  â”‚      â”‚ id â”‚ name  â”‚ op â”‚    â”‚ id â”‚ name  â”‚\n",
    "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ 1  â”‚ Alice â”‚      â”‚ 1  â”‚ Alice â”‚ U  â”‚ â†’  â”‚ 1  â”‚ ALICE â”‚ Updated\n",
    "â”‚ 2  â”‚ Bob   â”‚      â”‚ 3  â”‚ New   â”‚ I  â”‚ â†’  â”‚ 3  â”‚ New   â”‚ Inserted\n",
    "â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ 2  â”‚ Bob   â”‚ D  â”‚    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜ (Bob deleted)\n",
    "                    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE INTO - Exemple complet\n",
    "\n",
    "merge_sql = '''\n",
    "MERGE INTO target_table AS target\n",
    "USING source_cdc AS source\n",
    "ON target.id = source.id\n",
    "\n",
    "WHEN MATCHED AND source.op = 'D' THEN DELETE\n",
    "\n",
    "WHEN MATCHED AND source.op = 'U' THEN\n",
    "    UPDATE SET target.name = source.name, target.amount = source.amount\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (id, name, amount) VALUES (source.id, source.name, source.amount)\n",
    "'''\n",
    "\n",
    "merge_python = '''\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "target = DeltaTable.forPath(spark, \"s3a://silver/customers/\")\n",
    "source = spark.read.parquet(\"s3a://bronze/cdc/\")\n",
    "\n",
    "target.alias(\"t\").merge(source.alias(\"s\"), \"t.id = s.id\") \\\n",
    "    .whenMatchedDelete(condition=\"s.op = 'D'\") \\\n",
    "    .whenMatchedUpdate(condition=\"s.op = 'U'\", set={\"name\": \"s.name\"}) \\\n",
    "    .whenNotMatchedInsert(values={\"id\": \"s.id\", \"name\": \"s.name\"}) \\\n",
    "    .execute()\n",
    "'''\n",
    "\n",
    "print(\"SQL:\", merge_sql)\n",
    "print(\"\\nPython:\", merge_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3 : CDC avec MERGE INTO\n",
    "\n",
    "```python\n",
    "# Initial\n",
    "initial = [(1, \"Alice\"), (2, \"Bob\")]\n",
    "\n",
    "# CDC\n",
    "cdc = [(1, \"ALICE\", \"U\"), (3, \"New\", \"I\"), (2, None, \"D\")]\n",
    "\n",
    "# TODO: CrÃ©er table, appliquer MERGE\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "target = DeltaTable.forPath(spark, \"/tmp/customers\")\n",
    "target.alias(\"t\").merge(df_cdc.alias(\"s\"), \"t.id = s.id\") \\\n",
    "    .whenMatchedDelete(condition=\"s.op = 'D'\") \\\n",
    "    .whenMatchedUpdate(condition=\"s.op = 'U'\", set={\"name\": \"s.name\"}) \\\n",
    "    .whenNotMatchedInsert(values={\"id\": \"s.id\", \"name\": \"s.name\"}) \\\n",
    "    .execute()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Time Travel\n",
    "\n",
    "```\n",
    "Version 0     Version 1     Version 2     Version 3\n",
    "(Create)      (INSERT)      (UPDATE)      (DELETE)\n",
    "   â”‚              â”‚              â”‚              â”‚\n",
    "   â–¼              â–¼              â–¼              â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ 1 row  â”‚ â†’ â”‚ 3 rows â”‚ â†’ â”‚ 3 rows â”‚ â†’ â”‚ 2 rows â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚modifiedâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Tu peux lire N'IMPORTE quelle version !\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Travel\n",
    "\n",
    "time_travel = '''\n",
    "# Voir l'historique\n",
    "spark.sql(\"DESCRIBE HISTORY sales\").show()\n",
    "\n",
    "# Lire par version\n",
    "df_v1 = spark.sql(\"SELECT * FROM sales VERSION AS OF 1\")\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"path/\")\n",
    "\n",
    "# Lire par timestamp\n",
    "df = spark.sql(\"SELECT * FROM sales TIMESTAMP AS OF '2024-01-03 12:00:00'\")\n",
    "\n",
    "# RESTORE : Revenir Ã  une version\n",
    "spark.sql(\"RESTORE TABLE sales TO VERSION AS OF 5\")\n",
    "'''\n",
    "\n",
    "print(time_travel)\n",
    "print(\"\\nğŸ’¡ Use cases: Audit, rollback, debug, ML reproductibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4 : Restaurer aprÃ¨s erreur\n",
    "\n",
    "```python\n",
    "# OUPS ! DELETE sans WHERE\n",
    "spark.sql(\"DELETE FROM sales\")\n",
    "\n",
    "# TODO: Utiliser Time Travel pour restaurer\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"DESCRIBE HISTORY sales\").show()  # Trouver version avant DELETE\n",
    "spark.sql(\"RESTORE TABLE sales TO VERSION AS OF 1\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Schema Management\n",
    "\n",
    "| Mode | Comportement | Option |\n",
    "|------|--------------|--------|\n",
    "| **Enforcement** | Rejette schÃ©mas diffÃ©rents | (dÃ©faut) |\n",
    "| **Evolution** | Ajoute nouvelles colonnes | `mergeSchema=true` |\n",
    "| **Overwrite** | Remplace schÃ©ma entier | `overwriteSchema=true` |\n",
    "\n",
    "```python\n",
    "# Schema Evolution\n",
    "df_new.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .save(\"path/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Apache Iceberg â€” Deep Dive\n",
    "\n",
    "### 5.1 Architecture\n",
    "\n",
    "```\n",
    "s3://bucket/sales/\n",
    "â”œâ”€â”€ metadata/\n",
    "â”‚   â”œâ”€â”€ v1.metadata.json     â† Metadata File\n",
    "â”‚   â”œâ”€â”€ snap-xxx.avro        â† Manifest List\n",
    "â”‚   â””â”€â”€ xxx-m0.avro          â† Manifest File\n",
    "â””â”€â”€ data/\n",
    "    â””â”€â”€ part-00000.parquet\n",
    "```\n",
    "\n",
    "Structure arborescente vs log sÃ©quentiel = plus rapide pour grandes tables.\n",
    "\n",
    "### 5.2 Hidden Partitioning (Killer Feature)\n",
    "\n",
    "```sql\n",
    "CREATE TABLE events (\n",
    "    event_id LONG,\n",
    "    event_ts TIMESTAMP,\n",
    "    user_id LONG\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (\n",
    "    days(event_ts),      -- Partition par jour (cachÃ© !)\n",
    "    bucket(16, user_id)  -- Bucket hash\n",
    ")\n",
    "\n",
    "-- L'utilisateur n'a PAS besoin de connaÃ®tre le partitionnement !\n",
    "SELECT * FROM events WHERE event_ts = '2024-01-15 10:00:00'\n",
    "-- Partition pruning automatique !\n",
    "```\n",
    "\n",
    "**Transforms** : `years()`, `months()`, `days()`, `hours()`, `bucket(N, col)`, `truncate(L, col)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5 : Table Iceberg avec Hidden Partitioning\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```sql\n",
    "CREATE TABLE my_catalog.db.page_views (\n",
    "    view_id LONG, view_timestamp TIMESTAMP, user_id LONG\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(view_timestamp), bucket(8, user_id))\n",
    "```\n",
    "</details>\n",
    "\n",
    "### 5.3 Schema Evolution avancÃ©e\n",
    "\n",
    "```sql\n",
    "ALTER TABLE events ADD COLUMN source STRING\n",
    "ALTER TABLE events RENAME COLUMN payload TO event_data  -- Sans rÃ©Ã©criture !\n",
    "ALTER TABLE events DROP COLUMN deprecated              -- Sans rÃ©Ã©criture !\n",
    "```\n",
    "\n",
    "### 5.4 Catalog (obligatoire)\n",
    "\n",
    "Iceberg nÃ©cessite un catalog : Hive Metastore, AWS Glue, Nessie, Unity Catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Maintenance & Optimisation\n",
    "\n",
    "### 7.1 Compaction (OPTIMIZE)\n",
    "\n",
    "```sql\n",
    "-- Delta\n",
    "OPTIMIZE sales\n",
    "OPTIMIZE sales WHERE date >= '2024-01-01'\n",
    "\n",
    "-- Iceberg\n",
    "CALL catalog.system.rewrite_data_files(table => 'db.sales')\n",
    "```\n",
    "\n",
    "### 7.2 Z-Ordering\n",
    "\n",
    "Organise donnÃ©es par colonnes filtrÃ©es â†’ meilleur Data Skipping.\n",
    "\n",
    "```sql\n",
    "OPTIMIZE sales ZORDER BY (customer_id, product_id)\n",
    "```\n",
    "\n",
    "**RÃ¨gle** : Partition par faible cardinalitÃ©, Z-ORDER par haute cardinalitÃ©.\n",
    "\n",
    "### 7.3 VACUUM\n",
    "\n",
    "```sql\n",
    "-- Supprimer fichiers > 7 jours\n",
    "VACUUM sales\n",
    "VACUUM sales RETAIN 168 HOURS\n",
    "\n",
    "-- Iceberg\n",
    "CALL catalog.system.expire_snapshots(table => 'db.sales', retain_last => 5)\n",
    "```\n",
    "\n",
    "âš ï¸ **AprÃ¨s VACUUM, Time Travel sur versions supprimÃ©es impossible !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7 : Compacter une table\n",
    "\n",
    "```python\n",
    "# VÃ©rifier fichiers avant\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`path`\").select(\"numFiles\").show()\n",
    "\n",
    "# TODO: OPTIMIZE + ZORDER\n",
    "\n",
    "# VÃ©rifier aprÃ¨s\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "spark.sql(\"OPTIMIZE delta.`path` ZORDER BY (id)\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Migration Parquet â†’ Delta\n",
    "\n",
    "### 8.1 Conversion in-place (SANS copie !)\n",
    "\n",
    "```sql\n",
    "CONVERT TO DELTA parquet.`s3a://bronze/old_table/`\n",
    "CONVERT TO DELTA parquet.`s3a://bronze/partitioned/`\n",
    "    PARTITIONED BY (date STRING)\n",
    "```\n",
    "\n",
    "CrÃ©e `_delta_log/` sans dÃ©placer les donnÃ©es. **Temps : secondes.**\n",
    "\n",
    "### 8.2 Migration avec CTAS\n",
    "\n",
    "```sql\n",
    "CREATE TABLE new_delta USING DELTA AS\n",
    "SELECT * FROM parquet.`s3a://bronze/old/`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Apache Hudi â€” AperÃ§u\n",
    "\n",
    "### Copy-on-Write vs Merge-on-Read\n",
    "\n",
    "| | CoW | MoR |\n",
    "|---|-----|-----|\n",
    "| **Ã‰criture** | RÃ©Ã©crit fichier entier (ğŸ¢) | Ã‰crit delta log (ğŸš€) |\n",
    "| **Lecture** | Lit fichier (ğŸš€) | Merge base + deltas (ğŸ¢) |\n",
    "| **Use case** | Lectures frÃ©quentes | CDC intensif |\n",
    "\n",
    "**Quand utiliser Hudi** : CDC massif, near-real-time, AWS EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Mini-Projet : Lakehouse avec MinIO\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    MinIO                            â”‚\n",
    "â”‚  bronze/          silver/           gold/           â”‚\n",
    "â”‚  (CSV)     â†’     (Delta)      â†’    (Delta)          â”‚\n",
    "â”‚  orders.csv       orders/           daily_sales/    â”‚\n",
    "â”‚                   _delta_log/       _delta_log/     â”‚\n",
    "â”‚      â†“               â†“                  â†“           â”‚\n",
    "â”‚   Upload       MERGE INTO          OPTIMIZE         â”‚\n",
    "â”‚               (Dedupe+CDC)         ZORDER BY        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Ã‰tapes** :\n",
    "\n",
    "1. DÃ©marrer MinIO\n",
    "2. Configurer Spark + Delta\n",
    "3. Bronze â†’ Silver (MERGE)\n",
    "4. Time Travel audit\n",
    "5. Silver â†’ Gold (agrÃ©gations)\n",
    "6. OPTIMIZE + ZORDER\n",
    "7. VACUUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quiz\n",
    "\n",
    "**Q1.** Pourquoi Parquet seul ne garantit pas ACID ?\n",
    "<details><summary>R</summary>Pas de Transaction Log pour atomicitÃ©/isolation.</details>\n",
    "\n",
    "**Q2.** DiffÃ©rence Schema Enforcement vs Evolution ?\n",
    "<details><summary>R</summary>Enforcement rejette, Evolution adapte.</details>\n",
    "\n",
    "**Q3.** Quel format supporte Hidden Partitioning ?\n",
    "<details><summary>R</summary>Apache Iceberg uniquement.</details>\n",
    "\n",
    "**Q4.** RÃ´le du Transaction Log Delta ?\n",
    "<details><summary>R</summary>Enregistre add/remove pour ACID et Time Travel.</details>\n",
    "\n",
    "**Q5.** Objectif de ZORDER BY ?\n",
    "<details><summary>R</summary>Organiser donnÃ©es pour meilleur Data Skipping.</details>\n",
    "\n",
    "**Q6.** Quelle opÃ©ration supprime les vieux fichiers ?\n",
    "<details><summary>R</summary>VACUUM (Delta) ou expire_snapshots (Iceberg).</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "- [Delta Lake Docs](https://docs.delta.io/)\n",
    "- [Apache Iceberg Docs](https://iceberg.apache.org/docs/)\n",
    "- [Apache Hudi Docs](https://hudi.apache.org/docs/)\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ğŸ‘‰ **Module 24 : `24_kafka_streaming`** â€” Kafka & Streaming\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ RÃ©capitulatif\n",
    "\n",
    "| Concept | Appris |\n",
    "|---------|--------|\n",
    "| Data Lakehouse | Lake + Warehouse |\n",
    "| ACID | AtomicitÃ©, Consistance, Isolation, DurabilitÃ© |\n",
    "| Delta Lake | Transaction Log, MERGE, Time Travel |\n",
    "| Iceberg | Hidden Partitioning, Schema Evolution |\n",
    "| Maintenance | OPTIMIZE, ZORDER, VACUUM |\n",
    "| Migration | CONVERT TO DELTA |\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Module Table Formats terminÃ©."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
