{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŠ Kafka, Python & Structured Streaming\n",
    "\n",
    "## Le Temps RÃ©el pour le Data Engineering\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le **streaming de donnÃ©es** â€” la capacitÃ© Ã  traiter des flux continus d'Ã©vÃ©nements en temps rÃ©el plutÃ´t que des batches pÃ©riodiques.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "| Module | CompÃ©tence | Pourquoi ? |\n",
    "|--------|------------|------------|\n",
    "| âœ… 19 | PySpark Advanced | DataFrame API |\n",
    "| âœ… 21 | Spark on K8s | DÃ©ploiement |\n",
    "| âœ… 23 | Table Formats | Delta Lake comme Sink |\n",
    "\n",
    "## ğŸ¯ Objectifs\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre les **architectures de streaming** (Lambda vs Kappa)\n",
    "- DÃ©ployer **Apache Kafka** et crÃ©er des topics\n",
    "- Ã‰crire des **producteurs/consommateurs Python** natifs\n",
    "- Construire des pipelines **Spark Structured Streaming**\n",
    "- MaÃ®triser **Watermarks** et **Windowing** pour le temps d'Ã©vÃ©nement\n",
    "- Utiliser **foreachBatch + MERGE INTO** pour des sinks transactionnels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š 1. Introduction â€” Pourquoi le Temps RÃ©el ?\n",
    "\n",
    "### 1.1 L'Ã©volution des architectures de donnÃ©es\n",
    "\n",
    "Historiquement, le traitement de donnÃ©es Ã©tait **batch** : on collecte les donnÃ©es pendant X heures, puis on les traite. Mais les besoins modernes exigent une latence plus faible.\n",
    "\n",
    "```\n",
    "ARCHITECTURE LAMBDA (2010s)           ARCHITECTURE KAPPA (2020s)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                             â”‚      â”‚                             â”‚\n",
    "â”‚  Source â”€â”€â”¬â”€â”€ Batch Layer   â”‚      â”‚  Source â”€â”€â”€ Stream Layer    â”‚\n",
    "â”‚           â”‚   (Spark Batch) â”‚      â”‚             (Kafka + SSS)   â”‚\n",
    "â”‚           â”‚        â”‚        â”‚      â”‚                  â”‚          â”‚\n",
    "â”‚           â””â”€â”€ Speed Layer   â”‚      â”‚                  â”‚          â”‚\n",
    "â”‚               (Storm)       â”‚      â”‚                  â”‚          â”‚\n",
    "â”‚                  â”‚          â”‚      â”‚                  â”‚          â”‚\n",
    "â”‚           Serving Layer     â”‚      â”‚           Data Lake         â”‚\n",
    "â”‚                             â”‚      â”‚           (Delta)           â”‚\n",
    "â”‚  âš ï¸ 2 codebases Ã  maintenir â”‚      â”‚  âœ… 1 seul pipeline         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.2 Cas d'usage du temps rÃ©el\n",
    "\n",
    "| Domaine | Exemple | Latence requise |\n",
    "|---------|---------|----------------|\n",
    "| **Fraude** | DÃ©tecter transaction suspecte | < 1 seconde |\n",
    "| **IoT** | Alerter si capteur anormal | < 5 secondes |\n",
    "| **E-commerce** | Recommandations live | < 100 ms |\n",
    "| **Monitoring** | Alerter si service down | < 30 secondes |\n",
    "| **Finance** | Trading algorithmique | < 10 ms |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Batch vs Streaming : Les diffÃ©rences fondamentales\n",
    "\n",
    "| Aspect | Batch | Streaming |\n",
    "|--------|-------|----------|\n",
    "| **DonnÃ©es** | BornÃ©es (bounded) | Non-bornÃ©es (unbounded) |\n",
    "| **Traitement** | PÃ©riodique (horaire, quotidien) | Continu |\n",
    "| **Latence** | Minutes Ã  heures | Secondes Ã  millisecondes |\n",
    "| **Ã‰tat** | RecalculÃ© Ã  chaque run | Maintenu entre Ã©vÃ©nements |\n",
    "| **ComplexitÃ©** | Plus simple | Plus complexe (temps, Ã©tat) |\n",
    "\n",
    "### 1.4 Micro-batch vs Continuous\n",
    "\n",
    "Il existe deux modÃ¨les de traitement streaming :\n",
    "\n",
    "```\n",
    "MICRO-BATCH (Spark SSS dÃ©faut)        CONTINUOUS (Flink, Kafka Streams)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                             â”‚      â”‚                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”  â”‚      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚\n",
    "â”‚  â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚  â”‚      â”‚  Traitement Ã©vÃ©nement par   â”‚\n",
    "â”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜  â”‚      â”‚  Ã©vÃ©nement                  â”‚\n",
    "â”‚  Batches de 100ms-1s       â”‚      â”‚                             â”‚\n",
    "â”‚                             â”‚      â”‚                             â”‚\n",
    "â”‚  Latence: ~1s              â”‚      â”‚  Latence: ~10ms             â”‚\n",
    "â”‚  Throughput: TrÃ¨s Ã©levÃ©    â”‚      â”‚  Throughput: Ã‰levÃ©          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Spark Structured Streaming** utilise le micro-batch par dÃ©faut (suffisant pour 90% des cas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Garanties de livraison\n",
    "\n",
    "Un concept **crucial** en streaming : que se passe-t-il si le systÃ¨me plante ?\n",
    "\n",
    "| Garantie | Description | Quand l'utiliser |\n",
    "|----------|-------------|------------------|\n",
    "| **At-most-once** | Message traitÃ© 0 ou 1 fois | Logs non critiques |\n",
    "| **At-least-once** | Message traitÃ© 1+ fois (doublons possibles) | Compteurs, mÃ©triques |\n",
    "| **Exactly-once** | Message traitÃ© exactement 1 fois | Transactions financiÃ¨res |\n",
    "\n",
    "```\n",
    "AT-MOST-ONCE              AT-LEAST-ONCE           EXACTLY-ONCE\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Fire & Forgetâ”‚          â”‚ Retry until â”‚         â”‚ Transactionalâ”‚\n",
    "â”‚             â”‚          â”‚ ACK         â”‚         â”‚ + Idempotent â”‚\n",
    "â”‚ Peut perdre â”‚          â”‚ Peut dupliquerâ”‚        â”‚ Parfait     â”‚\n",
    "â”‚ des messagesâ”‚          â”‚ des messages â”‚         â”‚             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     ğŸ˜¢                       ğŸ˜                      ğŸ¯\n",
    "```\n",
    "\n",
    "**Exactly-once** est le Saint Graal, mais plus complexe Ã  implÃ©menter. Spark SSS + Kafka + Delta Lake peuvent l'atteindre !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¶ 2. Apache Kafka â€” Le Bus de Messagerie\n",
    "\n",
    "### 2.1 Qu'est-ce que Kafka ?\n",
    "\n",
    "Apache Kafka est une **plateforme de streaming distribuÃ©e** crÃ©Ã©e par LinkedIn en 2011. C'est devenu le standard de facto pour le streaming de donnÃ©es.\n",
    "\n",
    "**Kafka n'est PAS** une base de donnÃ©es, mais un **log distribuÃ©** oÃ¹ les messages sont :\n",
    "- Ã‰crits de maniÃ¨re **append-only** (jamais modifiÃ©s)\n",
    "- **PersistÃ©s** sur disque (pas juste en mÃ©moire)\n",
    "- **RÃ©pliquÃ©s** pour la tolÃ©rance aux pannes\n",
    "\n",
    "### 2.2 Architecture de Kafka\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      KAFKA CLUSTER                              â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
    "â”‚  â”‚  Broker 1   â”‚  â”‚  Broker 2   â”‚  â”‚  Broker 3   â”‚            â”‚\n",
    "â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚            â”‚\n",
    "â”‚  â”‚ Topic A P0  â”‚  â”‚ Topic A P1  â”‚  â”‚ Topic A P2  â”‚  â† Partitionsâ”‚\n",
    "â”‚  â”‚ Topic B P1  â”‚  â”‚ Topic B P0  â”‚  â”‚ Topic B P2  â”‚    rÃ©parties â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\n",
    "â”‚         â”‚                â”‚                â”‚                    â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\n",
    "â”‚                          â”‚                                     â”‚\n",
    "â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\n",
    "â”‚                  â”‚  ZooKeeper/   â”‚  â† Coordination             â”‚\n",
    "â”‚                  â”‚  KRaft        â”‚    (metadata, leaders)      â”‚\n",
    "â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–²                                          â”‚\n",
    "         â”‚                                          â–¼\n",
    "   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   â”‚ Producers â”‚                            â”‚ Consumers â”‚\n",
    "   â”‚ (Python)  â”‚                            â”‚ (Spark)   â”‚\n",
    "   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Concepts clÃ©s\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Broker** | Serveur Kafka qui stocke les messages |\n",
    "| **Topic** | CatÃ©gorie/flux de messages (comme une table) |\n",
    "| **Partition** | Sous-division d'un topic pour le parallÃ©lisme |\n",
    "| **Offset** | Position d'un message dans une partition |\n",
    "| **Producer** | Application qui envoie des messages |\n",
    "| **Consumer** | Application qui lit des messages |\n",
    "| **Consumer Group** | Groupe de consumers qui se partagent les partitions |\n",
    "\n",
    "### 2.4 Topics et Partitions\n",
    "\n",
    "```\n",
    "Topic: \"orders\" avec 3 partitions\n",
    "\n",
    "Partition 0:  [msg0] [msg3] [msg6] [msg9]  ...  â†’ Offset croissant\n",
    "Partition 1:  [msg1] [msg4] [msg7] [msg10] ...\n",
    "Partition 2:  [msg2] [msg5] [msg8] [msg11] ...\n",
    "\n",
    "Chaque partition :\n",
    "â€¢ Est ordonnÃ©e (FIFO dans la partition)\n",
    "â€¢ Peut Ãªtre lue par UN consumer du groupe\n",
    "â€¢ Est rÃ©pliquÃ©e sur plusieurs brokers\n",
    "```\n",
    "\n",
    "**ClÃ© de message** : DÃ©termine la partition. Messages avec la mÃªme clÃ© â†’ mÃªme partition â†’ ordre garanti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Installation Kafka avec Docker\n",
    "\n",
    "Nous allons dÃ©ployer Kafka en local avec Docker Compose. Deux options :\n",
    "- **ZooKeeper** : Mode classique (stable)\n",
    "- **KRaft** : Nouveau mode sans ZooKeeper (Kafka 3.3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker Compose pour Kafka avec ZooKeeper\n",
    "\n",
    "docker_compose_kafka = '''\n",
    "version: \"3.8\"\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    container_name: zookeeper\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    container_name: kafka\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"      # Pour les clients externes\n",
    "      - \"29092:29092\"    # Pour les clients Docker internes\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n",
    "\n",
    "  # Schema Registry (optionnel mais recommandÃ©)\n",
    "  schema-registry:\n",
    "    image: confluentinc/cp-schema-registry:7.5.0\n",
    "    container_name: schema-registry\n",
    "    depends_on:\n",
    "      - kafka\n",
    "    ports:\n",
    "      - \"8081:8081\"\n",
    "    environment:\n",
    "      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n",
    "      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092\n",
    "'''\n",
    "\n",
    "print(docker_compose_kafka)\n",
    "print(\"\\n# DÃ©marrer avec : docker-compose up -d\")\n",
    "print(\"# VÃ©rifier : docker-compose ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commandes CLI Kafka essentielles\n",
    "\n",
    "kafka_cli = '''\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Gestion des Topics\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# CrÃ©er un topic\n",
    "docker exec kafka kafka-topics --create \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic orders \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 1\n",
    "\n",
    "# Lister les topics\n",
    "docker exec kafka kafka-topics --list \\\n",
    "  --bootstrap-server localhost:9092\n",
    "\n",
    "# DÃ©crire un topic\n",
    "docker exec kafka kafka-topics --describe \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic orders\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Produire et Consommer (test rapide)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Produire des messages (interactif)\n",
    "docker exec -it kafka kafka-console-producer \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic orders\n",
    "\n",
    "# Consommer des messages (depuis le dÃ©but)\n",
    "docker exec kafka kafka-console-consumer \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic orders \\\n",
    "  --from-beginning\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Consumer Groups\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Lister les consumer groups\n",
    "docker exec kafka kafka-consumer-groups --list \\\n",
    "  --bootstrap-server localhost:9092\n",
    "\n",
    "# Voir le lag d'un group\n",
    "docker exec kafka kafka-consumer-groups --describe \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --group my-consumer-group\n",
    "'''\n",
    "\n",
    "print(kafka_cli)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 1 : DÃ©ployer Kafka et crÃ©er un topic\n",
    "\n",
    "**Objectif** : Mettre en place l'infrastructure Kafka.\n",
    "\n",
    "```bash\n",
    "# 1. CrÃ©er docker-compose.yml avec le contenu ci-dessus\n",
    "\n",
    "# 2. DÃ©marrer Kafka\n",
    "docker-compose up -d\n",
    "\n",
    "# 3. CrÃ©er un topic \"events\" avec 3 partitions\n",
    "# TODO\n",
    "\n",
    "# 4. VÃ©rifier que le topic existe\n",
    "# TODO\n",
    "\n",
    "# 5. Tester avec console-producer et console-consumer\n",
    "# TODO\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```bash\n",
    "# 3. CrÃ©er topic\n",
    "docker exec kafka kafka-topics --create \\\n",
    "  --bootstrap-server localhost:9092 \\\n",
    "  --topic events --partitions 3 --replication-factor 1\n",
    "\n",
    "# 4. VÃ©rifier\n",
    "docker exec kafka kafka-topics --describe \\\n",
    "  --bootstrap-server localhost:9092 --topic events\n",
    "\n",
    "# 5. Test\n",
    "# Terminal 1: docker exec -it kafka kafka-console-producer --bootstrap-server localhost:9092 --topic events\n",
    "# Terminal 2: docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic events --from-beginning\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“œ 3. Schema Registry & SÃ©rialisation\n",
    "\n",
    "### 3.1 Le problÃ¨me de la sÃ©rialisation\n",
    "\n",
    "Kafka transporte des **bytes**. Il faut donc sÃ©rialiser/dÃ©sÃ©rialiser les donnÃ©es. Trois formats populaires :\n",
    "\n",
    "| Format | Avantages | InconvÃ©nients |\n",
    "|--------|-----------|---------------|\n",
    "| **JSON** | Lisible, flexible | Verbeux, pas de schÃ©ma |\n",
    "| **Avro** | Compact, schÃ©ma, Ã©volution | Moins lisible |\n",
    "| **Protobuf** | TrÃ¨s compact, typage fort | Plus complexe |\n",
    "\n",
    "### 3.2 Pourquoi un Schema Registry ?\n",
    "\n",
    "Sans Schema Registry, chaque producteur/consommateur doit connaÃ®tre le schÃ©ma. ProblÃ¨mes :\n",
    "- Comment Ã©voluer le schÃ©ma sans casser les consumers ?\n",
    "- Comment valider que les messages sont conformes ?\n",
    "\n",
    "```\n",
    "SANS SCHEMA REGISTRY              AVEC SCHEMA REGISTRY\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Producer            â”‚          â”‚ Producer            â”‚\n",
    "â”‚ {schÃ©ma hardcodÃ©}   â”‚          â”‚ â†’ Enregistre schÃ©ma â”‚\n",
    "â”‚         â”‚           â”‚          â”‚ â†’ Envoie schema_id  â”‚\n",
    "â”‚         â–¼           â”‚          â”‚         â”‚           â”‚\n",
    "â”‚    [message]        â”‚          â”‚    [id + message]   â”‚\n",
    "â”‚         â”‚           â”‚          â”‚         â”‚           â”‚\n",
    "â”‚         â–¼           â”‚          â”‚         â–¼           â”‚\n",
    "â”‚ Consumer            â”‚          â”‚ Consumer            â”‚\n",
    "â”‚ {schÃ©ma hardcodÃ©}   â”‚          â”‚ â†’ RÃ©cupÃ¨re schÃ©ma   â”‚\n",
    "â”‚                     â”‚          â”‚   par id            â”‚\n",
    "â”‚ ğŸ˜° SchÃ©ma dÃ©sync!   â”‚          â”‚ âœ… Toujours Ã  jour  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 CompatibilitÃ© de schÃ©ma\n",
    "\n",
    "Le Schema Registry vÃ©rifie la **compatibilitÃ©** lors de l'Ã©volution :\n",
    "\n",
    "| Mode | Description | Exemple autorisÃ© |\n",
    "|------|-------------|------------------|\n",
    "| **BACKWARD** | Nouveau schÃ©ma peut lire ancien | Ajouter champ optionnel |\n",
    "| **FORWARD** | Ancien schÃ©ma peut lire nouveau | Supprimer champ optionnel |\n",
    "| **FULL** | Les deux | Ajouter/supprimer champ optionnel |\n",
    "| **NONE** | Pas de vÃ©rification | Tout (dangereux) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de schÃ©ma Avro\n",
    "\n",
    "avro_schema = '''\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Order\",\n",
    "  \"namespace\": \"com.example\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"order_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"customer_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"amount\", \"type\": \"double\"},\n",
    "    {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"},\n",
    "    {\"name\": \"status\", \"type\": {\"type\": \"enum\", \"name\": \"Status\", \"symbols\": [\"PENDING\", \"COMPLETED\", \"CANCELLED\"]}},\n",
    "    {\"name\": \"notes\", \"type\": [\"null\", \"string\"], \"default\": null}  // Champ optionnel\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"SchÃ©ma Avro pour les commandes :\")\n",
    "print(avro_schema)\n",
    "print(\"\\nğŸ’¡ Points clÃ©s :\")\n",
    "print(\"â€¢ 'type': 'record' â†’ structure comme une classe\")\n",
    "print(\"â€¢ 'logicalType' â†’ types avancÃ©s (timestamp, date, decimal)\")\n",
    "print(\"â€¢ ['null', 'string'] â†’ champ optionnel (union type)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ 4. Python Natif pour Kafka\n",
    "\n",
    "Avant d'utiliser Spark, apprenons Ã  interagir avec Kafka en **Python pur**. Deux librairies principales :\n",
    "\n",
    "| Librairie | Avantages | Quand l'utiliser |\n",
    "|-----------|-----------|------------------|\n",
    "| **kafka-python** | Simple, pur Python | Scripts simples, prototypage |\n",
    "| **confluent-kafka** | Performant, Schema Registry | Production, Avro |\n",
    "\n",
    "### 4.1 Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies Kafka Python\n",
    "# !pip install kafka-python confluent-kafka fastavro\n",
    "\n",
    "print(\"Librairies Ã  installer :\")\n",
    "print(\"pip install kafka-python       # Client simple\")\n",
    "print(\"pip install confluent-kafka    # Client performant + Schema Registry\")\n",
    "print(\"pip install fastavro           # SÃ©rialisation Avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Producteur Python (kafka-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producteur Kafka simple avec kafka-python\n",
    "\n",
    "producer_simple = '''\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Configuration du producteur\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    \n",
    "    # SÃ©rialisation JSON\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    \n",
    "    # Configuration de fiabilitÃ©\n",
    "    acks='all',              # Attendre confirmation de tous les replicas\n",
    "    retries=3,               # RÃ©essayer en cas d'erreur\n",
    "    retry_backoff_ms=100,    # DÃ©lai entre retries\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Envoyer des messages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def send_order(order_id, customer_id, amount):\n",
    "    \"\"\"Envoyer une commande au topic 'orders'\"\"\"\n",
    "    message = {\n",
    "        \"order_id\": order_id,\n",
    "        \"customer_id\": customer_id,\n",
    "        \"amount\": amount,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"status\": \"PENDING\"\n",
    "    }\n",
    "    \n",
    "    # Envoyer avec une clÃ© (mÃªme customer â†’ mÃªme partition â†’ ordre garanti)\n",
    "    future = producer.send(\n",
    "        topic='orders',\n",
    "        key=customer_id,    # ClÃ© pour le partitionnement\n",
    "        value=message\n",
    "    )\n",
    "    \n",
    "    # Attendre confirmation (synchrone)\n",
    "    try:\n",
    "        record_metadata = future.get(timeout=10)\n",
    "        print(f\"âœ… Message envoyÃ©: partition={record_metadata.partition}, offset={record_metadata.offset}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erreur: {e}\")\n",
    "\n",
    "# Simuler un flux de commandes\n",
    "for i in range(10):\n",
    "    send_order(\n",
    "        order_id=f\"ORD-{i:04d}\",\n",
    "        customer_id=f\"CUST-{i % 3:03d}\",  # 3 customers\n",
    "        amount=round(100 + i * 10.5, 2)\n",
    "    )\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Important : flush avant de quitter\n",
    "producer.flush()\n",
    "producer.close()\n",
    "'''\n",
    "\n",
    "print(producer_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Consommateur Python (kafka-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consommateur Kafka simple avec kafka-python\n",
    "\n",
    "consumer_simple = '''\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Configuration du consommateur\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',                             # Topic(s) Ã  consommer\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    \n",
    "    # DÃ©sÃ©rialisation JSON\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "    \n",
    "    # Consumer Group (pour le parallÃ©lisme)\n",
    "    group_id='order-processor-group',\n",
    "    \n",
    "    # OÃ¹ commencer Ã  lire\n",
    "    auto_offset_reset='earliest',  # 'earliest' = depuis le dÃ©but, 'latest' = nouveaux messages seulement\n",
    "    \n",
    "    # Commit des offsets\n",
    "    enable_auto_commit=True,       # Commit automatique\n",
    "    auto_commit_interval_ms=5000,  # Toutes les 5 secondes\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Boucle de consommation\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ§ En attente de messages...\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        # MÃ©tadonnÃ©es du message\n",
    "        print(f\"\\nğŸ“© Message reÃ§u:\")\n",
    "        print(f\"   Topic: {message.topic}\")\n",
    "        print(f\"   Partition: {message.partition}\")\n",
    "        print(f\"   Offset: {message.offset}\")\n",
    "        print(f\"   Key: {message.key}\")\n",
    "        print(f\"   Timestamp: {message.timestamp}\")\n",
    "        \n",
    "        # Contenu du message\n",
    "        order = message.value\n",
    "        print(f\"   Order: {order}\")\n",
    "        \n",
    "        # Logique mÃ©tier : alerter si montant Ã©levÃ©\n",
    "        if order.get('amount', 0) > 500:\n",
    "            print(f\"   âš ï¸ ALERTE: Commande de {order['amount']}â‚¬ !\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ ArrÃªt du consommateur\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "'''\n",
    "\n",
    "print(consumer_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Producteur avec Avro et Schema Registry (confluent-kafka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producteur avec Avro et Schema Registry\n",
    "\n",
    "producer_avro = '''\n",
    "from confluent_kafka import Producer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Configuration Schema Registry\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "schema_registry_conf = {'url': 'http://localhost:8081'}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "# SchÃ©ma Avro\n",
    "order_schema = \"\"\"\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Order\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"order_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"customer_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"amount\", \"type\": \"double\"},\n",
    "    {\"name\": \"timestamp\", \"type\": \"long\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# SÃ©rialiseurs\n",
    "avro_serializer = AvroSerializer(\n",
    "    schema_registry_client,\n",
    "    order_schema,\n",
    "    lambda obj, ctx: obj  # Conversion dict â†’ Avro\n",
    ")\n",
    "string_serializer = StringSerializer('utf-8')\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Configuration producteur\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "producer_conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'acks': 'all'\n",
    "}\n",
    "producer = Producer(producer_conf)\n",
    "\n",
    "# Callback de confirmation\n",
    "def delivery_report(err, msg):\n",
    "    if err:\n",
    "        print(f\"âŒ Erreur: {err}\")\n",
    "    else:\n",
    "        print(f\"âœ… Message livrÃ©: {msg.topic()}[{msg.partition()}] @ {msg.offset()}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Envoyer des messages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    order = {\n",
    "        \"order_id\": f\"ORD-{i:04d}\",\n",
    "        \"customer_id\": f\"CUST-{i % 3:03d}\",\n",
    "        \"amount\": 100.0 + i * 25.5,\n",
    "        \"timestamp\": int(time.time() * 1000)\n",
    "    }\n",
    "    \n",
    "    producer.produce(\n",
    "        topic='orders-avro',\n",
    "        key=string_serializer(order[\"customer_id\"]),\n",
    "        value=avro_serializer(order, SerializationContext('orders-avro', MessageField.VALUE)),\n",
    "        callback=delivery_report\n",
    "    )\n",
    "    producer.poll(0)  # DÃ©clencher les callbacks\n",
    "\n",
    "producer.flush()  # Attendre que tous les messages soient envoyÃ©s\n",
    "'''\n",
    "\n",
    "print(producer_avro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 2 : Producteur et Consommateur Python\n",
    "\n",
    "**Objectif** : CrÃ©er un systÃ¨me d'alertes en temps rÃ©el.\n",
    "\n",
    "```python\n",
    "# 1. CrÃ©er un producteur qui envoie des logs au format:\n",
    "# {\"level\": \"INFO|WARN|ERROR|FATAL\", \"message\": \"...\", \"timestamp\": \"...\"}\n",
    "\n",
    "# 2. CrÃ©er un consommateur qui:\n",
    "#    - Affiche tous les messages\n",
    "#    - Alerte (print spÃ©cial) si level == \"FATAL\"\n",
    "\n",
    "# TODO: ImplÃ©menter\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "# Producer\n",
    "import random\n",
    "levels = [\"INFO\", \"INFO\", \"INFO\", \"WARN\", \"ERROR\", \"FATAL\"]\n",
    "for i in range(20):\n",
    "    log = {\n",
    "        \"level\": random.choice(levels),\n",
    "        \"message\": f\"Event {i}\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    producer.send('logs', value=log)\n",
    "\n",
    "# Consumer\n",
    "for msg in consumer:\n",
    "    log = msg.value\n",
    "    if log['level'] == 'FATAL':\n",
    "        print(f\"ğŸš¨ FATAL ALERT: {log['message']}\")\n",
    "    else:\n",
    "        print(f\"[{log['level']}] {log['message']}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 AperÃ§u de Faust (Stream Processing Python)\n",
    "\n",
    "**Faust** est un framework Python pour le traitement de streams, inspirÃ© de Kafka Streams (Java). IdÃ©al pour du traitement lÃ©ger sans Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple Faust (aperÃ§u)\n",
    "\n",
    "faust_example = '''\n",
    "import faust\n",
    "\n",
    "# CrÃ©er l'application Faust\n",
    "app = faust.App(\n",
    "    'order-processor',\n",
    "    broker='kafka://localhost:9092',\n",
    "    value_serializer='json'\n",
    ")\n",
    "\n",
    "# DÃ©finir le schÃ©ma du message\n",
    "class Order(faust.Record):\n",
    "    order_id: str\n",
    "    customer_id: str\n",
    "    amount: float\n",
    "\n",
    "# Topic source\n",
    "orders_topic = app.topic('orders', value_type=Order)\n",
    "\n",
    "# Agent de traitement (comme un consumer intelligent)\n",
    "@app.agent(orders_topic)\n",
    "async def process_orders(orders):\n",
    "    async for order in orders:\n",
    "        print(f\"Processing: {order.order_id}\")\n",
    "        \n",
    "        # Logique mÃ©tier\n",
    "        if order.amount > 1000:\n",
    "            print(f\"âš ï¸ High-value order: {order.amount}\")\n",
    "            # Envoyer vers un autre topic\n",
    "            await high_value_topic.send(value=order)\n",
    "\n",
    "# Table pour agrÃ©gation (state)\n",
    "order_counts = app.Table('order-counts', default=int)\n",
    "\n",
    "@app.agent(orders_topic)\n",
    "async def count_by_customer(orders):\n",
    "    async for order in orders:\n",
    "        order_counts[order.customer_id] += 1\n",
    "        print(f\"{order.customer_id}: {order_counts[order.customer_id]} orders\")\n",
    "\n",
    "# Lancer avec: faust -A myapp worker -l info\n",
    "'''\n",
    "\n",
    "print(faust_example)\n",
    "print(\"\\nğŸ’¡ Quand utiliser Faust vs Spark SSS :\")\n",
    "print(\"â€¢ Faust : Traitement lÃ©ger, alertes, routing, < 100K events/s\")\n",
    "print(\"â€¢ Spark SSS : AgrÃ©gations complexes, ML, joins, gros volumes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ 5. Spark Structured Streaming (SSS)\n",
    "\n",
    "### 5.1 Le modÃ¨le de programmation\n",
    "\n",
    "L'idÃ©e gÃ©niale de Spark Structured Streaming : **traiter un stream comme un DataFrame qui grandit Ã  l'infini**.\n",
    "\n",
    "```\n",
    "                     Temps â†’\n",
    "                     \n",
    "Stream d'Ã©vÃ©nements: [e1] [e2] [e3] [e4] [e5] [e6] ...\n",
    "                      â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n",
    "                      â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              DataFrame \"illimitÃ©\"                   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚ e1  â”‚ e2  â”‚ e3  â”‚ e4  â”‚ e5  â”‚ e6  â”‚ ... â”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                     â”‚\n",
    "â”‚  Tu Ã©cris le mÃªme code que pour un batch !          â”‚\n",
    "â”‚  df.filter().groupBy().agg()                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Avantage** : Tu utilises la mÃªme API DataFrame que tu connais dÃ©jÃ  !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Spark pour le streaming avec Kafka\n",
    "\n",
    "spark_streaming_config = '''\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka Streaming Demo\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RÃ©duire les logs pour plus de clartÃ©\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "'''\n",
    "\n",
    "print(spark_streaming_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Lire depuis Kafka avec readStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire un stream Kafka\n",
    "\n",
    "read_kafka = '''\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Lecture du stream Kafka\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"orders\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Le DataFrame brut contient ces colonnes :\n",
    "# key (binary), value (binary), topic, partition, offset, timestamp, timestampType\n",
    "\n",
    "print(\"SchÃ©ma Kafka brut:\")\n",
    "kafka_df.printSchema()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# DÃ©sÃ©rialisation JSON\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# DÃ©finir le schÃ©ma du message JSON\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Parser le JSON\n",
    "orders_df = kafka_df \\\n",
    "    .selectExpr(\"CAST(key AS STRING) as customer_key\", \"CAST(value AS STRING) as json_value\") \\\n",
    "    .select(\n",
    "        col(\"customer_key\"),\n",
    "        from_json(col(\"json_value\"), order_schema).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"customer_key\", \"data.*\")\n",
    "\n",
    "print(\"SchÃ©ma aprÃ¨s parsing:\")\n",
    "orders_df.printSchema()\n",
    "'''\n",
    "\n",
    "print(read_kafka)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Output Modes et Sinks\n",
    "\n",
    "| Output Mode | Description | Quand l'utiliser |\n",
    "|-------------|-------------|------------------|\n",
    "| **Append** | Seulement les nouvelles lignes | Sans agrÃ©gation |\n",
    "| **Complete** | Toute la table rÃ©sultat | Avec agrÃ©gation, petits rÃ©sultats |\n",
    "| **Update** | Seulement les lignes modifiÃ©es | Avec agrÃ©gation, grands rÃ©sultats |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰crire le stream (diffÃ©rents sinks)\n",
    "\n",
    "write_stream = '''\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Sink Console (pour debug)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "query_console = orders_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Sink Parquet/Delta avec Checkpointing\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "query_delta = orders_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/orders\") \\\n",
    "    .option(\"path\", \"s3a://silver/orders_streaming/\") \\\n",
    "    .start()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Sink Kafka (pour pipeline)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "query_kafka = orders_df \\\n",
    "    .selectExpr(\"customer_id AS key\", \"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"topic\", \"orders-processed\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/orders-kafka\") \\\n",
    "    .start()\n",
    "\n",
    "# Attendre la terminaison\n",
    "query_console.awaitTermination()\n",
    "'''\n",
    "\n",
    "print(write_stream)\n",
    "print(\"\\nğŸ’¡ Le checkpointLocation est CRUCIAL pour :\")\n",
    "print(\"â€¢ Reprendre aprÃ¨s un crash (exactly-once)\")\n",
    "print(\"â€¢ Stocker l'Ã©tat des agrÃ©gations\")\n",
    "print(\"â€¢ Suivre les offsets Kafka\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 3 : Pipeline Kafka â†’ Spark SSS â†’ Console\n",
    "\n",
    "```python\n",
    "# 1. Lire le topic 'events' crÃ©Ã© Ã  l'exercice 1\n",
    "# 2. Parser le JSON\n",
    "# 3. Filtrer les events avec level = 'ERROR' ou 'FATAL'\n",
    "# 4. Afficher dans la console\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "schema = StructType([\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"message\", StringType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "events_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"events\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .filter(col(\"level\").isin(\"ERROR\", \"FATAL\"))\n",
    "\n",
    "query = events_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â° 6. Gestion du Temps et de l'Ã‰tat\n",
    "\n",
    "### 6.1 Le problÃ¨me du temps d'Ã©vÃ©nement\n",
    "\n",
    "En streaming, il y a **deux temps** diffÃ©rents :\n",
    "\n",
    "| Temps | Description | Exemple |\n",
    "|-------|-------------|--------|\n",
    "| **Event Time** | Quand l'Ã©vÃ©nement s'est produit | Timestamp dans le message |\n",
    "| **Processing Time** | Quand Spark traite l'Ã©vÃ©nement | Heure du serveur |\n",
    "\n",
    "```\n",
    "ProblÃ¨me des messages dÃ©sordonnÃ©s :\n",
    "\n",
    "Event Time:    10:00   10:01   10:02   10:03   10:04\n",
    "                 â”‚       â”‚       â”‚       â”‚       â”‚\n",
    "                 â–¼       â–¼       â–¼       â–¼       â–¼\n",
    "ArrivÃ©e:       [e1]    [e3]    [e2]    [e5]    [e4]   â† DÃ©sordonnÃ©s !\n",
    "                 â”‚       â”‚       â”‚       â”‚       â”‚\n",
    "Processing:   10:05   10:05   10:06   10:06   10:07\n",
    "\n",
    "Si tu agrÃ¨ges par Processing Time â†’ rÃ©sultat faux\n",
    "Si tu agrÃ¨ges par Event Time â†’ rÃ©sultat correct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Windowing (FenÃªtrage temporel)\n",
    "\n",
    "Pour agrÃ©ger sur le temps, on utilise des **fenÃªtres** :\n",
    "\n",
    "```\n",
    "TUMBLING WINDOW (non-chevauchantes)     SLIDING WINDOW (chevauchantes)\n",
    "\n",
    "   [â”€â”€â”€â”€5minâ”€â”€â”€â”€][â”€â”€â”€â”€5minâ”€â”€â”€â”€]         [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n",
    "                                             [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n",
    "   10:00      10:05      10:10                  [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n",
    "                                        \n",
    "   Chaque event appartient Ã             Chaque event peut appartenir\n",
    "   UNE SEULE fenÃªtre                    Ã  PLUSIEURS fenÃªtres\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windowing avec Spark SSS\n",
    "\n",
    "windowing_example = '''\n",
    "from pyspark.sql.functions import window, col, count, sum as spark_sum, to_timestamp\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PrÃ©parer le timestamp d'Ã©vÃ©nement\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "orders_with_ts = orders_df \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Tumbling Window : AgrÃ©gation par fenÃªtre de 5 minutes\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "tumbling_agg = orders_with_ts \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),  # FenÃªtre de 5 min\n",
    "        col(\"customer_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\"),\n",
    "        spark_sum(\"amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Sliding Window : FenÃªtre de 10 min, glissant toutes les 5 min\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sliding_agg = orders_with_ts \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"),  # 10 min, slide 5 min\n",
    "        col(\"customer_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    )\n",
    "\n",
    "# Le rÃ©sultat contient une colonne \"window\" avec start/end\n",
    "'''\n",
    "\n",
    "print(windowing_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Watermarks : GÃ©rer les donnÃ©es en retard\n",
    "\n",
    "**ProblÃ¨me** : Combien de temps attendre les messages en retard avant de fermer une fenÃªtre ?\n",
    "\n",
    "```\n",
    "Sans Watermark :                    Avec Watermark (10 min) :\n",
    "                                    \n",
    "Ã‰tat infini ! ğŸ˜±                    Ã‰tat limitÃ© âœ…\n",
    "                                    \n",
    "FenÃªtre 10:00-10:05                 FenÃªtre 10:00-10:05\n",
    "  â””â”€ Garde l'Ã©tat POUR TOUJOURS       â””â”€ Ferme Ã  10:15 (event time)\n",
    "     en attendant les retards            Late data aprÃ¨s â†’ ignorÃ©\n",
    "                                    \n",
    "MÃ©moire : EXPLOSE ğŸ’¥               MÃ©moire : Stable âœ…\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watermarks avec Spark SSS\n",
    "\n",
    "watermark_example = '''\n",
    "from pyspark.sql.functions import window, col, count, to_timestamp\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# AgrÃ©gation avec Watermark\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Le watermark dit : \"Je tolÃ¨re jusqu'Ã  10 minutes de retard\"\n",
    "# AprÃ¨s 10 min, les donnÃ©es en retard sont ignorÃ©es et l'Ã©tat nettoyÃ©\n",
    "\n",
    "windowed_counts = orders_with_ts \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\  # â† Crucial !\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),\n",
    "        col(\"customer_id\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Comment Ã§a marche ?\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "# 1. Spark suit le \"max event time\" vu jusqu'ici\n",
    "# 2. Watermark = max_event_time - 10 minutes\n",
    "# 3. Les fenÃªtres dont window.end < watermark sont finalisÃ©es\n",
    "# 4. Les donnÃ©es avec event_time < watermark sont ignorÃ©es\n",
    "#\n",
    "# Exemple :\n",
    "# - Max event time vu : 10:30\n",
    "# - Watermark : 10:20\n",
    "# - FenÃªtre 10:00-10:05 : FINALISÃ‰E (end 10:05 < 10:20)\n",
    "# - Message avec event_time 10:18 : ACCEPTÃ‰\n",
    "# - Message avec event_time 10:15 : IGNORÃ‰ (< watermark)\n",
    "\n",
    "# Ã‰crire en mode Update (pour les agrÃ©gations)\n",
    "query = windowed_counts.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "print(watermark_example)\n",
    "print(\"\\nğŸ’¡ Choisir le bon watermark :\")\n",
    "print(\"â€¢ Trop court (1 min) â†’ Perd des donnÃ©es en retard\")\n",
    "print(\"â€¢ Trop long (1 heure) â†’ Trop de mÃ©moire utilisÃ©e\")\n",
    "print(\"â€¢ RÃ¨gle : Analyser le retard typique de tes donnÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 4 : AgrÃ©gation avec Watermark\n",
    "\n",
    "```python\n",
    "# Calculer le nombre d'erreurs par fenÃªtre de 5 minutes\n",
    "# avec un watermark de 10 minutes\n",
    "\n",
    "# TODO: Lire depuis 'events', filtrer ERROR/FATAL, agrÃ©ger par window\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "error_counts = events_df \\\n",
    "    .filter(col(\"level\").isin(\"ERROR\", \"FATAL\")) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(window(col(\"event_time\"), \"5 minutes\")) \\\n",
    "    .count()\n",
    "\n",
    "query = error_counts.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— 7. OpÃ©rations AvancÃ©es et Sinks Transactionnelles\n",
    "\n",
    "### 7.1 Streaming Joins\n",
    "\n",
    "Spark SSS supporte plusieurs types de joins :\n",
    "\n",
    "| Type | Description | Exemple |\n",
    "|------|-------------|--------|\n",
    "| **Stream-Static** | Stream JOIN table batch | Enrichir orders avec customers |\n",
    "| **Stream-Stream** | Deux streams | JOIN clicks avec impressions |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming Joins\n",
    "\n",
    "join_examples = '''\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Stream-Static Join : Enrichir avec une dimension\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Table statique (dimension)\n",
    "customers_df = spark.read.parquet(\"s3a://dims/customers/\")\n",
    "\n",
    "# Stream\n",
    "orders_stream = spark.readStream.format(\"kafka\")...\n",
    "\n",
    "# Join\n",
    "enriched_orders = orders_stream.join(\n",
    "    customers_df,\n",
    "    orders_stream.customer_id == customers_df.id,\n",
    "    \"left\"  # LEFT JOIN pour garder toutes les commandes\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Stream-Stream Join : Deux flux\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Deux streams\n",
    "clicks_stream = spark.readStream.format(\"kafka\").option(\"subscribe\", \"clicks\")...\n",
    "impressions_stream = spark.readStream.format(\"kafka\").option(\"subscribe\", \"impressions\")...\n",
    "\n",
    "# Join avec watermark (obligatoire pour stream-stream)\n",
    "clicks_with_wm = clicks_stream \\\n",
    "    .withWatermark(\"click_time\", \"10 minutes\")\n",
    "\n",
    "impressions_with_wm = impressions_stream \\\n",
    "    .withWatermark(\"impression_time\", \"10 minutes\")\n",
    "\n",
    "# Join avec condition de temps\n",
    "matched = clicks_with_wm.join(\n",
    "    impressions_with_wm,\n",
    "    expr(\"\"\"\n",
    "        clicks.ad_id = impressions.ad_id AND\n",
    "        click_time >= impression_time AND\n",
    "        click_time <= impression_time + interval 1 hour\n",
    "    \"\"\"),\n",
    "    \"inner\"\n",
    ")\n",
    "'''\n",
    "\n",
    "print(join_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 foreachBatch : Le pont entre streaming et batch\n",
    "\n",
    "`foreachBatch` permet d'appliquer n'importe quelle logique batch sur chaque micro-batch. C'est **crucial** pour les upserts avec MERGE INTO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreachBatch avec MERGE INTO Delta\n",
    "\n",
    "foreach_batch_example = '''\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Fonction de traitement par micro-batch\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def upsert_to_delta(micro_batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    AppelÃ©e pour chaque micro-batch.\n",
    "    micro_batch_df : DataFrame Spark normal (pas streaming)\n",
    "    batch_id : Identifiant unique du batch\n",
    "    \"\"\"\n",
    "    print(f\"Processing batch {batch_id} with {micro_batch_df.count()} records\")\n",
    "    \n",
    "    # VÃ©rifier si la table existe\n",
    "    if DeltaTable.isDeltaTable(spark, \"s3a://silver/customers/\"):\n",
    "        # Table existe â†’ MERGE (upsert)\n",
    "        target = DeltaTable.forPath(spark, \"s3a://silver/customers/\")\n",
    "        \n",
    "        target.alias(\"target\").merge(\n",
    "            micro_batch_df.alias(\"source\"),\n",
    "            \"target.customer_id = source.customer_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\n",
    "                \"name\": \"source.name\",\n",
    "                \"email\": \"source.email\",\n",
    "                \"updated_at\": \"current_timestamp()\"\n",
    "            }\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"customer_id\": \"source.customer_id\",\n",
    "                \"name\": \"source.name\",\n",
    "                \"email\": \"source.email\",\n",
    "                \"created_at\": \"current_timestamp()\",\n",
    "                \"updated_at\": \"current_timestamp()\"\n",
    "            }\n",
    "        ).execute()\n",
    "    else:\n",
    "        # Table n'existe pas â†’ CREATE\n",
    "        micro_batch_df.write \\\n",
    "            .format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(\"s3a://silver/customers/\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Utiliser foreachBatch\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "query = customers_stream.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/customers-upsert\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "'''\n",
    "\n",
    "print(foreach_batch_example)\n",
    "print(\"\\nğŸ’¡ Avantages de foreachBatch :\")\n",
    "print(\"â€¢ Utiliser MERGE INTO (upserts)\")\n",
    "print(\"â€¢ Ã‰crire vers plusieurs sinks\")\n",
    "print(\"â€¢ Appliquer n'importe quelle logique batch\")\n",
    "print(\"â€¢ Exactly-once avec checkpointing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 5 : Upsert streaming avec foreachBatch\n",
    "\n",
    "```python\n",
    "# CrÃ©er un pipeline qui :\n",
    "# 1. Lit des updates de statut de commande depuis Kafka\n",
    "# 2. Fait un MERGE INTO vers une table Delta 'order_status'\n",
    "# 3. Met Ã  jour le statut si la commande existe, sinon insÃ¨re\n",
    "```\n",
    "\n",
    "<details><summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```python\n",
    "def upsert_order_status(df, batch_id):\n",
    "    if DeltaTable.isDeltaTable(spark, \"/tmp/order_status\"):\n",
    "        target = DeltaTable.forPath(spark, \"/tmp/order_status\")\n",
    "        target.alias(\"t\").merge(\n",
    "            df.alias(\"s\"), \"t.order_id = s.order_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set={\"status\": \"s.status\", \"updated_at\": \"current_timestamp()\"}\n",
    "        ).whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        df.write.format(\"delta\").save(\"/tmp/order_status\")\n",
    "\n",
    "status_stream.writeStream \\\n",
    "    .foreachBatch(upsert_order_status) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/cp/status\") \\\n",
    "    .start()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”Œ 8. Kafka Connect & Debezium (AperÃ§u)\n",
    "\n",
    "### 8.1 Qu'est-ce que Kafka Connect ?\n",
    "\n",
    "**Kafka Connect** est un framework pour connecter Kafka Ã  d'autres systÃ¨mes **sans code** :\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     KAFKA CONNECT                               â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   SOURCES                      SINKS                           â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚   â”‚ PostgreSQL   â”‚             â”‚ Elasticsearchâ”‚                â”‚\n",
    "â”‚   â”‚ MySQL        â”‚ â”€â”€â–¶ KAFKA â”€â”€â–¶â”‚ S3           â”‚                â”‚\n",
    "â”‚   â”‚ MongoDB      â”‚             â”‚ Snowflake    â”‚                â”‚\n",
    "â”‚   â”‚ Files (CSV)  â”‚             â”‚ BigQuery     â”‚                â”‚\n",
    "â”‚   â”‚ APIs         â”‚             â”‚ Redis        â”‚                â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Configuration JSON, pas de code !                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 8.2 Debezium : CDC temps rÃ©el\n",
    "\n",
    "**Debezium** est un connecteur Kafka Connect pour le **Change Data Capture** :\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   PostgreSQL   â”‚         â”‚   Debezium   â”‚         â”‚   Kafka    â”‚\n",
    "â”‚                â”‚         â”‚              â”‚         â”‚            â”‚\n",
    "â”‚   WAL logs â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Connector   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Topic     â”‚\n",
    "â”‚   (changes)    â”‚         â”‚              â”‚         â”‚  per table â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Chaque INSERT/UPDATE/DELETE â†’ Message Kafka automatique !\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de configuration Debezium\n",
    "\n",
    "debezium_config = '''\n",
    "{\n",
    "  \"name\": \"postgres-connector\",\n",
    "  \"config\": {\n",
    "    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "    \"database.hostname\": \"postgres\",\n",
    "    \"database.port\": \"5432\",\n",
    "    \"database.user\": \"debezium\",\n",
    "    \"database.password\": \"secret\",\n",
    "    \"database.dbname\": \"inventory\",\n",
    "    \"database.server.name\": \"dbserver1\",\n",
    "    \"table.include.list\": \"public.customers,public.orders\",\n",
    "    \"plugin.name\": \"pgoutput\",\n",
    "    \n",
    "    \"transforms\": \"unwrap\",\n",
    "    \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n",
    "    \"transforms.unwrap.drop.tombstones\": \"false\"\n",
    "  }\n",
    "}\n",
    "'''\n",
    "\n",
    "print(\"Configuration Debezium pour PostgreSQL :\")\n",
    "print(debezium_config)\n",
    "print(\"\\nğŸ’¡ Quand utiliser Debezium vs code custom :\")\n",
    "print(\"â€¢ Debezium : CDC standard depuis DB, pas de code Ã  maintenir\")\n",
    "print(\"â€¢ Code custom : Logique complexe, sources non supportÃ©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š 9. DÃ©ploiement & ObservabilitÃ©\n",
    "\n",
    "### 9.1 DÃ©ployer SSS sur Kubernetes\n",
    "\n",
    "Pour la production, on utilise le **Spark Operator** (voir Module 21) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkApplication pour un job streaming\n",
    "\n",
    "spark_streaming_k8s = '''\n",
    "apiVersion: sparkoperator.k8s.io/v1beta2\n",
    "kind: SparkApplication\n",
    "metadata:\n",
    "  name: kafka-to-delta-streaming\n",
    "  namespace: spark\n",
    "spec:\n",
    "  type: Python\n",
    "  pythonVersion: \"3\"\n",
    "  mode: cluster\n",
    "  image: my-registry/spark-streaming:latest\n",
    "  mainApplicationFile: s3a://code/streaming_job.py\n",
    "  sparkVersion: \"3.5.0\"\n",
    "  \n",
    "  # Important pour le streaming !\n",
    "  restartPolicy:\n",
    "    type: Always  # RedÃ©marrer automatiquement si crash\n",
    "    onFailureRetries: 3\n",
    "    onFailureRetryInterval: 60\n",
    "  \n",
    "  driver:\n",
    "    cores: 1\n",
    "    memory: \"2g\"\n",
    "  \n",
    "  executor:\n",
    "    cores: 2\n",
    "    instances: 3\n",
    "    memory: \"4g\"\n",
    "  \n",
    "  deps:\n",
    "    packages:\n",
    "      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\n",
    "      - io.delta:delta-spark_2.12:3.1.0\n",
    "'''\n",
    "\n",
    "print(spark_streaming_k8s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 MÃ©triques Ã  surveiller\n",
    "\n",
    "| MÃ©trique | Description | Seuil d'alerte |\n",
    "|----------|-------------|----------------|\n",
    "| **Input Rate** | Messages/sec entrants | - |\n",
    "| **Processing Rate** | Messages/sec traitÃ©s | < Input Rate |\n",
    "| **Batch Duration** | Temps de traitement | > Trigger interval |\n",
    "| **Backlog** | Messages en attente | Croissant |\n",
    "| **Watermark Lag** | Retard du watermark | > Seuil attendu |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring du streaming\n",
    "\n",
    "monitoring = '''\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# AccÃ©der aux mÃ©triques dans le code\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "query = df.writeStream...\n",
    "\n",
    "# Progression du dernier batch\n",
    "print(query.lastProgress)\n",
    "\n",
    "# Status actuel\n",
    "print(query.status)\n",
    "\n",
    "# Exemple de lastProgress :\n",
    "# {\n",
    "#   \"id\": \"abc123\",\n",
    "#   \"runId\": \"def456\",\n",
    "#   \"batchId\": 42,\n",
    "#   \"numInputRows\": 1000,\n",
    "#   \"inputRowsPerSecond\": 500.0,\n",
    "#   \"processedRowsPerSecond\": 450.0,\n",
    "#   \"durationMs\": {\n",
    "#     \"triggerExecution\": 2000,\n",
    "#     \"getBatch\": 100,\n",
    "#     \"queryPlanning\": 50\n",
    "#   },\n",
    "#   \"eventTime\": {\n",
    "#     \"watermark\": \"2024-01-15T10:20:00.000Z\"\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Listener pour mÃ©triques custom\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class MetricsListener(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        print(f\"Query started: {event.id}\")\n",
    "    \n",
    "    def onQueryProgress(self, event):\n",
    "        # Envoyer vers Prometheus/Grafana\n",
    "        metrics.gauge(\"streaming_input_rate\", event.progress.inputRowsPerSecond)\n",
    "        metrics.gauge(\"streaming_batch_duration\", event.progress.durationMs[\"triggerExecution\"])\n",
    "    \n",
    "    def onQueryTerminated(self, event):\n",
    "        print(f\"Query terminated: {event.id}\")\n",
    "\n",
    "spark.streams.addListener(MetricsListener())\n",
    "'''\n",
    "\n",
    "print(monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ 10. Mini-Projet : Pipeline Temps RÃ©el Complet\n",
    "\n",
    "### ğŸ¯ Objectif\n",
    "\n",
    "Construire un pipeline d'ingestion transactionnel : **Python Producer â†’ Kafka â†’ Spark SSS â†’ Delta Lake**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Producteur   â”‚   â”‚     Kafka     â”‚   â”‚ Spark Structured â”‚   â”‚    Delta     â”‚\n",
    "â”‚   (Python)     â”‚â”€â”€â–¶â”‚   (Docker)    â”‚â”€â”€â–¶â”‚    Streaming     â”‚â”€â”€â–¶â”‚    Lake      â”‚\n",
    "â”‚                â”‚   â”‚               â”‚   â”‚                  â”‚   â”‚              â”‚\n",
    "â”‚ Simule des     â”‚   â”‚ Topic:        â”‚   â”‚ â€¢ Watermark      â”‚   â”‚ â€¢ MERGE INTO â”‚\n",
    "â”‚ transactions   â”‚   â”‚ transactions  â”‚   â”‚ â€¢ Window 5 min   â”‚   â”‚ â€¢ Silver     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰TAPE 1 : Producteur Python\n",
    "\n",
    "producer_code = '''\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[\"localhost:9092\"],\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "customers = [\"CUST-001\", \"CUST-002\", \"CUST-003\", \"CUST-004\", \"CUST-005\"]\n",
    "products = [\"Laptop\", \"Phone\", \"Tablet\", \"Watch\", \"Headphones\"]\n",
    "\n",
    "print(\"ğŸš€ Sending transactions...\")\n",
    "\n",
    "for i in range(100):\n",
    "    transaction = {\n",
    "        \"transaction_id\": f\"TXN-{i:06d}\",\n",
    "        \"customer_id\": random.choice(customers),\n",
    "        \"product\": random.choice(products),\n",
    "        \"amount\": round(random.uniform(10, 500), 2),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    producer.send(\"transactions\", value=transaction)\n",
    "    print(f\"Sent: {transaction[\\'transaction_id\\']}\")\n",
    "    time.sleep(0.5)  # Simuler un flux\n",
    "\n",
    "producer.flush()\n",
    "print(\"âœ… Done!\")\n",
    "'''\n",
    "\n",
    "print(\"# producer.py\")\n",
    "print(producer_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰TAPE 2 : Job Spark Streaming\n",
    "\n",
    "streaming_job = '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, sum as spark_sum, count, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Realtime Transactions\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "            \"io.delta:delta-spark_2.12:3.1.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SchÃ©ma\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"product\", StringType()),\n",
    "    StructField(\"amount\", DoubleType()),\n",
    "    StructField(\"timestamp\", StringType())\n",
    "])\n",
    "\n",
    "# Lire Kafka\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parser JSON\n",
    "transactions = raw_stream \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\") \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
    "\n",
    "# AgrÃ©gation par fenÃªtre de 5 min avec watermark 10 min\n",
    "windowed_stats = transactions \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),\n",
    "        col(\"customer_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        spark_sum(\"amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "# Fonction pour upsert vers Delta\n",
    "def upsert_to_delta(batch_df, batch_id):\n",
    "    # Flatten window column\n",
    "    flat_df = batch_df.selectExpr(\n",
    "        \"window.start as window_start\",\n",
    "        \"window.end as window_end\",\n",
    "        \"customer_id\",\n",
    "        \"transaction_count\",\n",
    "        \"total_amount\"\n",
    "    )\n",
    "    \n",
    "    if flat_df.count() == 0:\n",
    "        return\n",
    "    \n",
    "    target_path = \"/tmp/delta/customer_stats\"\n",
    "    \n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "        target = DeltaTable.forPath(spark, target_path)\n",
    "        target.alias(\"t\").merge(\n",
    "            flat_df.alias(\"s\"),\n",
    "            \"t.window_start = s.window_start AND t.customer_id = s.customer_id\"\n",
    "        ).whenMatchedUpdate(set={\n",
    "            \"transaction_count\": \"s.transaction_count\",\n",
    "            \"total_amount\": \"s.total_amount\"\n",
    "        }).whenNotMatchedInsertAll().execute()\n",
    "    else:\n",
    "        flat_df.write.format(\"delta\").save(target_path)\n",
    "    \n",
    "    print(f\"Batch {batch_id}: Processed {flat_df.count()} records\")\n",
    "\n",
    "# Lancer le stream\n",
    "query = windowed_stats.writeStream \\\n",
    "    .foreachBatch(upsert_to_delta) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/customer_stats\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "'''\n",
    "\n",
    "print(\"# streaming_job.py\")\n",
    "print(streaming_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Quiz\n",
    "\n",
    "**Q1.** DiffÃ©rence entre Event Time et Processing Time ?\n",
    "<details><summary>R</summary>Event Time = quand l'Ã©vÃ©nement s'est produit. Processing Time = quand Spark le traite.</details>\n",
    "\n",
    "**Q2.** RÃ´le du Watermark ?\n",
    "<details><summary>R</summary>DÃ©finir la tolÃ©rance au retard et permettre le nettoyage de l'Ã©tat.</details>\n",
    "\n",
    "**Q3.** DiffÃ©rence At-least-once vs Exactly-once ?\n",
    "<details><summary>R</summary>At-least-once peut dupliquer. Exactly-once garantit un seul traitement.</details>\n",
    "\n",
    "**Q4.** Pourquoi utiliser foreachBatch ?\n",
    "<details><summary>R</summary>Pour appliquer une logique batch (MERGE INTO) sur chaque micro-batch.</details>\n",
    "\n",
    "**Q5.** Qu'est-ce qu'un Consumer Group ?\n",
    "<details><summary>R</summary>Groupe de consumers qui se partagent les partitions pour parallÃ©liser.</details>\n",
    "\n",
    "**Q6.** Tumbling vs Sliding Window ?\n",
    "<details><summary>R</summary>Tumbling = non-chevauchantes. Sliding = chevauchantes.</details>\n",
    "\n",
    "**Q7.** RÃ´le du checkpointLocation ?\n",
    "<details><summary>R</summary>Stocker les offsets et l'Ã©tat pour recovery et exactly-once.</details>\n",
    "\n",
    "**Q8.** Quand utiliser Debezium ?\n",
    "<details><summary>R</summary>Pour du CDC temps rÃ©el depuis une base de donnÃ©es vers Kafka.</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "- [Kafka Documentation](https://kafka.apache.org/documentation/)\n",
    "- [Spark Structured Streaming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n",
    "- [Debezium Documentation](https://debezium.io/documentation/)\n",
    "- [Faust Documentation](https://faust.readthedocs.io/)\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ğŸ‘‰ **Module 25 : `25_dbt_data_quality.ipynb`** â€” dbt + Data Quality\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ RÃ©capitulatif\n",
    "\n",
    "| Concept | Appris |\n",
    "|---------|--------|\n",
    "| **Kafka** | Topics, Partitions, Offsets, Consumer Groups |\n",
    "| **Python Kafka** | kafka-python, confluent-kafka, Faust |\n",
    "| **Spark SSS** | readStream, writeStream, Output Modes |\n",
    "| **Temps** | Event Time, Watermarks, Windowing |\n",
    "| **AvancÃ©** | foreachBatch, MERGE INTO, Stream Joins |\n",
    "| **Ops** | Checkpointing, Monitoring, K8s |\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu maÃ®trises maintenant le streaming de donnÃ©es."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
