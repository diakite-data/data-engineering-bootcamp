{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Spark SQL Deep Dive & Optimisation\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser **Spark SQL** en profondeur. Tu dÃ©couvriras les Window Functions, les agrÃ©gations avancÃ©es, le reshaping de donnÃ©es, et comment optimiser tes requÃªtes SQL.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Module 19 : PySpark Advanced |\n",
    "| âœ… Requis | Connaissances SQL de base |\n",
    "| ğŸ’¡ RecommandÃ© | ExpÃ©rience avec des requÃªtes analytiques |\n",
    "\n",
    "## Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- MaÃ®triser les **Window Functions** (ranking, lag/lead, frames)\n",
    "- Utiliser **PIVOT/UNPIVOT** pour reshaper les donnÃ©es\n",
    "- Appliquer **GROUPING SETS, CUBE, ROLLUP** pour des agrÃ©gations multidimensionnelles\n",
    "- Manipuler les donnÃ©es semi-structurÃ©es avec **EXPLODE**\n",
    "- Structurer des requÃªtes complexes avec **CTEs**\n",
    "- **Optimiser** les requÃªtes SQL (hints, statistiques)\n",
    "- Construire un **datamart analytique** complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# CrÃ©er une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL Deep Dive\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark {spark.version} initialisÃ©\")\n",
    "print(f\"ğŸ” Spark UI : {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamentals",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. SQL dans Spark : Rappels & Fondamentaux\n",
    "\n",
    "> âš¡ **Rappel rapide** â€” Les dÃ©tails sur Catalyst et l'architecture sont dans le module 19.\n",
    "\n",
    "### 1.1 CrÃ©er et utiliser des vues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "views_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des donnÃ©es de test\n",
    "sales_data = [\n",
    "    (1, \"Electronics\", \"North\", 1200, \"2024-01-15\"),\n",
    "    (2, \"Electronics\", \"South\", 800, \"2024-01-16\"),\n",
    "    (3, \"Clothing\", \"North\", 450, \"2024-01-15\"),\n",
    "    (4, \"Clothing\", \"South\", 650, \"2024-01-17\"),\n",
    "    (5, \"Electronics\", \"North\", 950, \"2024-01-18\"),\n",
    "    (6, \"Food\", \"South\", 320, \"2024-01-15\"),\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, \n",
    "    [\"id\", \"category\", \"region\", \"amount\", \"date\"])\n",
    "\n",
    "# CrÃ©er une vue temporaire\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Utiliser SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT category, SUM(amount) as total_sales\n",
    "    FROM sales\n",
    "    WHERE amount > 500\n",
    "    GROUP BY category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df_vs_sql",
   "metadata": {},
   "source": [
    "### 1.2 DataFrame API vs SQL\n",
    "\n",
    "| Aspect | DataFrame API | SQL |\n",
    "|--------|---------------|-----|\n",
    "| **Typage** | Compile-time | Runtime |\n",
    "| **LisibilitÃ©** | Code Python | Familier aux analystes |\n",
    "| **RÃ©utilisabilitÃ©** | Fonctions Python | CTEs, Vues |\n",
    "| **Performance** | Identique | Identique |\n",
    "| **Debug** | Stack trace Python | Erreurs SQL |\n",
    "| **IDE Support** | AutocomplÃ©tion | Variable |\n",
    "\n",
    "> ğŸ’¡ **RÃ¨gle** : Utilise ce qui est le plus lisible pour ton Ã©quipe. Les deux sont optimisÃ©s par Catalyst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰quivalence DataFrame API vs SQL\n",
    "\n",
    "# === DataFrame API ===\n",
    "result_df = sales_df \\\n",
    "    .filter(col(\"amount\") > 500) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "# === SQL ===\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT category, SUM(amount) as total_sales\n",
    "    FROM sales\n",
    "    WHERE amount > 500\n",
    "    GROUP BY category\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"DataFrame API:\")\n",
    "result_df.show()\n",
    "\n",
    "print(\"SQL:\")\n",
    "result_sql.show()\n",
    "\n",
    "# Les plans sont identiques !\n",
    "print(\"\\nâœ… Les deux approches gÃ©nÃ¨rent le mÃªme plan d'exÃ©cution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_plan",
   "metadata": {},
   "source": [
    "### 1.3 Lire un plan d'exÃ©cution SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des donnÃ©es pour dÃ©montrer les joins\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\"),\n",
    "    (2, \"T-Shirt\", \"Clothing\"),\n",
    "    (3, \"Apple\", \"Food\"),\n",
    "]\n",
    "products_df = spark.createDataFrame(products_data, [\"product_id\", \"name\", \"category\"])\n",
    "products_df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Voir le plan d'exÃ©cution\n",
    "spark.sql(\"\"\"\n",
    "    EXPLAIN FORMATTED\n",
    "    SELECT s.*, p.name\n",
    "    FROM sales s\n",
    "    JOIN products p ON s.category = p.category\n",
    "    WHERE s.amount > 500\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explain_guide",
   "metadata": {},
   "source": [
    "**Ce qu'il faut repÃ©rer dans le plan :**\n",
    "\n",
    "| Ã‰lÃ©ment | Signification | Bon/Mauvais |\n",
    "|---------|---------------|-------------|\n",
    "| `BroadcastHashJoin` | Petite table broadcastÃ©e | âœ… Bon |\n",
    "| `SortMergeJoin` | Shuffle des deux tables | âš ï¸ CoÃ»teux |\n",
    "| `PushedFilters` | Filtre appliquÃ© Ã  la source | âœ… Bon |\n",
    "| `Exchange` | Shuffle (redistribution) | âš ï¸ Ã€ surveiller |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window_functions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Window Functions â€” Le cÅ“ur analytique\n",
    "\n",
    "> ğŸ”¥ **Section la plus importante du module.** Les Window Functions sont essentielles pour l'analytics.\n",
    "\n",
    "### 2.1 Syntaxe et concepts\n",
    "\n",
    "```sql\n",
    "FUNCTION() OVER (\n",
    "  PARTITION BY column1, column2   -- Grouper les donnÃ©es\n",
    "  ORDER BY column3                -- Ordonner dans chaque groupe\n",
    "  ROWS BETWEEN start AND end      -- DÃ©finir la fenÃªtre de calcul\n",
    ")\n",
    "```\n",
    "\n",
    "```text\n",
    "ğŸ–¼ï¸ Comment fonctionne une Window Function :\n",
    "\n",
    "DonnÃ©es originales          PARTITION BY category    ORDER BY date       Calcul sur la fenÃªtre\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ cat â”‚ date â”‚ amtâ”‚         â”‚ Electronics     â”‚     â”‚ 01-15 â”‚ 1200   â”‚  â”‚ ROW_NUMBER = 1  â”‚\n",
    "â”‚ Elecâ”‚ 01-15â”‚1200â”‚   â†’     â”‚ â”œâ”€ 01-15 â”‚ 1200â”‚  â†’  â”‚ 01-16 â”‚  800   â”‚  â”‚ ROW_NUMBER = 2  â”‚\n",
    "â”‚ Elecâ”‚ 01-16â”‚ 800â”‚         â”‚ â”œâ”€ 01-16 â”‚  800â”‚     â”‚ 01-18 â”‚  950   â”‚  â”‚ ROW_NUMBER = 3  â”‚\n",
    "â”‚ Elecâ”‚ 01-18â”‚ 950â”‚         â”‚ â””â”€ 01-18 â”‚  950â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚ Clthâ”‚ 01-15â”‚ 450â”‚         â”‚                 â”‚\n",
    "â”‚ Clthâ”‚ 01-17â”‚ 650â”‚         â”‚ Clothing        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”œâ”€ 01-15 â”‚  450â”‚\n",
    "                            â”‚ â””â”€ 01-17 â”‚  650â”‚\n",
    "                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_data_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des donnÃ©es plus riches pour les window functions\n",
    "orders_data = [\n",
    "    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n",
    "    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n",
    "    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n",
    "    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n",
    "    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n",
    "    (6, 103, \"2024-01-10\", 400.0, \"Premium\"),\n",
    "    (7, 103, \"2024-01-25\", 400.0, \"Premium\"),  # Ex-aequo intentionnel\n",
    "    (8, 103, \"2024-02-10\", 350.0, \"Premium\"),\n",
    "    (9, 104, \"2024-01-03\", 100.0, \"Standard\"),\n",
    "    (10, 104, \"2024-02-15\", 120.0, \"Standard\"),\n",
    "]\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data,\n",
    "    [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\n",
    "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking_functions",
   "metadata": {},
   "source": [
    "### 2.2 Fonctions de Ranking\n",
    "\n",
    "| Fonction | Description | GÃ¨re les ex-aequo |\n",
    "|----------|-------------|-------------------|\n",
    "| `ROW_NUMBER()` | NumÃ©ro unique sÃ©quentiel | Non (arbitraire) |\n",
    "| `RANK()` | Rang avec gaps aprÃ¨s ex-aequo | Oui (1,1,3) |\n",
    "| `DENSE_RANK()` | Rang sans gaps | Oui (1,1,2) |\n",
    "| `NTILE(n)` | Divise en n groupes Ã©gaux | - |\n",
    "| `PERCENT_RANK()` | Rang en percentile (0-1) | Oui |\n",
    "| `CUME_DIST()` | Distribution cumulative | Oui |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DÃ©monstration des fonctions de ranking\n",
    "ranking_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as row_num,\n",
    "        RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as dense_rank,\n",
    "        NTILE(2) OVER (PARTITION BY customer_id ORDER BY amount DESC) as ntile_2\n",
    "    FROM orders\n",
    "    WHERE customer_id = 103\n",
    "    ORDER BY customer_id, amount DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š Comparaison ROW_NUMBER vs RANK vs DENSE_RANK (customer 103 avec ex-aequo):\")\n",
    "ranking_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top_n_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cas d'usage : Top 2 commandes par client\n",
    "top_orders = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            order_date,\n",
    "            amount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rn\n",
    "        FROM orders\n",
    "    )\n",
    "    WHERE rn <= 2\n",
    "    ORDER BY customer_id, rn\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ† Top 2 commandes par client:\")\n",
    "top_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lag_lead",
   "metadata": {},
   "source": [
    "### 2.3 Fonctions de dÃ©calage (LAG/LEAD)\n",
    "\n",
    "| Fonction | Description |\n",
    "|----------|-------------|\n",
    "| `LAG(col, n, default)` | Valeur **n lignes AVANT** |\n",
    "| `LEAD(col, n, default)` | Valeur **n lignes APRÃˆS** |\n",
    "| `FIRST_VALUE(col)` | PremiÃ¨re valeur de la fenÃªtre |\n",
    "| `LAST_VALUE(col)` | DerniÃ¨re valeur de la fenÃªtre |\n",
    "| `NTH_VALUE(col, n)` | N-iÃ¨me valeur de la fenÃªtre |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lag_lead_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cas d'usage : DÃ©lai entre commandes\n",
    "order_gaps = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n",
    "        LEAD(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date,\n",
    "        DATEDIFF(\n",
    "            order_date, \n",
    "            LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date)\n",
    "        ) as days_since_last_order\n",
    "    FROM orders\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“… Analyse du dÃ©lai entre commandes:\")\n",
    "order_gaps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first_last_value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST_VALUE et LAST_VALUE\n",
    "first_last = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        FIRST_VALUE(order_date) OVER (\n",
    "            PARTITION BY customer_id ORDER BY order_date\n",
    "        ) as first_order_date,\n",
    "        FIRST_VALUE(amount) OVER (\n",
    "            PARTITION BY customer_id ORDER BY order_date\n",
    "        ) as first_order_amount\n",
    "    FROM orders\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š FIRST_VALUE - Date et montant de la premiÃ¨re commande:\")\n",
    "first_last.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window_aggregations",
   "metadata": {},
   "source": [
    "### 2.4 AgrÃ©gations dans une fenÃªtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_agg_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running total, Running average, % du total\n",
    "window_agg = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        \n",
    "        -- Somme cumulative\n",
    "        SUM(amount) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY order_date\n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as running_total,\n",
    "        \n",
    "        -- Total du client (pour calculer %)\n",
    "        SUM(amount) OVER (PARTITION BY customer_id) as customer_total,\n",
    "        \n",
    "        -- % de chaque commande\n",
    "        ROUND(amount / SUM(amount) OVER (PARTITION BY customer_id) * 100, 1) as pct_of_customer_total\n",
    "        \n",
    "    FROM orders\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š AgrÃ©gations fenÃªtrÃ©es:\")\n",
    "window_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "window_frames",
   "metadata": {},
   "source": [
    "### 2.5 Window Frames (PiÃ¨ges Spark)\n",
    "\n",
    "```sql\n",
    "ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW  -- Du dÃ©but jusqu'Ã  maintenant\n",
    "ROWS BETWEEN 6 PRECEDING AND CURRENT ROW          -- 7 derniÃ¨res lignes (rolling)\n",
    "ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING  -- De maintenant jusqu'Ã  la fin\n",
    "ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING          -- 3 lignes (prÃ©cÃ©dente, courante, suivante)\n",
    "```\n",
    "\n",
    "| Clause | BasÃ©e sur | Comportement |\n",
    "|--------|-----------|---------------|\n",
    "| **ROWS** | Position physique | PrÃ©visible, recommandÃ© |\n",
    "| **RANGE** | Valeur logique | âš ï¸ Peut inclure plus de lignes si doublons |\n",
    "\n",
    "> ğŸ’¡ **En cas de doute, utilise ROWS** (plus prÃ©visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolling_avg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling average (moyenne mobile)\n",
    "# CrÃ©er plus de donnÃ©es pour dÃ©montrer\n",
    "daily_sales = spark.createDataFrame([\n",
    "    (\"2024-01-01\", 100), (\"2024-01-02\", 150), (\"2024-01-03\", 120),\n",
    "    (\"2024-01-04\", 180), (\"2024-01-05\", 90), (\"2024-01-06\", 200),\n",
    "    (\"2024-01-07\", 170), (\"2024-01-08\", 140), (\"2024-01-09\", 160),\n",
    "    (\"2024-01-10\", 190),\n",
    "], [\"date\", \"sales\"])\n",
    "daily_sales = daily_sales.withColumn(\"date\", to_date(col(\"date\")))\n",
    "daily_sales.createOrReplaceTempView(\"daily_sales\")\n",
    "\n",
    "# Moyenne mobile sur 3 jours\n",
    "rolling = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        sales,\n",
    "        ROUND(AVG(sales) OVER (\n",
    "            ORDER BY date\n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ), 1) as rolling_avg_3d,\n",
    "        SUM(sales) OVER (\n",
    "            ORDER BY date\n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as cumulative_sum\n",
    "    FROM daily_sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“ˆ Moyenne mobile 3 jours et somme cumulative:\")\n",
    "rolling.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pivot_unpivot",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PIVOT & UNPIVOT â€” Reshaping des donnÃ©es\n",
    "\n",
    "### 3.1 PIVOT : lignes â†’ colonnes\n",
    "\n",
    "Transforme les valeurs d'une colonne en colonnes distinctes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DonnÃ©es mensuelles\n",
    "monthly_data = spark.createDataFrame([\n",
    "    (\"Alice\", \"Jan\", 1000), (\"Alice\", \"Feb\", 1200), (\"Alice\", \"Mar\", 1100),\n",
    "    (\"Bob\", \"Jan\", 800), (\"Bob\", \"Feb\", 900), (\"Bob\", \"Mar\", 950),\n",
    "    (\"Charlie\", \"Jan\", 1500), (\"Charlie\", \"Feb\", 1400), (\"Charlie\", \"Mar\", 1600),\n",
    "], [\"salesperson\", \"month\", \"revenue\"])\n",
    "\n",
    "monthly_data.createOrReplaceTempView(\"monthly_sales\")\n",
    "\n",
    "print(\"DonnÃ©es originales (format long):\")\n",
    "monthly_data.show()\n",
    "\n",
    "# PIVOT : transformer les mois en colonnes\n",
    "pivoted = spark.sql(\"\"\"\n",
    "    SELECT * FROM monthly_sales\n",
    "    PIVOT (\n",
    "        SUM(revenue)\n",
    "        FOR month IN ('Jan', 'Feb', 'Mar')\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"AprÃ¨s PIVOT (format large):\")\n",
    "pivoted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pivot_df_api",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIVOT avec DataFrame API\n",
    "pivoted_df = monthly_data.groupBy(\"salesperson\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).sum(\"revenue\")\n",
    "\n",
    "print(\"PIVOT avec DataFrame API:\")\n",
    "pivoted_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unpivot",
   "metadata": {},
   "source": [
    "### 3.2 UNPIVOT : colonnes â†’ lignes\n",
    "\n",
    "L'opÃ©ration inverse de PIVOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unpivot_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une vue du DataFrame pivotÃ©\n",
    "pivoted_df.createOrReplaceTempView(\"pivoted_sales\")\n",
    "\n",
    "# UNPIVOT (Spark 3.4+)\n",
    "# Pour les versions antÃ©rieures, utiliser stack()\n",
    "unpivoted = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        salesperson,\n",
    "        stack(3, \n",
    "            'Jan', Jan, \n",
    "            'Feb', Feb, \n",
    "            'Mar', Mar\n",
    "        ) as (month, revenue)\n",
    "    FROM pivoted_sales\n",
    "\"\"\")\n",
    "\n",
    "print(\"AprÃ¨s UNPIVOT (retour au format long):\")\n",
    "unpivoted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grouping_sets",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. AgrÃ©gations AvancÃ©es â€” GROUPING SETS, CUBE, ROLLUP\n",
    "\n",
    "Ces fonctions permettent de calculer **plusieurs niveaux d'agrÃ©gation** en une seule requÃªte.\n",
    "\n",
    "### 4.1 GROUPING SETS : agrÃ©gations multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouping_sets_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING SETS : plusieurs agrÃ©gations en une requÃªte\n",
    "grouping_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        region,\n",
    "        SUM(amount) as total_sales,\n",
    "        COUNT(*) as num_transactions\n",
    "    FROM sales\n",
    "    GROUP BY GROUPING SETS (\n",
    "        (category, region),  -- Par catÃ©gorie ET rÃ©gion\n",
    "        (category),          -- Par catÃ©gorie seulement\n",
    "        (region),            -- Par rÃ©gion seulement\n",
    "        ()                   -- Total global\n",
    "    )\n",
    "    ORDER BY category NULLS LAST, region NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š GROUPING SETS - AgrÃ©gations multiples:\")\n",
    "grouping_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rollup_cube",
   "metadata": {},
   "source": [
    "### 4.2 ROLLUP : hiÃ©rarchie d'agrÃ©gations\n",
    "\n",
    "`ROLLUP(a, b, c)` = `GROUPING SETS ((a,b,c), (a,b), (a), ())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rollup_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLLUP : hiÃ©rarchie (catÃ©gorie â†’ rÃ©gion â†’ total)\n",
    "rollup_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        region,\n",
    "        SUM(amount) as total_sales\n",
    "    FROM sales\n",
    "    GROUP BY ROLLUP (category, region)\n",
    "    ORDER BY category NULLS LAST, region NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š ROLLUP - HiÃ©rarchie d'agrÃ©gations:\")\n",
    "rollup_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cube",
   "metadata": {},
   "source": [
    "### 4.3 CUBE : toutes les combinaisons\n",
    "\n",
    "`CUBE(a, b)` = `GROUPING SETS ((a,b), (a), (b), ())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cube_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUBE : toutes les combinaisons possibles\n",
    "cube_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        region,\n",
    "        SUM(amount) as total_sales\n",
    "    FROM sales\n",
    "    GROUP BY CUBE (category, region)\n",
    "    ORDER BY category NULLS LAST, region NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š CUBE - Toutes les combinaisons:\")\n",
    "cube_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouping_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPING() : identifier les sous-totaux\n",
    "grouping_with_flags = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        region,\n",
    "        SUM(amount) as total_sales,\n",
    "        GROUPING(category) as is_category_subtotal,\n",
    "        GROUPING(region) as is_region_subtotal,\n",
    "        CASE \n",
    "            WHEN GROUPING(category) = 1 AND GROUPING(region) = 1 THEN 'GRAND TOTAL'\n",
    "            WHEN GROUPING(category) = 1 THEN 'Region Subtotal'\n",
    "            WHEN GROUPING(region) = 1 THEN 'Category Subtotal'\n",
    "            ELSE 'Detail'\n",
    "        END as row_type\n",
    "    FROM sales\n",
    "    GROUP BY CUBE (category, region)\n",
    "    ORDER BY GROUPING(category), GROUPING(region), category, region\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š GROUPING() pour identifier les sous-totaux:\")\n",
    "grouping_with_flags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explode",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. DonnÃ©es Semi-StructurÃ©es â€” EXPLODE & JSON\n",
    "\n",
    "### 5.1 EXPLODE : Ã©clater arrays et maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explode_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des donnÃ©es avec arrays\n",
    "customers_with_tags = spark.createDataFrame([\n",
    "    (1, \"Alice\", [\"premium\", \"loyal\", \"newsletter\"]),\n",
    "    (2, \"Bob\", [\"new\", \"newsletter\"]),\n",
    "    (3, \"Charlie\", [\"premium\", \"vip\"]),\n",
    "], [\"id\", \"name\", \"tags\"])\n",
    "\n",
    "customers_with_tags.createOrReplaceTempView(\"customers_tags\")\n",
    "\n",
    "print(\"DonnÃ©es avec arrays:\")\n",
    "customers_with_tags.show(truncate=False)\n",
    "\n",
    "# EXPLODE : une ligne par tag\n",
    "exploded = spark.sql(\"\"\"\n",
    "    SELECT id, name, tag\n",
    "    FROM customers_tags\n",
    "    LATERAL VIEW EXPLODE(tags) t AS tag\n",
    "\"\"\")\n",
    "\n",
    "print(\"AprÃ¨s EXPLODE:\")\n",
    "exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posexplode_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSEXPLODE : avec position\n",
    "posexploded = spark.sql(\"\"\"\n",
    "    SELECT id, name, pos, tag\n",
    "    FROM customers_tags\n",
    "    LATERAL VIEW POSEXPLODE(tags) t AS pos, tag\n",
    "\"\"\")\n",
    "\n",
    "print(\"POSEXPLODE (avec index):\")\n",
    "posexploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explode_map_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLODE avec Map\n",
    "customers_with_attrs = spark.createDataFrame([\n",
    "    (1, \"Alice\", {\"city\": \"Paris\", \"country\": \"France\"}),\n",
    "    (2, \"Bob\", {\"city\": \"London\", \"country\": \"UK\", \"postal\": \"SW1\"}),\n",
    "], [\"id\", \"name\", \"attributes\"])\n",
    "\n",
    "customers_with_attrs.createOrReplaceTempView(\"customers_attrs\")\n",
    "\n",
    "print(\"DonnÃ©es avec Map:\")\n",
    "customers_with_attrs.show(truncate=False)\n",
    "\n",
    "# EXPLODE sur Map â†’ (key, value)\n",
    "exploded_map = spark.sql(\"\"\"\n",
    "    SELECT id, name, key, value\n",
    "    FROM customers_attrs\n",
    "    LATERAL VIEW EXPLODE(attributes) t AS key, value\n",
    "\"\"\")\n",
    "\n",
    "print(\"EXPLODE sur Map:\")\n",
    "exploded_map.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json_access",
   "metadata": {},
   "source": [
    "### 5.2 AccÃ¨s JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "json_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DonnÃ©es JSON\n",
    "events = spark.createDataFrame([\n",
    "    (1, '{\"user\": \"alice\", \"action\": \"click\", \"details\": {\"page\": \"home\", \"duration\": 5}}'),\n",
    "    (2, '{\"user\": \"bob\", \"action\": \"purchase\", \"details\": {\"page\": \"cart\", \"amount\": 99.99}}'),\n",
    "], [\"id\", \"json_data\"])\n",
    "\n",
    "events.createOrReplaceTempView(\"events\")\n",
    "\n",
    "# Extraire des champs JSON\n",
    "json_extracted = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        get_json_object(json_data, '$.user') as user,\n",
    "        get_json_object(json_data, '$.action') as action,\n",
    "        get_json_object(json_data, '$.details.page') as page\n",
    "    FROM events\n",
    "\"\"\")\n",
    "\n",
    "print(\"Extraction JSON avec get_json_object:\")\n",
    "json_extracted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "from_json_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser JSON complet avec schema\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "details_schema = StructType([\n",
    "    StructField(\"page\", StringType()),\n",
    "    StructField(\"duration\", DoubleType()),\n",
    "    StructField(\"amount\", DoubleType())\n",
    "])\n",
    "\n",
    "event_schema = StructType([\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"action\", StringType()),\n",
    "    StructField(\"details\", details_schema)\n",
    "])\n",
    "\n",
    "parsed = events.withColumn(\"parsed\", from_json(col(\"json_data\"), event_schema))\n",
    "parsed.select(\"id\", \"parsed.user\", \"parsed.action\", \"parsed.details.page\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ctes",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. CTEs & Subqueries â€” Structurer ses requÃªtes\n",
    "\n",
    "### 6.1 Common Table Expressions (CTEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cte_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTE : structurer une requÃªte complexe\n",
    "cte_result = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    -- Ã‰tape 1 : AgrÃ©gation par client\n",
    "    customer_stats AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(*) as num_orders,\n",
    "            SUM(amount) as total_spent,\n",
    "            AVG(amount) as avg_order\n",
    "        FROM orders\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    \n",
    "    -- Ã‰tape 2 : Moyenne globale\n",
    "    global_avg AS (\n",
    "        SELECT AVG(total_spent) as avg_total_spent\n",
    "        FROM customer_stats\n",
    "    )\n",
    "    \n",
    "    -- RequÃªte finale : clients au-dessus de la moyenne\n",
    "    SELECT \n",
    "        c.*,\n",
    "        g.avg_total_spent,\n",
    "        CASE WHEN c.total_spent > g.avg_total_spent THEN 'Above Average' ELSE 'Below Average' END as status\n",
    "    FROM customer_stats c\n",
    "    CROSS JOIN global_avg g\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š Analyse client avec CTEs:\")\n",
    "cte_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cte_vs_subquery",
   "metadata": {},
   "source": [
    "### 6.2 CTEs vs Subqueries\n",
    "\n",
    "| Aspect | CTE | Subquery |\n",
    "|--------|-----|----------|\n",
    "| **LisibilitÃ©** | âœ… Excellent | âš ï¸ Peut Ãªtre confus |\n",
    "| **RÃ©utilisabilitÃ©** | âœ… Peut Ãªtre rÃ©fÃ©rencÃ© plusieurs fois | âŒ RÃ©pÃ©tition nÃ©cessaire |\n",
    "| **Performance** | âš ï¸ Pas toujours optimisÃ© | âš ï¸ Variable |\n",
    "\n",
    "> âš ï¸ **Mythe** : Les CTEs ne sont PAS toujours plus rapides. Catalyst les traite comme des subqueries inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subquery_vs_window",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ Subquery corrÃ©lÃ©e (potentiellement lent)\n",
    "# SELECT * FROM orders o\n",
    "# WHERE amount > (SELECT AVG(amount) FROM orders WHERE customer_id = o.customer_id)\n",
    "\n",
    "# âœ… Window function (plus efficace)\n",
    "efficient_query = spark.sql(\"\"\"\n",
    "    SELECT * FROM (\n",
    "        SELECT \n",
    "            *,\n",
    "            AVG(amount) OVER (PARTITION BY customer_id) as avg_customer_amount\n",
    "        FROM orders\n",
    "    )\n",
    "    WHERE amount > avg_customer_amount\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Commandes au-dessus de la moyenne du client (window function):\")\n",
    "efficient_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimization",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Optimisation SQL dans Spark\n",
    "\n",
    "### 7.1 Join Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "join_hints_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des tables pour dÃ©montrer les hints\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_segment\")\n",
    "segments = spark.createDataFrame([\n",
    "    (\"Premium\", 1.2), (\"Standard\", 1.0)\n",
    "], [\"segment\", \"multiplier\"])\n",
    "segments.createOrReplaceTempView(\"dim_segment\")\n",
    "\n",
    "# BROADCAST hint\n",
    "broadcast_join = spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(dim_segment) */ \n",
    "        o.*,\n",
    "        s.multiplier,\n",
    "        o.amount * s.multiplier as adjusted_amount\n",
    "    FROM orders o\n",
    "    JOIN dim_segment s ON o.segment = s.segment\n",
    "\"\"\")\n",
    "\n",
    "print(\"Plan avec BROADCAST hint:\")\n",
    "broadcast_join.explain()\n",
    "print(\"\\nRÃ©sultat:\")\n",
    "broadcast_join.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hints_reference",
   "metadata": {},
   "source": [
    "**Hints disponibles :**\n",
    "\n",
    "| Hint | Usage | Quand l'utiliser |\n",
    "|------|-------|------------------|\n",
    "| `/*+ BROADCAST(table) */` | Force broadcast | Petite table (< 100 MB) |\n",
    "| `/*+ MERGE(t1, t2) */` | Force sort-merge join | Grandes tables triÃ©es |\n",
    "| `/*+ SHUFFLE_HASH(t1) */` | Force shuffle hash | Tables moyennes |\n",
    "| `/*+ COALESCE(n) */` | RÃ©duit les partitions | Avant Ã©criture |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter les statistiques (amÃ©liore l'optimiseur)\n",
    "# Note : fonctionne sur des tables persistÃ©es, pas des vues temporaires\n",
    "\n",
    "print(\"ğŸ’¡ Pour collecter des statistiques sur une table persistÃ©e :\")\n",
    "print(\"\"\"\n",
    "-- Statistiques de base\n",
    "ANALYZE TABLE my_table COMPUTE STATISTICS\n",
    "\n",
    "-- Statistiques sur colonnes spÃ©cifiques\n",
    "ANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1, col2\n",
    "\n",
    "-- Voir les statistiques\n",
    "DESCRIBE EXTENDED my_table\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anti_patterns",
   "metadata": {},
   "source": [
    "### 7.2 Anti-patterns SQL\n",
    "\n",
    "| Anti-pattern | ProblÃ¨me | Solution |\n",
    "|--------------|----------|----------|\n",
    "| `SELECT *` | Lit toutes les colonnes | `SELECT col1, col2` |\n",
    "| `ORDER BY` sans `LIMIT` | Tri global coÃ»teux | Ajouter `LIMIT` |\n",
    "| UDF dans `WHERE` | Pas de pushdown | Expression native |\n",
    "| `COUNT(DISTINCT)` haute cardinalitÃ© | TrÃ¨s lent | `APPROX_COUNT_DISTINCT` |\n",
    "| `NOT IN` avec NULL | RÃ©sultats inattendus | `NOT EXISTS` ou `LEFT JOIN` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approx_count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation vs COUNT DISTINCT\n",
    "comparison = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT customer_id) as exact_count,\n",
    "        APPROX_COUNT_DISTINCT(customer_id) as approx_count\n",
    "    FROM orders\n",
    "\"\"\")\n",
    "\n",
    "print(\"COUNT DISTINCT vs APPROX_COUNT_DISTINCT:\")\n",
    "comparison.show()\n",
    "print(\"ğŸ’¡ APPROX_COUNT_DISTINCT est ~10x plus rapide sur de grandes tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini-Projet : Datamart Customer Analytics\n",
    "\n",
    "### Objectif\n",
    "Construire un datamart analytique client complet en utilisant toutes les techniques apprises.\n",
    "\n",
    "### MÃ©triques Ã  calculer\n",
    "- Date premiÃ¨re commande (`FIRST_VALUE`)\n",
    "- Nombre de commandes (`COUNT OVER`)\n",
    "- Revenu total client (`SUM OVER`)\n",
    "- DÃ©lai moyen entre commandes (`LAG` + `AVG`)\n",
    "- Rang du client par segment (`RANK`)\n",
    "- % du revenu du segment (`SUM OVER partition`)\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Orders    â”‚     â”‚  Segments   â”‚\n",
    "â”‚   (fact)    â”‚     â”‚   (dim)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                   â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚  CTE: enriched    â”‚\n",
    "       â”‚  - joins          â”‚\n",
    "       â”‚  - window funcs   â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚  CTE: customer    â”‚\n",
    "       â”‚      stats        â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "       â”‚    Datamart       â”‚\n",
    "       â”‚   [Parquet]       â”‚\n",
    "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# CrÃ©er des donnÃ©es plus riches\n",
    "orders_extended = spark.createDataFrame([\n",
    "    # Customer 101 - Premium, 3 commandes\n",
    "    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n",
    "    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n",
    "    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n",
    "    # Customer 102 - Standard, 4 commandes\n",
    "    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n",
    "    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n",
    "    (6, 102, \"2024-02-10\", 275.0, \"Standard\"),\n",
    "    (7, 102, \"2024-03-01\", 320.0, \"Standard\"),\n",
    "    # Customer 103 - Premium, 2 commandes\n",
    "    (8, 103, \"2024-01-10\", 400.0, \"Premium\"),\n",
    "    (9, 103, \"2024-02-15\", 450.0, \"Premium\"),\n",
    "    # Customer 104 - Standard, 3 commandes\n",
    "    (10, 104, \"2024-01-03\", 100.0, \"Standard\"),\n",
    "    (11, 104, \"2024-01-25\", 120.0, \"Standard\"),\n",
    "    (12, 104, \"2024-02-20\", 90.0, \"Standard\"),\n",
    "    # Customer 105 - Premium, 1 commande\n",
    "    (13, 105, \"2024-02-01\", 500.0, \"Premium\"),\n",
    "], [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\n",
    "\n",
    "orders_extended = orders_extended.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "orders_extended.createOrReplaceTempView(\"orders_ext\")\n",
    "\n",
    "print(\"ğŸ“¦ DonnÃ©es source:\")\n",
    "orders_extended.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_datamart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction du Datamart avec CTEs et Window Functions\n",
    "datamart = spark.sql(\"\"\"\n",
    "    WITH \n",
    "    -- Ã‰tape 1 : Enrichir les commandes avec mÃ©triques temporelles\n",
    "    orders_enriched AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            segment,\n",
    "            order_date,\n",
    "            amount,\n",
    "            \n",
    "            -- NumÃ©ro de commande du client\n",
    "            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_number,\n",
    "            \n",
    "            -- Date de la commande prÃ©cÃ©dente\n",
    "            LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n",
    "            \n",
    "            -- DÃ©lai depuis la derniÃ¨re commande\n",
    "            DATEDIFF(\n",
    "                order_date,\n",
    "                LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date)\n",
    "            ) as days_since_last\n",
    "        FROM orders_ext\n",
    "    ),\n",
    "    \n",
    "    -- Ã‰tape 2 : AgrÃ©ger par client\n",
    "    customer_stats AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            segment,\n",
    "            \n",
    "            -- PremiÃ¨re commande\n",
    "            MIN(order_date) as first_order_date,\n",
    "            \n",
    "            -- DerniÃ¨re commande\n",
    "            MAX(order_date) as last_order_date,\n",
    "            \n",
    "            -- Nombre de commandes\n",
    "            COUNT(*) as total_orders,\n",
    "            \n",
    "            -- Revenus\n",
    "            SUM(amount) as total_revenue,\n",
    "            AVG(amount) as avg_order_value,\n",
    "            \n",
    "            -- DÃ©lai moyen entre commandes (exclut NULL de la premiÃ¨re commande)\n",
    "            AVG(days_since_last) as avg_days_between_orders\n",
    "        FROM orders_enriched\n",
    "        GROUP BY customer_id, segment\n",
    "    ),\n",
    "    \n",
    "    -- Ã‰tape 3 : Ajouter des mÃ©triques de segment\n",
    "    customer_with_segment_stats AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            \n",
    "            -- Total du segment\n",
    "            SUM(total_revenue) OVER (PARTITION BY segment) as segment_total_revenue,\n",
    "            \n",
    "            -- % du revenu du segment\n",
    "            ROUND(total_revenue / SUM(total_revenue) OVER (PARTITION BY segment) * 100, 2) as pct_of_segment,\n",
    "            \n",
    "            -- Rang dans le segment\n",
    "            RANK() OVER (PARTITION BY segment ORDER BY total_revenue DESC) as rank_in_segment,\n",
    "            \n",
    "            -- Nombre de clients dans le segment\n",
    "            COUNT(*) OVER (PARTITION BY segment) as customers_in_segment\n",
    "        FROM customer_stats\n",
    "    )\n",
    "    \n",
    "    -- RÃ©sultat final\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        segment,\n",
    "        first_order_date,\n",
    "        last_order_date,\n",
    "        total_orders,\n",
    "        ROUND(total_revenue, 2) as total_revenue,\n",
    "        ROUND(avg_order_value, 2) as avg_order_value,\n",
    "        ROUND(avg_days_between_orders, 1) as avg_days_between_orders,\n",
    "        rank_in_segment,\n",
    "        pct_of_segment,\n",
    "        customers_in_segment\n",
    "    FROM customer_with_segment_stats\n",
    "    ORDER BY segment, rank_in_segment\n",
    "\"\"\")\n",
    "\n",
    "print(\"ğŸ“Š DATAMART CUSTOMER ANALYTICS:\")\n",
    "datamart.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter le datamart\n",
    "output_path = \"/tmp/customer_datamart\"\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "\n",
    "datamart.write.partitionBy(\"segment\").parquet(output_path)\n",
    "\n",
    "print(f\"âœ… Datamart exportÃ© : {output_path}\")\n",
    "print(f\"ğŸ“ PartitionnÃ© par segment\")\n",
    "\n",
    "# VÃ©rifier la structure\n",
    "for root, dirs, files in os.walk(output_path):\n",
    "    level = root.replace(output_path, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©sumÃ© du mini-projet\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           ğŸ“Š RÃ‰SUMÃ‰ DU DATAMART                              â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘                                                              â•‘\n",
    "â•‘  Techniques utilisÃ©es :                                      â•‘\n",
    "â•‘  âœ… CTEs pour structurer la transformation                   â•‘\n",
    "â•‘  âœ… ROW_NUMBER() pour numÃ©roter les commandes                â•‘\n",
    "â•‘  âœ… LAG() pour calculer le dÃ©lai entre commandes             â•‘\n",
    "â•‘  âœ… RANK() pour classer les clients par segment              â•‘\n",
    "â•‘  âœ… SUM() OVER pour calculer les totaux de segment           â•‘\n",
    "â•‘  âœ… Partitionnement Parquet par segment                      â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  MÃ©triques calculÃ©es :                                       â•‘\n",
    "â•‘  â€¢ Date premiÃ¨re/derniÃ¨re commande                           â•‘\n",
    "â•‘  â€¢ Nombre total de commandes                                 â•‘\n",
    "â•‘  â€¢ Revenu total et moyen                                     â•‘\n",
    "â•‘  â€¢ DÃ©lai moyen entre commandes                               â•‘\n",
    "â•‘  â€¢ Rang dans le segment                                      â•‘\n",
    "â•‘  â€¢ % du revenu du segment                                    â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  ğŸ’¡ Ce type de transformation sera automatisÃ©                â•‘\n",
    "â•‘     avec dbt dans le module 26                               â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quiz de fin de module\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Quelle est la diffÃ©rence entre ROW_NUMBER() et RANK() ?\n",
    "a) ROW_NUMBER() est plus rapide  \n",
    "b) RANK() gÃ¨re les ex-aequo avec des gaps, ROW_NUMBER() donne des numÃ©ros uniques  \n",
    "c) ROW_NUMBER() nÃ©cessite ORDER BY, pas RANK()  \n",
    "d) Aucune diffÃ©rence\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” RANK() donne le mÃªme rang aux ex-aequo (1,1,3), ROW_NUMBER() donne toujours des numÃ©ros uniques (1,2,3).\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Quelle est la diffÃ©rence entre ROWS et RANGE dans une window frame ?\n",
    "a) ROWS est plus rapide  \n",
    "b) RANGE supporte plus de fonctions  \n",
    "c) ROWS se base sur la position physique, RANGE sur la valeur logique  \n",
    "d) RANGE ne fonctionne qu'avec des dates\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” ROWS compte les lignes physiquement, RANGE peut inclure plusieurs lignes si elles ont la mÃªme valeur.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Que fait la fonction GROUPING() ?\n",
    "a) Groupe les donnÃ©es  \n",
    "b) Identifie si une colonne est agrÃ©gÃ©e (sous-total)  \n",
    "c) Compte le nombre de groupes  \n",
    "d) Trie les groupes\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” GROUPING() retourne 1 si la colonne est agrÃ©gÃ©e (NULL dans un sous-total), 0 sinon.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Comment forcer un broadcast join en SQL Spark ?\n",
    "a) `FORCE BROADCAST`  \n",
    "b) `/*+ BROADCAST(table) */`  \n",
    "c) `BROADCAST JOIN`  \n",
    "d) `SET spark.broadcast = true`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” On utilise le hint SQL `/*+ BROADCAST(table) */` aprÃ¨s SELECT.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. PIVOT transforme... ?\n",
    "a) Colonnes en lignes  \n",
    "b) Lignes en colonnes  \n",
    "c) JSON en colonnes  \n",
    "d) Arrays en lignes\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” PIVOT transforme les valeurs distinctes d'une colonne en colonnes sÃ©parÃ©es.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. EXPLODE est utilisÃ© pour... ?\n",
    "a) Compresser les donnÃ©es  \n",
    "b) Supprimer les doublons  \n",
    "c) Transformer un array en plusieurs lignes  \n",
    "d) Joindre des tables\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” EXPLODE Ã©clate un array (ou map) en crÃ©ant une ligne pour chaque Ã©lÃ©ment.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Les CTEs sont-ils toujours plus performants que les subqueries ?\n",
    "a) Oui, toujours  \n",
    "b) Non, Catalyst les traite de maniÃ¨re similaire  \n",
    "c) Oui, car ils sont matÃ©rialisÃ©s  \n",
    "d) Non, ils sont toujours plus lents\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” C'est un mythe ! Catalyst inline les CTEs comme des subqueries. L'avantage est la lisibilitÃ©.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Quel hint utiliser pour Ã©viter un shuffle lors d'un join avec une petite table ?\n",
    "a) `/*+ MERGE */`  \n",
    "b) `/*+ SHUFFLE_HASH */`  \n",
    "c) `/*+ BROADCAST */`  \n",
    "d) `/*+ COALESCE */`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” BROADCAST envoie la petite table Ã  tous les executors, Ã©vitant le shuffle.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q9. UNBOUNDED PRECEDING signifie... ?\n",
    "a) La ligne prÃ©cÃ©dente  \n",
    "b) Toutes les lignes depuis le dÃ©but de la partition  \n",
    "c) La premiÃ¨re ligne de la table  \n",
    "d) Aucune limite de mÃ©moire\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” UNBOUNDED PRECEDING inclut toutes les lignes depuis le dÃ©but de la partition jusqu'Ã  la position actuelle.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q10. Comment collecter des statistiques sur une table pour amÃ©liorer l'optimiseur ?\n",
    "a) `COMPUTE STATS table`  \n",
    "b) `ANALYZE TABLE table COMPUTE STATISTICS`  \n",
    "c) `COLLECT STATISTICS table`  \n",
    "d) `DESCRIBE STATISTICS table`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `ANALYZE TABLE table COMPUTE STATISTICS` collecte les statistiques pour l'optimiseur Catalyst.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-ref.html)\n",
    "- [Window Functions](https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html#window-functions)\n",
    "- [SQL Syntax](https://spark.apache.org/docs/latest/sql-ref-syntax.html)\n",
    "\n",
    "### ğŸ“– Articles & Tutoriels\n",
    "- [Databricks - Window Functions](https://docs.databricks.com/en/sql/language-manual/sql-ref-functions-builtin.html)\n",
    "- [Advanced SQL Optimization](https://www.databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises Spark SQL, passons Ã  spark sur K8S !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `21_spark_on_kubernetes`** - Spark & Kubernetes\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ RÃ©capitulatif de ce module\n",
    "\n",
    "| Concept | Ce que tu as appris |\n",
    "|---------|--------------------|\n",
    "| **Window Functions** | ROW_NUMBER, RANK, LAG/LEAD, FIRST_VALUE, frames |\n",
    "| **PIVOT/UNPIVOT** | Reshaping des donnÃ©es |\n",
    "| **GROUPING SETS** | CUBE, ROLLUP, agrÃ©gations multidimensionnelles |\n",
    "| **EXPLODE** | DonnÃ©es semi-structurÃ©es, JSON |\n",
    "| **CTEs** | Structurer les requÃªtes complexes |\n",
    "| **Optimisation** | Hints, statistiques, anti-patterns |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module Spark SQL Deep Dive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage\n",
    "spark.stop()\n",
    "print(\"âœ… SparkSession arrÃªtÃ©e\")\n",
    "\n",
    "# Nettoyage des fichiers temporaires (optionnel)\n",
    "# import shutil\n",
    "# if os.path.exists(\"/tmp/customer_datamart\"):\n",
    "#     shutil.rmtree(\"/tmp/customer_datamart\")\n",
    "# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}