{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# â˜ï¸ Cloud & Object Storage for Data Engineers\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le **Cloud Computing** et l'**Object Storage**, les fondations de tout Data Lake moderne.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "| Niveau | Module | CompÃ©tence |\n",
    "|--------|--------|------------|\n",
    "| âœ… Requis | Module 14 | Docker Fundamentals |\n",
    "| âœ… Requis | Module 19 | PySpark Advanced |\n",
    "| âœ… Requis | Module 21 | Spark on Kubernetes |\n",
    "| ğŸ’¡ RecommandÃ© | Module 08 | Big Data Introduction (Medallion Architecture) |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre les **modÃ¨les Cloud** (IaaS, PaaS, SaaS)\n",
    "- ConnaÃ®tre les **services Data Engineering** sur AWS, GCP, Azure\n",
    "- MaÃ®triser les concepts de l'**Object Storage** (buckets, keys, prefixes)\n",
    "- Lire/Ã©crire sur **S3, Azure Blob, GCS** avec Python et Spark\n",
    "- DÃ©ployer **MinIO** pour pratiquer localement\n",
    "- **Optimiser** les performances (formats, partitionnement, small files)\n",
    "- GÃ©rer la **sÃ©curitÃ©** et les **coÃ»ts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro_cloud",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š 1. Introduction â€” Pourquoi le Cloud pour le Data Engineering ?\n",
    "\n",
    "### 1.1 L'Ã©volution du Data Engineering\n",
    "\n",
    "```text\n",
    "2000s                    2010s                    2020s+\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  On-Premise â”‚         â”‚   Hadoop    â”‚         â”‚    Cloud    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ RDBMS â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚ HDFS  â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚  S3   â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚  CoÃ»teux    â”‚         â”‚  Complexe   â”‚         â”‚  Scalable   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.2 Avantages du Cloud pour le Data Engineering\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **ScalabilitÃ©** | Stockage et compute illimitÃ©s Ã  la demande |\n",
    "| **CoÃ»t** | Pay-as-you-go, pas d'investissement initial |\n",
    "| **Managed Services** | Moins d'ops, plus de focus sur les donnÃ©es |\n",
    "| **SÃ©paration compute/storage** | Scaler indÃ©pendamment |\n",
    "| **DurabilitÃ©** | 99.999999999% (11 nines) pour S3 |\n",
    "| **Global** | DonnÃ©es accessibles partout |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud_computing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â˜ï¸ 2. Introduction au Cloud Computing\n",
    "\n",
    "### 2.1 C'est quoi le Cloud ?\n",
    "\n",
    "Le Cloud = des serveurs, du stockage et des services accessibles via Internet, gÃ©rÃ©s par un provider.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        LE CLOUD                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ Compute â”‚  â”‚ Storage â”‚  â”‚ Network â”‚  â”‚  ....   â”‚       â”‚\n",
    "â”‚  â”‚  (VMs)  â”‚  â”‚ (Disks) â”‚  â”‚  (VPC)  â”‚  â”‚         â”‚       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Tu ne gÃ¨res pas le hardware, juste les services            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                              â–²\n",
    "                              â”‚ Internet\n",
    "                              â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Ton application  â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.2 Les 3 modÃ¨les de service\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    RESPONSABILITÃ‰                              â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚   On-Premise      IaaS           PaaS           SaaS          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â—€â”€â”€ Provider â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚  Data    â”‚  â”‚  Data    â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚ Runtime  â”‚  â”‚ Runtime  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚   OS     â”‚  â”‚   OS     â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚ Virtual  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚ Servers  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚ Storage  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â”‚ Network  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚  â–ˆ = GÃ©rÃ© par le Cloud Provider                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "| ModÃ¨le | Description | Exemples | Tu gÃ¨res |\n",
    "|--------|-------------|----------|----------|\n",
    "| **IaaS** | Infrastructure as a Service | EC2, VMs, VPC | OS, Runtime, App |\n",
    "| **PaaS** | Platform as a Service | RDS, Cloud SQL, EKS | App, Data |\n",
    "| **SaaS** | Software as a Service | Snowflake, Databricks | Rien (juste utiliser) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud_providers",
   "metadata": {},
   "source": [
    "### 2.3 Les 3 grands Cloud Providers\n",
    "\n",
    "| Provider | Part de marchÃ© | Forces | Faiblesse |\n",
    "|----------|----------------|--------|------------|\n",
    "| **AWS** | ~32% | Leader, plus de services, maturitÃ© | ComplexitÃ©, coÃ»ts |\n",
    "| **Azure** | ~23% | IntÃ©gration Microsoft, entreprises | UX parfois confuse |\n",
    "| **GCP** | ~10% | BigQuery, ML/AI, Kubernetes | Moins de services |\n",
    "\n",
    "### 2.4 RÃ©gions & Zones de disponibilitÃ©\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     AWS Global                              â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚  â”‚  Region:        â”‚    â”‚  Region:        â”‚                â”‚\n",
    "â”‚  â”‚  eu-west-1      â”‚    â”‚  us-east-1      â”‚                â”‚\n",
    "â”‚  â”‚  (Ireland)      â”‚    â”‚  (N. Virginia)  â”‚                â”‚\n",
    "â”‚  â”‚                 â”‚    â”‚                 â”‚                â”‚\n",
    "â”‚  â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚    â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚                â”‚\n",
    "â”‚  â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚    â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚                â”‚\n",
    "â”‚  â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚    â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚                â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  AZ = Availability Zone = Data Center isolÃ©                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "- **Region** : Zone gÃ©ographique (Paris, Dublin, N. Virginia)\n",
    "- **Availability Zone** : Data center isolÃ© dans une rÃ©gion\n",
    "- **Latence** : Choisir la rÃ©gion proche des utilisateurs\n",
    "- **Compliance** : GDPR â†’ donnÃ©es en Europe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_1",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 1 : Identifier IaaS / PaaS / SaaS\n",
    "\n",
    "Classe ces services dans la bonne catÃ©gorie :\n",
    "\n",
    "| Service | IaaS / PaaS / SaaS ? |\n",
    "|---------|----------------------|\n",
    "| Amazon EC2 | ? |\n",
    "| Google BigQuery | ? |\n",
    "| Snowflake | ? |\n",
    "| Azure Kubernetes Service | ? |\n",
    "| Amazon S3 | ? |\n",
    "| Databricks | ? |\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir les rÃ©ponses</summary>\n",
    "\n",
    "| Service | CatÃ©gorie | Explication |\n",
    "|---------|-----------|-------------|\n",
    "| Amazon EC2 | **IaaS** | Tu gÃ¨res l'OS et tout ce qu'il y a dessus |\n",
    "| Google BigQuery | **PaaS/SaaS** | Serverless, tu gÃ¨res juste les requÃªtes |\n",
    "| Snowflake | **SaaS** | EntiÃ¨rement managÃ© |\n",
    "| Azure Kubernetes Service | **PaaS** | K8s managÃ©, tu gÃ¨res les workloads |\n",
    "| Amazon S3 | **PaaS** | Stockage managÃ©, tu gÃ¨res les donnÃ©es |\n",
    "| Databricks | **PaaS/SaaS** | Spark managÃ© |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloud_services_de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ› ï¸ 3. Services Cloud pour le Data Engineering\n",
    "\n",
    "### 3.1 Tableau comparatif complet\n",
    "\n",
    "| CatÃ©gorie | AWS | GCP | Azure |\n",
    "|-----------|-----|-----|-------|\n",
    "| **Object Storage** | S3 | GCS | Blob Storage |\n",
    "| **Data Warehouse** | Redshift | BigQuery | Synapse Analytics |\n",
    "| **ETL Serverless** | Glue | Dataflow | Data Factory |\n",
    "| **Query Engine** | Athena | BigQuery | Synapse Serverless |\n",
    "| **Orchestration** | MWAA (Airflow) | Composer (Airflow) | Data Factory |\n",
    "| **Streaming** | Kinesis | Pub/Sub + Dataflow | Event Hubs |\n",
    "| **Catalog** | Glue Catalog | Data Catalog | Purview |\n",
    "| **Kubernetes** | EKS | GKE | AKS |\n",
    "| **Serverless Compute** | Lambda | Cloud Functions | Functions |\n",
    "| **NoSQL** | DynamoDB | Firestore/Bigtable | CosmosDB |\n",
    "| **Message Queue** | SQS/SNS | Pub/Sub | Service Bus |\n",
    "\n",
    "### 3.2 Focus sur les services Data Engineering\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  ARCHITECTURE DATA CLOUD                        â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Sources              Ingestion           Storage               â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”‚ API â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Kinesis â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   S3    â”‚           â”‚\n",
    "â”‚  â”‚ DB  â”‚             â”‚ Pub/Sub â”‚         â”‚  GCS    â”‚           â”‚\n",
    "â”‚  â”‚ Filesâ”‚             â”‚ EventHubâ”‚         â”‚  Blob   â”‚           â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\n",
    "â”‚                                               â”‚                 â”‚\n",
    "â”‚                                               â–¼                 â”‚\n",
    "â”‚  Processing                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Glue   â”‚           â”‚\n",
    "â”‚  â”‚                                       â”‚Dataflow â”‚           â”‚\n",
    "â”‚  â”‚                                       â”‚  ADF    â”‚           â”‚\n",
    "â”‚  â”‚                                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\n",
    "â”‚  â”‚                                            â”‚                 â”‚\n",
    "â”‚  â”‚                                            â–¼                 â”‚\n",
    "â”‚  â”‚  Serving                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚Redshift â”‚           â”‚\n",
    "â”‚  â””â”€â”€â”‚ Athena  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚BigQuery â”‚           â”‚\n",
    "â”‚     â”‚ Synapse â”‚                          â”‚ Synapse â”‚           â”‚\n",
    "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "storage_models",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ 4. Storage Models : Block vs File vs Object\n",
    "\n",
    "### 4.1 Comparaison dÃ©taillÃ©e\n",
    "\n",
    "| CritÃ¨re | Block Storage | File Storage | Object Storage |\n",
    "|---------|---------------|--------------|----------------|\n",
    "| **Structure** | Blocs bruts | HiÃ©rarchie fichiers | ClÃ©-valeur plat |\n",
    "| **AccÃ¨s** | Bas niveau (disque) | POSIX (NFS, SMB) | API HTTP (REST) |\n",
    "| **Metadata** | Minimales | Basiques | Riches, custom |\n",
    "| **ScalabilitÃ©** | LimitÃ©e (TB) | LimitÃ©e (TB) | IllimitÃ©e (PB+) |\n",
    "| **Performance** | TrÃ¨s haute | Moyenne | Variable |\n",
    "| **CoÃ»t** | Ã‰levÃ© | Moyen | Faible |\n",
    "| **Use case** | Bases de donnÃ©es | Partage fichiers | Data Lakes |\n",
    "| **Exemples** | EBS, Azure Disk | EFS, Azure Files | S3, GCS, Blob |\n",
    "\n",
    "### 4.2 Pourquoi Object Storage pour les Data Lakes ?\n",
    "\n",
    "```text\n",
    "âœ… ScalabilitÃ© illimitÃ©e      âœ… CoÃ»t faible\n",
    "âœ… DurabilitÃ© 11 nines        âœ… API HTTP standard\n",
    "âœ… SÃ©paration compute/storage âœ… Metadata riches\n",
    "âœ… Formats natifs (Parquet)   âœ… Multi-cloud possible\n",
    "```\n",
    "\n",
    "### 4.3 SÃ©paration Compute / Storage\n",
    "\n",
    "```text\n",
    "AVANT (Hadoop/HDFS)                    APRÃˆS (Cloud/Object Storage)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Node 1         â”‚               â”‚    Compute (Spark)   â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚  â”‚Computeâ”‚ â”‚Data â”‚  â”‚               â”‚    â”‚  Executor â”‚     â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚               â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚      CouplÃ©s !      â”‚               â”‚          â”‚           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                 â”‚ Network\n",
    "                                                 â–¼\n",
    "                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                      â”‚   Storage (S3)      â”‚\n",
    "                                      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "                                      â”‚    â”‚  Data   â”‚      â”‚\n",
    "                                      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "                                      â”‚   Scale sÃ©parÃ©ment  â”‚\n",
    "                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_2",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 2 : Choisir le bon type de storage\n",
    "\n",
    "Quel type de storage pour chaque use case ?\n",
    "\n",
    "| Use case | Block / File / Object ? |\n",
    "|----------|-------------------------|\n",
    "| Base de donnÃ©es PostgreSQL | ? |\n",
    "| Data Lake avec fichiers Parquet | ? |\n",
    "| Partage de documents entre Ã©quipes | ? |\n",
    "| Logs d'application (PB de donnÃ©es) | ? |\n",
    "| VM avec OS Windows | ? |\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir les rÃ©ponses</summary>\n",
    "\n",
    "| Use case | Type | Raison |\n",
    "|----------|------|--------|\n",
    "| PostgreSQL | **Block** | IOPS Ã©levÃ©s, accÃ¨s bas niveau |\n",
    "| Data Lake Parquet | **Object** | Scalable, coÃ»t faible |\n",
    "| Partage documents | **File** | AccÃ¨s POSIX, permissions |\n",
    "| Logs application | **Object** | Volume Ã©norme, coÃ»t faible |\n",
    "| VM Windows | **Block** | Disque systÃ¨me |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "object_storage_concepts",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸª£ 5. Object Storage â€” Concepts fondamentaux\n",
    "\n",
    "### 5.1 Buckets, Keys, Prefixes\n",
    "\n",
    "```text\n",
    "s3://my-bucket/bronze/2024/01/transactions.parquet\n",
    "     â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "       Bucket           Prefix            Object Key\n",
    "\n",
    "âš ï¸ IMPORTANT : Les prÃ©fixes NE SONT PAS des dossiers !\n",
    "   C'est juste une convention de nommage (clÃ©-valeur plat)\n",
    "```\n",
    "\n",
    "| Concept | Description | Exemple |\n",
    "|---------|-------------|---------|\n",
    "| **Bucket** | Conteneur racine, nom unique global | `my-company-datalake` |\n",
    "| **Key** | Identifiant unique de l'objet | `bronze/2024/01/data.parquet` |\n",
    "| **Prefix** | \"Faux dossier\", filtre de listing | `bronze/2024/` |\n",
    "| **Object** | Le fichier + ses metadata | Parquet, CSV, JSON... |\n",
    "\n",
    "### 5.2 Metadata & Tags\n",
    "\n",
    "Chaque objet peut avoir des metadata custom :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metadata_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de metadata sur un objet S3\n",
    "metadata_example = {\n",
    "    # Metadata systÃ¨me (automatiques)\n",
    "    \"Content-Type\": \"application/octet-stream\",\n",
    "    \"Content-Length\": 1048576,\n",
    "    \"Last-Modified\": \"2024-01-15T10:30:00Z\",\n",
    "    \"ETag\": \"d41d8cd98f00b204e9800998ecf8427e\",\n",
    "    \n",
    "    # Metadata custom (x-amz-meta-*)\n",
    "    \"x-amz-meta-source\": \"kafka-topic-orders\",\n",
    "    \"x-amz-meta-pipeline\": \"etl-daily\",\n",
    "    \"x-amz-meta-schema-version\": \"2.1\",\n",
    "}\n",
    "\n",
    "# Tags (pour billing, governance)\n",
    "tags = {\n",
    "    \"Environment\": \"production\",\n",
    "    \"Team\": \"data-engineering\",\n",
    "    \"CostCenter\": \"DE-001\",\n",
    "}\n",
    "\n",
    "print(\"Metadata et Tags permettent de :\")\n",
    "print(\"- Tracer l'origine des donnÃ©es\")\n",
    "print(\"- Filtrer pour la gouvernance\")\n",
    "print(\"- Allouer les coÃ»ts par Ã©quipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "versioning_lifecycle",
   "metadata": {},
   "source": [
    "### 5.3 Versioning & Lifecycle Policies\n",
    "\n",
    "**Versioning** : Garder plusieurs versions d'un mÃªme objet\n",
    "\n",
    "```text\n",
    "s3://bucket/data.csv\n",
    "   â”‚\n",
    "   â”œâ”€â”€ Version 1 (2024-01-01) â† Ancienne\n",
    "   â”œâ”€â”€ Version 2 (2024-01-15) â† Ancienne\n",
    "   â””â”€â”€ Version 3 (2024-02-01) â† Current\n",
    "```\n",
    "\n",
    "**Lifecycle Policies** : Automatiser la gestion du stockage\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    30 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   90 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Standard  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Infrequent  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   Glacier   â”‚\n",
    "â”‚   $0.023/GB â”‚                â”‚   $0.0125/GBâ”‚               â”‚  $0.004/GB  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     Hot                           Cool                          Archive\n",
    "```\n",
    "\n",
    "### 5.4 Classes de stockage\n",
    "\n",
    "| Classe | Usage | Latence | CoÃ»t stockage |\n",
    "|--------|-------|---------|---------------|\n",
    "| **Standard** | AccÃ¨s frÃ©quent | ms | $0.023/GB |\n",
    "| **IA (Infrequent Access)** | AccÃ¨s rare | ms | $0.0125/GB |\n",
    "| **Glacier** | Archivage | minutes-heures | $0.004/GB |\n",
    "| **Glacier Deep Archive** | Compliance | heures | $0.00099/GB |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protocols_drivers",
   "metadata": {},
   "source": [
    "### 5.5 Protocoles & Drivers\n",
    "\n",
    "> ğŸ”¥ **Section essentielle** â€” Source de confusion frÃ©quente !\n",
    "\n",
    "| Protocole | Cloud | Usage | Driver/SDK | Exemple |\n",
    "|-----------|-------|-------|------------|----------|\n",
    "| `s3://` | AWS | CLI, Python | boto3, aws cli | `s3://bucket/key` |\n",
    "| `s3a://` | AWS | **Spark/Hadoop** | hadoop-aws | `s3a://bucket/key` |\n",
    "| `abfs://` | Azure | Spark (legacy) | hadoop-azure | `abfs://container@account.dfs.core.windows.net/` |\n",
    "| `abfss://` | Azure | **Spark (TLS)** | hadoop-azure | `abfss://container@account.dfs.core.windows.net/` |\n",
    "| `gs://` | GCP | **Spark, CLI** | gcs-connector | `gs://bucket/key` |\n",
    "| `https://` | Tous | Direct HTTP | requests | Signed URLs |\n",
    "\n",
    "```text\n",
    "âš ï¸ ATTENTION :\n",
    "\n",
    "s3://  â‰   s3a://\n",
    "\n",
    "- s3://  â†’ AWS CLI, boto3 (haut niveau)\n",
    "- s3a:// â†’ Hadoop/Spark (bas niveau, optimisÃ© Big Data)\n",
    "\n",
    "Dans Spark, TOUJOURS utiliser s3a://, abfss://, ou gs://\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metadata_catalogs",
   "metadata": {},
   "source": [
    "### 5.6 Metadata Catalogs â€” Transition vers le module 23\n",
    "\n",
    "L'Object Storage stocke des **fichiers bruts**. Mais pour faire du SQL, on a besoin de :\n",
    "- SchÃ©ma des tables (colonnes, types)\n",
    "- Localisation des partitions\n",
    "- Statistiques pour l'optimiseur\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    METADATA CATALOG                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚  Table: sales                                           â”‚   â”‚\n",
    "â”‚  â”‚  Location: s3://bucket/silver/sales/                    â”‚   â”‚\n",
    "â”‚  â”‚  Schema: id INT, amount DOUBLE, date DATE               â”‚   â”‚\n",
    "â”‚  â”‚  Partitions: date=2024-01-01, date=2024-01-02, ...      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                â”‚\n",
    "                                â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    OBJECT STORAGE (S3)                          â”‚\n",
    "â”‚  s3://bucket/silver/sales/date=2024-01-01/part-00000.parquet   â”‚\n",
    "â”‚  s3://bucket/silver/sales/date=2024-01-02/part-00000.parquet   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Solutions Catalog :**\n",
    "\n",
    "| Solution | Type | UtilisÃ© par |\n",
    "|----------|------|-------------|\n",
    "| **Hive Metastore** | Open-source | Spark, Presto, Trino |\n",
    "| **AWS Glue Catalog** | Managed | Athena, Glue, EMR |\n",
    "| **GCP Data Catalog** | Managed | BigQuery, Dataproc |\n",
    "| **Azure Purview** | Managed | Synapse, Databricks |\n",
    "\n",
    "> ğŸ’¡ **Preview Module 23** : Delta Lake et Iceberg intÃ¨grent leur propre **Transaction Log** directement dans l'Object Storage. Plus besoin de catalogue externe !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_lake_layout",
   "metadata": {},
   "source": [
    "### 5.7 Layout Data Lake â€” Rappel (rÃ©f module 08)\n",
    "\n",
    "```text\n",
    "s3://my-datalake/\n",
    "â”‚\n",
    "â”œâ”€â”€ bronze/                    â† Raw, immutable, source of truth\n",
    "â”‚   â”œâ”€â”€ orders/\n",
    "â”‚   â”‚   â””â”€â”€ 2024/01/01/\n",
    "â”‚   â”‚       â””â”€â”€ orders_raw.json\n",
    "â”‚   â””â”€â”€ customers/\n",
    "â”‚       â””â”€â”€ customers_full.csv\n",
    "â”‚\n",
    "â”œâ”€â”€ silver/                    â† Cleaned, validated, deduplicated\n",
    "â”‚   â”œâ”€â”€ orders/\n",
    "â”‚   â”‚   â””â”€â”€ date=2024-01-01/\n",
    "â”‚   â”‚       â””â”€â”€ part-00000.parquet\n",
    "â”‚   â””â”€â”€ customers/\n",
    "â”‚       â””â”€â”€ part-00000.parquet\n",
    "â”‚\n",
    "â””â”€â”€ gold/                      â† Aggregated, business-ready\n",
    "    â”œâ”€â”€ daily_sales/\n",
    "    â”‚   â””â”€â”€ part-00000.parquet\n",
    "    â””â”€â”€ customer_360/\n",
    "        â””â”€â”€ part-00000.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_3",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 3 : Calculer le coÃ»t de stockage\n",
    "\n",
    "**ScÃ©nario** : Tu as un Data Lake avec :\n",
    "- Bronze : 500 GB (accÃ¨s rare)\n",
    "- Silver : 200 GB (accÃ¨s frÃ©quent)\n",
    "- Gold : 50 GB (accÃ¨s trÃ¨s frÃ©quent)\n",
    "\n",
    "Calcule le coÃ»t mensuel optimal sur S3.\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la solution</summary>\n",
    "\n",
    "| Layer | Taille | Classe | Prix/GB | CoÃ»t |\n",
    "|-------|--------|--------|---------|------|\n",
    "| Bronze | 500 GB | S3 IA | $0.0125 | $6.25 |\n",
    "| Silver | 200 GB | Standard | $0.023 | $4.60 |\n",
    "| Gold | 50 GB | Standard | $0.023 | $1.15 |\n",
    "| **Total** | 750 GB | | | **$12.00/mois** |\n",
    "\n",
    "Sans optimisation (tout en Standard) : 750 Ã— $0.023 = **$17.25/mois**\n",
    "\n",
    "Ã‰conomie : **30%** !\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aws_s3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŸ  6. AWS S3 â€” Deep Dive\n",
    "\n",
    "### 6.1 Concepts & Classes de stockage\n",
    "\n",
    "| Classe | DurabilitÃ© | DisponibilitÃ© | Min storage | Retrieval |\n",
    "|--------|------------|---------------|-------------|----------|\n",
    "| Standard | 11 nines | 99.99% | - | ImmÃ©diat |\n",
    "| Intelligent-Tiering | 11 nines | 99.9% | - | ImmÃ©diat |\n",
    "| Standard-IA | 11 nines | 99.9% | 30 jours | ImmÃ©diat |\n",
    "| Glacier Instant | 11 nines | 99.9% | 90 jours | ms |\n",
    "| Glacier Flexible | 11 nines | 99.99% | 90 jours | 1-12h |\n",
    "| Glacier Deep Archive | 11 nines | 99.99% | 180 jours | 12-48h |\n",
    "\n",
    "### 6.2 OpÃ©rations CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s3_cli",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cli_commands = \"\"\"\n",
    "# Lister les buckets\n",
    "aws s3 ls\n",
    "\n",
    "# Lister le contenu d'un bucket\n",
    "aws s3 ls s3://my-bucket/bronze/\n",
    "\n",
    "# Copier un fichier local vers S3\n",
    "aws s3 cp data.csv s3://my-bucket/bronze/data.csv\n",
    "\n",
    "# Copier un fichier S3 vers local\n",
    "aws s3 cp s3://my-bucket/bronze/data.csv ./data.csv\n",
    "\n",
    "# Synchroniser un dossier\n",
    "aws s3 sync ./local-folder/ s3://my-bucket/bronze/\n",
    "\n",
    "# Supprimer un fichier\n",
    "aws s3 rm s3://my-bucket/bronze/old-data.csv\n",
    "\n",
    "# Supprimer rÃ©cursivement\n",
    "aws s3 rm s3://my-bucket/temp/ --recursive\n",
    "\"\"\"\n",
    "print(s3_cli_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3_python",
   "metadata": {},
   "source": [
    "### 6.3 Python avec boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boto3_examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install boto3\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# CrÃ©er un client S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# --- Upload ---\n",
    "def upload_file(file_path, bucket, key):\n",
    "    \"\"\"Upload un fichier vers S3.\"\"\"\n",
    "    s3.upload_file(file_path, bucket, key)\n",
    "    print(f\"âœ… Uploaded {file_path} to s3://{bucket}/{key}\")\n",
    "\n",
    "# --- Download ---\n",
    "def download_file(bucket, key, file_path):\n",
    "    \"\"\"Download un fichier depuis S3.\"\"\"\n",
    "    s3.download_file(bucket, key, file_path)\n",
    "    print(f\"âœ… Downloaded s3://{bucket}/{key} to {file_path}\")\n",
    "\n",
    "# --- List objects ---\n",
    "def list_objects(bucket, prefix=\"\"):\n",
    "    \"\"\"Liste les objets dans un bucket.\"\"\"\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            print(f\"  {obj['Key']} ({obj['Size']} bytes)\")\n",
    "    return response.get('Contents', [])\n",
    "\n",
    "# --- Presigned URL ---\n",
    "def generate_presigned_url(bucket, key, expiration=3600):\n",
    "    \"\"\"GÃ©nÃ¨re une URL temporaire pour accÃ¨s direct.\"\"\"\n",
    "    url = s3.generate_presigned_url(\n",
    "        'get_object',\n",
    "        Params={'Bucket': bucket, 'Key': key},\n",
    "        ExpiresIn=expiration\n",
    "    )\n",
    "    return url\n",
    "\n",
    "# Exemple d'utilisation (commentÃ© car pas de credentials)\n",
    "# upload_file('data.csv', 'my-bucket', 'bronze/data.csv')\n",
    "# list_objects('my-bucket', 'bronze/')\n",
    "print(\"ğŸ“ Fonctions boto3 dÃ©finies (upload, download, list, presigned URL)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3_spark",
   "metadata": {},
   "source": [
    "### 6.4 S3 avec Spark (s3a://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_s3_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Spark pour S3\n",
    "spark_s3_config = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Access\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lire depuis S3\n",
    "df = spark.read.parquet(\"s3a://my-bucket/silver/sales/\")\n",
    "\n",
    "# Ã‰crire vers S3\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://my-bucket/gold/aggregates/\")\n",
    "\"\"\"\n",
    "\n",
    "# Optimisations S3A\n",
    "s3a_optimizations = \"\"\"\n",
    "# Performance optimizations\n",
    "spark.hadoop.fs.s3a.connection.maximum=200\n",
    "spark.hadoop.fs.s3a.threads.max=64\n",
    "spark.hadoop.fs.s3a.fast.upload=true\n",
    "spark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer\n",
    "spark.hadoop.fs.s3a.multipart.size=104857600  # 100MB\n",
    "\"\"\"\n",
    "print(spark_s3_config)\n",
    "print(\"\\n--- Optimisations S3A ---\")\n",
    "print(s3a_optimizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3_auth",
   "metadata": {},
   "source": [
    "### 6.5 Authentification & IAM\n",
    "\n",
    "| MÃ©thode | SÃ©curitÃ© | Usage | Production ? |\n",
    "|---------|----------|-------|--------------|\n",
    "| **Access Keys** | âš ï¸ Faible | Dev local | âŒ Non |\n",
    "| **Instance Profile** | âœ… Haute | EC2 | âœ… Oui |\n",
    "| **IRSA** (IAM Roles for Service Accounts) | âœ… Haute | EKS/K8s | âœ… Oui |\n",
    "| **AssumeRole** | âœ… Haute | Cross-account | âœ… Oui |\n",
    "\n",
    "```text\n",
    "ğŸ” Best Practice : JAMAIS de credentials dans le code !\n",
    "\n",
    "Utiliser :\n",
    "- Variables d'environnement\n",
    "- Instance Profiles (EC2)\n",
    "- IRSA (Kubernetes) â† Module 21\n",
    "- AWS Secrets Manager\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_4",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 4 : Upload/Download avec boto3\n",
    "\n",
    "Ã‰cris un script Python qui :\n",
    "1. CrÃ©e un fichier CSV local avec 3 lignes\n",
    "2. L'upload vers S3 (ou MinIO)\n",
    "3. Liste les fichiers du bucket\n",
    "4. TÃ©lÃ©charge le fichier sous un autre nom\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la solution</summary>\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import csv\n",
    "\n",
    "# 1. CrÃ©er un fichier CSV\n",
    "with open('test_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'name', 'amount'])\n",
    "    writer.writerow([1, 'Alice', 100])\n",
    "    writer.writerow([2, 'Bob', 200])\n",
    "    writer.writerow([3, 'Charlie', 150])\n",
    "\n",
    "# 2. Upload\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file('test_data.csv', 'my-bucket', 'bronze/test_data.csv')\n",
    "\n",
    "# 3. List\n",
    "response = s3.list_objects_v2(Bucket='my-bucket', Prefix='bronze/')\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])\n",
    "\n",
    "# 4. Download\n",
    "s3.download_file('my-bucket', 'bronze/test_data.csv', 'downloaded_data.csv')\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azure_blob",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”µ 7. Azure Blob Storage â€” Deep Dive\n",
    "\n",
    "### 7.1 Concepts\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   AZURE STORAGE ACCOUNT                     â”‚\n",
    "â”‚                   (mystorageaccount)                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
    "â”‚  â”‚   Container:    â”‚  â”‚   Container:    â”‚                  â”‚\n",
    "â”‚  â”‚   bronze        â”‚  â”‚   silver        â”‚                  â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚\n",
    "â”‚  â”‚  â”‚ data.csv  â”‚  â”‚  â”‚  â”‚ data.parq â”‚  â”‚                  â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "| Concept Azure | Ã‰quivalent S3 |\n",
    "|---------------|---------------|\n",
    "| Storage Account | (pas d'Ã©quivalent, niveau compte) |\n",
    "| Container | Bucket |\n",
    "| Blob | Object |\n",
    "| ADLS Gen2 | S3 + Glue Catalog intÃ©grÃ© |\n",
    "\n",
    "### 7.2 Access Tiers\n",
    "\n",
    "| Tier | Usage | CoÃ»t stockage | CoÃ»t accÃ¨s |\n",
    "|------|-------|---------------|------------|\n",
    "| Hot | FrÃ©quent | Ã‰levÃ© | Faible |\n",
    "| Cool | Rare (30+ jours) | Moyen | Moyen |\n",
    "| Archive | Archivage (180+ jours) | TrÃ¨s faible | Ã‰levÃ© |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "azure_python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install azure-storage-blob\n",
    "\n",
    "azure_blob_example = \"\"\"\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient\n",
    "\n",
    "# Connection string (dev only !)\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# CrÃ©er le client\n",
    "blob_service = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Upload\n",
    "blob_client = blob_service.get_blob_client(container=\"bronze\", blob=\"data.csv\")\n",
    "with open(\"data.csv\", \"rb\") as f:\n",
    "    blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "# Download\n",
    "with open(\"downloaded.csv\", \"wb\") as f:\n",
    "    f.write(blob_client.download_blob().readall())\n",
    "\n",
    "# List blobs\n",
    "container_client = blob_service.get_container_client(\"bronze\")\n",
    "for blob in container_client.list_blobs():\n",
    "    print(blob.name)\n",
    "\"\"\"\n",
    "print(azure_blob_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azure_spark",
   "metadata": {},
   "source": [
    "### 7.3 Azure Blob avec Spark (abfss://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_azure_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_azure_config = \"\"\"\n",
    "# Configuration Spark pour Azure Blob (ADLS Gen2)\n",
    "storage_account = \"mystorageaccount\"\n",
    "container = \"bronze\"\n",
    "\n",
    "# MÃ©thode 1 : Access Key (dev only)\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n",
    "    \"YOUR_ACCESS_KEY\"\n",
    ")\n",
    "\n",
    "# MÃ©thode 2 : Service Principal (production)\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n",
    "               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", \"CLIENT_ID\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", \"CLIENT_SECRET\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n",
    "               \"https://login.microsoftonline.com/TENANT_ID/oauth2/token\")\n",
    "\n",
    "# Lire\n",
    "df = spark.read.parquet(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/data/\")\n",
    "\"\"\"\n",
    "print(spark_azure_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "azure_auth",
   "metadata": {},
   "source": [
    "### 7.4 Authentification Azure\n",
    "\n",
    "| MÃ©thode | SÃ©curitÃ© | Usage | Production ? |\n",
    "|---------|----------|-------|--------------|\n",
    "| **Access Keys** | âš ï¸ Faible | Dev | âŒ Non |\n",
    "| **SAS Token** | âš ï¸ Moyenne | Temporaire, externe | âš ï¸ LimitÃ© |\n",
    "| **Service Principal** | âœ… Haute | Apps, CI/CD | âœ… Oui |\n",
    "| **Managed Identity** | âœ… TrÃ¨s haute | VMs, AKS | âœ… Oui |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_5",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 5 : GÃ©nÃ©rer un SAS Token\n",
    "\n",
    "Un SAS Token permet de donner un accÃ¨s temporaire Ã  un blob.\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la solution</summary>\n",
    "\n",
    "```python\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "account_name = \"mystorageaccount\"\n",
    "account_key = \"YOUR_KEY\"\n",
    "container = \"bronze\"\n",
    "blob_name = \"data.csv\"\n",
    "\n",
    "# GÃ©nÃ©rer SAS Token (valide 1 heure)\n",
    "sas_token = generate_blob_sas(\n",
    "    account_name=account_name,\n",
    "    container_name=container,\n",
    "    blob_name=blob_name,\n",
    "    account_key=account_key,\n",
    "    permission=BlobSasPermissions(read=True),\n",
    "    expiry=datetime.utcnow() + timedelta(hours=1)\n",
    ")\n",
    "\n",
    "# URL complÃ¨te\n",
    "url = f\"https://{account_name}.blob.core.windows.net/{container}/{blob_name}?{sas_token}\"\n",
    "print(url)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcs",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŸ¡ 8. Google Cloud Storage â€” Deep Dive\n",
    "\n",
    "### 8.1 Concepts\n",
    "\n",
    "| Classe | Usage | SLA | CoÃ»t |\n",
    "|--------|-------|-----|------|\n",
    "| Standard | FrÃ©quent | 99.99% | $0.020/GB |\n",
    "| Nearline | 1x/mois | 99.9% | $0.010/GB |\n",
    "| Coldline | 1x/trimestre | 99.9% | $0.004/GB |\n",
    "| Archive | 1x/an | 99.9% | $0.0012/GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcs_cli",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_cli_commands = \"\"\"\n",
    "# gsutil - CLI pour GCS\n",
    "\n",
    "# Lister les buckets\n",
    "gsutil ls\n",
    "\n",
    "# Lister le contenu d'un bucket\n",
    "gsutil ls gs://my-bucket/bronze/\n",
    "\n",
    "# Copier\n",
    "gsutil cp data.csv gs://my-bucket/bronze/\n",
    "gsutil cp gs://my-bucket/bronze/data.csv ./\n",
    "\n",
    "# Synchroniser (comme rsync)\n",
    "gsutil rsync -r ./local/ gs://my-bucket/bronze/\n",
    "\n",
    "# Copie parallÃ¨le (gros fichiers)\n",
    "gsutil -m cp -r ./data/ gs://my-bucket/bronze/\n",
    "\"\"\"\n",
    "print(gcs_cli_commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gcs_python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install google-cloud-storage\n",
    "\n",
    "gcs_python_example = \"\"\"\n",
    "from google.cloud import storage\n",
    "\n",
    "# CrÃ©er le client (utilise GOOGLE_APPLICATION_CREDENTIALS)\n",
    "client = storage.Client()\n",
    "\n",
    "# AccÃ©der au bucket\n",
    "bucket = client.bucket(\"my-bucket\")\n",
    "\n",
    "# Upload\n",
    "blob = bucket.blob(\"bronze/data.csv\")\n",
    "blob.upload_from_filename(\"data.csv\")\n",
    "\n",
    "# Download\n",
    "blob.download_to_filename(\"downloaded.csv\")\n",
    "\n",
    "# List blobs\n",
    "blobs = bucket.list_blobs(prefix=\"bronze/\")\n",
    "for blob in blobs:\n",
    "    print(blob.name)\n",
    "\n",
    "# Signed URL (temporaire)\n",
    "url = blob.generate_signed_url(expiration=3600)  # 1 heure\n",
    "\"\"\"\n",
    "print(gcs_python_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcs_spark",
   "metadata": {},
   "source": [
    "### 8.3 GCS avec Spark (gs://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_gcs_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_gcs_config = \"\"\"\n",
    "# Configuration Spark pour GCS\n",
    "\n",
    "# Option 1 : Service Account Key (dev)\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/keyfile.json\")\n",
    "\n",
    "# Option 2 : Application Default Credentials (GKE, Cloud Functions)\n",
    "# Pas de config nÃ©cessaire si ADC est configurÃ©\n",
    "\n",
    "# Lire depuis GCS\n",
    "df = spark.read.parquet(\"gs://my-bucket/silver/data/\")\n",
    "\n",
    "# Ã‰crire vers GCS\n",
    "df.write.mode(\"overwrite\").parquet(\"gs://my-bucket/gold/aggregates/\")\n",
    "\"\"\"\n",
    "print(spark_gcs_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gcs_auth",
   "metadata": {},
   "source": [
    "### 8.4 Authentification GCP\n",
    "\n",
    "| MÃ©thode | SÃ©curitÃ© | Usage | Production ? |\n",
    "|---------|----------|-------|--------------|\n",
    "| **Service Account Key** | âš ï¸ Moyenne | Dev, CI/CD | âš ï¸ Avec prÃ©caution |\n",
    "| **Workload Identity** | âœ… Haute | GKE | âœ… Oui |\n",
    "| **ADC** (Application Default Credentials) | âœ… Haute | Cloud Functions, Cloud Run | âœ… Oui |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_6",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 6 : Lister et tÃ©lÃ©charger depuis GCS\n",
    "\n",
    "Ã‰cris un script qui liste tous les fichiers `.parquet` dans un bucket et tÃ©lÃ©charge le premier.\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la solution</summary>\n",
    "\n",
    "```python\n",
    "from google.cloud import storage\n",
    "\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(\"my-bucket\")\n",
    "\n",
    "# Lister les fichiers .parquet\n",
    "parquet_files = []\n",
    "for blob in bucket.list_blobs(prefix=\"silver/\"):\n",
    "    if blob.name.endswith('.parquet'):\n",
    "        parquet_files.append(blob)\n",
    "        print(f\"Found: {blob.name}\")\n",
    "\n",
    "# TÃ©lÃ©charger le premier\n",
    "if parquet_files:\n",
    "    first_file = parquet_files[0]\n",
    "    first_file.download_to_filename(\"downloaded.parquet\")\n",
    "    print(f\"Downloaded: {first_file.name}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minio",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŸ¢ 9. MinIO â€” Object Storage Local\n",
    "\n",
    "### 9.1 Pourquoi MinIO ?\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **100% S3 compatible** | MÃªme API, mÃªme code boto3 |\n",
    "| **Gratuit** | Open-source, pas de compte cloud |\n",
    "| **Local** | Parfait pour dev/test |\n",
    "| **LÃ©ger** | Docker, un seul binaire |\n",
    "| **Production-ready** | UtilisÃ© aussi en production (on-premise) |\n",
    "\n",
    "```text\n",
    "ğŸ’¡ Le code Ã©crit pour MinIO fonctionne sur S3 sans modification !\n",
    "   Il suffit de changer l'endpoint.\n",
    "```\n",
    "\n",
    "### 9.2 Installation avec Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minio_docker",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /tmp/minio/docker-compose.yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  minio:\n",
    "    image: minio/minio:latest\n",
    "    container_name: minio\n",
    "    ports:\n",
    "      - \"9000:9000\"   # API S3\n",
    "      - \"9001:9001\"   # Console Web\n",
    "    environment:\n",
    "      MINIO_ROOT_USER: minioadmin\n",
    "      MINIO_ROOT_PASSWORD: minioadmin\n",
    "    command: server /data --console-address \":9001\"\n",
    "    volumes:\n",
    "      - minio-data:/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n",
    "      interval: 30s\n",
    "      timeout: 10s\n",
    "      retries: 3\n",
    "\n",
    "volumes:\n",
    "  minio-data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minio_commands",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_commands = \"\"\"\n",
    "# DÃ©marrer MinIO\n",
    "docker-compose up -d\n",
    "\n",
    "# AccÃ©der Ã  la console web\n",
    "# http://localhost:9001\n",
    "# Login: minioadmin / minioadmin\n",
    "\n",
    "# Installer mc (MinIO Client)\n",
    "# Linux\n",
    "wget https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "chmod +x mc\n",
    "sudo mv mc /usr/local/bin/\n",
    "\n",
    "# Mac\n",
    "brew install minio/stable/mc\n",
    "\n",
    "# Configurer mc\n",
    "mc alias set myminio http://localhost:9000 minioadmin minioadmin\n",
    "\n",
    "# CrÃ©er des buckets\n",
    "mc mb myminio/bronze\n",
    "mc mb myminio/silver\n",
    "mc mb myminio/gold\n",
    "\n",
    "# Lister\n",
    "mc ls myminio/\n",
    "\n",
    "# Upload\n",
    "mc cp data.csv myminio/bronze/\n",
    "\"\"\"\n",
    "print(minio_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minio_python",
   "metadata": {},
   "source": [
    "### 9.4 Python avec MinIO (boto3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minio_boto3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le mÃªme code boto3 fonctionne avec MinIO !\n",
    "# Il suffit de spÃ©cifier endpoint_url\n",
    "\n",
    "minio_boto3_example = \"\"\"\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "# Configuration pour MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9000',  # â† La seule diffÃ©rence !\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin',\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "# CrÃ©er un bucket\n",
    "s3.create_bucket(Bucket='bronze')\n",
    "\n",
    "# Upload (identique Ã  S3)\n",
    "s3.upload_file('data.csv', 'bronze', 'raw/data.csv')\n",
    "\n",
    "# List (identique Ã  S3)\n",
    "response = s3.list_objects_v2(Bucket='bronze')\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])\n",
    "\n",
    "# Download (identique Ã  S3)\n",
    "s3.download_file('bronze', 'raw/data.csv', 'downloaded.csv')\n",
    "\"\"\"\n",
    "print(minio_boto3_example)\n",
    "print(\"\\nğŸ’¡ Ce code fonctionne sur S3 en enlevant juste endpoint_url !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minio_spark",
   "metadata": {},
   "source": [
    "### 9.5 Spark avec MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark_minio_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_minio_config = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Access\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lire depuis MinIO (mÃªme syntaxe que S3 !)\n",
    "df = spark.read.csv(\"s3a://bronze/raw/data.csv\", header=True)\n",
    "\n",
    "# Ã‰crire vers MinIO\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://silver/clean/data/\")\n",
    "\"\"\"\n",
    "print(spark_minio_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_7",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 7 : DÃ©ployer MinIO et crÃ©er un bucket\n",
    "\n",
    "1. Lance MinIO avec Docker\n",
    "2. AccÃ¨de Ã  la console (http://localhost:9001)\n",
    "3. CrÃ©e les buckets bronze, silver, gold\n",
    "4. Upload un fichier via la console\n",
    "\n",
    "```bash\n",
    "# Solution rapide\n",
    "docker run -d --name minio \\\n",
    "  -p 9000:9000 -p 9001:9001 \\\n",
    "  -e MINIO_ROOT_USER=minioadmin \\\n",
    "  -e MINIO_ROOT_PASSWORD=minioadmin \\\n",
    "  minio/minio server /data --console-address \":9001\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ 10. Performance & Optimisation\n",
    "\n",
    "### 10.1 Le problÃ¨me des petits fichiers\n",
    "\n",
    "```text\n",
    "âŒ MAUVAIS : 10,000 fichiers Ã— 1 MB = 10 GB\n",
    "   - 10,000 requÃªtes API (LIST + GET)\n",
    "   - Overhead Ã©norme pour Spark\n",
    "   - Temps de lecture : minutes\n",
    "\n",
    "âœ… BON : 100 fichiers Ã— 100 MB = 10 GB\n",
    "   - 100 requÃªtes API\n",
    "   - ParallÃ©lisme optimal\n",
    "   - Temps de lecture : secondes\n",
    "```\n",
    "\n",
    "**Taille idÃ©ale** : 100 MB - 1 GB par fichier\n",
    "\n",
    "**Solutions** :\n",
    "- `df.coalesce(n)` ou `df.repartition(n)` avant Ã©criture\n",
    "- Compaction pÃ©riodique\n",
    "- Delta Lake / Iceberg (Module 23) = compaction automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small_files_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰viter les petits fichiers avec Spark\n",
    "small_files_solution = \"\"\"\n",
    "# Lecture de beaucoup de petits fichiers\n",
    "df = spark.read.parquet(\"s3a://bronze/data/\")  # 10,000 fichiers\n",
    "\n",
    "# âŒ Ã‰criture directe = mÃªme nombre de fichiers\n",
    "# df.write.parquet(\"s3a://silver/data/\")\n",
    "\n",
    "# âœ… Repartitionner avant d'Ã©crire\n",
    "df.repartition(100) \\  # 100 fichiers de ~100 MB\n",
    "  .write.mode(\"overwrite\") \\\n",
    "  .parquet(\"s3a://silver/data/\")\n",
    "\n",
    "# âœ… Ou coalesce (moins de shuffle)\n",
    "df.coalesce(100) \\\n",
    "  .write.mode(\"overwrite\") \\\n",
    "  .parquet(\"s3a://silver/data/\")\n",
    "\"\"\"\n",
    "print(small_files_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partitioning",
   "metadata": {},
   "source": [
    "### 10.2 Partitionnement\n",
    "\n",
    "```text\n",
    "s3://bucket/silver/sales/\n",
    "â”œâ”€â”€ year=2024/\n",
    "â”‚   â”œâ”€â”€ month=01/\n",
    "â”‚   â”‚   â”œâ”€â”€ day=01/\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ part-00000.parquet\n",
    "â”‚   â”‚   â””â”€â”€ day=02/\n",
    "â”‚   â”‚       â””â”€â”€ part-00000.parquet\n",
    "â”‚   â””â”€â”€ month=02/\n",
    "â”‚       â””â”€â”€ ...\n",
    "â””â”€â”€ year=2023/\n",
    "    â””â”€â”€ ...\n",
    "```\n",
    "\n",
    "**Avantages** :\n",
    "- Partition pruning (Spark ne lit que les partitions nÃ©cessaires)\n",
    "- RequÃªtes plus rapides\n",
    "\n",
    "**âš ï¸ Attention au over-partitioning** :\n",
    "- Trop de partitions = trop de petits fichiers\n",
    "- RÃ¨gle : max 10,000 partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partitioning_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitionnement avec Spark\n",
    "partitioning_example = \"\"\"\n",
    "# Ã‰criture partitionnÃ©e\n",
    "df.write \\\n",
    "  .partitionBy(\"year\", \"month\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .parquet(\"s3a://silver/sales/\")\n",
    "\n",
    "# Lecture avec partition pruning\n",
    "df = spark.read.parquet(\"s3a://silver/sales/\")\n",
    "\n",
    "# Cette requÃªte ne lit QUE year=2024/month=01\n",
    "df.filter(\"year = 2024 AND month = 1\").show()\n",
    "\"\"\"\n",
    "print(partitioning_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "file_formats",
   "metadata": {},
   "source": [
    "### 10.3 Formats de fichiers\n",
    "\n",
    "| Format | Type | Compression | Lecture colonnes | Use case |\n",
    "|--------|------|-------------|------------------|----------|\n",
    "| **CSV** | Row | Non | âŒ | Ã‰change, debug |\n",
    "| **JSON** | Row | Non | âŒ | APIs, logs |\n",
    "| **Avro** | Row | Oui | âŒ | Streaming, Kafka |\n",
    "| **Parquet** | Columnar | Oui | âœ… | Analytics, Data Lake |\n",
    "| **ORC** | Columnar | Oui | âœ… | Hive, analytics |\n",
    "\n",
    "```text\n",
    "ğŸ’¡ Pour le Data Engineering : PARQUET est le standard\n",
    "   - Compression excellente (snappy, zstd)\n",
    "   - Lecture par colonnes\n",
    "   - Predicate pushdown\n",
    "   - Schema intÃ©grÃ©\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_8",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 8 : Comparer Parquet vs CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercice : Comparer la taille et le temps de lecture\n",
    "\n",
    "format_comparison = \"\"\"\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Format Comparison\").getOrCreate()\n",
    "\n",
    "# CrÃ©er un DataFrame de test (1M lignes)\n",
    "df = spark.range(1000000).toDF(\"id\") \\\n",
    "    .withColumn(\"name\", lit(\"test_name\")) \\\n",
    "    .withColumn(\"amount\", rand() * 1000)\n",
    "\n",
    "# Ã‰crire en CSV\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").csv(\"s3a://test/csv/\")\n",
    "csv_write_time = time.time() - start\n",
    "\n",
    "# Ã‰crire en Parquet\n",
    "start = time.time()\n",
    "df.write.mode(\"overwrite\").parquet(\"s3a://test/parquet/\")\n",
    "parquet_write_time = time.time() - start\n",
    "\n",
    "# Lire CSV\n",
    "start = time.time()\n",
    "df_csv = spark.read.csv(\"s3a://test/csv/\").count()\n",
    "csv_read_time = time.time() - start\n",
    "\n",
    "# Lire Parquet\n",
    "start = time.time()\n",
    "df_parquet = spark.read.parquet(\"s3a://test/parquet/\").count()\n",
    "parquet_read_time = time.time() - start\n",
    "\n",
    "print(f\"CSV Write: {csv_write_time:.2f}s, Read: {csv_read_time:.2f}s\")\n",
    "print(f\"Parquet Write: {parquet_write_time:.2f}s, Read: {parquet_read_time:.2f}s\")\n",
    "\"\"\"\n",
    "print(format_comparison)\n",
    "print(\"\\nğŸ’¡ RÃ©sultat attendu : Parquet 5-10x plus rapide et 5-10x plus petit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_9",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 9 : Impact de la taille des fichiers\n",
    "\n",
    "**Objectif** : Mesurer l'impact du nombre de fichiers sur les performances.\n",
    "\n",
    "```python\n",
    "# ScÃ©nario A : 1000 petits fichiers\n",
    "df.repartition(1000).write.parquet(\"s3a://test/small-files/\")\n",
    "\n",
    "# ScÃ©nario B : 10 gros fichiers\n",
    "df.repartition(10).write.parquet(\"s3a://test/large-files/\")\n",
    "\n",
    "# Mesurer le temps de lecture pour chaque\n",
    "```\n",
    "\n",
    "**RÃ©sultat attendu** : ScÃ©nario B sera beaucoup plus rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "security",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”’ 11. SÃ©curitÃ© & Gouvernance\n",
    "\n",
    "### 11.1 Encryption\n",
    "\n",
    "| Type | Description | Qui gÃ¨re la clÃ© ? |\n",
    "|------|-------------|-------------------|\n",
    "| **SSE-S3** | Encryption cÃ´tÃ© serveur, clÃ© AWS | AWS |\n",
    "| **SSE-KMS** | Encryption avec AWS KMS | AWS (tu choisis la clÃ©) |\n",
    "| **SSE-C** | Encryption avec clÃ© client | Toi |\n",
    "| **Client-side** | Encryption avant upload | Toi |\n",
    "\n",
    "```text\n",
    "ğŸ’¡ Best Practice : SSE-KMS pour la plupart des cas\n",
    "   - Rotation automatique des clÃ©s\n",
    "   - Audit dans CloudTrail\n",
    "   - ContrÃ´le d'accÃ¨s fin\n",
    "```\n",
    "\n",
    "### 11.2 Access Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bucket_policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de bucket policy S3\n",
    "bucket_policy_example = \"\"\"\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AllowDataTeamRead\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": \"arn:aws:iam::123456789:role/DataEngineerRole\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::my-datalake\",\n",
    "                \"arn:aws:s3:::my-datalake/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"DenyPublicAccess\",\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": \"arn:aws:s3:::my-datalake/*\",\n",
    "            \"Condition\": {\n",
    "                \"Bool\": {\n",
    "                    \"aws:SecureTransport\": \"false\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "print(bucket_policy_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "audit",
   "metadata": {},
   "source": [
    "### 11.3 Audit & Compliance\n",
    "\n",
    "| Service | Cloud | Ce qu'il trace |\n",
    "|---------|-------|----------------|\n",
    "| **S3 Access Logs** | AWS | Qui accÃ¨de Ã  quoi |\n",
    "| **CloudTrail** | AWS | Toutes les API calls |\n",
    "| **Activity Logs** | Azure | OpÃ©rations sur Blob |\n",
    "| **Audit Logs** | GCP | AccÃ¨s aux ressources |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "costs",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’° 12. CoÃ»ts & Optimisation financiÃ¨re\n",
    "\n",
    "### 12.1 Composantes du coÃ»t\n",
    "\n",
    "| Composante | Description | Exemple S3 |\n",
    "|------------|-------------|------------|\n",
    "| **Stockage** | GB/mois | $0.023/GB (Standard) |\n",
    "| **PUT/POST** | Ã‰critures | $0.005 / 1000 requÃªtes |\n",
    "| **GET** | Lectures | $0.0004 / 1000 requÃªtes |\n",
    "| **LIST** | Listing | $0.005 / 1000 requÃªtes |\n",
    "| **Egress** | Sortie du cloud | $0.09/GB (vers Internet) |\n",
    "\n",
    "### 12.2 CoÃ»ts cachÃ©s\n",
    "\n",
    "```text\n",
    "âš ï¸ Attention aux coÃ»ts cachÃ©s :\n",
    "\n",
    "1. LISTING frÃ©quent\n",
    "   - Spark fait un LIST avant chaque lecture\n",
    "   - 1M de fichiers = 1000 LIST calls = $5\n",
    "\n",
    "2. EGRESS\n",
    "   - DonnÃ©es sortant du cloud = coÃ»teux\n",
    "   - Cross-region = $0.02/GB\n",
    "   - Vers Internet = $0.09/GB\n",
    "\n",
    "3. Small files\n",
    "   - Plus de requÃªtes API\n",
    "   - Plus de listing\n",
    "```\n",
    "\n",
    "### 12.3 Lifecycle Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lifecycle_policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de lifecycle policy S3\n",
    "lifecycle_policy = \"\"\"\n",
    "{\n",
    "    \"Rules\": [\n",
    "        {\n",
    "            \"ID\": \"ArchiveBronzeData\",\n",
    "            \"Status\": \"Enabled\",\n",
    "            \"Filter\": {\n",
    "                \"Prefix\": \"bronze/\"\n",
    "            },\n",
    "            \"Transitions\": [\n",
    "                {\n",
    "                    \"Days\": 30,\n",
    "                    \"StorageClass\": \"STANDARD_IA\"\n",
    "                },\n",
    "                {\n",
    "                    \"Days\": 90,\n",
    "                    \"StorageClass\": \"GLACIER\"\n",
    "                }\n",
    "            ],\n",
    "            \"Expiration\": {\n",
    "                \"Days\": 365\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\"\"\"\n",
    "print(lifecycle_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise_10",
   "metadata": {},
   "source": [
    "### ğŸ‹ï¸ Exercice 10 : Estimer le coÃ»t mensuel d'un Data Lake\n",
    "\n",
    "**ScÃ©nario** :\n",
    "- Bronze : 1 TB (accÃ¨s rare)\n",
    "- Silver : 500 GB (accÃ¨s frÃ©quent)\n",
    "- Gold : 100 GB (accÃ¨s trÃ¨s frÃ©quent)\n",
    "- 100,000 requÃªtes GET/jour\n",
    "- 10,000 requÃªtes PUT/jour\n",
    "- 50 GB egress/mois\n",
    "\n",
    "Calcule le coÃ»t mensuel sur AWS S3.\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la solution</summary>\n",
    "\n",
    "| Ã‰lÃ©ment | Calcul | CoÃ»t |\n",
    "|---------|--------|------|\n",
    "| Bronze (S3 IA) | 1000 GB Ã— $0.0125 | $12.50 |\n",
    "| Silver (Standard) | 500 GB Ã— $0.023 | $11.50 |\n",
    "| Gold (Standard) | 100 GB Ã— $0.023 | $2.30 |\n",
    "| GET requests | 100K Ã— 30 / 1000 Ã— $0.0004 | $1.20 |\n",
    "| PUT requests | 10K Ã— 30 / 1000 Ã— $0.005 | $1.50 |\n",
    "| Egress | 50 GB Ã— $0.09 | $4.50 |\n",
    "| **Total** | | **~$33.50/mois** |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ 13. Mini-Projet : Data Lake avec MinIO\n",
    "\n",
    "### ğŸ¯ Objectif\n",
    "\n",
    "Construire un Data Lake local complet avec MinIO.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       MinIO (Docker)                            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\n",
    "â”‚  â”‚   bronze/   â”‚    â”‚   silver/   â”‚    â”‚    gold/    â”‚        â”‚\n",
    "â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\n",
    "â”‚  â”‚  raw.csv    â”‚â”€â”€â”€â–¶â”‚ clean.parq  â”‚â”€â”€â”€â–¶â”‚  agg.parq   â”‚        â”‚\n",
    "â”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\n",
    "â”‚        â–²                  â–²                  â–²                  â”‚\n",
    "â”‚        â”‚                  â”‚                  â”‚                  â”‚\n",
    "â”‚     Upload            Transform          Aggregate              â”‚\n",
    "â”‚    (boto3)           (PySpark)         (Spark SQL)             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Ã‰tapes du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰tape 1 : DÃ©marrer MinIO\n",
    "print(\"\"\"\n",
    "# Terminal 1 : DÃ©marrer MinIO\n",
    "docker run -d --name minio \\\n",
    "  -p 9000:9000 -p 9001:9001 \\\n",
    "  -e MINIO_ROOT_USER=minioadmin \\\n",
    "  -e MINIO_ROOT_PASSWORD=minioadmin \\\n",
    "  minio/minio server /data --console-address \":9001\"\n",
    "\n",
    "# VÃ©rifier que MinIO tourne\n",
    "curl http://localhost:9000/minio/health/live\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰tape 2 : CrÃ©er les buckets et uploader les donnÃ©es\n",
    "\n",
    "step2_code = \"\"\"\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import csv\n",
    "\n",
    "# Connexion Ã  MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url='http://localhost:9000',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin',\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "# CrÃ©er les buckets\n",
    "for bucket in ['bronze', 'silver', 'gold']:\n",
    "    try:\n",
    "        s3.create_bucket(Bucket=bucket)\n",
    "        print(f\"âœ… Bucket '{bucket}' crÃ©Ã©\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Bucket '{bucket}' existe dÃ©jÃ \")\n",
    "\n",
    "# CrÃ©er des donnÃ©es de test\n",
    "with open('sales_data.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['date', 'product', 'category', 'amount', 'quantity'])\n",
    "    writer.writerow(['2024-01-01', 'Laptop', 'Electronics', 1200, 1])\n",
    "    writer.writerow(['2024-01-01', 'Mouse', 'Electronics', 25, 3])\n",
    "    writer.writerow(['2024-01-02', 'Desk', 'Furniture', 350, 1])\n",
    "    writer.writerow(['2024-01-02', 'Chair', 'Furniture', 150, 2])\n",
    "    writer.writerow(['2024-01-03', 'Laptop', 'Electronics', 1200, 2])\n",
    "    writer.writerow(['2024-01-03', 'Monitor', 'Electronics', 400, 1])\n",
    "\n",
    "# Upload vers bronze\n",
    "s3.upload_file('sales_data.csv', 'bronze', 'raw/sales_data.csv')\n",
    "print(\"âœ… DonnÃ©es uploadÃ©es vers bronze/raw/sales_data.csv\")\n",
    "\"\"\"\n",
    "print(step2_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰tape 3 : Transformer avec PySpark (Bronze â†’ Silver)\n",
    "\n",
    "step3_code = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, upper\n",
    "\n",
    "# CrÃ©er SparkSession avec config MinIO\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Bronze to Silver\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lire depuis Bronze\n",
    "df_bronze = spark.read.csv(\n",
    "    \"s3a://bronze/raw/sales_data.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "print(\"ğŸ“¥ DonnÃ©es Bronze:\")\n",
    "df_bronze.show()\n",
    "\n",
    "# Transformer\n",
    "df_silver = df_bronze \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"))) \\\n",
    "    .withColumn(\"category\", upper(col(\"category\"))) \\\n",
    "    .withColumn(\"total\", col(\"amount\") * col(\"quantity\")) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "print(\"ğŸ”„ DonnÃ©es transformÃ©es:\")\n",
    "df_silver.show()\n",
    "\n",
    "# Ã‰crire vers Silver (Parquet partitionnÃ©)\n",
    "df_silver.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(\"s3a://silver/sales/\")\n",
    "\n",
    "print(\"âœ… DonnÃ©es Ã©crites vers silver/sales/\")\n",
    "\"\"\"\n",
    "print(step3_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_step4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰tape 4 : AgrÃ©ger avec Spark SQL (Silver â†’ Gold)\n",
    "\n",
    "step4_code = \"\"\"\n",
    "# Lire depuis Silver\n",
    "df_silver = spark.read.parquet(\"s3a://silver/sales/\")\n",
    "\n",
    "# CrÃ©er une vue temporaire\n",
    "df_silver.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# AgrÃ©gation avec Spark SQL\n",
    "df_gold = spark.sql(\\\"\\\"\\\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as num_transactions,\n",
    "        SUM(quantity) as total_quantity,\n",
    "        SUM(total) as total_revenue,\n",
    "        AVG(total) as avg_transaction\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "\\\"\\\"\\\")\n",
    "\n",
    "print(\"ğŸ“Š AgrÃ©gations Gold:\")\n",
    "df_gold.show()\n",
    "\n",
    "# Ã‰crire vers Gold\n",
    "df_gold.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3a://gold/category_summary/\")\n",
    "\n",
    "print(\"âœ… DonnÃ©es Ã©crites vers gold/category_summary/\")\n",
    "\"\"\"\n",
    "print(step4_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_step5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰tape 5 : VÃ©rifier les rÃ©sultats\n",
    "\n",
    "step5_code = \"\"\"\n",
    "# Lister les fichiers crÃ©Ã©s\n",
    "import subprocess\n",
    "\n",
    "# Avec mc CLI\n",
    "print(\"ğŸ“ Contenu de bronze/:\")\n",
    "!mc ls myminio/bronze/ --recursive\n",
    "\n",
    "print(\"\\nğŸ“ Contenu de silver/:\")\n",
    "!mc ls myminio/silver/ --recursive\n",
    "\n",
    "print(\"\\nğŸ“ Contenu de gold/:\")\n",
    "!mc ls myminio/gold/ --recursive\n",
    "\n",
    "# Ou avec boto3\n",
    "for bucket in ['bronze', 'silver', 'gold']:\n",
    "    print(f\"\\nğŸ“ {bucket}/\")\n",
    "    response = s3.list_objects_v2(Bucket=bucket)\n",
    "    for obj in response.get('Contents', []):\n",
    "        print(f\"   {obj['Key']} ({obj['Size']} bytes)\")\n",
    "\"\"\"\n",
    "print(step5_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Quiz de fin de module\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Quelle est la diffÃ©rence entre un prefix et un dossier dans S3 ?\n",
    "a) Aucune diffÃ©rence  \n",
    "b) Un prefix est un vrai dossier crÃ©Ã© par S3  \n",
    "c) Un prefix est juste une convention de nommage, S3 est un key-value store plat  \n",
    "d) Un dossier peut contenir des sous-dossiers, pas un prefix\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” S3 est un key-value store plat. Les \"/\" dans les clÃ©s sont juste des caractÃ¨res comme les autres.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Quel protocole utiliser pour lire S3 avec Spark ?\n",
    "a) `s3://`  \n",
    "b) `s3a://`  \n",
    "c) `https://`  \n",
    "d) `hdfs://`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `s3a://` est le protocole Hadoop optimisÃ© pour Spark. `s3://` est pour AWS CLI/boto3.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Pourquoi les petits fichiers sont-ils un problÃ¨me ?\n",
    "a) Ils prennent plus de place  \n",
    "b) Ils gÃ©nÃ¨rent trop de requÃªtes API et d'overhead  \n",
    "c) Ils ne sont pas supportÃ©s par Parquet  \n",
    "d) Ils ne peuvent pas Ãªtre partitionnÃ©s\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Chaque fichier = une requÃªte API. 10,000 fichiers = 10,000 GET requests = lent et coÃ»teux.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Quelle est la diffÃ©rence entre SAS Token et Managed Identity sur Azure ?\n",
    "a) SAS Token est plus sÃ©curisÃ©  \n",
    "b) Managed Identity est temporaire, SAS est permanent  \n",
    "c) SAS Token est temporaire et partageable, Managed Identity est liÃ©e Ã  une ressource Azure  \n",
    "d) Aucune diffÃ©rence\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” SAS Token = URL temporaire partageable. Managed Identity = identitÃ© attachÃ©e Ã  une VM/AKS, plus sÃ©curisÃ©.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Pourquoi MinIO est-il compatible avec S3 ?\n",
    "a) C'est un produit AWS  \n",
    "b) Il implÃ©mente la mÃªme API REST que S3  \n",
    "c) Il utilise les mÃªmes serveurs  \n",
    "d) Il copie les donnÃ©es depuis S3\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” MinIO implÃ©mente l'API S3 (REST). Le mÃªme code boto3 fonctionne avec les deux.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Quelle classe de stockage pour des donnÃ©es rarement lues ?\n",
    "a) S3 Standard  \n",
    "b) S3 Intelligent-Tiering  \n",
    "c) S3 Glacier  \n",
    "d) S3 One Zone-IA\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Glacier pour l'archivage (donnÃ©es rarement lues). Intelligent-Tiering si le pattern d'accÃ¨s est imprÃ©visible.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Quel est l'avantage du partitionnement dans un Data Lake ?\n",
    "a) Les fichiers sont plus petits  \n",
    "b) Spark peut ignorer les partitions non pertinentes (partition pruning)  \n",
    "c) Le stockage coÃ»te moins cher  \n",
    "d) Les donnÃ©es sont automatiquement compressÃ©es\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Partition pruning = Spark lit uniquement les partitions qui matchent le filtre.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Comment authentifier Spark on K8s vers S3 en production ?\n",
    "a) Access Keys dans le code  \n",
    "b) Variables d'environnement  \n",
    "c) IAM Roles for Service Accounts (IRSA)  \n",
    "d) Fichier de config local\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” IRSA permet d'associer un IAM Role Ã  un ServiceAccount K8s. Pas de credentials dans le code.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q9. Qu'est-ce qu'un Metadata Catalog (Glue, Hive Metastore) ?\n",
    "a) Un systÃ¨me de stockage  \n",
    "b) Un registre des schÃ©mas et partitions des tables  \n",
    "c) Un outil de requÃªtage SQL  \n",
    "d) Un systÃ¨me de cache\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Le catalog stocke les mÃ©tadonnÃ©es : schÃ©ma, localisation, partitions, stats. L'Object Storage ne stocke que les fichiers bruts.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q10. Quel format de fichier est recommandÃ© pour un Data Lake analytique ?\n",
    "a) CSV  \n",
    "b) JSON  \n",
    "c) Parquet  \n",
    "d) XML\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Parquet est columnar, compressÃ©, avec schema intÃ©grÃ©. IdÃ©al pour l'analytics.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)\n",
    "- [Azure Blob Storage](https://docs.microsoft.com/en-us/azure/storage/blobs/)\n",
    "- [Google Cloud Storage](https://cloud.google.com/storage/docs)\n",
    "- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)\n",
    "\n",
    "### ğŸ“– Articles & Tutoriels\n",
    "- [Spark + S3 Best Practices](https://spark.apache.org/docs/latest/cloud-integration.html)\n",
    "- [Data Lake Architecture](https://www.databricks.com/glossary/data-lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises l'Object Storage, passons aux **Table Formats** pour transformer ton Data Lake en **Lakehouse** !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `23_table_formats_delta_iceberg.ipynb`** â€” Delta Lake & Apache Iceberg\n",
    "\n",
    "Tu vas apprendre :\n",
    "- **Delta Lake** : ACID, Time Travel, Schema Evolution\n",
    "- **Apache Iceberg** : Table format open-source\n",
    "- **Transaction Log** : Comment Ã§a remplace le Metastore\n",
    "- **Optimisations** : Compaction, Z-Ordering, Vacuum\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ RÃ©capitulatif de ce module\n",
    "\n",
    "| Concept | Ce que tu as appris |\n",
    "|---------|--------------------|\n",
    "| **Cloud Computing** | IaaS, PaaS, SaaS, rÃ©gions |\n",
    "| **Services Cloud DE** | S3, Glue, BigQuery, Synapse... |\n",
    "| **Storage Models** | Block vs File vs Object |\n",
    "| **Object Storage** | Buckets, keys, prefixes, protocols |\n",
    "| **AWS S3** | boto3, s3a://, IAM |\n",
    "| **Azure Blob** | SDK, abfss://, SAS, Managed Identity |\n",
    "| **GCS** | gsutil, gs://, Workload Identity |\n",
    "| **MinIO** | Object Storage local S3-compatible |\n",
    "| **Performance** | Small files, partitioning, formats |\n",
    "| **SÃ©curitÃ©** | Encryption, bucket policies |\n",
    "| **CoÃ»ts** | Classes de stockage, lifecycle |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module Cloud & Object Storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
