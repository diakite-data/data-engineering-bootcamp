{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# âš¡ High Performance Python\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  **accÃ©lÃ©rer considÃ©rablement** tes pipelines Python. Tu dÃ©couvriras comment contourner les limitations du GIL, parallÃ©liser tes traitements, et gÃ©rer des fichiers plus grands que ta RAM !\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | MaÃ®triser Python (fonctions, classes) |\n",
    "| âœ… Requis | Avoir suivi le module `17_polars` |\n",
    "| ğŸ’¡ RecommandÃ© | ConnaÃ®tre Pandas |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre le **GIL** et ses implications sur la performance\n",
    "- **Profiler** ton code pour identifier les vrais goulots d'Ã©tranglement\n",
    "- Utiliser **concurrent.futures** pour parallÃ©liser simplement\n",
    "- MaÃ®triser **asyncio** pour l'I/O massivement parallÃ¨le\n",
    "- Exploiter **Dask** pour traiter des fichiers plus grands que la RAM\n",
    "- Choisir le **bon outil** selon ton problÃ¨me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/diakite-data/data-engineering-bootcamp/blob/main/notebooks/intermediate/18_high_performance_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
    "",
    "> ğŸ’¡ **Conseil** : Cliquez sur le badge ci-dessus pour exÃ©cuter ce notebook directement dans Google Colab (aucune installation requise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gil",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”’ 1. Le GIL : Comprendre la limitation fondamentale\n",
    "\n",
    "> ğŸ§  **Cette section est essentielle** pour comprendre pourquoi certaines techniques fonctionnent et d'autres non.\n",
    "\n",
    "### 1.1 Qu'est-ce que le GIL ?\n",
    "\n",
    "Le **Global Interpreter Lock (GIL)** est un verrou qui empÃªche Python d'exÃ©cuter plusieurs threads Python **simultanÃ©ment**.\n",
    "\n",
    "```text\n",
    "AVEC LE GIL (Python standard)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Thread 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘\n",
    "Thread 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps\n",
    "          \n",
    "â†’ Les threads s'exÃ©cutent en alternance, pas en parallÃ¨le !\n",
    "\n",
    "\n",
    "SANS GIL (multiprocessing)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Process 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "Process 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
    "           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps\n",
    "           \n",
    "â†’ Vraie exÃ©cution parallÃ¨le sur plusieurs CPUs !\n",
    "```\n",
    "\n",
    "### 1.2 CPU-bound vs I/O-bound\n",
    "\n",
    "| Type | Exemple | GIL bloquant ? | Solution |\n",
    "|------|---------|----------------|----------|\n",
    "| **CPU-bound** | Calculs, transformations, ETL | âœ… **Oui** | `multiprocessing`, `ProcessPoolExecutor` |\n",
    "| **I/O-bound** | API, fichiers, base de donnÃ©es | âŒ Non | `threading`, `asyncio` |\n",
    "\n",
    "**Pourquoi le GIL ne bloque pas l'I/O ?**\n",
    "\n",
    "Quand Python attend une rÃ©ponse rÃ©seau ou disque, il **relÃ¢che le GIL**. Un autre thread peut donc s'exÃ©cuter pendant ce temps d'attente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gil_info",
   "metadata": {},
   "source": [
    "> â„¹ï¸ **Le savais-tu ?**\n",
    ">\n",
    "> Le GIL a Ã©tÃ© introduit dans Python pour **simplifier la gestion de la mÃ©moire**. Il rend Python thread-safe par dÃ©faut, mais au prix de la performance multi-thread.\n",
    ">\n",
    "> Des projets comme **nogil** (Python 3.13+) et **subinterpreters** travaillent Ã  supprimer ou contourner cette limitation.\n",
    ">\n",
    "> En attendant, les Data Engineers utilisent **multiprocessing** pour contourner le GIL !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gil_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from multiprocessing import Process\n",
    "\n",
    "def cpu_intensive(n):\n",
    "    \"\"\"TÃ¢che CPU-bound : calcul intensif\"\"\"\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        total += i ** 2\n",
    "    return total\n",
    "\n",
    "N = 5_000_000\n",
    "\n",
    "# ============ SÃ‰QUENTIEL ============\n",
    "start = time.time()\n",
    "cpu_intensive(N)\n",
    "cpu_intensive(N)\n",
    "seq_time = time.time() - start\n",
    "print(f\"â±ï¸ SÃ©quentiel : {seq_time:.2f}s\")\n",
    "\n",
    "# ============ THREADING (bloquÃ© par GIL) ============\n",
    "start = time.time()\n",
    "t1 = threading.Thread(target=cpu_intensive, args=(N,))\n",
    "t2 = threading.Thread(target=cpu_intensive, args=(N,))\n",
    "t1.start(); t2.start()\n",
    "t1.join(); t2.join()\n",
    "thread_time = time.time() - start\n",
    "print(f\"ğŸ§µ Threading : {thread_time:.2f}s (GIL bloque!)\")\n",
    "\n",
    "# ============ MULTIPROCESSING (contourne GIL) ============\n",
    "start = time.time()\n",
    "p1 = Process(target=cpu_intensive, args=(N,))\n",
    "p2 = Process(target=cpu_intensive, args=(N,))\n",
    "p1.start(); p2.start()\n",
    "p1.join(); p2.join()\n",
    "proc_time = time.time() - start\n",
    "print(f\"ğŸš€ Multiprocessing : {proc_time:.2f}s (vraie parallÃ©lisation!)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speedup multiprocessing vs sÃ©quentiel : {seq_time/proc_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "profiling",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” 2. Identifier les goulots : Profiling\n",
    "\n",
    "> âš ï¸ **\"Premature optimization is the root of all evil\"** â€” Donald Knuth\n",
    ">\n",
    "> Avant d'optimiser, il faut **mesurer** pour identifier le vrai problÃ¨me.\n",
    "\n",
    "### 2.1 Outils de profiling\n",
    "\n",
    "| Outil | Usage | Comment l'utiliser |\n",
    "|-------|-------|--------------------|\n",
    "| `%%time` | Temps d'une cellule | Jupyter magic |\n",
    "| `%%timeit` | Temps moyen (plusieurs runs) | Jupyter magic |\n",
    "| `cProfile` | Profiling par fonction | `python -m cProfile script.py` |\n",
    "| `line_profiler` | Profiling ligne par ligne | `@profile` decorator |\n",
    "| `memory_profiler` | Usage RAM | `@profile` + `mprof run` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profiling_time",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time - mesure le temps d'exÃ©cution d'une cellule\n",
    "import time\n",
    "\n",
    "def slow_function():\n",
    "    total = 0\n",
    "    for i in range(1_000_000):\n",
    "        total += i\n",
    "    return total\n",
    "\n",
    "%time result = slow_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profiling_timeit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit - moyenne sur plusieurs exÃ©cutions\n",
    "def fast_function():\n",
    "    return sum(range(1_000_000))\n",
    "\n",
    "%timeit fast_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "profiling_cprofile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "\n",
    "def main_pipeline():\n",
    "    \"\"\"Pipeline simulÃ© avec plusieurs Ã©tapes\"\"\"\n",
    "    data = list(range(100_000))\n",
    "    \n",
    "    # Ã‰tape 1 : transformation\n",
    "    transformed = [x ** 2 for x in data]\n",
    "    \n",
    "    # Ã‰tape 2 : filtrage\n",
    "    filtered = [x for x in transformed if x % 2 == 0]\n",
    "    \n",
    "    # Ã‰tape 3 : agrÃ©gation\n",
    "    result = sum(filtered)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Profiler le code\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "result = main_pipeline()\n",
    "\n",
    "profiler.disable()\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "stream = StringIO()\n",
    "stats = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')\n",
    "stats.print_stats(10)\n",
    "print(stream.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overview",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ºï¸ 3. StratÃ©gies de performance : Vue d'ensemble\n",
    "\n",
    "| Besoin | Solution | Quand l'utiliser |\n",
    "|--------|----------|------------------|\n",
    "| Multi-CPU (CPU-bound) | `ProcessPoolExecutor` | ETL lourd, calculs |\n",
    "| I/O parallÃ¨le (simple) | `ThreadPoolExecutor` | < 20 requÃªtes/fichiers |\n",
    "| I/O parallÃ¨le (massif) | `asyncio` | 100+ requÃªtes API |\n",
    "| Gros fichiers (> RAM) | Polars streaming, Dask | 10-100+ Go |\n",
    "| ParallÃ©lisation simple | `joblib` | Boucles, ML |\n",
    "\n",
    "### ğŸŒ³ Arbre de dÃ©cision\n",
    "\n",
    "```text\n",
    "Ton problÃ¨me est...\n",
    "â”‚\n",
    "â”œâ”€â–¶ CPU-bound (calculs, transformations) ?\n",
    "â”‚   â”œâ”€â–¶ Simple/boucle â†’ joblib\n",
    "â”‚   â””â”€â–¶ Complexe/chunks â†’ ProcessPoolExecutor\n",
    "â”‚\n",
    "â”œâ”€â–¶ I/O-bound (API, fichiers, DB) ?\n",
    "â”‚   â”œâ”€â–¶ < 20 requÃªtes â†’ ThreadPoolExecutor\n",
    "â”‚   â””â”€â–¶ 100+ requÃªtes â†’ asyncio\n",
    "â”‚\n",
    "â””â”€â–¶ Gros fichiers (> RAM) ?\n",
    "    â”œâ”€â–¶ Single file, Polars-like â†’ Polars streaming\n",
    "    â”œâ”€â–¶ Multi-files, Pandas-like â†’ Dask\n",
    "    â””â”€â–¶ Cluster distribuÃ© â†’ Spark (module 19)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concurrent_futures",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ 4. concurrent.futures â€” L'API moderne\n",
    "\n",
    "> âœ… **Plus simple** que `multiprocessing` et `threading` bruts\n",
    "> âœ… **Interface unifiÃ©e** pour threads et processes\n",
    "\n",
    "### 4.1 ThreadPoolExecutor (I/O-bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threadpool_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def simulate_io_task(task_id):\n",
    "    \"\"\"Simule une tÃ¢che I/O (ex: requÃªte API)\"\"\"\n",
    "    time.sleep(0.5)  # Simule latence rÃ©seau\n",
    "    return f\"Task {task_id} completed\"\n",
    "\n",
    "tasks = list(range(10))\n",
    "\n",
    "# ============ SÃ‰QUENTIEL ============\n",
    "start = time.time()\n",
    "results_seq = [simulate_io_task(t) for t in tasks]\n",
    "print(f\"â±ï¸ SÃ©quentiel : {time.time() - start:.2f}s\")\n",
    "\n",
    "# ============ THREADPOOL ============\n",
    "start = time.time()\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results_parallel = list(executor.map(simulate_io_task, tasks))\n",
    "print(f\"ğŸš€ ThreadPool : {time.time() - start:.2f}s\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speedup : {5.0 / (time.time() - start + 0.01):.1f}x environ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processpool",
   "metadata": {},
   "source": [
    "### 4.2 ProcessPoolExecutor (CPU-bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processpool_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "def cpu_task(n):\n",
    "    \"\"\"TÃ¢che CPU-intensive\"\"\"\n",
    "    return sum(i ** 2 for i in range(n))\n",
    "\n",
    "# DonnÃ©es Ã  traiter\n",
    "data_chunks = [1_000_000] * 8  # 8 chunks\n",
    "\n",
    "# ============ SÃ‰QUENTIEL ============\n",
    "start = time.time()\n",
    "results_seq = [cpu_task(chunk) for chunk in data_chunks]\n",
    "seq_time = time.time() - start\n",
    "print(f\"â±ï¸ SÃ©quentiel : {seq_time:.2f}s\")\n",
    "\n",
    "# ============ PROCESSPOOL ============\n",
    "start = time.time()\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    results_parallel = list(executor.map(cpu_task, data_chunks))\n",
    "proc_time = time.time() - start\n",
    "print(f\"ğŸš€ ProcessPool : {proc_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Speedup : {seq_time/proc_time:.1f}x\")\n",
    "print(f\"ğŸ’» CPUs disponibles : {os.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "futures_advanced",
   "metadata": {},
   "source": [
    "### 4.3 Gestion avancÃ©e : submit() et as_completed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "futures_submit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import random\n",
    "\n",
    "def fetch_data(task_id):\n",
    "    \"\"\"Simule une requÃªte avec temps variable\"\"\"\n",
    "    delay = random.uniform(0.1, 1.0)\n",
    "    time.sleep(delay)\n",
    "    return {\"task_id\": task_id, \"delay\": delay}\n",
    "\n",
    "# Utiliser submit() pour plus de contrÃ´le\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Soumettre les tÃ¢ches\n",
    "    futures = {executor.submit(fetch_data, i): i for i in range(5)}\n",
    "    \n",
    "    # Traiter les rÃ©sultats au fur et Ã  mesure qu'ils arrivent\n",
    "    for future in as_completed(futures):\n",
    "        task_id = futures[future]\n",
    "        try:\n",
    "            result = future.result(timeout=5)  # Timeout de 5s\n",
    "            print(f\"âœ… Task {task_id} terminÃ©e en {result['delay']:.2f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Task {task_id} a Ã©chouÃ© : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "futures_comparison",
   "metadata": {},
   "source": [
    "### 4.4 Quand utiliser quoi ?\n",
    "\n",
    "| Executor | GIL contournÃ© ? | Usage | Exemple |\n",
    "|----------|-----------------|-------|----------|\n",
    "| `ThreadPoolExecutor` | âŒ Non | I/O : API, fichiers | TÃ©lÃ©charger 50 fichiers |\n",
    "| `ProcessPoolExecutor` | âœ… Oui | CPU : calculs, ETL | Transformer 8 chunks de donnÃ©es |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiprocessing",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ 5. multiprocessing â€” ContrÃ´le avancÃ©\n",
    "\n",
    "Pour les cas oÃ¹ `concurrent.futures` ne suffit pas.\n",
    "\n",
    "### 5.1 Pool avec map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiprocessing_pool",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Traite un chunk de donnÃ©es\"\"\"\n",
    "    return np.sum(chunk ** 2)\n",
    "\n",
    "# CrÃ©er des donnÃ©es\n",
    "big_array = np.random.rand(1_000_000)\n",
    "\n",
    "# Partitionner en chunks\n",
    "n_workers = cpu_count()\n",
    "chunks = np.array_split(big_array, n_workers)\n",
    "\n",
    "print(f\"ğŸ’» Nombre de workers : {n_workers}\")\n",
    "print(f\"ğŸ“¦ Taille des chunks : {[len(c) for c in chunks]}\")\n",
    "\n",
    "# Traitement parallÃ¨le\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    results = pool.map(process_chunk, chunks)\n",
    "\n",
    "total = sum(results)\n",
    "print(f\"\\nâœ… RÃ©sultat total : {total:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiprocessing_starmap",
   "metadata": {},
   "source": [
    "### 5.2 starmap pour plusieurs arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starmap_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def process_with_params(data, multiplier, offset):\n",
    "    \"\"\"Fonction avec plusieurs paramÃ¨tres\"\"\"\n",
    "    return sum(data) * multiplier + offset\n",
    "\n",
    "# PrÃ©parer les arguments\n",
    "args_list = [\n",
    "    ([1, 2, 3], 2, 10),\n",
    "    ([4, 5, 6], 3, 20),\n",
    "    ([7, 8, 9], 4, 30),\n",
    "]\n",
    "\n",
    "with Pool(3) as pool:\n",
    "    results = pool.starmap(process_with_params, args_list)\n",
    "\n",
    "print(\"RÃ©sultats:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiprocessing_limits",
   "metadata": {},
   "source": [
    "### 5.3 Limites et prÃ©cautions\n",
    "\n",
    "| âš ï¸ Limite | Explication |\n",
    "|-----------|-------------|\n",
    "| **Overhead** | CrÃ©er des process prend du temps (~100ms) |\n",
    "| **SÃ©rialisation** | Les donnÃ©es sont copiÃ©es (pickle) |\n",
    "| **`if __name__ == \"__main__\"`** | Obligatoire sur Windows |\n",
    "| **MÃ©moire** | Chaque process a sa propre mÃ©moire |\n",
    "\n",
    "```python\n",
    "# âš ï¸ Toujours protÃ©ger avec if __name__ == \"__main__\"\n",
    "if __name__ == \"__main__\":\n",
    "    with Pool(4) as pool:\n",
    "        results = pool.map(my_func, data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asyncio",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ 6. asyncio â€” I/O massivement parallÃ¨le\n",
    "\n",
    "> âœ… **IdÃ©al pour** : ingestion depuis API, requÃªtes DB/S3 massives\n",
    "> âŒ **Pas pour** : calculs CPU-intensive\n",
    "\n",
    "### 6.1 Concepts : async/await"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asyncio_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "async def fetch_data(task_id):\n",
    "    \"\"\"Simule une requÃªte API asynchrone\"\"\"\n",
    "    print(f\"ğŸš€ DÃ©but tÃ¢che {task_id}\")\n",
    "    await asyncio.sleep(1)  # Simule latence rÃ©seau (non-bloquant !)\n",
    "    print(f\"âœ… Fin tÃ¢che {task_id}\")\n",
    "    return f\"result_{task_id}\"\n",
    "\n",
    "async def main():\n",
    "    # Lancer 5 tÃ¢ches en parallÃ¨le\n",
    "    tasks = [fetch_data(i) for i in range(5)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n",
    "\n",
    "# ExÃ©cuter\n",
    "start = time.time()\n",
    "results = await main()  # Dans Jupyter, pas besoin de asyncio.run()\n",
    "print(f\"\\nâ±ï¸ Temps total : {time.time() - start:.2f}s (au lieu de 5s sÃ©quentiel)\")\n",
    "print(f\"ğŸ“Š RÃ©sultats : {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asyncio_aiohttp",
   "metadata": {},
   "source": [
    "### 6.2 Exemple rÃ©el avec aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aiohttp_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install aiohttp\n",
    "import asyncio\n",
    "\n",
    "# Simulons aiohttp pour l'exemple (sans vraies requÃªtes)\n",
    "async def fetch_url(session, url):\n",
    "    \"\"\"Simule une requÃªte HTTP\"\"\"\n",
    "    await asyncio.sleep(0.1)  # Simule latence\n",
    "    return {\"url\": url, \"status\": 200}\n",
    "\n",
    "async def fetch_all_urls(urls):\n",
    "    \"\"\"Fetch toutes les URLs en parallÃ¨le\"\"\"\n",
    "    session = None  # En vrai : async with aiohttp.ClientSession() as session:\n",
    "    \n",
    "    tasks = [fetch_url(session, url) for url in urls]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    return results\n",
    "\n",
    "# Simuler 20 URLs\n",
    "urls = [f\"https://api.example.com/data/{i}\" for i in range(20)]\n",
    "\n",
    "start = time.time()\n",
    "results = await fetch_all_urls(urls)\n",
    "print(f\"â±ï¸ 20 requÃªtes en {time.time() - start:.2f}s\")\n",
    "print(f\"âœ… SuccÃ¨s : {len([r for r in results if isinstance(r, dict)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asyncio_semaphore",
   "metadata": {},
   "source": [
    "### 6.3 Semaphore : limiter les connexions simultanÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "semaphore_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "# Limiter Ã  5 connexions simultanÃ©es\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "async def fetch_limited(task_id):\n",
    "    \"\"\"Fetch avec limite de concurrence\"\"\"\n",
    "    async with semaphore:  # Attend si dÃ©jÃ  5 en cours\n",
    "        print(f\"ğŸš€ TÃ¢che {task_id} dÃ©marre\")\n",
    "        await asyncio.sleep(0.5)\n",
    "        print(f\"âœ… TÃ¢che {task_id} terminÃ©e\")\n",
    "        return task_id\n",
    "\n",
    "async def main():\n",
    "    tasks = [fetch_limited(i) for i in range(15)]\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "start = time.time()\n",
    "results = await main()\n",
    "print(f\"\\nâ±ï¸ 15 tÃ¢ches (max 5 simultanÃ©es) en {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asyncio_when",
   "metadata": {},
   "source": [
    "### 6.4 Quand NE PAS utiliser asyncio\n",
    "\n",
    "| Situation | asyncio efficace ? | Alternative |\n",
    "|-----------|-------------------|-------------|\n",
    "| 100+ appels API | âœ… **Oui** | - |\n",
    "| Lecture S3/DB massives | âœ… **Oui** | - |\n",
    "| Calculs CPU | âŒ **Non** | ProcessPoolExecutor |\n",
    "| 5 requÃªtes simples | âŒ Overkill | ThreadPoolExecutor |\n",
    "| Code synchrone existant | âŒ Refactoring lourd | ThreadPoolExecutor |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dask",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š 7. Dask â€” Pandas distribuÃ©\n",
    "\n",
    "> ğŸ”¥ **Le plus utile avant Spark** pour traiter des fichiers plus grands que la RAM.\n",
    "\n",
    "### 7.1 Pourquoi Dask ?\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **API Pandas-like** | Migration facile |\n",
    "| **Lazy evaluation** | Optimisation automatique |\n",
    "| **ParallÃ©lisme** | Utilise tous les CPUs |\n",
    "| **Out-of-core** | Fichiers > RAM |\n",
    "| **Scalable** | Optionnellement sur cluster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install dask[complete]\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CrÃ©er des fichiers de test\n",
    "os.makedirs(\"data/dask_demo\", exist_ok=True)\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.DataFrame({\n",
    "        \"id\": range(i * 10000, (i + 1) * 10000),\n",
    "        \"category\": [f\"cat_{j % 5}\" for j in range(10000)],\n",
    "        \"amount\": [float(j % 1000) for j in range(10000)]\n",
    "    })\n",
    "    df.to_csv(f\"data/dask_demo/file_{i}.csv\", index=False)\n",
    "\n",
    "print(\"âœ… 5 fichiers CSV crÃ©Ã©s (50,000 lignes au total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# Lire TOUS les fichiers avec glob pattern (lazy !)\n",
    "ddf = dd.read_csv(\"data/dask_demo/*.csv\")\n",
    "\n",
    "print(\"Type:\", type(ddf))\n",
    "print(f\"Partitions: {ddf.npartitions}\")\n",
    "print(\"\\nâš ï¸ Les donnÃ©es ne sont PAS encore chargÃ©es !\")\n",
    "print(ddf)  # Affiche le schÃ©ma, pas les donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask_operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Dask (lazy)\n",
    "result = (\n",
    "    ddf\n",
    "    .filter(ddf.amount > 100)  # Filtrage\n",
    "    .assign(amount_doubled=ddf.amount * 2)  # Transformation\n",
    "    .groupby(\"category\")  # AgrÃ©gation\n",
    "    .amount.sum()\n",
    ")\n",
    "\n",
    "print(\"Pipeline dÃ©fini (lazy) :\")\n",
    "print(result)\n",
    "\n",
    "# ExÃ©cuter avec compute()\n",
    "start = time.time()\n",
    "final_result = result.compute()\n",
    "print(f\"\\nâ±ï¸ ExÃ©cution : {time.time() - start:.2f}s\")\n",
    "print(\"\\nRÃ©sultat :\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dask_vs_others",
   "metadata": {},
   "source": [
    "### 7.2 Dask vs Polars vs Spark\n",
    "\n",
    "| Aspect | Polars | Dask | Spark |\n",
    "|--------|--------|------|-------|\n",
    "| **Single machine** | â­â­â­ | â­â­ | â­ |\n",
    "| **Cluster** | âŒ | â­â­ | â­â­â­ |\n",
    "| **API** | Propre | Pandas-like | Propre |\n",
    "| **Setup** | Simple | Simple | Complexe |\n",
    "| **Vitesse (single node)** | â­â­â­ | â­â­ | â­ |\n",
    "| **Ã‰cosystÃ¨me** | Nouveau | Mature | TrÃ¨s mature |\n",
    "\n",
    "### 7.3 Quand utiliser Dask ?\n",
    "\n",
    "- âœ… Fichiers **> RAM** mais **< 100 Go**\n",
    "- âœ… Tu connais dÃ©jÃ  **Pandas**\n",
    "- âœ… Pas besoin d'un **cluster Spark**\n",
    "- âœ… Traitement **multi-fichiers**\n",
    "- âŒ Si single file < 10 Go â†’ utilise **Polars**\n",
    "- âŒ Si > 100 Go ou cluster â†’ utilise **Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joblib",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¨ 8. joblib â€” ParallÃ©lisation simple\n",
    "\n",
    "> âœ… **Ultra-simple** â€” parfait pour parallÃ©liser une boucle rapidement\n",
    "> âœ… TrÃ¨s utilisÃ© en **Data Science** (sklearn l'utilise en interne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joblib_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation : pip install joblib\n",
    "from joblib import Parallel, delayed\n",
    "import time\n",
    "\n",
    "def expensive_computation(x):\n",
    "    \"\"\"Calcul coÃ»teux\"\"\"\n",
    "    time.sleep(0.1)  # Simule un calcul\n",
    "    return x ** 2\n",
    "\n",
    "data = list(range(20))\n",
    "\n",
    "# ============ SÃ‰QUENTIEL ============\n",
    "start = time.time()\n",
    "results_seq = [expensive_computation(x) for x in data]\n",
    "print(f\"â±ï¸ SÃ©quentiel : {time.time() - start:.2f}s\")\n",
    "\n",
    "# ============ JOBLIB ============\n",
    "start = time.time()\n",
    "results_parallel = Parallel(n_jobs=-1)(  # -1 = tous les CPUs\n",
    "    delayed(expensive_computation)(x) for x in data\n",
    ")\n",
    "print(f\"ğŸš€ Joblib : {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joblib_options",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def io_task(x):\n",
    "    time.sleep(0.1)\n",
    "    return x\n",
    "\n",
    "# Options utiles\n",
    "results = Parallel(\n",
    "    n_jobs=4,              # Nombre de workers\n",
    "    backend=\"threading\",   # \"threading\" pour I/O, \"loky\" (dÃ©faut) pour CPU\n",
    "    verbose=1              # Affiche la progression\n",
    ")(delayed(io_task)(x) for x in range(10))\n",
    "\n",
    "print(f\"\\nâœ… RÃ©sultats : {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decision_tree",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒ³ 9. Choisir la bonne technologie\n",
    "\n",
    "### RÃ©capitulatif\n",
    "\n",
    "| Outil | Type | GIL contournÃ© | ComplexitÃ© | Use case |\n",
    "|-------|------|---------------|------------|----------|\n",
    "| `ThreadPoolExecutor` | I/O | âŒ Non | â­ | < 20 requÃªtes/fichiers |\n",
    "| `ProcessPoolExecutor` | CPU | âœ… Oui | â­â­ | ETL, calculs |\n",
    "| `asyncio` | I/O | âŒ Non | â­â­â­ | 100+ requÃªtes API |\n",
    "| `joblib` | CPU/I/O | âœ… (loky) | â­ | Boucles simples, ML |\n",
    "| `Dask` | Big Data | âœ… Oui | â­â­ | Fichiers > RAM |\n",
    "\n",
    "### ğŸ–¼ï¸ Arbre de dÃ©cision visuel\n",
    "\n",
    "```text\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Quel problÃ¨me? â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                             â”‚\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚                   â”‚                   â”‚\n",
    "         â–¼                   â–¼                   â–¼\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚CPU-boundâ”‚        â”‚I/O-boundâ”‚        â”‚Fichiers >RAMâ”‚\n",
    "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                  â”‚                    â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
    "    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n",
    "    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n",
    " Simple?  Complex?  <20 req?  100+ req?  <100Go?  >100Go?\n",
    "    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n",
    "    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n",
    " joblib   Process   Thread    asyncio    Dask     Spark\n",
    "          Pool      Pool                        (mod 19)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ 10. Bonnes pratiques & Erreurs frÃ©quentes\n",
    "\n",
    "### âŒ Erreurs frÃ©quentes\n",
    "\n",
    "| Erreur | Pourquoi c'est faux | Solution |\n",
    "|--------|---------------------|----------|\n",
    "| Threading pour CPU | GIL bloque | `ProcessPoolExecutor` |\n",
    "| Async pour calculs | Pas de gain | `ProcessPoolExecutor` |\n",
    "| Pandas sur 50 Go | Crash RAM | Dask ou Polars streaming |\n",
    "| 100 workers pour 10 tÃ¢ches | Overhead inutile | Adapter au workload |\n",
    "| Pas de profiling | Optimise au hasard | Toujours profiler d'abord |\n",
    "| Oublier `if __name__` | Crash sur Windows | Toujours protÃ©ger |\n",
    "\n",
    "### âœ… Bonnes pratiques\n",
    "\n",
    "| Pratique | Pourquoi |\n",
    "|----------|----------|\n",
    "| **Profiler d'abord** | Identifier le vrai goulot |\n",
    "| **Ã‰crire en Parquet** | I/O plus rapide |\n",
    "| **Partitionner intelligemment** | Ã‰vite surcharge mÃ©moire |\n",
    "| **Tester avec peu de workers** | Puis augmenter progressivement |\n",
    "| **`if __name__ == \"__main__\"`** | Obligatoire pour multiprocessing |\n",
    "| **Utiliser `n_jobs=-1`** | Utilise tous les CPUs disponibles |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Quiz de fin de module\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Qu'est-ce que le GIL empÃªche en Python ?\n",
    "a) L'exÃ©cution de code Python  \n",
    "b) L'exÃ©cution simultanÃ©e de plusieurs threads Python  \n",
    "c) L'utilisation de la mÃ©moire  \n",
    "d) La lecture de fichiers\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Le GIL (Global Interpreter Lock) empÃªche l'exÃ©cution simultanÃ©e de plusieurs threads Python, les forÃ§ant Ã  s'exÃ©cuter en alternance.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Pour un traitement CPU-intensive, quel outil utiliser ?\n",
    "a) `ThreadPoolExecutor`  \n",
    "b) `ProcessPoolExecutor`  \n",
    "c) `asyncio`  \n",
    "d) Tous sont Ã©quivalents\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `ProcessPoolExecutor` contourne le GIL en utilisant des processus sÃ©parÃ©s, permettant une vraie parallÃ©lisation CPU.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Quand utiliser `asyncio` ?\n",
    "a) Pour des calculs mathÃ©matiques complexes  \n",
    "b) Pour tÃ©lÃ©charger 100+ fichiers depuis une API  \n",
    "c) Pour trier un gros tableau  \n",
    "d) Pour compresser des fichiers\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `asyncio` est idÃ©al pour l'I/O massivement parallÃ¨le (requÃªtes API, tÃ©lÃ©chargements). Les autres sont CPU-bound.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Que fait `ddf.compute()` dans Dask ?\n",
    "a) DÃ©finit le pipeline  \n",
    "b) Affiche le schÃ©ma  \n",
    "c) DÃ©clenche l'exÃ©cution et retourne un DataFrame Pandas  \n",
    "d) Sauvegarde les donnÃ©es\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” `.compute()` dÃ©clenche l'exÃ©cution du pipeline lazy et retourne le rÃ©sultat sous forme de DataFrame Pandas.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Que signifie `n_jobs=-1` dans joblib ?\n",
    "a) DÃ©sactive le parallÃ©lisme  \n",
    "b) Utilise un seul CPU  \n",
    "c) Utilise tous les CPUs disponibles  \n",
    "d) Erreur de configuration\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” `n_jobs=-1` indique Ã  joblib d'utiliser tous les CPUs disponibles sur la machine.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Pourquoi `ThreadPoolExecutor` ne accÃ©lÃ¨re pas les calculs CPU ?\n",
    "a) Parce qu'il est mal implÃ©mentÃ©  \n",
    "b) Parce que le GIL force l'exÃ©cution sÃ©quentielle des threads Python  \n",
    "c) Parce qu'il utilise trop de mÃ©moire  \n",
    "d) Parce qu'il est obsolÃ¨te\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Le GIL empÃªche les threads Python de s'exÃ©cuter en parallÃ¨le. Pour du CPU-bound, il faut utiliser des processus.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Pour traiter un fichier de 50 Go avec une API Pandas-like, quel outil choisir ?\n",
    "a) Pandas  \n",
    "b) Polars  \n",
    "c) Dask  \n",
    "d) asyncio\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Dask permet de traiter des fichiers plus grands que la RAM avec une API similaire Ã  Pandas.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Quelle est la premiÃ¨re Ã©tape avant d'optimiser du code ?\n",
    "a) Ajouter du multiprocessing  \n",
    "b) RÃ©Ã©crire en Rust  \n",
    "c) Profiler pour identifier le goulot d'Ã©tranglement  \n",
    "d) Utiliser asyncio\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” \"Premature optimization is the root of all evil\". Il faut d'abord mesurer pour savoir oÃ¹ optimiser.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Mini-projet : Pipeline haute performance\n",
    "\n",
    "### ğŸ¯ Objectif\n",
    "Combiner plusieurs techniques pour crÃ©er un pipeline performant :\n",
    "- **ProcessPoolExecutor** pour transformation CPU-intensive\n",
    "- **Dask** pour agrÃ©gation\n",
    "- Export **Parquet**\n",
    "\n",
    "### ğŸ—ï¸ Architecture\n",
    "\n",
    "```text\n",
    "data/raw/*.csv\n",
    "      â”‚\n",
    "      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ ProcessPoolExecutor â”‚  Transformation parallÃ¨le (CPU)\n",
    "â”‚  - Nettoyage        â”‚\n",
    "â”‚  - Feature eng.     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "data/intermediate/*.csv\n",
    "           â”‚\n",
    "           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Dask DataFrame    â”‚  AgrÃ©gation (multi-fichiers)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â–¼\n",
    "data/processed/result.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup : crÃ©er les donnÃ©es de test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/intermediate\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# CrÃ©er 10 fichiers CSV (simule des logs)\n",
    "np.random.seed(42)\n",
    "categories = [\"web\", \"api\", \"db\", \"cache\", \"auth\"]\n",
    "statuses = [\"success\", \"error\", \"timeout\"]\n",
    "\n",
    "for i in range(10):\n",
    "    n_rows = 10000\n",
    "    df = pd.DataFrame({\n",
    "        \"timestamp\": pd.date_range(\"2024-01-01\", periods=n_rows, freq=\"s\"),\n",
    "        \"category\": np.random.choice(categories, n_rows),\n",
    "        \"status\": np.random.choice(statuses, n_rows, p=[0.8, 0.15, 0.05]),\n",
    "        \"response_time_ms\": np.random.exponential(100, n_rows),\n",
    "        \"bytes_sent\": np.random.randint(100, 10000, n_rows)\n",
    "    })\n",
    "    df.to_csv(f\"data/raw/logs_{i:02d}.csv\", index=False)\n",
    "\n",
    "print(\"âœ… 10 fichiers CSV crÃ©Ã©s (100,000 lignes au total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_transform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "\n",
    "def transform_file(filepath):\n",
    "    \"\"\"\n",
    "    Transformation CPU-intensive d'un fichier.\n",
    "    - Nettoyage\n",
    "    - Feature engineering\n",
    "    - Export intermÃ©diaire\n",
    "    \"\"\"\n",
    "    # Lire\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Nettoyage : filtrer les timeouts extrÃªmes\n",
    "    df = df[df[\"response_time_ms\"] < 10000]\n",
    "    \n",
    "    # Feature engineering (CPU-intensive)\n",
    "    df[\"response_time_log\"] = np.log1p(df[\"response_time_ms\"])\n",
    "    df[\"is_error\"] = (df[\"status\"] != \"success\").astype(int)\n",
    "    df[\"throughput\"] = df[\"bytes_sent\"] / (df[\"response_time_ms\"] + 1)\n",
    "    \n",
    "    # Extraction date\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n",
    "    \n",
    "    # Export intermÃ©diaire\n",
    "    output_path = filepath.replace(\"raw\", \"intermediate\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Liste des fichiers\n",
    "input_files = sorted(glob.glob(\"data/raw/*.csv\"))\n",
    "print(f\"ğŸ“ {len(input_files)} fichiers Ã  traiter\")\n",
    "\n",
    "# ============ TRANSFORMATION PARALLÃˆLE ============\n",
    "start = time.time()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=4) as executor:\n",
    "    output_files = list(executor.map(transform_file, input_files))\n",
    "\n",
    "print(f\"\\nâ±ï¸ Transformation : {time.time() - start:.2f}s\")\n",
    "print(f\"âœ… {len(output_files)} fichiers transformÃ©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import time\n",
    "\n",
    "# ============ AGRÃ‰GATION AVEC DASK ============\n",
    "start = time.time()\n",
    "\n",
    "# Lire tous les fichiers intermÃ©diaires (lazy)\n",
    "ddf = dd.read_csv(\"data/intermediate/*.csv\")\n",
    "\n",
    "# Pipeline d'agrÃ©gation\n",
    "result = (\n",
    "    ddf\n",
    "    .groupby([\"category\", \"status\", \"hour\"])\n",
    "    .agg({\n",
    "        \"response_time_ms\": [\"mean\", \"max\", \"count\"],\n",
    "        \"bytes_sent\": \"sum\",\n",
    "        \"is_error\": \"sum\",\n",
    "        \"throughput\": \"mean\"\n",
    "    })\n",
    "    .compute()  # ExÃ©cution\n",
    ")\n",
    "\n",
    "# Aplatir les colonnes multi-index\n",
    "result.columns = ['_'.join(col).strip() for col in result.columns.values]\n",
    "result = result.reset_index()\n",
    "\n",
    "print(f\"â±ï¸ AgrÃ©gation Dask : {time.time() - start:.2f}s\")\n",
    "print(f\"\\nğŸ“Š RÃ©sultat : {len(result)} lignes\")\n",
    "print(result.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ EXPORT PARQUET ============\n",
    "result.to_parquet(\"data/processed/aggregated_logs.parquet\", index=False)\n",
    "print(\"âœ… RÃ©sultat exportÃ© : data/processed/aggregated_logs.parquet\")\n",
    "\n",
    "# VÃ©rification\n",
    "import os\n",
    "size_bytes = os.path.getsize(\"data/processed/aggregated_logs.parquet\")\n",
    "print(f\"ğŸ“¦ Taille : {size_bytes / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ RÃ‰SUMÃ‰ DU PIPELINE ============\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“Š RÃ‰SUMÃ‰ DU PIPELINE HAUTE PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\"\"  \n",
    "1ï¸âƒ£ Input : 10 fichiers CSV (100K lignes)\n",
    "2ï¸âƒ£ Transformation : ProcessPoolExecutor (4 workers)\n",
    "   - Nettoyage\n",
    "   - Feature engineering\n",
    "3ï¸âƒ£ AgrÃ©gation : Dask DataFrame\n",
    "   - GroupBy multi-colonnes\n",
    "   - Aggregations multiples\n",
    "4ï¸âƒ£ Output : Parquet ({size_bytes / 1024:.1f} KB)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) â€” Doc Python\n",
    "- [asyncio](https://docs.python.org/3/library/asyncio.html) â€” Doc Python\n",
    "- [Dask Documentation](https://docs.dask.org/) â€” Guide complet\n",
    "- [joblib](https://joblib.readthedocs.io/) â€” ParallÃ©lisation simple\n",
    "\n",
    "### ğŸ“– Articles & Tutoriels\n",
    "- [Real Python - Async IO](https://realpython.com/async-io-python/) â€” Tutoriel complet\n",
    "- [Speed Up Your Python Code](https://realpython.com/python-concurrency/) â€” Guide concurrence\n",
    "\n",
    "### ğŸ”§ Outils de profiling\n",
    "- [py-spy](https://github.com/benfred/py-spy) â€” Sampling profiler\n",
    "- [Scalene](https://github.com/plasma-umass/scalene) â€” CPU + mÃ©moire + GPU profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises les techniques de performance en Python, passons au **traitement distribuÃ© Ã  grande Ã©chelle** avec Spark !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `19_pyspark_advanced.ipynb`** â€” PySpark AvancÃ©\n",
    "\n",
    "Tu vas apprendre :\n",
    "- Architecture Spark (Driver, Executors)\n",
    "- RDDs et DataFrames Spark\n",
    "- Optimisations (partitioning, caching)\n",
    "- Spark sur Kubernetes\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ RÃ©capitulatif de ce module\n",
    "\n",
    "| Outil | Type | Quand l'utiliser |\n",
    "|-------|------|------------------|\n",
    "| `ThreadPoolExecutor` | I/O | < 20 requÃªtes/fichiers |\n",
    "| `ProcessPoolExecutor` | CPU | Calculs, transformations |\n",
    "| `asyncio` | I/O massif | 100+ requÃªtes API |\n",
    "| `joblib` | Simple | ParallÃ©liser une boucle |\n",
    "| `Dask` | Big Data | Fichiers > RAM, API Pandas |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module High Performance Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des fichiers temporaires (optionnel)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# DÃ©commenter pour nettoyer\n",
    "# for folder in [\"data/raw\", \"data/intermediate\", \"data/processed\", \"data/dask_demo\"]:\n",
    "#     if os.path.exists(folder):\n",
    "#         shutil.rmtree(folder)\n",
    "# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}