{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ»â€â„ï¸ Polars pour Data Engineers\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir **Polars**, la bibliothÃ¨que DataFrame ultra-rapide qui rÃ©volutionne le traitement de donnÃ©es en Python. Tu apprendras pourquoi Polars surpasse Pandas, comment exploiter son moteur d'exÃ©cution lazy, et comment construire des pipelines ETL performants !\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Connaissances de base en Python |\n",
    "| âœ… Requis | Avoir utilisÃ© Pandas (mÃªme basiquement) |\n",
    "| ğŸ’¡ RecommandÃ© | Avoir suivi les modules prÃ©cÃ©dents du bootcamp |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre pourquoi Polars est **5-100x plus rapide** que Pandas\n",
    "- MaÃ®triser l'architecture **columnar** et le format **Apache Arrow**\n",
    "- Utiliser les **expressions Polars** pour des transformations efficaces\n",
    "- Exploiter l'exÃ©cution **Lazy** pour des pipelines optimisÃ©s\n",
    "- Migrer du code Pandas vers Polars\n",
    "- Construire des pipelines ETL performants en production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/diakite-data/data-engineering-bootcamp/blob/main/notebooks/intermediate/17_polars_for_data_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
    "",
    "> ğŸ’¡ **Conseil** : Cliquez sur le badge ci-dessus pour exÃ©cuter ce notebook directement dans Google Colab (aucune installation requise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars_vs_pandas",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ 1. Polars vs Pandas : Pourquoi changer ?\n",
    "\n",
    "Avant de plonger dans Polars, comprenons **pourquoi** cette bibliothÃ¨que existe et ce qu'elle apporte.\n",
    "\n",
    "### 1.1 Les limitations de Pandas\n",
    "\n",
    "Pandas est formidable pour l'exploration de donnÃ©es, mais il a des **limitations structurelles** :\n",
    "\n",
    "| Limitation | Explication | Impact |\n",
    "|------------|-------------|--------|\n",
    "| **Single-threaded** | Le GIL Python bloque le parallÃ©lisme | N'utilise qu'1 CPU |\n",
    "| **Row-based en mÃ©moire** | DonnÃ©es stockÃ©es par ligne | Cache CPU inefficace |\n",
    "| **Eager execution** | Chaque opÃ©ration s'exÃ©cute immÃ©diatement | Pas d'optimisation globale |\n",
    "| **Copies frÃ©quentes** | Beaucoup d'opÃ©rations copient les donnÃ©es | RAM x2 ou x3 |\n",
    "| **MÃ©moire gourmande** | ~5-10x la taille du fichier | Limite les gros datasets |\n",
    "\n",
    "### 1.2 Les forces de Polars\n",
    "\n",
    "| Aspect | Pandas | Polars |\n",
    "|--------|--------|--------|\n",
    "| **Backend** | NumPy (C) | Rust ğŸ¦€ |\n",
    "| **Threading** | Single (GIL) | Multi-threaded |\n",
    "| **MÃ©moire** | Row-based | Columnar (Arrow) |\n",
    "| **Execution** | Eager only | Eager + **Lazy** |\n",
    "| **Vitesse** | Baseline | **5-100x plus rapide** |\n",
    "| **Out-of-core** | âŒ | âœ… (streaming) |\n",
    "| **Optimiseur** | âŒ | âœ… Query planner |\n",
    "\n",
    "> ğŸ’¡ **En rÃ©sumÃ©** : Polars est conÃ§u dÃ¨s le dÃ©part pour la **performance** et les **gros volumes**, lÃ  oÃ¹ Pandas a Ã©tÃ© conÃ§u pour l'**exploration interactive**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polars_info",
   "metadata": {},
   "source": [
    "> â„¹ï¸ **Le savais-tu ?**\n",
    ">\n",
    "> Polars a Ã©tÃ© crÃ©Ã© en **2020** par **Ritchie Vink**, un ingÃ©nieur nÃ©erlandais frustrÃ© par la lenteur de Pandas.\n",
    ">\n",
    "> Le nom \"Polars\" fait rÃ©fÃ©rence Ã  l'**ours polaire** (ğŸ»â€â„ï¸) â€” un clin d'Å“il Ã  Pandas (ğŸ¼) tout en Ã©tant plus rapide et adaptÃ© aux environnements \"froids\" (haute performance).\n",
    ">\n",
    "> Polars est Ã©crit en **Rust**, un langage rÃ©putÃ© pour sa vitesse et sa sÃ©curitÃ© mÃ©moire.\n",
    ">\n",
    "> ğŸ“– [Site officiel Polars](https://www.pola.rs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark",
   "metadata": {},
   "source": [
    "### 1.3 Benchmark concret\n",
    "\n",
    "Comparons Pandas et Polars sur une opÃ©ration simple : lire un CSV et faire une agrÃ©gation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_benchmark_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un fichier de test\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# GÃ©nÃ©rer 500K lignes\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\n",
    "n_rows = 500_000\n",
    "\n",
    "with open(\"data/benchmark.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"category\", \"amount\", \"quantity\"])\n",
    "    for i in range(n_rows):\n",
    "        writer.writerow([\n",
    "            i,\n",
    "            random.choice(categories),\n",
    "            round(random.uniform(10, 1000), 2),\n",
    "            random.randint(1, 100)\n",
    "        ])\n",
    "\n",
    "print(f\"âœ… Fichier crÃ©Ã© : data/benchmark.csv ({n_rows:,} lignes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark_pandas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Benchmark Pandas\n",
    "start = time.time()\n",
    "\n",
    "df_pandas = pd.read_csv(\"data/benchmark.csv\")\n",
    "result_pandas = (\n",
    "    df_pandas\n",
    "    .groupby(\"category\")\n",
    "    .agg({\"amount\": \"sum\", \"quantity\": \"mean\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pandas_time = time.time() - start\n",
    "print(f\"ğŸ¼ Pandas : {pandas_time:.3f} secondes\")\n",
    "print(result_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark_polars",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "# Benchmark Polars (Eager)\n",
    "start = time.time()\n",
    "\n",
    "df_polars = pl.read_csv(\"data/benchmark.csv\")\n",
    "result_polars = (\n",
    "    df_polars\n",
    "    .group_by(\"category\")\n",
    "    .agg(\n",
    "        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n",
    "        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n",
    "    )\n",
    ")\n",
    "\n",
    "polars_time = time.time() - start\n",
    "print(f\"ğŸ»â€â„ï¸ Polars : {polars_time:.3f} secondes\")\n",
    "print(f\"âš¡ Polars est {pandas_time/polars_time:.1f}x plus rapide !\")\n",
    "print(result_polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark_polars_lazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Polars (Lazy) - encore plus rapide !\n",
    "start = time.time()\n",
    "\n",
    "result_lazy = (\n",
    "    pl.scan_csv(\"data/benchmark.csv\")  # Lazy !\n",
    "    .group_by(\"category\")\n",
    "    .agg(\n",
    "        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n",
    "        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n",
    "    )\n",
    "    .collect()  # ExÃ©cution optimisÃ©e\n",
    ")\n",
    "\n",
    "lazy_time = time.time() - start\n",
    "print(f\"ğŸš€ Polars Lazy : {lazy_time:.3f} secondes\")\n",
    "print(f\"âš¡ Polars Lazy est {pandas_time/lazy_time:.1f}x plus rapide que Pandas !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ 2. Comprendre l'architecture de Polars\n",
    "\n",
    "Pour bien utiliser Polars, il faut comprendre **pourquoi** il est si rapide.\n",
    "\n",
    "### 2.1 Columnar vs Row-based\n",
    "\n",
    "```text\n",
    "ROW-BASED (Pandas)                    COLUMNAR (Polars)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ id  â”‚ name â”‚ age â”‚                  â”‚ id:   [1, 2, 3]   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1  â”‚ Ana  â”‚ 25  â”‚  â† Ligne 1       â”‚ name: [A, B, C]   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  2  â”‚ Bob  â”‚ 30  â”‚  â† Ligne 2       â”‚ age:  [25, 30, 22]â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚  3  â”‚ Cat  â”‚ 22  â”‚  â† Ligne 3              â†‘\n",
    "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                   Colonnes contiguÃ«s\n",
    "        â†‘                              en mÃ©moire\n",
    "  Lignes contiguÃ«s\n",
    "  en mÃ©moire\n",
    "```\n",
    "\n",
    "**Pourquoi columnar est plus rapide ?**\n",
    "\n",
    "| Avantage | Explication |\n",
    "|----------|-------------|\n",
    "| **Cache CPU** | DonnÃ©es contiguÃ«s = moins de cache misses |\n",
    "| **SIMD** | OpÃ©rations vectorisÃ©es sur colonnes entiÃ¨res |\n",
    "| **Compression** | Colonnes homogÃ¨nes = meilleure compression |\n",
    "| **SÃ©lection** | Lire seulement les colonnes nÃ©cessaires |\n",
    "\n",
    "### 2.2 Apache Arrow : le format sous-jacent\n",
    "\n",
    "Polars utilise **Apache Arrow** comme format mÃ©moire :\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Zero-copy** | Partage de donnÃ©es sans copie |\n",
    "| **InteropÃ©rabilitÃ©** | Compatible Spark, DuckDB, PyArrow |\n",
    "| **StandardisÃ©** | Format ouvert et documentÃ© |\n",
    "\n",
    "### 2.3 Eager vs Lazy execution\n",
    "\n",
    "| Mode | Description | Quand l'utiliser |\n",
    "|------|-------------|------------------|\n",
    "| **Eager** | ExÃ©cute immÃ©diatement chaque opÃ©ration | Exploration, debug, petits datasets |\n",
    "| **Lazy** | Construit un plan, optimise, puis exÃ©cute | Production, gros fichiers, pipelines |\n",
    "\n",
    "```python\n",
    "# Eager : rÃ©sultat immÃ©diat\n",
    "df = pl.read_csv(\"data.csv\")        # Lit maintenant\n",
    "df = df.filter(pl.col(\"x\") > 5)     # Filtre maintenant\n",
    "\n",
    "# Lazy : plan d'exÃ©cution\n",
    "lf = pl.scan_csv(\"data.csv\")        # CrÃ©e un plan\n",
    "lf = lf.filter(pl.col(\"x\") > 5)     # Ajoute au plan\n",
    "df = lf.collect()                    # ExÃ©cute tout (optimisÃ©)\n",
    "```\n",
    "\n",
    "### 2.4 Query Optimizer\n",
    "\n",
    "Le **Query Optimizer** de Polars applique automatiquement des optimisations :\n",
    "\n",
    "| Optimisation | Description |\n",
    "|--------------|-------------|\n",
    "| **Predicate pushdown** | Filtres appliquÃ©s le plus tÃ´t possible |\n",
    "| **Projection pruning** | Colonnes inutiles jamais lues |\n",
    "| **Common subexpression** | Calculs redondants factorisÃ©s |\n",
    "| **Parallelization** | OpÃ©rations distribuÃ©es sur tous les CPUs |\n",
    "\n",
    "```text\n",
    "PLAN ORIGINAL                    PLAN OPTIMISÃ‰\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "scan_csv(all cols)               scan_csv(only needed cols)\n",
    "      â”‚                                â”‚\n",
    "      â–¼                                â–¼\n",
    "with_columns(...)                filter(amount > 100)  â† Pushdown!\n",
    "      â”‚                                â”‚\n",
    "      â–¼                                â–¼\n",
    "filter(amount > 100)             with_columns(...)\n",
    "      â”‚                                â”‚\n",
    "      â–¼                                â–¼\n",
    "   result                           result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "installation",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’» 3. Installation & Configuration\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Installation de base\n",
    "pip install polars\n",
    "\n",
    "# Avec toutes les features (recommandÃ©)\n",
    "pip install 'polars[all]'\n",
    "\n",
    "# Features spÃ©cifiques\n",
    "pip install 'polars[pyarrow,pandas,numpy,fsspec]'\n",
    "```\n",
    "\n",
    "### VÃ©rification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_install",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "print(f\"âœ… Polars version : {pl.__version__}\")\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pl.Config.set_tbl_rows(10)           # Lignes affichÃ©es\n",
    "pl.Config.set_tbl_cols(12)           # Colonnes affichÃ©es\n",
    "pl.Config.set_fmt_str_lengths(50)    # Longueur des strings\n",
    "\n",
    "# Voir le nombre de threads utilisÃ©s\n",
    "print(f\"ğŸ”§ Threads disponibles : {pl.thread_pool_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load_data",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“‚ 4. Charger & Exporter des donnÃ©es\n",
    "\n",
    "### 4.1 Formats supportÃ©s\n",
    "\n",
    "| Format | Read (Eager) | Scan (Lazy) | Write |\n",
    "|--------|--------------|-------------|-------|\n",
    "| **CSV** | `read_csv()` | `scan_csv()` | `write_csv()` |\n",
    "| **Parquet** | `read_parquet()` | `scan_parquet()` | `write_parquet()` |\n",
    "| **JSON** | `read_json()` | `scan_ndjson()` | `write_json()` |\n",
    "| **Excel** | `read_excel()` | âŒ | `write_excel()` |\n",
    "| **Database** | `read_database()` | âŒ | âŒ |\n",
    "| **IPC/Feather** | `read_ipc()` | `scan_ipc()` | `write_ipc()` |\n",
    "\n",
    "### 4.2 Lecture Eager vs Lazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# ============ EAGER (tout en mÃ©moire) ============\n",
    "df = pl.read_csv(\"data/benchmark.csv\")\n",
    "print(\"Eager - Type:\", type(df))\n",
    "print(df.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# ============ LAZY (plan d'exÃ©cution) ============\n",
    "lf = pl.scan_csv(\"data/benchmark.csv\")\n",
    "print(\"Lazy - Type:\", type(lf))\n",
    "print(lf)  # Affiche le plan, pas les donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read_multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er plusieurs fichiers pour l'exemple\n",
    "import os\n",
    "os.makedirs(\"data/multi\", exist_ok=True)\n",
    "\n",
    "for i in range(3):\n",
    "    pl.DataFrame({\n",
    "        \"id\": range(i*100, (i+1)*100),\n",
    "        \"value\": [i*10 + j for j in range(100)]\n",
    "    }).write_csv(f\"data/multi/file_{i}.csv\")\n",
    "\n",
    "print(\"âœ… Fichiers crÃ©Ã©s\")\n",
    "\n",
    "# Lire plusieurs fichiers avec glob pattern\n",
    "lf = pl.scan_csv(\"data/multi/*.csv\")\n",
    "print(f\"\\nNombre de lignes : {lf.collect().height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã‰criture\n",
    "df = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"age\": [25, 30, 35],\n",
    "    \"city\": [\"Paris\", \"Lyon\", \"Marseille\"]\n",
    "})\n",
    "\n",
    "# CSV\n",
    "df.write_csv(\"data/output.csv\")\n",
    "\n",
    "# Parquet (recommandÃ© pour la production)\n",
    "df.write_parquet(\"data/output.parquet\")\n",
    "\n",
    "# JSON\n",
    "df.write_json(\"data/output.json\")\n",
    "\n",
    "print(\"âœ… Fichiers exportÃ©s\")\n",
    "\n",
    "# VÃ©rifier avec Parquet\n",
    "df_parquet = pl.read_parquet(\"data/output.parquet\")\n",
    "print(df_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressions",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¥ 5. Expressions Polars â€” Le cÅ“ur du moteur\n",
    "\n",
    "> ğŸ§  **Les expressions sont ce qui rend Polars si puissant.** C'est un changement de paradigme par rapport Ã  Pandas.\n",
    "\n",
    "### 5.1 Philosophie : tout est expression\n",
    "\n",
    "```python\n",
    "# âŒ Pandas : opÃ©rations sur colonnes\n",
    "df[\"new_col\"] = df[\"a\"] + df[\"b\"]\n",
    "\n",
    "# âœ… Polars : expressions\n",
    "df.with_columns(\n",
    "    (pl.col(\"a\") + pl.col(\"b\")).alias(\"new_col\")\n",
    ")\n",
    "```\n",
    "\n",
    "### 5.2 Expressions de base\n",
    "\n",
    "| Expression | Description | Exemple |\n",
    "|------------|-------------|---------|\n",
    "| `pl.col(\"x\")` | SÃ©lectionner une colonne | `pl.col(\"amount\")` |\n",
    "| `pl.col(\"x\", \"y\")` | Plusieurs colonnes | `pl.col(\"a\", \"b\", \"c\")` |\n",
    "| `pl.all()` | Toutes les colonnes | `df.select(pl.all())` |\n",
    "| `pl.exclude(\"x\")` | Toutes sauf x | `pl.exclude(\"id\")` |\n",
    "| `pl.lit(42)` | Valeur littÃ©rale | `pl.lit(\"constant\")` |\n",
    "| `pl.col(\"*\")` | Toutes (autre syntaxe) | `pl.col(\"*\")` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressions_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n",
    "    \"age\": [25, 30, 35, 28],\n",
    "    \"salary\": [50000, 60000, 75000, 55000],\n",
    "    \"department\": [\"IT\", \"HR\", \"IT\", \"Finance\"]\n",
    "})\n",
    "\n",
    "print(\"DataFrame original :\")\n",
    "print(df)\n",
    "\n",
    "# SÃ©lectionner des colonnes avec expressions\n",
    "print(\"\\nSÃ©lection avec expressions :\")\n",
    "print(\n",
    "    df.select(\n",
    "        pl.col(\"name\"),\n",
    "        pl.col(\"salary\") / 12,  # Salaire mensuel\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressions_conditional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions conditionnelles : when/then/otherwise\n",
    "print(\"Expressions conditionnelles :\")\n",
    "print(\n",
    "    df.with_columns(\n",
    "        pl.when(pl.col(\"age\") >= 30)\n",
    "          .then(pl.lit(\"Senior\"))\n",
    "          .otherwise(pl.lit(\"Junior\"))\n",
    "          .alias(\"level\"),\n",
    "        \n",
    "        pl.when(pl.col(\"salary\") > 60000)\n",
    "          .then(pl.lit(\"High\"))\n",
    "          .when(pl.col(\"salary\") > 50000)\n",
    "          .then(pl.lit(\"Medium\"))\n",
    "          .otherwise(pl.lit(\"Low\"))\n",
    "          .alias(\"salary_band\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressions_chaining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChaÃ®nage d'expressions\n",
    "print(\"ChaÃ®nage d'expressions :\")\n",
    "print(\n",
    "    df.with_columns(\n",
    "        # String operations\n",
    "        pl.col(\"name\").str.to_uppercase().alias(\"NAME_UPPER\"),\n",
    "        pl.col(\"name\").str.len_chars().alias(\"name_length\"),\n",
    "        \n",
    "        # Math operations\n",
    "        (pl.col(\"salary\") * 1.1).round(2).alias(\"salary_raised\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manipulations",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ› ï¸ 6. Manipulations de donnÃ©es essentielles\n",
    "\n",
    "### 6.1 SÃ©lection de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "select",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"data/benchmark.csv\")\n",
    "\n",
    "# SÃ©lection simple\n",
    "print(\"SÃ©lection simple :\")\n",
    "print(df.select(\"category\", \"amount\").head(3))\n",
    "\n",
    "# SÃ©lection avec transformation\n",
    "print(\"\\nSÃ©lection avec transformation :\")\n",
    "print(\n",
    "    df.select(\n",
    "        pl.col(\"category\"),\n",
    "        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n",
    "    ).head(3)\n",
    ")\n",
    "\n",
    "# SÃ©lection par type\n",
    "print(\"\\nColonnes numÃ©riques uniquement :\")\n",
    "print(df.select(pl.col(pl.Float64, pl.Int64)).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filter_section",
   "metadata": {},
   "source": [
    "### 6.2 Filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre simple\n",
    "print(\"Filtre simple (amount > 500) :\")\n",
    "print(df.filter(pl.col(\"amount\") > 500).head(3))\n",
    "\n",
    "# Filtres multiples (AND)\n",
    "print(\"\\nFiltres multiples (AND) :\")\n",
    "print(\n",
    "    df.filter(\n",
    "        (pl.col(\"amount\") > 500) & \n",
    "        (pl.col(\"category\") == \"Electronics\")\n",
    "    ).head(3)\n",
    ")\n",
    "\n",
    "# Filtres multiples (OR)\n",
    "print(\"\\nFiltres multiples (OR) :\")\n",
    "print(\n",
    "    df.filter(\n",
    "        (pl.col(\"category\") == \"Electronics\") | \n",
    "        (pl.col(\"category\") == \"Books\")\n",
    "    ).head(3)\n",
    ")\n",
    "\n",
    "# Filtre avec is_in\n",
    "print(\"\\nFiltre avec is_in :\")\n",
    "print(\n",
    "    df.filter(\n",
    "        pl.col(\"category\").is_in([\"Electronics\", \"Books\"])\n",
    "    ).head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "with_columns_section",
   "metadata": {},
   "source": [
    "### 6.3 Ajout / modification de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "with_columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ajout de colonnes :\")\n",
    "result = df.with_columns(\n",
    "    # Calcul\n",
    "    (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n",
    "    \n",
    "    # Valeur constante\n",
    "    pl.lit(\"USD\").alias(\"currency\"),\n",
    "    \n",
    "    # Transformation de colonne existante\n",
    "    pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\"),\n",
    "    \n",
    "    # Conditionnel\n",
    "    pl.when(pl.col(\"amount\") > 500)\n",
    "      .then(pl.lit(\"High\"))\n",
    "      .otherwise(pl.lit(\"Low\"))\n",
    "      .alias(\"amount_level\")\n",
    ")\n",
    "\n",
    "print(result.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "groupby_section",
   "metadata": {},
   "source": [
    "### 6.4 GroupBy & Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "groupby",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GroupBy simple :\")\n",
    "print(\n",
    "    df.group_by(\"category\").agg(\n",
    "        pl.col(\"amount\").sum().alias(\"total_amount\"),\n",
    "        pl.col(\"amount\").mean().alias(\"avg_amount\"),\n",
    "        pl.col(\"amount\").max().alias(\"max_amount\"),\n",
    "        pl.len().alias(\"count\")\n",
    "    ).sort(\"total_amount\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "groupby_advanced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations avancÃ©es\n",
    "print(\"Aggregations avancÃ©es :\")\n",
    "print(\n",
    "    df.group_by(\"category\").agg(\n",
    "        # Statistiques\n",
    "        pl.col(\"amount\").mean().alias(\"avg\"),\n",
    "        pl.col(\"amount\").std().alias(\"std\"),\n",
    "        pl.col(\"amount\").quantile(0.5).alias(\"median\"),\n",
    "        \n",
    "        # Comptages conditionnels\n",
    "        (pl.col(\"amount\") > 500).sum().alias(\"high_amount_count\"),\n",
    "        \n",
    "        # Premier/Dernier\n",
    "        pl.col(\"amount\").first().alias(\"first_amount\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sort_rename_section",
   "metadata": {},
   "source": [
    "### 6.5 Tri, renommage, suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sort_rename",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tri\n",
    "print(\"Tri dÃ©croissant :\")\n",
    "print(df.sort(\"amount\", descending=True).head(3))\n",
    "\n",
    "# Tri multiple\n",
    "print(\"\\nTri multiple :\")\n",
    "print(df.sort([\"category\", \"amount\"], descending=[True, False]).head(5))\n",
    "\n",
    "# Renommer\n",
    "print(\"\\nRenommer :\")\n",
    "print(df.rename({\"amount\": \"montant\", \"quantity\": \"quantite\"}).head(2))\n",
    "\n",
    "# Supprimer des colonnes\n",
    "print(\"\\nSupprimer colonnes :\")\n",
    "print(df.drop(\"id\").head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joins_section",
   "metadata": {},
   "source": [
    "### 6.6 Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des DataFrames pour les joins\n",
    "orders = pl.DataFrame({\n",
    "    \"order_id\": [1, 2, 3, 4],\n",
    "    \"customer_id\": [101, 102, 101, 103],\n",
    "    \"amount\": [100, 200, 150, 300]\n",
    "})\n",
    "\n",
    "customers = pl.DataFrame({\n",
    "    \"customer_id\": [101, 102, 104],\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Diana\"]\n",
    "})\n",
    "\n",
    "print(\"Orders:\", orders)\n",
    "print(\"\\nCustomers:\", customers)\n",
    "\n",
    "# Inner join\n",
    "print(\"\\nInner Join :\")\n",
    "print(orders.join(customers, on=\"customer_id\", how=\"inner\"))\n",
    "\n",
    "# Left join\n",
    "print(\"\\nLeft Join :\")\n",
    "print(orders.join(customers, on=\"customer_id\", how=\"left\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dates_section",
   "metadata": {},
   "source": [
    "### 6.7 Dates et timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "\n",
    "df_dates = pl.DataFrame({\n",
    "    \"event\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "    \"timestamp\": [\n",
    "        datetime(2024, 1, 15, 10, 30),\n",
    "        datetime(2024, 3, 20, 14, 45),\n",
    "        datetime(2024, 6, 5, 9, 0),\n",
    "        datetime(2024, 12, 25, 18, 30)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"DataFrame avec dates :\")\n",
    "print(df_dates)\n",
    "\n",
    "print(\"\\nExtractions de dates :\")\n",
    "print(\n",
    "    df_dates.with_columns(\n",
    "        pl.col(\"timestamp\").dt.year().alias(\"year\"),\n",
    "        pl.col(\"timestamp\").dt.month().alias(\"month\"),\n",
    "        pl.col(\"timestamp\").dt.day().alias(\"day\"),\n",
    "        pl.col(\"timestamp\").dt.hour().alias(\"hour\"),\n",
    "        pl.col(\"timestamp\").dt.weekday().alias(\"weekday\"),\n",
    "        pl.col(\"timestamp\").dt.strftime(\"%Y-%m-%d\").alias(\"date_str\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy_execution",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ 7. Lazy Execution â€” Le Game Changer\n",
    "\n",
    "> ğŸ¯ **C'est ce qui rend Polars adaptÃ© Ã  la production et aux gros volumes.**\n",
    "\n",
    "### 7.1 CrÃ©er un LazyFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy_create",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depuis un fichier (recommandÃ©)\n",
    "lf = pl.scan_csv(\"data/benchmark.csv\")\n",
    "print(\"Type:\", type(lf))\n",
    "print(\"\\nLazyFrame (pas encore exÃ©cutÃ©) :\")\n",
    "print(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy_from_df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depuis un DataFrame existant\n",
    "df = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n",
    "lf = df.lazy()\n",
    "print(\"Converti en LazyFrame:\", type(lf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy_pipeline",
   "metadata": {},
   "source": [
    "### 7.2 Construire le pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy_build",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline complet en Lazy\n",
    "pipeline = (\n",
    "    pl.scan_csv(\"data/benchmark.csv\")\n",
    "    .filter(pl.col(\"amount\") > 100)\n",
    "    .with_columns(\n",
    "        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n",
    "        pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\")\n",
    "    )\n",
    "    .group_by(\"CATEGORY\")\n",
    "    .agg(\n",
    "        pl.col(\"total\").sum().alias(\"total_revenue\"),\n",
    "        pl.len().alias(\"transaction_count\")\n",
    "    )\n",
    "    .sort(\"total_revenue\", descending=True)\n",
    ")\n",
    "\n",
    "print(\"Pipeline dÃ©fini (pas encore exÃ©cutÃ©) :\")\n",
    "print(pipeline)\n",
    "print(\"\\nâš ï¸ Rien n'a Ã©tÃ© lu ou calculÃ© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy_collect",
   "metadata": {},
   "source": [
    "### 7.3 ExÃ©cuter avec .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lazy_execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "result = pipeline.collect()  # MAINTENANT Ã§a s'exÃ©cute\n",
    "print(f\"â±ï¸ Temps d'exÃ©cution : {time.time() - start:.3f}s\")\n",
    "print(\"\\nRÃ©sultat :\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy_explain",
   "metadata": {},
   "source": [
    "### 7.4 Voir le plan d'exÃ©cution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan logique (ce que tu as Ã©crit)\n",
    "print(\"=== PLAN LOGIQUE ===\")\n",
    "print(pipeline.explain())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Plan optimisÃ© (ce que Polars exÃ©cute rÃ©ellement)\n",
    "print(\"=== PLAN OPTIMISÃ‰ ===\")\n",
    "print(pipeline.explain(optimized=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming",
   "metadata": {},
   "source": [
    "### 7.5 Streaming pour fichiers massifs\n",
    "\n",
    "Le mode **streaming** permet de traiter des fichiers plus grands que la RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming : traite par chunks\n",
    "result_streaming = (\n",
    "    pl.scan_csv(\"data/benchmark.csv\")\n",
    "    .filter(pl.col(\"category\") == \"Electronics\")\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.col(\"amount\").sum())\n",
    "    .collect(streaming=True)  # Mode streaming\n",
    ")\n",
    "\n",
    "print(\"RÃ©sultat avec streaming :\")\n",
    "print(result_streaming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lazy_diagram",
   "metadata": {},
   "source": [
    "### ğŸ–¼ï¸ SchÃ©ma : Pipeline Lazy\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  scan_csv() â”‚â”€â”€â”€â”€â–¶â”‚  filter()   â”‚â”€â”€â”€â”€â–¶â”‚with_columns()â”‚â”€â”€â”€â–¶â”‚  group_by() â”‚\n",
    "â”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                                                                   â–¼\n",
    "                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                            â”‚  collect()  â”‚\n",
    "                                                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                    â”‚    Query Optimizer          â”‚\n",
    "                                                    â”‚  â€¢ Predicate pushdown       â”‚\n",
    "                                                    â”‚  â€¢ Column pruning           â”‚\n",
    "                                                    â”‚  â€¢ Parallel execution       â”‚\n",
    "                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                                                                   â–¼\n",
    "                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                            â”‚  DataFrame  â”‚\n",
    "                                                            â”‚  (rÃ©sultat) â”‚\n",
    "                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”„ 8. Migration Pandas â†’ Polars\n",
    "\n",
    "### 8.1 Tableau de correspondance\n",
    "\n",
    "| OpÃ©ration | Pandas | Polars |\n",
    "|-----------|--------|--------|\n",
    "| Lire CSV | `pd.read_csv()` | `pl.read_csv()` / `pl.scan_csv()` |\n",
    "| Lire Parquet | `pd.read_parquet()` | `pl.read_parquet()` / `pl.scan_parquet()` |\n",
    "| SÃ©lection colonne | `df[\"col\"]` | `df.select(\"col\")` |\n",
    "| Plusieurs colonnes | `df[[\"a\", \"b\"]]` | `df.select(\"a\", \"b\")` |\n",
    "| Filtre | `df[df[\"x\"] > 5]` | `df.filter(pl.col(\"x\") > 5)` |\n",
    "| Nouvelle colonne | `df[\"new\"] = df[\"a\"] + 1` | `df.with_columns((pl.col(\"a\") + 1).alias(\"new\"))` |\n",
    "| GroupBy | `df.groupby(\"x\").agg({\"y\": \"sum\"})` | `df.group_by(\"x\").agg(pl.col(\"y\").sum())` |\n",
    "| Tri | `df.sort_values(\"x\")` | `df.sort(\"x\")` |\n",
    "| Renommer | `df.rename(columns={\"a\": \"b\"})` | `df.rename({\"a\": \"b\"})` |\n",
    "| Drop | `df.drop(columns=[\"x\"])` | `df.drop(\"x\")` |\n",
    "| Reset index | `df.reset_index()` | N/A (pas d'index) |\n",
    "| Apply | `df.apply(func)` | `df.map_rows(func)` âš ï¸ Ã©viter |\n",
    "\n",
    "### 8.2 InteropÃ©rabilitÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# CrÃ©er un DataFrame Pandas\n",
    "pandas_df = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\"],\n",
    "    \"age\": [25, 30]\n",
    "})\n",
    "\n",
    "# Pandas â†’ Polars\n",
    "polars_df = pl.from_pandas(pandas_df)\n",
    "print(\"Pandas â†’ Polars :\")\n",
    "print(polars_df)\n",
    "\n",
    "# Polars â†’ Pandas\n",
    "back_to_pandas = polars_df.to_pandas()\n",
    "print(\"\\nPolars â†’ Pandas :\")\n",
    "print(back_to_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "migration_diff",
   "metadata": {},
   "source": [
    "### 8.3 DiffÃ©rences clÃ©s Ã  retenir\n",
    "\n",
    "| Aspect | Pandas | Polars |\n",
    "|--------|--------|--------|\n",
    "| **Index** | âœ… Index par dÃ©faut | âŒ Pas d'index |\n",
    "| **Modification in-place** | âœ… `inplace=True` | âŒ Toujours immutable |\n",
    "| **Typage** | Flexible | Strict |\n",
    "| **NaN vs null** | NaN (float) | null (natif) |\n",
    "| **ChaÃ®nage** | LimitÃ© | Naturel et optimisÃ© |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "migration_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de migration complÃ¨te\n",
    "\n",
    "# ============ VERSION PANDAS ============\n",
    "# df = pd.read_csv(\"data.csv\")\n",
    "# df = df[df[\"amount\"] > 100]\n",
    "# df[\"total\"] = df[\"amount\"] * df[\"quantity\"]\n",
    "# result = df.groupby(\"category\").agg({\"total\": \"sum\"}).reset_index()\n",
    "\n",
    "# ============ VERSION POLARS (Eager) ============\n",
    "result_eager = (\n",
    "    pl.read_csv(\"data/benchmark.csv\")\n",
    "    .filter(pl.col(\"amount\") > 100)\n",
    "    .with_columns(\n",
    "        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n",
    "    )\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.col(\"total\").sum())\n",
    ")\n",
    "\n",
    "# ============ VERSION POLARS (Lazy - recommandÃ©) ============\n",
    "result_lazy = (\n",
    "    pl.scan_csv(\"data/benchmark.csv\")\n",
    "    .filter(pl.col(\"amount\") > 100)\n",
    "    .with_columns(\n",
    "        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n",
    "    )\n",
    "    .group_by(\"category\")\n",
    "    .agg(pl.col(\"total\").sum())\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "print(\"RÃ©sultat :\")\n",
    "print(result_lazy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš ï¸ 9. Bonnes pratiques & Erreurs frÃ©quentes\n",
    "\n",
    "### âŒ Erreurs frÃ©quentes\n",
    "\n",
    "| Erreur | ProblÃ¨me | Solution |\n",
    "|--------|----------|----------|\n",
    "| `.apply()` sur chaque ligne | ExtrÃªmement lent | Utiliser expressions natives |\n",
    "| `df[\"col\"]` style Pandas | Ne fonctionne pas | `df.select(\"col\")` ou `pl.col()` |\n",
    "| `read_csv()` sur 100 fichiers | Lent, beaucoup de RAM | `scan_csv(\"*.csv\")` + glob |\n",
    "| Oublier `.collect()` | Pas d'exÃ©cution | Toujours `.collect()` Ã  la fin |\n",
    "| MÃ©langer eager/lazy | Erreurs de type | Rester cohÃ©rent dans le pipeline |\n",
    "| Pas d'alias sur les expressions | Noms de colonnes illisibles | Toujours `.alias(\"nom\")` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad_vs_good",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ MAUVAIS : apply() ligne par ligne\n",
    "# df.map_rows(lambda row: row[0] * 2)  # TRÃˆS LENT\n",
    "\n",
    "# âœ… BON : expression native\n",
    "df = pl.DataFrame({\"x\": [1, 2, 3]})\n",
    "result = df.with_columns((pl.col(\"x\") * 2).alias(\"x_doubled\"))\n",
    "print(\"âœ… Expression native :\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "good_practices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âŒ MAUVAIS : read_csv sur plusieurs fichiers sÃ©parÃ©ment\n",
    "# dfs = [pl.read_csv(f) for f in files]  # Pas optimisÃ©\n",
    "\n",
    "# âœ… BON : scan_csv avec glob\n",
    "lf = pl.scan_csv(\"data/multi/*.csv\")\n",
    "print(\"âœ… Scan avec glob :\")\n",
    "print(lf.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices_table",
   "metadata": {},
   "source": [
    "### âœ… Bonnes pratiques\n",
    "\n",
    "| Pratique | Pourquoi |\n",
    "|----------|----------|\n",
    "| **Utiliser Lazy en production** | Optimisation automatique |\n",
    "| **PrÃ©fÃ©rer Parquet** | 10x plus rapide que CSV, compression |\n",
    "| **ChaÃ®ner les expressions** | Plus lisible, plus optimisÃ© |\n",
    "| **Ã‰viter `.apply()`** | Utiliser expressions natives |\n",
    "| **Profiler avec `.explain()`** | Comprendre l'exÃ©cution |\n",
    "| **Toujours `.alias()`** | Noms de colonnes explicites |\n",
    "| **`scan_*` pour gros fichiers** | Lazy = optimisations |\n",
    "| **Streaming pour > RAM** | `collect(streaming=True)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiz",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Quiz de fin de module\n",
    "\n",
    "RÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Quel est le principal avantage de l'architecture columnar de Polars ?\n",
    "a) Plus facile Ã  lire pour les humains  \n",
    "b) OpÃ©rations vectorisÃ©es plus rapides et meilleure utilisation du cache CPU  \n",
    "c) Compatible avec Excel  \n",
    "d) Utilise moins de colonnes\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Le stockage columnar permet des opÃ©rations vectorisÃ©es (SIMD) et une meilleure utilisation du cache CPU car les donnÃ©es d'une colonne sont contiguÃ«s en mÃ©moire.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Quelle est la diffÃ©rence entre `pl.read_csv()` et `pl.scan_csv()` ?\n",
    "a) `read_csv` est plus rapide  \n",
    "b) `scan_csv` crÃ©e un LazyFrame et permet l'optimisation  \n",
    "c) `scan_csv` ne supporte pas les gros fichiers  \n",
    "d) Aucune diffÃ©rence\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `scan_csv` crÃ©e un LazyFrame (plan d'exÃ©cution) qui sera optimisÃ© avant exÃ©cution, tandis que `read_csv` charge immÃ©diatement tout en mÃ©moire.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Comment ajouter une nouvelle colonne en Polars ?\n",
    "a) `df[\"new\"] = df[\"old\"] * 2`  \n",
    "b) `df.with_columns((pl.col(\"old\") * 2).alias(\"new\"))`  \n",
    "c) `df.add_column(\"new\", df[\"old\"] * 2)`  \n",
    "d) `df.insert(\"new\", df[\"old\"] * 2)`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” En Polars, on utilise `with_columns()` avec des expressions. La syntaxe `df[\"col\"]` style Pandas ne fonctionne pas.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Que fait le Query Optimizer avec \"predicate pushdown\" ?\n",
    "a) Supprime les colonnes inutiles  \n",
    "b) Applique les filtres le plus tÃ´t possible dans le pipeline  \n",
    "c) ParallÃ©lise les calculs  \n",
    "d) Compresse les donnÃ©es\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Le predicate pushdown dÃ©place les filtres le plus tÃ´t possible, rÃ©duisant ainsi la quantitÃ© de donnÃ©es Ã  traiter dans les Ã©tapes suivantes.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Quand utiliser `.collect()` ?\n",
    "a) AprÃ¨s chaque opÃ©ration  \n",
    "b) Ã€ la fin du pipeline Lazy pour dÃ©clencher l'exÃ©cution  \n",
    "c) Pour convertir en Pandas  \n",
    "d) Pour Ã©crire un fichier\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `.collect()` dÃ©clenche l'exÃ©cution d'un LazyFrame et retourne un DataFrame. Sans `.collect()`, rien n'est calculÃ©.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Pourquoi Ã©viter `.apply()` en Polars ?\n",
    "a) Ce n'est pas supportÃ©  \n",
    "b) C'est lent car Ã§a passe par Python pour chaque ligne  \n",
    "c) Ã‡a modifie les donnÃ©es en place  \n",
    "d) Ã‡a consomme trop de mÃ©moire\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `.apply()` (ou `map_rows`) passe par Python pour chaque ligne, perdant tous les avantages du moteur Rust vectorisÃ©. PrÃ©fÃ©rer les expressions natives.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Quel format de fichier est recommandÃ© en production avec Polars ?\n",
    "a) CSV  \n",
    "b) JSON  \n",
    "c) Parquet  \n",
    "d) Excel\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Parquet est columnar (comme Polars), compressÃ©, et supporte les types. Il est 10x+ plus rapide que CSV.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Comment voir le plan d'exÃ©cution optimisÃ© d'un LazyFrame ?\n",
    "a) `lf.show_plan()`  \n",
    "b) `lf.explain(optimized=True)`  \n",
    "c) `lf.describe()`  \n",
    "d) `print(lf)`\n",
    "\n",
    "<details><summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” `.explain(optimized=True)` affiche le plan d'exÃ©cution aprÃ¨s les optimisations du Query Optimizer.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸš€ Mini-projet : Pipeline ETL Polars\n",
    "\n",
    "### ğŸ¯ Objectif\n",
    "Construire un pipeline ETL **complet en mode Lazy** qui :\n",
    "- Lit plusieurs fichiers CSV\n",
    "- Nettoie et transforme les donnÃ©es\n",
    "- AgrÃ¨ge par catÃ©gorie et pÃ©riode\n",
    "- Exporte en Parquet\n",
    "\n",
    "### ğŸ—ï¸ Architecture\n",
    "\n",
    "```text\n",
    "data/raw/*.csv\n",
    "      â”‚\n",
    "      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   scan_csv()    â”‚  Lazy read (glob pattern)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    filter()     â”‚  Nettoyage (nulls, invalides)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ with_columns()  â”‚  Enrichissement\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   group_by()    â”‚  AgrÃ©gation\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    collect()    â”‚  ExÃ©cution optimisÃ©e\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "data/processed/output.parquet\n",
    "```\n",
    "\n",
    "### ğŸ“ Structure projet\n",
    "\n",
    "```text\n",
    "polars-etl-project/\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â”œâ”€â”€ raw/\n",
    "â”‚   â”‚   â”œâ”€â”€ transactions_01.csv\n",
    "â”‚   â”‚   â”œâ”€â”€ transactions_02.csv\n",
    "â”‚   â”‚   â””â”€â”€ transactions_03.csv\n",
    "â”‚   â””â”€â”€ processed/\n",
    "â”‚       â””â”€â”€ output.parquet\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â””â”€â”€ etl_pipeline.py\n",
    "â””â”€â”€ requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup : crÃ©er les donnÃ©es de test\n",
    "import polars as pl\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "categories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\n",
    "base_date = datetime(2024, 1, 1)\n",
    "\n",
    "# GÃ©nÃ©rer 3 fichiers CSV\n",
    "for file_num in range(1, 4):\n",
    "    n_rows = 10000\n",
    "    data = {\n",
    "        \"transaction_id\": range(file_num * 10000, file_num * 10000 + n_rows),\n",
    "        \"timestamp\": [base_date + timedelta(days=random.randint(0, 365)) for _ in range(n_rows)],\n",
    "        \"category\": [random.choice(categories) for _ in range(n_rows)],\n",
    "        \"amount\": [round(random.uniform(-50, 1000), 2) for _ in range(n_rows)],  # Certains nÃ©gatifs !\n",
    "        \"quantity\": [random.randint(0, 100) for _ in range(n_rows)],  # Certains Ã  0 !\n",
    "        \"customer_id\": [random.randint(1000, 9999) for _ in range(n_rows)]\n",
    "    }\n",
    "    df = pl.DataFrame(data)\n",
    "    df.write_csv(f\"data/raw/transactions_{file_num:02d}.csv\")\n",
    "\n",
    "print(\"âœ… DonnÃ©es de test crÃ©Ã©es (3 fichiers x 10,000 lignes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mini_project_solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ DÃ©marrage du pipeline ETL Polars...\\n\")\n",
    "start = time.time()\n",
    "\n",
    "# ============ PIPELINE LAZY ============\n",
    "result = (\n",
    "    # 1. EXTRACT : Lire tous les CSV avec glob pattern\n",
    "    pl.scan_csv(\"data/raw/*.csv\")\n",
    "    \n",
    "    # 2. CLEAN : Filtrer les donnÃ©es invalides\n",
    "    .filter(\n",
    "        (pl.col(\"amount\") > 0) &           # Montants positifs\n",
    "        (pl.col(\"quantity\") > 0) &         # QuantitÃ©s positives\n",
    "        (pl.col(\"customer_id\").is_not_null())  # Pas de null\n",
    "    )\n",
    "    \n",
    "    # 3. TRANSFORM : Enrichir les donnÃ©es\n",
    "    .with_columns(\n",
    "        # Calculer le total\n",
    "        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total_revenue\"),\n",
    "        \n",
    "        # Extraire annÃ©e et mois\n",
    "        pl.col(\"timestamp\").str.to_datetime().dt.year().alias(\"year\"),\n",
    "        pl.col(\"timestamp\").str.to_datetime().dt.month().alias(\"month\"),\n",
    "        \n",
    "        # CatÃ©goriser les montants\n",
    "        pl.when(pl.col(\"amount\") > 500)\n",
    "          .then(pl.lit(\"High\"))\n",
    "          .when(pl.col(\"amount\") > 100)\n",
    "          .then(pl.lit(\"Medium\"))\n",
    "          .otherwise(pl.lit(\"Low\"))\n",
    "          .alias(\"amount_tier\"),\n",
    "        \n",
    "        # Uppercase category\n",
    "        pl.col(\"category\").str.to_uppercase().alias(\"category_upper\")\n",
    "    )\n",
    "    \n",
    "    # 4. AGGREGATE : Par catÃ©gorie et mois\n",
    "    .group_by([\"year\", \"month\", \"category_upper\"])\n",
    "    .agg(\n",
    "        pl.col(\"total_revenue\").sum().alias(\"total_revenue\"),\n",
    "        pl.col(\"total_revenue\").mean().alias(\"avg_revenue\"),\n",
    "        pl.len().alias(\"transaction_count\"),\n",
    "        pl.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n",
    "        (pl.col(\"amount_tier\") == \"High\").sum().alias(\"high_value_count\")\n",
    "    )\n",
    "    \n",
    "    # 5. SORT\n",
    "    .sort([\"year\", \"month\", \"total_revenue\"], descending=[False, False, True])\n",
    "    \n",
    "    # 6. EXECUTE\n",
    "    .collect()\n",
    ")\n",
    "\n",
    "execution_time = time.time() - start\n",
    "print(f\"â±ï¸ Pipeline exÃ©cutÃ© en {execution_time:.3f} secondes\")\n",
    "print(f\"ğŸ“Š RÃ©sultat : {result.height} lignes, {result.width} colonnes\\n\")\n",
    "\n",
    "# Afficher un aperÃ§u\n",
    "print(\"AperÃ§u des rÃ©sultats :\")\n",
    "print(result.head(10))\n",
    "\n",
    "# 7. EXPORT en Parquet\n",
    "result.write_parquet(\"data/processed/monthly_summary.parquet\")\n",
    "print(\"\\nâœ… RÃ©sultat exportÃ© : data/processed/monthly_summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify_output",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VÃ©rifier le fichier Parquet\n",
    "print(\"ğŸ“– Lecture du fichier Parquet exportÃ© :\")\n",
    "df_check = pl.read_parquet(\"data/processed/monthly_summary.parquet\")\n",
    "print(df_check.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [Polars User Guide](https://docs.pola.rs/) â€” Documentation complÃ¨te\n",
    "- [Polars API Reference](https://docs.pola.rs/api/python/stable/reference/) â€” RÃ©fÃ©rence API\n",
    "- [Polars GitHub](https://github.com/pola-rs/polars) â€” Code source\n",
    "\n",
    "### ğŸ“– Tutoriels & Articles\n",
    "- [Polars vs Pandas Benchmark](https://www.pola.rs/benchmarks.html) â€” Benchmarks officiels\n",
    "- [Modern Polars](https://kevinheavey.github.io/modern-polars/) â€” Guide approfondi\n",
    "\n",
    "### ğŸ”§ Outils complÃ©mentaires\n",
    "- [DuckDB](https://duckdb.org/) â€” SQL analytique ultra-rapide (compatible Polars)\n",
    "- [PyArrow](https://arrow.apache.org/docs/python/) â€” Format Arrow sous-jacent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises Polars, dÃ©couvrons d'autres outils **haute performance** pour Python !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `18_high_performance_python.ipynb`** â€” Python Haute Performance\n",
    "\n",
    "Tu vas apprendre :\n",
    "- **Dask** : parallÃ©lisation de Pandas/NumPy\n",
    "- **Vaex** : traitement out-of-core\n",
    "- **multiprocessing** : parallÃ©lisme CPU\n",
    "- **concurrent.futures** : ThreadPool et ProcessPool\n",
    "- **async/await** : I/O asynchrone\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module Polars pour Data Engineers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage des fichiers temporaires (optionnel)\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# DÃ©commenter pour nettoyer\n",
    "# if os.path.exists(\"data\"):\n",
    "#     shutil.rmtree(\"data\")\n",
    "#     print(\"ğŸ§¹ Dossier data/ supprimÃ©\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}