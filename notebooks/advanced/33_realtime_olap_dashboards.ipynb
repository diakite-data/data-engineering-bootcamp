{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# âš¡ Real-Time OLAP & Dashboards\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir les **moteurs OLAP temps rÃ©el** â€” des bases de donnÃ©es optimisÃ©es pour des requÃªtes analytiques ultra-rapides sur des donnÃ©es en streaming. Tu apprendras Ã  construire des **dashboards live** qui se rafraÃ®chissent en temps rÃ©el.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Kafka & Spark Streaming (M24) |\n",
    "| âœ… Requis | SQL avancÃ© |\n",
    "| âœ… Requis | Docker |\n",
    "| ðŸ’¡ RecommandÃ© | Distributed Messaging (M29) |\n",
    "\n",
    "## ðŸŽ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre quand utiliser un **OLAP engine** vs Spark\n",
    "- DÃ©ployer et configurer **ClickHouse**\n",
    "- IngÃ©rer des donnÃ©es depuis **Kafka** vers ClickHouse\n",
    "- CrÃ©er des **Materialized Views** pour prÃ©-agrÃ©gation\n",
    "- Construire des **dashboards temps rÃ©el** avec Grafana\n",
    "- ConnaÃ®tre les alternatives : **Druid** et **Pinot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Introduction : Pourquoi un OLAP Engine ?\n",
    "\n",
    "### 1.1 Rappel : Architecture Streaming (M24)\n",
    "\n",
    "Dans le module M24, tu as appris Ã  construire des pipelines streaming :\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CE QU'ON A VU EN M24                                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Source â”€â”€â–¶ Kafka â”€â”€â–¶ Spark Streaming â”€â”€â–¶ Delta Lake                      â”‚\n",
    "â”‚                              â”‚                                              â”‚\n",
    "â”‚                              â””â”€â”€ Transformations                            â”‚\n",
    "â”‚                                  AgrÃ©gations                                â”‚\n",
    "â”‚                                  Windowing                                  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   âœ… Ingestion temps rÃ©el                                                   â”‚\n",
    "â”‚   âœ… Transformations complexes                                              â”‚\n",
    "â”‚   âœ… Exactly-once semantics                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.2 Le ProblÃ¨me : Queries Interactives\n",
    "\n",
    "Spark est excellent pour le **traitement** mais moins pour les **queries interactives** :\n",
    "\n",
    "| Besoin | Spark | OLAP Engine |\n",
    "|--------|-------|-------------|\n",
    "| Query latency | Secondes | Millisecondes |\n",
    "| Concurrent users | ~10 | ~1000 |\n",
    "| Ad-hoc queries | Lent Ã  dÃ©marrer | InstantanÃ© |\n",
    "| Dashboard refresh | CoÃ»teux | OptimisÃ© |\n",
    "\n",
    "### 1.3 OLAP vs OLTP vs Streaming\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    OLTP vs OLAP vs STREAMING                                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   OLTP (PostgreSQL)         STREAMING (Kafka+Spark)    OLAP (ClickHouse)   â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â€¢ Row-oriented            â€¢ Event processing         â€¢ Column-oriented   â”‚\n",
    "â”‚   â€¢ Single row ops          â€¢ Continuous               â€¢ Analytical        â”‚\n",
    "â”‚   â€¢ ACID transactions       â€¢ Transformations          â€¢ Fast aggregations â”‚\n",
    "â”‚   â€¢ Low latency writes      â€¢ State management         â€¢ Low latency reads â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Use: Applications         Use: Pipelines             Use: Analytics      â”‚\n",
    "â”‚   Ex: User signup           Ex: ETL, enrichment        Ex: Dashboards      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.4 Architecture ComplÃ¨te Real-Time Analytics\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    REAL-TIME ANALYTICS ARCHITECTURE                         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚   â”‚  Apps   â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚   Spark     â”‚â”€â”€â”€â”€â–¶â”‚ Delta Lake  â”‚     â”‚\n",
    "â”‚   â”‚  IoT    â”‚     â”‚         â”‚     â”‚  Streaming  â”‚     â”‚ (historique)â”‚     â”‚\n",
    "â”‚   â”‚  Events â”‚     â”‚         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚         â”‚                                              â”‚\n",
    "â”‚                   â”‚         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚                   â”‚         â”‚â”€â”€â”€â”€â–¶â”‚ ClickHouse  â”‚â”€â”€â”€â”€â–¶â”‚  Grafana    â”‚     â”‚\n",
    "â”‚                   â”‚         â”‚     â”‚   (OLAP)    â”‚     â”‚ (Dashboard) â”‚     â”‚\n",
    "â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   M24: Kafka + Spark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚\n",
    "â”‚   M33: OLAP + Dashboards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â—€â”€â”€ CE MODULE                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clickhouse_intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ClickHouse : Le Moteur OLAP Ultra-Rapide\n",
    "\n",
    "### 2.1 Qu'est-ce que ClickHouse ?\n",
    "\n",
    "**ClickHouse** est un SGBD OLAP open-source crÃ©Ã© par Yandex, conÃ§u pour :\n",
    "- RequÃªtes analytiques sur des **milliards de lignes**\n",
    "- Latence de **millisecondes**\n",
    "- Ingestion Ã  **haute vitesse** (millions de lignes/sec)\n",
    "\n",
    "### 2.2 Pourquoi ClickHouse est Rapide ?\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CLICKHOUSE : SECRETS DE PERFORMANCE                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   1. COLUMNAR STORAGE                                                       â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\n",
    "â”‚   Row-based:    [id, name, amount, date] [id, name, amount, date] ...      â”‚\n",
    "â”‚   Column-based: [id, id, id...] [name, name...] [amount, amount...] âœ…     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â†’ Lit uniquement les colonnes nÃ©cessaires                                â”‚\n",
    "â”‚   â†’ Compression excellente (valeurs similaires groupÃ©es)                   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   2. VECTORIZED EXECUTION                                                   â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚\n",
    "â”‚   Traite les donnÃ©es par blocs (SIMD), pas ligne par ligne                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   3. DATA SKIPPING                                                          â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚\n",
    "â”‚   Indexes sparse + min/max par granule â†’ skip des blocs inutiles           â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   4. COMPRESSION                                                            â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\n",
    "â”‚   LZ4/ZSTD par dÃ©faut, 10-20x compression ratio                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2.3 Architecture ClickHouse\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CLICKHOUSE ARCHITECTURE                                  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                         CLUSTER                                     â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚   Shard 1   â”‚   â”‚   Shard 2   â”‚   â”‚   Shard 3   â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚             â”‚   â”‚             â”‚   â”‚             â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â”‚Replica 1â”‚ â”‚   â”‚ â”‚Replica 1â”‚ â”‚   â”‚ â”‚Replica 1â”‚ â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â”‚Replica 2â”‚ â”‚   â”‚ â”‚Replica 2â”‚ â”‚   â”‚ â”‚Replica 2â”‚ â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚  â”‚\n",
    "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   Sharding: Distribution horizontale des donnÃ©es                   â”‚  â”‚\n",
    "â”‚   â”‚   Replication: Haute disponibilitÃ©                                 â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                      ZOOKEEPER / CLICKHOUSE KEEPER                  â”‚  â”‚\n",
    "â”‚   â”‚                      (coordination, replication)                    â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clickhouse_setup",
   "metadata": {},
   "source": [
    "### 2.4 Installation avec Docker\n",
    "\n",
    "```bash\n",
    "# DÃ©marrer ClickHouse (single node)\n",
    "docker run -d \\\n",
    "    --name clickhouse-server \\\n",
    "    -p 8123:8123 \\\n",
    "    -p 9000:9000 \\\n",
    "    -v clickhouse_data:/var/lib/clickhouse \\\n",
    "    -v clickhouse_logs:/var/log/clickhouse-server \\\n",
    "    clickhouse/clickhouse-server:latest\n",
    "\n",
    "# AccÃ©der au client CLI\n",
    "docker exec -it clickhouse-server clickhouse-client\n",
    "\n",
    "# Ou via HTTP (port 8123)\n",
    "curl 'http://localhost:8123/?query=SELECT%201'\n",
    "```\n",
    "\n",
    "### 2.5 Docker Compose (ClickHouse + Kafka + Grafana)\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports:\n",
    "      - \"2181:2181\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - \"9092:9092\"\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "\n",
    "  clickhouse:\n",
    "    image: clickhouse/clickhouse-server:latest\n",
    "    ports:\n",
    "      - \"8123:8123\"  # HTTP\n",
    "      - \"9000:9000\"  # Native\n",
    "    volumes:\n",
    "      - clickhouse_data:/var/lib/clickhouse\n",
    "    ulimits:\n",
    "      nofile:\n",
    "        soft: 262144\n",
    "        hard: 262144\n",
    "\n",
    "  grafana:\n",
    "    image: grafana/grafana:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource\n",
    "    volumes:\n",
    "      - grafana_data:/var/lib/grafana\n",
    "\n",
    "volumes:\n",
    "  clickhouse_data:\n",
    "  grafana_data:\n",
    "```\n",
    "\n",
    "```bash\n",
    "# DÃ©marrer tout\n",
    "docker-compose up -d\n",
    "\n",
    "# AccÃ¨s :\n",
    "# - ClickHouse : http://localhost:8123\n",
    "# - Grafana : http://localhost:3000 (admin/admin)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clickhouse_tables",
   "metadata": {},
   "source": [
    "### 2.6 Table Engines\n",
    "\n",
    "ClickHouse propose diffÃ©rents **engines** selon le use case :\n",
    "\n",
    "| Engine | Use Case | CaractÃ©ristiques |\n",
    "|--------|----------|------------------|\n",
    "| **MergeTree** | Analytics standard | Le plus utilisÃ©, tri, partitioning |\n",
    "| **ReplacingMergeTree** | DÃ©duplication | Garde derniÃ¨re version par clÃ© |\n",
    "| **SummingMergeTree** | PrÃ©-agrÃ©gation | Somme automatique par clÃ© |\n",
    "| **AggregatingMergeTree** | AgrÃ©gations complexes | States d'agrÃ©gation |\n",
    "| **Kafka** | Ingestion Kafka | Consomme directement un topic |\n",
    "| **Buffer** | Write buffering | Accumule avant d'Ã©crire |\n",
    "\n",
    "### 2.7 CrÃ©er une Table MergeTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple SQL ClickHouse\n",
    "\n",
    "create_table_sql = \"\"\"\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- TABLE : events (MergeTree)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS events\n",
    "(\n",
    "    -- Colonnes\n",
    "    event_id        UUID DEFAULT generateUUIDv4(),\n",
    "    event_time      DateTime64(3),          -- Millisecond precision\n",
    "    event_date      Date DEFAULT toDate(event_time),\n",
    "    user_id         String,\n",
    "    event_type      LowCardinality(String), -- OptimisÃ© pour peu de valeurs distinctes\n",
    "    page            String,\n",
    "    country         LowCardinality(String),\n",
    "    device          LowCardinality(String),\n",
    "    session_id      String,\n",
    "    duration_ms     UInt32,\n",
    "    revenue         Decimal(10, 2) DEFAULT 0,\n",
    "    properties      String                   -- JSON stockÃ© comme String\n",
    ")\n",
    "ENGINE = MergeTree()\n",
    "PARTITION BY toYYYYMM(event_date)           -- Partition par mois\n",
    "ORDER BY (event_date, event_type, user_id)  -- ClÃ© de tri (crucial pour performance)\n",
    "TTL event_date + INTERVAL 90 DAY            -- Retention 90 jours\n",
    "SETTINGS index_granularity = 8192;          -- GranularitÃ© de l'index\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- InsÃ©rer des donnÃ©es\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "INSERT INTO events (event_time, user_id, event_type, page, country, device, session_id, duration_ms, revenue)\n",
    "VALUES\n",
    "    (now(), 'user_001', 'page_view', '/home', 'FR', 'mobile', 'sess_abc', 1500, 0),\n",
    "    (now(), 'user_001', 'click', '/products', 'FR', 'mobile', 'sess_abc', 200, 0),\n",
    "    (now(), 'user_002', 'purchase', '/checkout', 'US', 'desktop', 'sess_xyz', 5000, 99.99),\n",
    "    (now(), 'user_003', 'page_view', '/home', 'DE', 'tablet', 'sess_123', 800, 0);\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- RequÃªtes analytiques\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "-- Events par type aujourd'hui\n",
    "SELECT \n",
    "    event_type,\n",
    "    count() AS event_count,\n",
    "    uniq(user_id) AS unique_users,\n",
    "    avg(duration_ms) AS avg_duration\n",
    "FROM events\n",
    "WHERE event_date = today()\n",
    "GROUP BY event_type\n",
    "ORDER BY event_count DESC;\n",
    "\n",
    "-- Revenue par pays (derniÃ¨re heure)\n",
    "SELECT \n",
    "    country,\n",
    "    sum(revenue) AS total_revenue,\n",
    "    count() AS purchases\n",
    "FROM events\n",
    "WHERE event_type = 'purchase'\n",
    "  AND event_time >= now() - INTERVAL 1 HOUR\n",
    "GROUP BY country\n",
    "ORDER BY total_revenue DESC;\n",
    "\n",
    "-- Funnel analysis\n",
    "SELECT\n",
    "    countIf(event_type = 'page_view') AS views,\n",
    "    countIf(event_type = 'click') AS clicks,\n",
    "    countIf(event_type = 'purchase') AS purchases,\n",
    "    round(clicks / views * 100, 2) AS click_rate,\n",
    "    round(purchases / clicks * 100, 2) AS conversion_rate\n",
    "FROM events\n",
    "WHERE event_date = today();\n",
    "\"\"\"\n",
    "\n",
    "print(create_table_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clickhouse_order_by",
   "metadata": {},
   "source": [
    "### 2.8 ORDER BY : La ClÃ© de la Performance\n",
    "\n",
    "Le `ORDER BY` est **crucial** dans ClickHouse. Il dÃ©finit :\n",
    "- L'ordre physique des donnÃ©es sur disque\n",
    "- L'index primaire (sparse index)\n",
    "- Les colonnes Ã  utiliser dans les filtres WHERE\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    ORDER BY BEST PRACTICES                                  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   RÃˆGLE 1 : Mettre les colonnes de filtre frÃ©quent en premier              â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\n",
    "â”‚   ORDER BY (date, user_id, event_type)                                     â”‚\n",
    "â”‚             ^^^^                                                            â”‚\n",
    "â”‚   Si tu filtres souvent par date, mets-la en premier                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   RÃˆGLE 2 : Du moins cardinal au plus cardinal                             â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚\n",
    "â”‚   ORDER BY (country, city, user_id)                                        â”‚\n",
    "â”‚             ~200      ~50K   ~10M  valeurs distinctes                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   RÃˆGLE 3 : Ne pas mettre trop de colonnes                                 â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚\n",
    "â”‚   3-5 colonnes max, sinon l'index grossit trop                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "```sql\n",
    "-- BON : filtre sur les premiÃ¨res colonnes du ORDER BY\n",
    "SELECT * FROM events\n",
    "WHERE event_date = '2024-01-15' AND event_type = 'purchase';\n",
    "\n",
    "-- MOINS BON : filtre sur une colonne non dans ORDER BY\n",
    "SELECT * FROM events\n",
    "WHERE session_id = 'abc123';  -- Full scan possible\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kafka_engine",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Ingestion Kafka â†’ ClickHouse\n",
    "\n",
    "### 3.1 Architecture d'Ingestion\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    KAFKA â†’ CLICKHOUSE INGESTION                             â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚   â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚ Kafka Engineâ”‚â”€â”€â”€â”€â–¶â”‚ Materializedâ”‚â”€â”€â”€â”€â–¶â”‚ MergeTree   â”‚ â”‚\n",
    "â”‚   â”‚  Topic  â”‚     â”‚   (source)  â”‚     â”‚    View     â”‚     â”‚  (storage)  â”‚ â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Le pattern recommandÃ© :                                                  â”‚\n",
    "â”‚   1. Table Kafka Engine consomme le topic                                  â”‚\n",
    "â”‚   2. Materialized View transforme et insÃ¨re dans la table finale          â”‚\n",
    "â”‚   3. Table MergeTree stocke les donnÃ©es                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 3.2 Configuration ComplÃ¨te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kafka_ingestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Kafka â†’ ClickHouse\n",
    "\n",
    "kafka_ingestion_sql = \"\"\"\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- Ã‰TAPE 1 : Table de stockage final (MergeTree)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS events_final\n",
    "(\n",
    "    event_time      DateTime64(3),\n",
    "    event_date      Date DEFAULT toDate(event_time),\n",
    "    user_id         String,\n",
    "    event_type      LowCardinality(String),\n",
    "    page            String,\n",
    "    country         LowCardinality(String),\n",
    "    amount          Decimal(10, 2),\n",
    "    _kafka_topic    LowCardinality(String),\n",
    "    _kafka_offset   UInt64,\n",
    "    _inserted_at    DateTime DEFAULT now()\n",
    ")\n",
    "ENGINE = MergeTree()\n",
    "PARTITION BY toYYYYMM(event_date)\n",
    "ORDER BY (event_date, event_type, user_id)\n",
    "TTL event_date + INTERVAL 180 DAY;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- Ã‰TAPE 2 : Table Kafka Engine (source)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS events_kafka\n",
    "(\n",
    "    raw String\n",
    ")\n",
    "ENGINE = Kafka\n",
    "SETTINGS\n",
    "    kafka_broker_list = 'kafka:29092',\n",
    "    kafka_topic_list = 'events',\n",
    "    kafka_group_name = 'clickhouse_consumer',\n",
    "    kafka_format = 'JSONAsString',\n",
    "    kafka_num_consumers = 2,                    -- ParallÃ©lisme\n",
    "    kafka_max_block_size = 65536,\n",
    "    kafka_skip_broken_messages = 100;           -- TolÃ©rance aux erreurs\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- Ã‰TAPE 3 : Materialized View (transformation + insertion)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS events_kafka_mv\n",
    "TO events_final\n",
    "AS SELECT\n",
    "    -- Parser le JSON\n",
    "    parseDateTime64BestEffort(JSONExtractString(raw, 'timestamp')) AS event_time,\n",
    "    JSONExtractString(raw, 'user_id') AS user_id,\n",
    "    JSONExtractString(raw, 'event_type') AS event_type,\n",
    "    JSONExtractString(raw, 'page') AS page,\n",
    "    JSONExtractString(raw, 'country') AS country,\n",
    "    toDecimal64(JSONExtractFloat(raw, 'amount'), 2) AS amount,\n",
    "    _topic AS _kafka_topic,\n",
    "    _offset AS _kafka_offset\n",
    "FROM events_kafka;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- VÃ©rifier l'ingestion\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "-- Nombre d'events ingÃ©rÃ©s\n",
    "SELECT count() FROM events_final;\n",
    "\n",
    "-- Lag Kafka (derniers offsets)\n",
    "SELECT \n",
    "    _kafka_topic,\n",
    "    max(_kafka_offset) AS latest_offset,\n",
    "    max(_inserted_at) AS last_insert\n",
    "FROM events_final\n",
    "GROUP BY _kafka_topic;\n",
    "\n",
    "-- Events par minute (monitoring)\n",
    "SELECT \n",
    "    toStartOfMinute(event_time) AS minute,\n",
    "    count() AS events\n",
    "FROM events_final\n",
    "WHERE event_time >= now() - INTERVAL 1 HOUR\n",
    "GROUP BY minute\n",
    "ORDER BY minute DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "print(kafka_ingestion_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "producer_example",
   "metadata": {},
   "source": [
    "### 3.3 Producer Python pour Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kafka_producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producer Kafka pour envoyer des events\n",
    "\n",
    "producer_code = '''\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "event_types = ['page_view', 'click', 'scroll', 'purchase', 'signup']\n",
    "pages = ['/home', '/products', '/cart', '/checkout', '/profile']\n",
    "countries = ['FR', 'US', 'DE', 'GB', 'ES', 'IT']\n",
    "\n",
    "def generate_event():\n",
    "    event_type = random.choice(event_types)\n",
    "    return {\n",
    "        'timestamp': datetime.utcnow().isoformat(),\n",
    "        'user_id': f'user_{random.randint(1, 1000):04d}',\n",
    "        'event_type': event_type,\n",
    "        'page': random.choice(pages),\n",
    "        'country': random.choice(countries),\n",
    "        'amount': round(random.uniform(10, 500), 2) if event_type == 'purchase' else 0\n",
    "    }\n",
    "\n",
    "# Envoyer des events en continu\n",
    "print(\"Sending events to Kafka...\")\n",
    "try:\n",
    "    while True:\n",
    "        event = generate_event()\n",
    "        producer.send('events', value=event)\n",
    "        print(f\"Sent: {event['event_type']} from {event['country']}\")\n",
    "        time.sleep(0.1)  # 10 events/sec\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped\")\n",
    "finally:\n",
    "    producer.close()\n",
    "'''\n",
    "\n",
    "print(\"# kafka_producer.py\")\n",
    "print(producer_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "materialized_views",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Materialized Views pour PrÃ©-AgrÃ©gation\n",
    "\n",
    "### 4.1 Pourquoi PrÃ©-AgrÃ©ger ?\n",
    "\n",
    "| Approche | Query Time | Storage | FlexibilitÃ© |\n",
    "|----------|------------|---------|-------------|\n",
    "| Raw data + query Ã  la volÃ©e | Lent sur gros volumes | Minimal | Maximum |\n",
    "| **Materialized View** | Ultra-rapide | ModÃ©rÃ© | PrÃ©-dÃ©fini |\n",
    "\n",
    "### 4.2 SummingMergeTree : AgrÃ©gation Automatique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summing_mv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialized View avec SummingMergeTree\n",
    "\n",
    "mv_summing_sql = \"\"\"\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- AGRÃ‰GATION HORAIRE : events par type, pays, heure\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "-- Table de destination (SummingMergeTree)\n",
    "CREATE TABLE IF NOT EXISTS events_hourly\n",
    "(\n",
    "    event_hour      DateTime,\n",
    "    event_type      LowCardinality(String),\n",
    "    country         LowCardinality(String),\n",
    "    event_count     UInt64,\n",
    "    unique_users    AggregateFunction(uniq, String),  -- HyperLogLog\n",
    "    total_amount    Decimal(18, 2),\n",
    "    avg_amount      AggregateFunction(avg, Decimal(10, 2))\n",
    ")\n",
    "ENGINE = SummingMergeTree((event_count, total_amount))\n",
    "PARTITION BY toYYYYMM(event_hour)\n",
    "ORDER BY (event_hour, event_type, country);\n",
    "\n",
    "-- Materialized View qui alimente la table\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS events_hourly_mv\n",
    "TO events_hourly\n",
    "AS SELECT\n",
    "    toStartOfHour(event_time) AS event_hour,\n",
    "    event_type,\n",
    "    country,\n",
    "    count() AS event_count,\n",
    "    uniqState(user_id) AS unique_users,        -- State pour merge\n",
    "    sum(amount) AS total_amount,\n",
    "    avgState(amount) AS avg_amount             -- State pour merge\n",
    "FROM events_final\n",
    "GROUP BY event_hour, event_type, country;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- REQUÃŠTES SUR L'AGRÃ‰GAT (ultra-rapides !)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "-- Events par heure (derniÃ¨res 24h)\n",
    "SELECT \n",
    "    event_hour,\n",
    "    sum(event_count) AS total_events,\n",
    "    uniqMerge(unique_users) AS unique_users,   -- Merge les HLL\n",
    "    sum(total_amount) AS revenue\n",
    "FROM events_hourly\n",
    "WHERE event_hour >= now() - INTERVAL 24 HOUR\n",
    "GROUP BY event_hour\n",
    "ORDER BY event_hour;\n",
    "\n",
    "-- Top pays par revenue\n",
    "SELECT \n",
    "    country,\n",
    "    sum(event_count) AS events,\n",
    "    sum(total_amount) AS revenue,\n",
    "    avgMerge(avg_amount) AS avg_order_value\n",
    "FROM events_hourly\n",
    "WHERE event_type = 'purchase'\n",
    "  AND event_hour >= today()\n",
    "GROUP BY country\n",
    "ORDER BY revenue DESC;\n",
    "\"\"\"\n",
    "\n",
    "print(mv_summing_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agg_mv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Materialized View pour mÃ©triques temps rÃ©el (par minute)\n",
    "\n",
    "mv_realtime_sql = \"\"\"\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- MÃ‰TRIQUES TEMPS RÃ‰EL (par minute, pour dashboards)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS events_minute\n",
    "(\n",
    "    event_minute    DateTime,\n",
    "    event_type      LowCardinality(String),\n",
    "    event_count     UInt64,\n",
    "    unique_users    UInt64,\n",
    "    total_amount    Decimal(18, 2)\n",
    ")\n",
    "ENGINE = SummingMergeTree((event_count, unique_users, total_amount))\n",
    "ORDER BY (event_minute, event_type)\n",
    "TTL event_minute + INTERVAL 7 DAY;  -- Garder 7 jours seulement\n",
    "\n",
    "CREATE MATERIALIZED VIEW IF NOT EXISTS events_minute_mv\n",
    "TO events_minute\n",
    "AS SELECT\n",
    "    toStartOfMinute(event_time) AS event_minute,\n",
    "    event_type,\n",
    "    count() AS event_count,\n",
    "    uniq(user_id) AS unique_users,\n",
    "    sum(amount) AS total_amount\n",
    "FROM events_final\n",
    "GROUP BY event_minute, event_type;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- QUERIES POUR DASHBOARD TEMPS RÃ‰EL\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "-- DerniÃ¨res 5 minutes (refresh toutes les 5 sec)\n",
    "SELECT \n",
    "    event_minute,\n",
    "    sum(event_count) AS events,\n",
    "    sum(unique_users) AS users,\n",
    "    sum(total_amount) AS revenue\n",
    "FROM events_minute\n",
    "WHERE event_minute >= now() - INTERVAL 5 MINUTE\n",
    "GROUP BY event_minute\n",
    "ORDER BY event_minute;\n",
    "\n",
    "-- Events par seconde (approximation)\n",
    "SELECT \n",
    "    sum(event_count) / 60 AS events_per_second\n",
    "FROM events_minute\n",
    "WHERE event_minute >= now() - INTERVAL 1 MINUTE;\n",
    "\n",
    "-- Comparaison vs mÃªme heure hier\n",
    "SELECT \n",
    "    'today' AS period,\n",
    "    sum(event_count) AS events,\n",
    "    sum(total_amount) AS revenue\n",
    "FROM events_minute\n",
    "WHERE event_minute >= toStartOfHour(now())\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'yesterday' AS period,\n",
    "    sum(event_count) AS events,\n",
    "    sum(total_amount) AS revenue\n",
    "FROM events_minute\n",
    "WHERE event_minute >= toStartOfHour(now() - INTERVAL 1 DAY)\n",
    "  AND event_minute < toStartOfHour(now() - INTERVAL 1 DAY) + INTERVAL 1 HOUR;\n",
    "\"\"\"\n",
    "\n",
    "print(mv_realtime_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "druid_pinot",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Alternatives : Apache Druid & Pinot\n",
    "\n",
    "### 5.1 Apache Druid\n",
    "\n",
    "**Druid** est un OLAP engine optimisÃ© pour les **time-series** et le **real-time**.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    APACHE DRUID ARCHITECTURE                                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚   â”‚   Kafka     â”‚â”€â”€â–¶â”‚   Middle    â”‚â”€â”€â–¶â”‚   Historicalâ”‚                      â”‚\n",
    "â”‚   â”‚   (stream)  â”‚   â”‚   Manager   â”‚   â”‚   (segments)â”‚                      â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                                              â”‚                              â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚                              â”‚\n",
    "â”‚   â”‚   Batch     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚   â”‚   (HDFS/S3) â”‚                                                           â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚\n",
    "â”‚                                              â”‚                              â”‚\n",
    "â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                              â”‚\n",
    "â”‚                           â”‚     Broker      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€ Queries              â”‚\n",
    "â”‚                           â”‚   (scatter/gather)                              â”‚\n",
    "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**CaractÃ©ristiques Druid :**\n",
    "- Columnar storage avec compression\n",
    "- Ingestion real-time ET batch\n",
    "- Roll-up automatique (prÃ©-agrÃ©gation Ã  l'ingestion)\n",
    "- OptimisÃ© pour GROUP BY sur time-series\n",
    "\n",
    "**Quand utiliser Druid :**\n",
    "- Time-series analytics (monitoring, IoT)\n",
    "- TrÃ¨s hauts volumes (trillions de rows)\n",
    "- Besoin de roll-up Ã  l'ingestion\n",
    "\n",
    "### 5.2 Apache Pinot\n",
    "\n",
    "**Pinot** est un OLAP engine crÃ©Ã© par LinkedIn, optimisÃ© pour les **user-facing analytics**.\n",
    "\n",
    "**CaractÃ©ristiques Pinot :**\n",
    "- Latence ultra-basse (<100ms P99)\n",
    "- OptimisÃ© pour queries concurrentes (1000+ QPS)\n",
    "- Star-tree index pour agrÃ©gations prÃ©-calculÃ©es\n",
    "- Upsert support (contrairement Ã  Druid)\n",
    "\n",
    "**Quand utiliser Pinot :**\n",
    "- Analytics user-facing (dashboards clients)\n",
    "- TrÃ¨s haute concurrence\n",
    "- Besoin d'upserts\n",
    "\n",
    "### 5.3 Comparaison ClickHouse vs Druid vs Pinot\n",
    "\n",
    "| Feature | ClickHouse | Druid | Pinot |\n",
    "|---------|------------|-------|-------|\n",
    "| **Type** | OLAP DB | Time-series OLAP | User-facing OLAP |\n",
    "| **SQL** | Full SQL | Druid SQL (limitÃ©) | PQL + SQL |\n",
    "| **Latency** | ~10-100ms | ~100-500ms | ~10-50ms |\n",
    "| **Concurrency** | ~100 | ~100 | ~1000+ |\n",
    "| **Upserts** | Oui (ReplacingMergeTree) | Non | Oui |\n",
    "| **Joins** | Oui | LimitÃ© | LimitÃ© |\n",
    "| **Complexity** | Simple | Complexe | Medium |\n",
    "| **Best for** | General analytics | Time-series | User-facing |\n",
    "\n",
    "### 5.4 Recommandations\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    QUEL OLAP CHOISIR ?                                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Tu veux de l'analytics interne (dashboards, ad-hoc) ?              â”‚  â”‚\n",
    "â”‚   â”‚  â†’ ClickHouse (simple, SQL complet, flexible)                       â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Tu as des time-series Ã  trÃ¨s haut volume avec roll-up ?            â”‚  â”‚\n",
    "â”‚   â”‚  â†’ Druid (optimisÃ© pour Ã§a, mais plus complexe)                     â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Tu as des dashboards user-facing avec 1000+ users concurrents ?    â”‚  â”‚\n",
    "â”‚   â”‚  â†’ Pinot (conÃ§u pour Ã§a, latence garantie)                          â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Dans le doute : commence par ClickHouse (plus simple Ã  opÃ©rer)           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dashboards",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Real-Time Dashboards\n",
    "\n",
    "### 6.1 Architecture Dashboard Temps RÃ©el\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    REAL-TIME DASHBOARD ARCHITECTURE                         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Option 1: PULL (Polling)                                                  â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Dashboard â”€â”€(every 5s)â”€â”€â–¶ ClickHouse â”€â”€â–¶ Response                        â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   âœ… Simple                                                                 â”‚\n",
    "â”‚   âŒ Latence = intervalle de refresh                                        â”‚\n",
    "â”‚   âŒ Charge DB si beaucoup de clients                                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Option 2: PUSH (WebSocket/SSE)                                            â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Kafka â”€â”€â–¶ Stream Processor â”€â”€â–¶ WebSocket â”€â”€â–¶ Dashboard                   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   âœ… Latence minimale                                                       â”‚\n",
    "â”‚   âœ… Efficient (pas de polling)                                             â”‚\n",
    "â”‚   âŒ Plus complexe Ã  implÃ©menter                                            â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Option 3: HYBRIDE (recommandÃ©)                                            â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â€¢ DonnÃ©es historiques : Query ClickHouse                                 â”‚\n",
    "â”‚   â€¢ MÃ©triques live : WebSocket depuis Kafka                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 6.2 Grafana + ClickHouse\n",
    "\n",
    "**Grafana** est l'outil le plus populaire pour les dashboards temps rÃ©el.\n",
    "\n",
    "#### Configuration du Data Source\n",
    "\n",
    "```yaml\n",
    "# grafana/provisioning/datasources/clickhouse.yaml\n",
    "apiVersion: 1\n",
    "\n",
    "datasources:\n",
    "  - name: ClickHouse\n",
    "    type: grafana-clickhouse-datasource\n",
    "    access: proxy\n",
    "    url: http://clickhouse:8123\n",
    "    jsonData:\n",
    "      defaultDatabase: default\n",
    "      port: 9000\n",
    "      server: clickhouse\n",
    "      username: default\n",
    "      tlsSkipVerify: true\n",
    "    secureJsonData:\n",
    "      password: \"\"\n",
    "```\n",
    "\n",
    "#### Exemples de Queries pour Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grafana_queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries Grafana pour ClickHouse\n",
    "\n",
    "grafana_queries = \"\"\"\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 1 : Time Series - Events par minute\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT \n",
    "    $__timeInterval(event_minute) AS time,\n",
    "    sum(event_count) AS events\n",
    "FROM events_minute\n",
    "WHERE $__timeFilter(event_minute)\n",
    "GROUP BY time\n",
    "ORDER BY time;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 2 : Stat - Total events (derniÃ¨re heure)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT sum(event_count) AS total_events\n",
    "FROM events_minute\n",
    "WHERE event_minute >= now() - INTERVAL 1 HOUR;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 3 : Pie Chart - Events par type\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT \n",
    "    event_type,\n",
    "    sum(event_count) AS count\n",
    "FROM events_minute\n",
    "WHERE event_minute >= now() - INTERVAL 1 HOUR\n",
    "GROUP BY event_type\n",
    "ORDER BY count DESC;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 4 : Bar Chart - Top 10 pays par revenue\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT \n",
    "    country,\n",
    "    sum(total_amount) AS revenue\n",
    "FROM events_hourly\n",
    "WHERE event_hour >= today()\n",
    "  AND event_type = 'purchase'\n",
    "GROUP BY country\n",
    "ORDER BY revenue DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 5 : Gauge - Conversion rate (temps rÃ©el)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT \n",
    "    round(\n",
    "        sumIf(event_count, event_type = 'purchase') / \n",
    "        sumIf(event_count, event_type = 'page_view') * 100, \n",
    "        2\n",
    "    ) AS conversion_rate\n",
    "FROM events_minute\n",
    "WHERE event_minute >= now() - INTERVAL 1 HOUR;\n",
    "\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "-- PANEL 6 : Table - Derniers events (live)\n",
    "-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "SELECT \n",
    "    event_time,\n",
    "    user_id,\n",
    "    event_type,\n",
    "    country,\n",
    "    amount\n",
    "FROM events_final\n",
    "WHERE event_time >= now() - INTERVAL 5 MINUTE\n",
    "ORDER BY event_time DESC\n",
    "LIMIT 100;\n",
    "\"\"\"\n",
    "\n",
    "print(grafana_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grafana_dashboard",
   "metadata": {},
   "source": [
    "### 6.3 Dashboard JSON (Import dans Grafana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grafana_json",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "grafana_dashboard = {\n",
    "    \"dashboard\": {\n",
    "        \"title\": \"Real-Time Analytics\",\n",
    "        \"tags\": [\"clickhouse\", \"realtime\"],\n",
    "        \"timezone\": \"browser\",\n",
    "        \"refresh\": \"5s\",  # Auto-refresh toutes les 5 secondes\n",
    "        \"panels\": [\n",
    "            {\n",
    "                \"id\": 1,\n",
    "                \"title\": \"Events per Minute\",\n",
    "                \"type\": \"timeseries\",\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 0, \"w\": 12, \"h\": 8},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT $__timeInterval(event_minute) AS time, sum(event_count) AS events FROM events_minute WHERE $__timeFilter(event_minute) GROUP BY time ORDER BY time\",\n",
    "                    \"format\": \"time_series\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 2,\n",
    "                \"title\": \"Total Events (1h)\",\n",
    "                \"type\": \"stat\",\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 0, \"w\": 4, \"h\": 4},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT sum(event_count) AS total FROM events_minute WHERE event_minute >= now() - INTERVAL 1 HOUR\",\n",
    "                    \"format\": \"table\"\n",
    "                }],\n",
    "                \"options\": {\n",
    "                    \"colorMode\": \"value\",\n",
    "                    \"graphMode\": \"none\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": 3,\n",
    "                \"title\": \"Revenue (1h)\",\n",
    "                \"type\": \"stat\",\n",
    "                \"gridPos\": {\"x\": 16, \"y\": 0, \"w\": 4, \"h\": 4},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT sum(total_amount) AS revenue FROM events_minute WHERE event_minute >= now() - INTERVAL 1 HOUR\",\n",
    "                    \"format\": \"table\"\n",
    "                }],\n",
    "                \"options\": {\n",
    "                    \"colorMode\": \"value\"\n",
    "                },\n",
    "                \"fieldConfig\": {\n",
    "                    \"defaults\": {\n",
    "                        \"unit\": \"currencyEUR\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"id\": 4,\n",
    "                \"title\": \"Events by Type\",\n",
    "                \"type\": \"piechart\",\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 4, \"w\": 8, \"h\": 8},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT event_type, sum(event_count) AS count FROM events_minute WHERE event_minute >= now() - INTERVAL 1 HOUR GROUP BY event_type\",\n",
    "                    \"format\": \"table\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 5,\n",
    "                \"title\": \"Top Countries by Revenue\",\n",
    "                \"type\": \"barchart\",\n",
    "                \"gridPos\": {\"x\": 0, \"y\": 8, \"w\": 12, \"h\": 8},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT country, sum(total_amount) AS revenue FROM events_hourly WHERE event_hour >= today() AND event_type = 'purchase' GROUP BY country ORDER BY revenue DESC LIMIT 10\",\n",
    "                    \"format\": \"table\"\n",
    "                }]\n",
    "            },\n",
    "            {\n",
    "                \"id\": 6,\n",
    "                \"title\": \"Live Events\",\n",
    "                \"type\": \"table\",\n",
    "                \"gridPos\": {\"x\": 12, \"y\": 12, \"w\": 12, \"h\": 8},\n",
    "                \"datasource\": \"ClickHouse\",\n",
    "                \"targets\": [{\n",
    "                    \"rawSql\": \"SELECT event_time, user_id, event_type, country, amount FROM events_final WHERE event_time >= now() - INTERVAL 5 MINUTE ORDER BY event_time DESC LIMIT 50\",\n",
    "                    \"format\": \"table\"\n",
    "                }]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"overwrite\": True\n",
    "}\n",
    "\n",
    "print(\"ðŸ“Š Grafana Dashboard JSON:\")\n",
    "print(json.dumps(grafana_dashboard, indent=2)[:2000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superset",
   "metadata": {},
   "source": [
    "### 6.4 Apache Superset (Alternative)\n",
    "\n",
    "**Apache Superset** est une alternative open-source Ã  Grafana, plus orientÃ©e BI.\n",
    "\n",
    "```bash\n",
    "# Docker Compose pour Superset\n",
    "docker run -d -p 8088:8088 \\\n",
    "    --name superset \\\n",
    "    -e SUPERSET_SECRET_KEY='your-secret-key' \\\n",
    "    apache/superset\n",
    "\n",
    "# Setup initial\n",
    "docker exec -it superset superset fab create-admin \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --email admin@example.com \\\n",
    "    --password admin\n",
    "\n",
    "docker exec -it superset superset db upgrade\n",
    "docker exec -it superset superset init\n",
    "\n",
    "# AccÃ¨s : http://localhost:8088\n",
    "```\n",
    "\n",
    "**Connexion ClickHouse dans Superset :**\n",
    "```\n",
    "clickhousedb://default:@clickhouse:8123/default\n",
    "```\n",
    "\n",
    "### 6.5 Comparaison Grafana vs Superset\n",
    "\n",
    "| Feature | Grafana | Superset |\n",
    "|---------|---------|----------|\n",
    "| **Focus** | Monitoring, time-series | BI, exploration |\n",
    "| **Refresh** | Excellent (auto, push) | Bon (polling) |\n",
    "| **SQL Editor** | Basique | Excellent |\n",
    "| **Exploration** | LimitÃ©e | TrÃ¨s bonne |\n",
    "| **Alerting** | IntÃ©grÃ© | Via plugin |\n",
    "| **Best for** | Ops dashboards | Business analytics |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best_practices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Patterns et Best Practices\n",
    "\n",
    "### 7.1 Pre-Aggregation Patterns\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    PRE-AGGREGATION PATTERNS                                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   PATTERN 1 : Multi-Level Aggregation                                       â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Raw Events â”€â”€â–¶ Per-Minute â”€â”€â–¶ Per-Hour â”€â”€â–¶ Per-Day                       â”‚\n",
    "â”‚   (dÃ©tail)       (7 jours)      (90 jours)   (1+ an)                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â†’ Queries rapides Ã  chaque niveau                                        â”‚\n",
    "â”‚   â†’ Retention diffÃ©rente selon granularitÃ©                                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   PATTERN 2 : Dimension-Specific Tables                                     â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   events_by_country  (agrÃ©gÃ© par pays)                                     â”‚\n",
    "â”‚   events_by_product  (agrÃ©gÃ© par produit)                                  â”‚\n",
    "â”‚   events_by_user     (agrÃ©gÃ© par user)                                     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â†’ Ultra-rapide pour les dimensions connues                               â”‚\n",
    "â”‚   â†’ Moins flexible pour ad-hoc                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 7.2 Tiered Storage\n",
    "\n",
    "```sql\n",
    "-- ClickHouse : Storage policies\n",
    "CREATE TABLE events_tiered\n",
    "(\n",
    "    event_time DateTime,\n",
    "    event_type String,\n",
    "    data String\n",
    ")\n",
    "ENGINE = MergeTree()\n",
    "ORDER BY event_time\n",
    "TTL \n",
    "    event_time + INTERVAL 7 DAY TO VOLUME 'hot',     -- SSD\n",
    "    event_time + INTERVAL 30 DAY TO VOLUME 'warm',   -- HDD\n",
    "    event_time + INTERVAL 365 DAY TO VOLUME 'cold';  -- S3\n",
    "```\n",
    "\n",
    "### 7.3 Retention Policies\n",
    "\n",
    "```sql\n",
    "-- TTL pour auto-delete\n",
    "ALTER TABLE events_minute\n",
    "MODIFY TTL event_minute + INTERVAL 7 DAY;\n",
    "\n",
    "-- Voir l'espace utilisÃ©\n",
    "SELECT \n",
    "    table,\n",
    "    formatReadableSize(sum(bytes_on_disk)) AS size,\n",
    "    sum(rows) AS rows\n",
    "FROM system.parts\n",
    "WHERE active\n",
    "GROUP BY table\n",
    "ORDER BY sum(bytes_on_disk) DESC;\n",
    "```\n",
    "\n",
    "### 7.4 Monitoring des Pipelines\n",
    "\n",
    "```sql\n",
    "-- Lag d'ingestion (Kafka offset vs current)\n",
    "SELECT \n",
    "    max(_kafka_offset) AS latest_offset,\n",
    "    max(event_time) AS latest_event,\n",
    "    dateDiff('second', max(event_time), now()) AS lag_seconds\n",
    "FROM events_final;\n",
    "\n",
    "-- Throughput (events/sec)\n",
    "SELECT \n",
    "    toStartOfMinute(event_time) AS minute,\n",
    "    count() / 60 AS events_per_second\n",
    "FROM events_final\n",
    "WHERE event_time >= now() - INTERVAL 10 MINUTE\n",
    "GROUP BY minute\n",
    "ORDER BY minute DESC;\n",
    "\n",
    "-- Alerter si lag > 5 minutes\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN dateDiff('minute', max(event_time), now()) > 5 \n",
    "        THEN 'ALERT: Ingestion lag > 5 min!'\n",
    "        ELSE 'OK'\n",
    "    END AS status\n",
    "FROM events_final;\n",
    "```\n",
    "\n",
    "### 7.5 Cost Optimization\n",
    "\n",
    "| Technique | Impact | ImplÃ©mentation |\n",
    "|-----------|--------|----------------|\n",
    "| **Compression** | -80% storage | LZ4 (dÃ©faut) ou ZSTD |\n",
    "| **TTL** | Limite le volume | `TTL date + INTERVAL X DAY` |\n",
    "| **Partitioning** | Pruning efficace | `PARTITION BY toYYYYMM(date)` |\n",
    "| **Materialized Views** | -90% query cost | PrÃ©-agrÃ©gation |\n",
    "| **LowCardinality** | -50% pour enums | `LowCardinality(String)` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "python_client",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Client Python pour ClickHouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clickhouse_python",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client Python pour ClickHouse\n",
    "\n",
    "python_client_code = '''\n",
    "# pip install clickhouse-connect\n",
    "\n",
    "import clickhouse_connect\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# CONNEXION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "client = clickhouse_connect.get_client(\n",
    "    host='localhost',\n",
    "    port=8123,\n",
    "    username='default',\n",
    "    password=''\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# QUERIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Query simple\n",
    "result = client.query(\"SELECT count() FROM events_final\")\n",
    "print(f\"Total events: {result.result_rows[0][0]}\")\n",
    "\n",
    "# Query avec paramÃ¨tres\n",
    "result = client.query(\n",
    "    \"SELECT event_type, count() FROM events_final \"\n",
    "    \"WHERE event_time >= {start:DateTime} \"\n",
    "    \"GROUP BY event_type\",\n",
    "    parameters={\"start\": datetime.now() - timedelta(hours=1)}\n",
    ")\n",
    "\n",
    "for row in result.result_rows:\n",
    "    print(f\"{row[0]}: {row[1]}\")\n",
    "\n",
    "# Query vers DataFrame\n",
    "df = client.query_df(\n",
    "    \"SELECT toStartOfMinute(event_time) AS minute, count() AS events \"\n",
    "    \"FROM events_final \"\n",
    "    \"WHERE event_time >= now() - INTERVAL 1 HOUR \"\n",
    "    \"GROUP BY minute ORDER BY minute\"\n",
    ")\n",
    "print(df.head())\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSERT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Insert batch\n",
    "data = [\n",
    "    [datetime.now(), \"user_001\", \"page_view\", \"FR\", 0],\n",
    "    [datetime.now(), \"user_002\", \"click\", \"US\", 0],\n",
    "    [datetime.now(), \"user_003\", \"purchase\", \"DE\", 99.99],\n",
    "]\n",
    "\n",
    "client.insert(\n",
    "    \"events_final\",\n",
    "    data,\n",
    "    column_names=[\"event_time\", \"user_id\", \"event_type\", \"country\", \"amount\"]\n",
    ")\n",
    "\n",
    "# Insert depuis DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"event_time\": [datetime.now()] * 100,\n",
    "    \"user_id\": [f\"user_{i:04d}\" for i in range(100)],\n",
    "    \"event_type\": [\"page_view\"] * 100,\n",
    "    \"country\": [\"FR\"] * 100,\n",
    "    \"amount\": [0.0] * 100\n",
    "})\n",
    "\n",
    "client.insert_df(\"events_final\", df)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ASYNC (pour haute performance)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import asyncio\n",
    "import clickhouse_connect.driver.asyncclient as async_client\n",
    "\n",
    "async def async_queries():\n",
    "    client = await async_client.create_async_client(host=\"localhost\")\n",
    "    \n",
    "    result = await client.query(\"SELECT count() FROM events_final\")\n",
    "    print(f\"Async count: {result.result_rows[0][0]}\")\n",
    "    \n",
    "    await client.close()\n",
    "\n",
    "# asyncio.run(async_queries())\n",
    "'''\n",
    "\n",
    "print(python_client_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Setup ClickHouse + Kafka Ingestion\n",
    "\n",
    "1. DÃ©marrer ClickHouse et Kafka avec Docker Compose\n",
    "2. CrÃ©er un topic Kafka `events`\n",
    "3. CrÃ©er la table `events_final` (MergeTree)\n",
    "4. CrÃ©er la table Kafka Engine + Materialized View\n",
    "5. Envoyer des events avec le producer Python\n",
    "6. VÃ©rifier l'ingestion dans ClickHouse\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 2 : Materialized Views Multi-Niveaux\n",
    "\n",
    "CrÃ©er 3 niveaux d'agrÃ©gation :\n",
    "- `events_minute` : TTL 7 jours\n",
    "- `events_hourly` : TTL 90 jours\n",
    "- `events_daily` : TTL 2 ans\n",
    "\n",
    "VÃ©rifier que les queries sur chaque niveau sont rapides.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 3 : Dashboard Grafana\n",
    "\n",
    "CrÃ©er un dashboard avec :\n",
    "- Time series : events par minute\n",
    "- Stats : total events, revenue, unique users\n",
    "- Pie chart : events par type\n",
    "- Table : derniers events live\n",
    "\n",
    "Configurer auto-refresh Ã  5 secondes.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 4 : Alerting\n",
    "\n",
    "CrÃ©er des alertes Grafana pour :\n",
    "- Lag d'ingestion > 5 minutes\n",
    "- Events/sec < 10 (drop de trafic)\n",
    "- Revenue = 0 depuis 30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 5 : Benchmark\n",
    "\n",
    "1. GÃ©nÃ©rer 10 millions d'events\n",
    "2. Comparer les temps de query sur :\n",
    "   - Table raw (MergeTree)\n",
    "   - Materialized View minute\n",
    "   - Materialized View horaire\n",
    "3. Documenter les gains de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Mini-Projet : Real-Time Analytics Platform\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Construire une plateforme d'analytics temps rÃ©el complÃ¨te.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   MINI-PROJET : REAL-TIME ANALYTICS                         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
    "â”‚   â”‚   Event     â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚ ClickHouse  â”‚                      â”‚\n",
    "â”‚   â”‚  Generator  â”‚     â”‚  Topic  â”‚     â”‚             â”‚                      â”‚\n",
    "â”‚   â”‚  (Python)   â”‚     â”‚         â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚  Raw    â”‚ â”‚â”€â”€â”€â”€â–¶â”‚   Grafana   â”‚ â”‚\n",
    "â”‚                                       â”‚ â”‚  Table  â”‚ â”‚     â”‚  Dashboard  â”‚ â”‚\n",
    "â”‚   Throughput:                         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚     â”‚             â”‚ â”‚\n",
    "â”‚   1000 events/sec                     â”‚      â”‚      â”‚     â”‚  â€¢ Live     â”‚ â”‚\n",
    "â”‚                                       â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚     â”‚  â€¢ Refresh  â”‚ â”‚\n",
    "â”‚                                       â”‚ â”‚  MVs    â”‚ â”‚     â”‚    5 sec    â”‚ â”‚\n",
    "â”‚                                       â”‚ â”‚ min/hr  â”‚ â”‚     â”‚             â”‚ â”‚\n",
    "â”‚                                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   MÃ©triques Ã  afficher :                                                   â”‚\n",
    "â”‚   â€¢ Events/minute (time series)                                            â”‚\n",
    "â”‚   â€¢ Revenue temps rÃ©el                                                     â”‚\n",
    "â”‚   â€¢ Top pays, Top produits                                                 â”‚\n",
    "â”‚   â€¢ Conversion funnel                                                      â”‚\n",
    "â”‚   â€¢ Alertes si anomalie                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Livrables\n",
    "\n",
    "1. **docker-compose.yaml** : Kafka + ClickHouse + Grafana\n",
    "2. **Event Generator** : Python script gÃ©nÃ©rant 1000 events/sec\n",
    "3. **ClickHouse Schema** : Tables + Materialized Views\n",
    "4. **Grafana Dashboard** : 6+ panels avec auto-refresh\n",
    "5. **Alerting** : 3+ alertes configurÃ©es\n",
    "6. **Documentation** : README avec architecture et setup\n",
    "\n",
    "### Structure du Projet\n",
    "\n",
    "```text\n",
    "realtime-analytics/\n",
    "â”œâ”€â”€ docker-compose.yaml\n",
    "â”œâ”€â”€ clickhouse/\n",
    "â”‚   â””â”€â”€ init.sql                # Schema + MVs\n",
    "â”œâ”€â”€ producer/\n",
    "â”‚   â”œâ”€â”€ requirements.txt\n",
    "â”‚   â””â”€â”€ event_generator.py\n",
    "â”œâ”€â”€ grafana/\n",
    "â”‚   â”œâ”€â”€ provisioning/\n",
    "â”‚   â”‚   â”œâ”€â”€ datasources/\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ clickhouse.yaml\n",
    "â”‚   â”‚   â””â”€â”€ dashboards/\n",
    "â”‚   â”‚       â””â”€â”€ realtime.json\n",
    "â”‚   â””â”€â”€ dashboards/\n",
    "â”‚       â””â”€â”€ realtime_analytics.json\n",
    "â””â”€â”€ README.md\n",
    "```\n",
    "\n",
    "### CritÃ¨res de SuccÃ¨s\n",
    "\n",
    "- [ ] Ingestion Kafka â†’ ClickHouse fonctionne\n",
    "- [ ] Materialized Views crÃ©Ã©es (minute + heure)\n",
    "- [ ] Dashboard avec 6+ panels\n",
    "- [ ] Auto-refresh 5 secondes\n",
    "- [ ] Alertes configurÃ©es\n",
    "- [ ] Latence < 10 secondes end-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Ressources\n",
    "\n",
    "### Documentation\n",
    "- [ClickHouse Documentation](https://clickhouse.com/docs/)\n",
    "- [Grafana ClickHouse Plugin](https://grafana.com/grafana/plugins/grafana-clickhouse-datasource/)\n",
    "- [Apache Druid Docs](https://druid.apache.org/docs/latest/)\n",
    "- [Apache Pinot Docs](https://docs.pinot.apache.org/)\n",
    "\n",
    "### Articles\n",
    "- [ClickHouse vs Druid vs Pinot](https://clickhouse.com/blog/clickhouse-vs-druid-vs-pinot-olap-comparison)\n",
    "- [Real-Time Analytics at Scale](https://www.uber.com/blog/real-time-analytics/)\n",
    "- [Materialized Views Best Practices](https://clickhouse.com/docs/en/guides/developer/cascading-materialized-views)\n",
    "\n",
    "### Tutoriels\n",
    "- [ClickHouse + Kafka Tutorial](https://clickhouse.com/docs/en/integrations/kafka)\n",
    "- [Grafana + ClickHouse Setup](https://grafana.com/docs/grafana/latest/datasources/clickhouse/)\n",
    "\n",
    "---\n",
    "\n",
    "## âž¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ðŸ‘‰ **Module suivant : `34_cloud_data_platform`** â€” Cloud Data Platforms (AWS/GCP/Azure)\n",
    "\n",
    "---\n",
    "\n",
    "ðŸŽ‰ **FÃ©licitations !** Tu maÃ®trises maintenant les OLAP engines et les dashboards temps rÃ©el."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
