{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ¤– Data Engineering for ML\n",
    "\n",
    "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  construire l'**infrastructure data** qui alimente les systÃ¨mes de Machine Learning. En tant que Data Engineer, tu ne crÃ©es pas les modÃ¨les, mais tu construis les **pipelines**, **Feature Stores**, et **systÃ¨mes de monitoring** qui rendent le ML possible en production.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | PySpark DataFrame API (M19) |\n",
    "| âœ… Requis | Delta Lake (M20) |\n",
    "| âœ… Requis | Airflow (M22, M28) |\n",
    "| âœ… Requis | Data Quality avec Great Expectations (M23) |\n",
    "| ğŸ’¡ RecommandÃ© | Notions de base en ML (features, training, inference) |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Construire des **Feature Pipelines** robustes avec Spark\n",
    "- DÃ©ployer et alimenter un **Feature Store** (Feast)\n",
    "- CrÃ©er des **Training Datasets** sans data leakage\n",
    "- ImplÃ©menter la **Data Validation** spÃ©cifique au ML\n",
    "- Mettre en place le **Data Monitoring** (drift detection)\n",
    "- Comprendre l'intÃ©gration avec MLflow (cÃ´tÃ© data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Le RÃ´le du Data Engineer dans le ML\n",
    "\n",
    "### 1.1 ML Lifecycle vu par le Data Engineer\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     DATA ENGINEER SCOPE IN ML                               â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                    DATA ENGINEER CONSTRUIT                          â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   Raw Data â”€â”€â–¶ Data Pipelines â”€â”€â–¶ Feature Pipelines â”€â”€â–¶ Feature    â”‚  â”‚\n",
    "â”‚   â”‚                                                          Store     â”‚  â”‚\n",
    "â”‚   â”‚                                                            â”‚        â”‚  â”‚\n",
    "â”‚   â”‚   Training Data â—€â”€â”€ Serving Data â—€â”€â”€ Data Validation â—€â”€â”€â”€â”€â”˜        â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                          â”‚                                  â”‚\n",
    "â”‚                                          â–¼                                  â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                    DATA SCIENTIST UTILISE                           â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   Feature Store â”€â”€â–¶ Model Training â”€â”€â–¶ Model Registry â”€â”€â–¶ Serving  â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 1.2 Data Engineer vs Data Scientist vs ML Engineer\n",
    "\n",
    "| RÃ´le | ResponsabilitÃ©s |\n",
    "|------|----------------|\n",
    "| **Data Engineer** | Pipelines de donnÃ©es, Feature Store infra, Data Quality, Monitoring data |\n",
    "| **Data Scientist** | Feature design, Model training, Experimentation, Evaluation |\n",
    "| **ML Engineer** | Model deployment, Model serving, Model monitoring, MLOps |\n",
    "\n",
    "### 1.3 ProblÃ¨mes classiques que le DE doit rÃ©soudre\n",
    "\n",
    "| ProblÃ¨me | Description | Solution DE |\n",
    "|----------|-------------|-------------|\n",
    "| **Training-Serving Skew** | Features diffÃ©rentes en training vs production | Feature Store unique |\n",
    "| **Data Leakage** | Utiliser des donnÃ©es du futur pour prÃ©dire le passÃ© | Point-in-time joins |\n",
    "| **Reproducibility** | Impossible de recrÃ©er un training dataset | Dataset versioning |\n",
    "| **Feature Inconsistency** | Calcul diffÃ©rent selon les Ã©quipes | Feature pipelines centralisÃ©s |\n",
    "| **Stale Features** | Features pas Ã  jour en production | Refresh pipelines, CDC |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_pipelines",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Feature Pipelines avec Spark\n",
    "\n",
    "### 2.1 Qu'est-ce qu'une Feature ?\n",
    "\n",
    "Une **feature** est une variable dÃ©rivÃ©e des donnÃ©es brutes, utilisÃ©e comme input pour un modÃ¨le ML.\n",
    "\n",
    "| Raw Data | Features dÃ©rivÃ©es |\n",
    "|----------|------------------|\n",
    "| Transactions individuelles | `total_transactions_30d`, `avg_amount_30d` |\n",
    "| Clics sur un site | `pages_viewed_7d`, `time_on_site_avg` |\n",
    "| Historique d'achats | `days_since_last_purchase`, `favorite_category` |\n",
    "\n",
    "### 2.2 Transformations courantes\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeaturePipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_sample_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er des donnÃ©es d'exemple\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FeaturePipeline\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# DonnÃ©es de transactions\n",
    "transactions_data = [\n",
    "    (\"C001\", \"TXN001\", 150.0, \"Electronics\", \"2024-01-15\"),\n",
    "    (\"C001\", \"TXN002\", 25.0, \"Food\", \"2024-01-18\"),\n",
    "    (\"C001\", \"TXN003\", 200.0, \"Electronics\", \"2024-01-25\"),\n",
    "    (\"C002\", \"TXN004\", 75.0, \"Clothing\", \"2024-01-10\"),\n",
    "    (\"C002\", \"TXN005\", 50.0, \"Food\", \"2024-01-20\"),\n",
    "    (\"C003\", \"TXN006\", 500.0, \"Electronics\", \"2024-01-05\"),\n",
    "    (\"C003\", \"TXN007\", 30.0, \"Food\", \"2024-01-08\"),\n",
    "    (\"C003\", \"TXN008\", 120.0, \"Clothing\", \"2024-01-22\"),\n",
    "    (\"C001\", \"TXN009\", 80.0, \"Food\", \"2024-02-01\"),\n",
    "    (\"C002\", \"TXN010\", 300.0, \"Electronics\", \"2024-02-05\"),\n",
    "]\n",
    "\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"category\", StringType(), False),\n",
    "    StructField(\"transaction_date\", StringType(), False),\n",
    "])\n",
    "\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_schema) \\\n",
    "    .withColumn(\"transaction_date\", F.to_date(\"transaction_date\"))\n",
    "\n",
    "print(\"ğŸ“¦ Transactions brutes :\")\n",
    "transactions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregation_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FEATURE 1 : AgrÃ©gations simples\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "aggregation_features = transactions_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_transactions\"),\n",
    "        F.sum(\"amount\").alias(\"total_spent\"),\n",
    "        F.avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "        F.min(\"amount\").alias(\"min_transaction_amount\"),\n",
    "        F.max(\"amount\").alias(\"max_transaction_amount\"),\n",
    "        F.stddev(\"amount\").alias(\"stddev_transaction_amount\"),\n",
    "        F.countDistinct(\"category\").alias(\"unique_categories\"),\n",
    "        F.max(\"transaction_date\").alias(\"last_transaction_date\"),\n",
    "        F.min(\"transaction_date\").alias(\"first_transaction_date\"),\n",
    "    )\n",
    "\n",
    "print(\"ğŸ“Š Features d'agrÃ©gation :\")\n",
    "aggregation_features.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "window_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FEATURE 2 : Window Functions (features temporelles)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# DÃ©finir les fenÃªtres\n",
    "window_30d = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(F.col(\"transaction_date\").cast(\"long\")) \\\n",
    "    .rangeBetween(-30 * 86400, 0)  # 30 jours en secondes\n",
    "\n",
    "window_7d = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(F.col(\"transaction_date\").cast(\"long\")) \\\n",
    "    .rangeBetween(-7 * 86400, 0)\n",
    "\n",
    "# Features rolling\n",
    "rolling_features = transactions_df \\\n",
    "    .withColumn(\"transaction_ts\", F.col(\"transaction_date\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"amount_sum_30d\", F.sum(\"amount\").over(window_30d)) \\\n",
    "    .withColumn(\"amount_avg_30d\", F.avg(\"amount\").over(window_30d)) \\\n",
    "    .withColumn(\"txn_count_30d\", F.count(\"*\").over(window_30d)) \\\n",
    "    .withColumn(\"amount_sum_7d\", F.sum(\"amount\").over(window_7d)) \\\n",
    "    .withColumn(\"txn_count_7d\", F.count(\"*\").over(window_7d))\n",
    "\n",
    "print(\"ğŸ“ˆ Features avec Window Functions :\")\n",
    "rolling_features.select(\n",
    "    \"customer_id\", \"transaction_date\", \"amount\",\n",
    "    \"amount_sum_30d\", \"txn_count_30d\", \"amount_sum_7d\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FEATURE 3 : Encoding catÃ©goriel\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# One-Hot Encoding manuel (pivot)\n",
    "category_pivot = transactions_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .pivot(\"category\") \\\n",
    "    .agg(F.count(\"*\")) \\\n",
    "    .fillna(0)\n",
    "\n",
    "print(\"ğŸ·ï¸ One-Hot Encoding des catÃ©gories :\")\n",
    "category_pivot.show()\n",
    "\n",
    "# CatÃ©gorie favorite\n",
    "favorite_category = transactions_df \\\n",
    "    .groupBy(\"customer_id\", \"category\") \\\n",
    "    .agg(F.sum(\"amount\").alias(\"category_total\")) \\\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"customer_id\").orderBy(F.desc(\"category_total\"))\n",
    "        )\n",
    "    ) \\\n",
    "    .filter(F.col(\"rank\") == 1) \\\n",
    "    .select(\"customer_id\", F.col(\"category\").alias(\"favorite_category\"))\n",
    "\n",
    "print(\"â­ CatÃ©gorie favorite par client :\")\n",
    "favorite_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recency_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FEATURE 4 : Features de rÃ©cence (RFM)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "reference_date = \"2024-02-10\"\n",
    "\n",
    "rfm_features = transactions_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        F.max(\"transaction_date\").alias(\"last_transaction\"),\n",
    "        F.count(\"*\").alias(\"frequency\"),\n",
    "        F.sum(\"amount\").alias(\"monetary\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"recency_days\",\n",
    "        F.datediff(F.lit(reference_date), F.col(\"last_transaction\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"is_active_30d\",\n",
    "        F.when(F.col(\"recency_days\") <= 30, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "print(\"ğŸ“… Features RFM (Recency, Frequency, Monetary) :\")\n",
    "rfm_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_feature_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# PIPELINE COMPLET : Assembler toutes les features\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def build_customer_features(transactions_df, reference_date):\n",
    "    \"\"\"\n",
    "    Pipeline complet de feature engineering pour les clients.\n",
    "    \n",
    "    Args:\n",
    "        transactions_df: DataFrame des transactions\n",
    "        reference_date: Date de rÃ©fÃ©rence pour les calculs de rÃ©cence\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec toutes les features client\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. AgrÃ©gations de base\n",
    "    base_features = transactions_df \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"total_transactions\"),\n",
    "            F.sum(\"amount\").alias(\"total_spent\"),\n",
    "            F.avg(\"amount\").alias(\"avg_transaction_amount\"),\n",
    "            F.stddev(\"amount\").alias(\"stddev_amount\"),\n",
    "            F.countDistinct(\"category\").alias(\"unique_categories\"),\n",
    "            F.max(\"transaction_date\").alias(\"last_transaction_date\"),\n",
    "            F.min(\"transaction_date\").alias(\"first_transaction_date\"),\n",
    "        )\n",
    "    \n",
    "    # 2. Features de rÃ©cence\n",
    "    recency_features = base_features \\\n",
    "        .withColumn(\n",
    "            \"recency_days\",\n",
    "            F.datediff(F.lit(reference_date), F.col(\"last_transaction_date\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"customer_tenure_days\",\n",
    "            F.datediff(F.lit(reference_date), F.col(\"first_transaction_date\"))\n",
    "        ) \\\n",
    "        .withColumn(\n",
    "            \"is_active_30d\",\n",
    "            F.when(F.col(\"recency_days\") <= 30, 1).otherwise(0)\n",
    "        )\n",
    "    \n",
    "    # 3. CatÃ©gorie favorite\n",
    "    favorite_cat = transactions_df \\\n",
    "        .groupBy(\"customer_id\", \"category\") \\\n",
    "        .agg(F.count(\"*\").alias(\"cat_count\")) \\\n",
    "        .withColumn(\n",
    "            \"rank\",\n",
    "            F.row_number().over(\n",
    "                Window.partitionBy(\"customer_id\").orderBy(F.desc(\"cat_count\"))\n",
    "            )\n",
    "        ) \\\n",
    "        .filter(F.col(\"rank\") == 1) \\\n",
    "        .select(\"customer_id\", F.col(\"category\").alias(\"favorite_category\"))\n",
    "    \n",
    "    # 4. One-hot des catÃ©gories\n",
    "    category_ohe = transactions_df \\\n",
    "        .groupBy(\"customer_id\") \\\n",
    "        .pivot(\"category\") \\\n",
    "        .agg(F.count(\"*\")) \\\n",
    "        .fillna(0)\n",
    "    \n",
    "    # Renommer les colonnes OHE\n",
    "    for col_name in category_ohe.columns:\n",
    "        if col_name != \"customer_id\":\n",
    "            category_ohe = category_ohe.withColumnRenamed(\n",
    "                col_name, f\"category_{col_name.lower()}_count\"\n",
    "            )\n",
    "    \n",
    "    # 5. Joindre toutes les features\n",
    "    final_features = recency_features \\\n",
    "        .join(favorite_cat, \"customer_id\", \"left\") \\\n",
    "        .join(category_ohe, \"customer_id\", \"left\") \\\n",
    "        .withColumn(\"feature_timestamp\", F.lit(reference_date).cast(\"timestamp\"))\n",
    "    \n",
    "    return final_features\n",
    "\n",
    "# ExÃ©cuter le pipeline\n",
    "customer_features = build_customer_features(transactions_df, \"2024-02-10\")\n",
    "\n",
    "print(\"ğŸ¯ Features client complÃ¨tes :\")\n",
    "customer_features.show(truncate=False)\n",
    "customer_features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "point_in_time",
   "metadata": {},
   "source": [
    "### 2.3 Point-in-Time Correctness (Ã‰viter le Data Leakage)\n",
    "\n",
    "**Data Leakage** = utiliser des informations du futur pour prÃ©dire le passÃ©.\n",
    "\n",
    "```text\n",
    "âŒ MAUVAIS (Data Leakage) :\n",
    "   Pour prÃ©dire si le client achÃ¨te le 15 janvier,\n",
    "   on utilise ses transactions du 20 janvier â†’ TRICHE !\n",
    "\n",
    "âœ… BON (Point-in-Time Correct) :\n",
    "   Pour prÃ©dire si le client achÃ¨te le 15 janvier,\n",
    "   on utilise UNIQUEMENT ses transactions AVANT le 15 janvier.\n",
    "```\n",
    "\n",
    "```python\n",
    "def build_features_as_of(transactions_df, as_of_date):\n",
    "    \"\"\"\n",
    "    Construire les features en utilisant UNIQUEMENT les donnÃ©es\n",
    "    disponibles AVANT as_of_date.\n",
    "    \"\"\"\n",
    "    # Filtrer les transactions AVANT la date\n",
    "    filtered = transactions_df.filter(\n",
    "        F.col(\"transaction_date\") < as_of_date\n",
    "    )\n",
    "    \n",
    "    return build_customer_features(filtered, as_of_date)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_store_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Infrastructure Feature Store\n",
    "\n",
    "### 3.1 Pourquoi un Feature Store ?\n",
    "\n",
    "| ProblÃ¨me sans Feature Store | Solution avec Feature Store |\n",
    "|-----------------------------|-----------------------------||\n",
    "| Features calculÃ©es diffÃ©remment en training vs serving | **Single source of truth** |\n",
    "| Duplication du code de features | **RÃ©utilisation** |\n",
    "| Pas de dÃ©couverte des features existantes | **Feature discovery & catalog** |\n",
    "| Point-in-time joins complexes | **Built-in time-travel** |\n",
    "| Latence Ã©levÃ©e en serving | **Online store low-latency** |\n",
    "\n",
    "### 3.2 Architecture Feature Store\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         FEATURE STORE ARCHITECTURE                          â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚  Feature        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  Pipelines      â”‚â”€â”€â”€â”€â–¶â”‚           OFFLINE STORE                     â”‚  â”‚\n",
    "â”‚   â”‚  (Spark/Airflow)â”‚     â”‚  (Data Warehouse / Delta Lake / Parquet)    â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  - Historical features                      â”‚  â”‚\n",
    "â”‚                           â”‚  - Training data generation                 â”‚  â”‚\n",
    "â”‚                           â”‚  - Backfill support                         â”‚  â”‚\n",
    "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                              â”‚                              â”‚\n",
    "â”‚                                    Materialization Job                      â”‚\n",
    "â”‚                                              â”‚                              â”‚\n",
    "â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚                           â”‚           ONLINE STORE                      â”‚  â”‚\n",
    "â”‚                           â”‚  (Redis / DynamoDB / Cassandra)             â”‚  â”‚\n",
    "â”‚                           â”‚  - Latest feature values only               â”‚  â”‚\n",
    "â”‚                           â”‚  - Low-latency serving (<10ms)              â”‚  â”‚\n",
    "â”‚                           â”‚  - Real-time inference                      â”‚  â”‚\n",
    "â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 3.3 Feast : Feature Store Open Source\n",
    "\n",
    "**Feast** (Feature Store) est le Feature Store open-source le plus populaire.\n",
    "\n",
    "#### Installation\n",
    "\n",
    "```bash\n",
    "pip install feast[redis]\n",
    "\n",
    "# CrÃ©er un projet Feast\n",
    "feast init my_feature_store\n",
    "cd my_feature_store\n",
    "```\n",
    "\n",
    "#### Structure du projet\n",
    "\n",
    "```text\n",
    "my_feature_store/\n",
    "â”œâ”€â”€ feature_store.yaml      # Configuration\n",
    "â”œâ”€â”€ features.py             # DÃ©finition des features\n",
    "â””â”€â”€ data/\n",
    "    â””â”€â”€ customer_features.parquet\n",
    "```\n",
    "\n",
    "#### Configuration `feature_store.yaml`\n",
    "\n",
    "```yaml\n",
    "project: my_ml_project\n",
    "registry: data/registry.db\n",
    "provider: local\n",
    "\n",
    "offline_store:\n",
    "  type: file\n",
    "  # En production : type: snowflake / bigquery / redshift\n",
    "\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: localhost:6379\n",
    "  # Alternatives : dynamodb, datastore, sqlite (local)\n",
    "\n",
    "entity_key_serialization_version: 2\n",
    "```\n",
    "\n",
    "#### DÃ©finition des features `features.py`\n",
    "\n",
    "```python\n",
    "from datetime import timedelta\n",
    "from feast import Entity, Feature, FeatureView, FileSource, ValueType\n",
    "from feast.types import Float64, Int64, String\n",
    "\n",
    "# 1. DÃ©finir l'entitÃ© (la clÃ©)\n",
    "customer = Entity(\n",
    "    name=\"customer_id\",\n",
    "    value_type=ValueType.STRING,\n",
    "    description=\"Unique customer identifier\"\n",
    ")\n",
    "\n",
    "# 2. DÃ©finir la source de donnÃ©es\n",
    "customer_features_source = FileSource(\n",
    "    path=\"data/customer_features.parquet\",\n",
    "    timestamp_field=\"feature_timestamp\",\n",
    ")\n",
    "\n",
    "# 3. DÃ©finir la Feature View\n",
    "customer_features_view = FeatureView(\n",
    "    name=\"customer_features\",\n",
    "    entities=[customer],\n",
    "    ttl=timedelta(days=1),  # Time-to-live dans l'online store\n",
    "    schema=[\n",
    "        Feature(name=\"total_transactions\", dtype=Int64),\n",
    "        Feature(name=\"total_spent\", dtype=Float64),\n",
    "        Feature(name=\"avg_transaction_amount\", dtype=Float64),\n",
    "        Feature(name=\"recency_days\", dtype=Int64),\n",
    "        Feature(name=\"is_active_30d\", dtype=Int64),\n",
    "        Feature(name=\"favorite_category\", dtype=String),\n",
    "    ],\n",
    "    source=customer_features_source,\n",
    "    online=True,  # MatÃ©rialiser dans l'online store\n",
    ")\n",
    "```\n",
    "\n",
    "#### Commandes Feast\n",
    "\n",
    "```bash\n",
    "# Appliquer les dÃ©finitions\n",
    "feast apply\n",
    "\n",
    "# MatÃ©rialiser dans l'online store\n",
    "feast materialize 2024-01-01 2024-02-10\n",
    "\n",
    "# MatÃ©rialisation incrÃ©mentale\n",
    "feast materialize-incremental $(date +%Y-%m-%d)\n",
    "```\n",
    "\n",
    "#### Utilisation Python\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# OFFLINE : RÃ©cupÃ©rer des features historiques (training)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "entity_df = pd.DataFrame({\n",
    "    \"customer_id\": [\"C001\", \"C002\", \"C003\"],\n",
    "    \"event_timestamp\": [datetime(2024, 2, 1)] * 3\n",
    "})\n",
    "\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"customer_features:total_transactions\",\n",
    "        \"customer_features:total_spent\",\n",
    "        \"customer_features:recency_days\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ONLINE : RÃ©cupÃ©rer les features en temps rÃ©el (serving)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "online_features = store.get_online_features(\n",
    "    features=[\n",
    "        \"customer_features:total_spent\",\n",
    "        \"customer_features:is_active_30d\",\n",
    "    ],\n",
    "    entity_rows=[{\"customer_id\": \"C001\"}]\n",
    ").to_dict()\n",
    "\n",
    "print(online_features)\n",
    "# {'customer_id': ['C001'], 'total_spent': [375.0], 'is_active_30d': [1]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feast_example",
   "metadata": {},
   "source": [
    "### 3.4 Pipeline d'alimentation du Feature Store\n",
    "\n",
    "```python\n",
    "# feature_pipeline_dag.py (Airflow)\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': True,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'feature_pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # 1. Calculer les features avec Spark\n",
    "    compute_features = SparkSubmitOperator(\n",
    "        task_id='compute_customer_features',\n",
    "        application='/jobs/compute_features.py',\n",
    "        application_args=['--date', '{{ ds }}'],\n",
    "        conf={\n",
    "            'spark.executor.memory': '4g',\n",
    "            'spark.executor.cores': '2',\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # 2. Valider les features\n",
    "    validate_features = PythonOperator(\n",
    "        task_id='validate_features',\n",
    "        python_callable=run_feature_validation,\n",
    "        op_kwargs={'date': '{{ ds }}'},\n",
    "    )\n",
    "    \n",
    "    # 3. MatÃ©rialiser dans le Feature Store\n",
    "    materialize = PythonOperator(\n",
    "        task_id='materialize_features',\n",
    "        python_callable=materialize_to_feast,\n",
    "        op_kwargs={'end_date': '{{ ds }}'},\n",
    "    )\n",
    "    \n",
    "    compute_features >> validate_features >> materialize\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_data",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Training Data Pipelines\n",
    "\n",
    "### 4.1 GÃ©nÃ©rer des Datasets Reproductibles\n",
    "\n",
    "Un bon training dataset doit Ãªtre :\n",
    "- **Reproductible** : on peut le recrÃ©er exactement\n",
    "- **VersionnÃ©** : on sait quelle version a Ã©tÃ© utilisÃ©e\n",
    "- **Point-in-time correct** : pas de data leakage\n",
    "\n",
    "### 4.2 Point-in-Time Joins\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       POINT-IN-TIME JOIN                                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Events (ce qu'on prÃ©dit)          Features (inputs du modÃ¨le)            â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   customer_id | event_date          customer_id | feature_date | features  â”‚\n",
    "â”‚   C001        | 2024-02-01          C001        | 2024-01-15   | {...}     â”‚\n",
    "â”‚   C001        | 2024-02-15          C001        | 2024-02-01   | {...}     â”‚\n",
    "â”‚                                     C001        | 2024-02-10   | {...}     â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Pour l'event du 2024-02-01 :                                             â”‚\n",
    "â”‚   â†’ Utiliser les features du 2024-01-15 (la plus rÃ©cente AVANT l'event)    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Pour l'event du 2024-02-15 :                                             â”‚\n",
    "â”‚   â†’ Utiliser les features du 2024-02-10 (la plus rÃ©cente AVANT l'event)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "point_in_time_join",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# POINT-IN-TIME JOIN avec Spark\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# CrÃ©er des donnÃ©es d'exemple\n",
    "\n",
    "# Events : ce qu'on veut prÃ©dire (ex: churn, achat)\n",
    "events_data = [\n",
    "    (\"C001\", \"2024-02-01\", 1),  # Client C001 a churnÃ© le 1er fÃ©vrier\n",
    "    (\"C002\", \"2024-02-05\", 0),  # Client C002 n'a pas churnÃ©\n",
    "    (\"C003\", \"2024-02-10\", 1),  # Client C003 a churnÃ© le 10 fÃ©vrier\n",
    "]\n",
    "events_df = spark.createDataFrame(\n",
    "    events_data, \n",
    "    [\"customer_id\", \"event_date\", \"label\"]\n",
    ").withColumn(\"event_date\", F.to_date(\"event_date\"))\n",
    "\n",
    "# Features avec timestamps (plusieurs versions par client)\n",
    "features_data = [\n",
    "    (\"C001\", \"2024-01-15\", 5, 500.0),\n",
    "    (\"C001\", \"2024-01-25\", 6, 550.0),\n",
    "    (\"C002\", \"2024-01-20\", 3, 200.0),\n",
    "    (\"C002\", \"2024-02-01\", 4, 280.0),\n",
    "    (\"C003\", \"2024-01-10\", 10, 1000.0),\n",
    "    (\"C003\", \"2024-02-05\", 11, 1100.0),\n",
    "]\n",
    "features_df = spark.createDataFrame(\n",
    "    features_data,\n",
    "    [\"customer_id\", \"feature_date\", \"total_transactions\", \"total_spent\"]\n",
    ").withColumn(\"feature_date\", F.to_date(\"feature_date\"))\n",
    "\n",
    "print(\"ğŸ“… Events (labels) :\")\n",
    "events_df.show()\n",
    "\n",
    "print(\"ğŸ“Š Features (avec historique) :\")\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pit_join_implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_time_join(events_df, features_df, entity_col, event_ts_col, feature_ts_col):\n",
    "    \"\"\"\n",
    "    Join point-in-time correct : pour chaque event, rÃ©cupÃ©rer\n",
    "    les features les plus rÃ©centes AVANT l'event.\n",
    "    \n",
    "    Args:\n",
    "        events_df: DataFrame avec les events et leurs timestamps\n",
    "        features_df: DataFrame avec les features et leurs timestamps\n",
    "        entity_col: Colonne de jointure (ex: customer_id)\n",
    "        event_ts_col: Colonne timestamp dans events_df\n",
    "        feature_ts_col: Colonne timestamp dans features_df\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec events + features point-in-time correct\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Joindre sur l'entitÃ© + feature_date < event_date\n",
    "    joined = events_df.alias(\"e\").join(\n",
    "        features_df.alias(\"f\"),\n",
    "        (F.col(f\"e.{entity_col}\") == F.col(f\"f.{entity_col}\")) &\n",
    "        (F.col(f\"f.{feature_ts_col}\") < F.col(f\"e.{event_ts_col}\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # 2. Garder uniquement la feature la plus rÃ©cente avant l'event\n",
    "    window = Window.partitionBy(f\"e.{entity_col}\", f\"e.{event_ts_col}\") \\\n",
    "                   .orderBy(F.col(f\"f.{feature_ts_col}\").desc())\n",
    "    \n",
    "    result = joined \\\n",
    "        .withColumn(\"_rank\", F.row_number().over(window)) \\\n",
    "        .filter(F.col(\"_rank\") == 1) \\\n",
    "        .drop(\"_rank\", f\"f.{entity_col}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ExÃ©cuter le join point-in-time\n",
    "training_data = point_in_time_join(\n",
    "    events_df, \n",
    "    features_df,\n",
    "    entity_col=\"customer_id\",\n",
    "    event_ts_col=\"event_date\",\n",
    "    feature_ts_col=\"feature_date\"\n",
    ")\n",
    "\n",
    "print(\"ğŸ¯ Training Dataset (Point-in-Time Correct) :\")\n",
    "training_data.select(\n",
    "    \"customer_id\", \"event_date\", \"label\", \n",
    "    \"feature_date\", \"total_transactions\", \"total_spent\"\n",
    ").show()\n",
    "\n",
    "# VÃ©rification : feature_date est toujours < event_date âœ“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_versioning",
   "metadata": {},
   "source": [
    "### 4.3 Dataset Versioning avec Delta Lake\n",
    "\n",
    "```python\n",
    "# Sauvegarder le training dataset avec versioning\n",
    "\n",
    "training_data.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"data/training_datasets/churn_model\")\n",
    "\n",
    "# Ajouter des mÃ©tadonnÃ©es\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"data/training_datasets/churn_model\")\n",
    "\n",
    "# Voir l'historique des versions\n",
    "delta_table.history().select(\n",
    "    \"version\", \"timestamp\", \"operation\", \"operationMetrics\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Time travel : rÃ©cupÃ©rer une version spÃ©cifique\n",
    "training_v2 = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"versionAsOf\", 2) \\\n",
    "    .load(\"data/training_datasets/churn_model\")\n",
    "\n",
    "# Ou par timestamp\n",
    "training_at_date = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-15 10:00:00\") \\\n",
    "    .load(\"data/training_datasets/churn_model\")\n",
    "```\n",
    "\n",
    "### 4.4 Data Splits\n",
    "\n",
    "```python\n",
    "def create_train_val_test_split(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Split un dataset en train/validation/test de maniÃ¨re reproductible.\n",
    "    \n",
    "    Pour les donnÃ©es temporelles, prÃ©fÃ©rer un split par date !\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 0.001\n",
    "    \n",
    "    # Random split\n",
    "    train_df, val_df, test_df = df.randomSplit(\n",
    "        [train_ratio, val_ratio, test_ratio], \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def create_temporal_split(df, date_col, train_end, val_end):\n",
    "    \"\"\"\n",
    "    Split temporel (recommandÃ© pour Ã©viter le leakage) :\n",
    "    - Train : donnÃ©es avant train_end\n",
    "    - Val : donnÃ©es entre train_end et val_end  \n",
    "    - Test : donnÃ©es aprÃ¨s val_end\n",
    "    \"\"\"\n",
    "    train_df = df.filter(F.col(date_col) < train_end)\n",
    "    val_df = df.filter(\n",
    "        (F.col(date_col) >= train_end) & \n",
    "        (F.col(date_col) < val_end)\n",
    "    )\n",
    "    test_df = df.filter(F.col(date_col) >= val_end)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_validation_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Data Validation pour ML\n",
    "\n",
    "### 5.1 Pourquoi la Data Quality est Critique pour ML\n",
    "\n",
    "```text\n",
    "\"Garbage In, Garbage Out\" â€” mais en pire pour le ML !\n",
    "\n",
    "ProblÃ¨me de data â†’ ModÃ¨le apprend le bruit â†’ PrÃ©dictions fausses en production\n",
    "```\n",
    "\n",
    "| ProblÃ¨me de donnÃ©es | Impact sur le ML |\n",
    "|---------------------|------------------|\n",
    "| Missing values | ModÃ¨le biaisÃ© ou crash |\n",
    "| Outliers extrÃªmes | Poids aberrants |\n",
    "| Data leakage | MÃ©triques sur-optimistes, crash en prod |\n",
    "| Distribution drift | Performance dÃ©gradÃ©e en prod |\n",
    "| Class imbalance non dÃ©tectÃ© | ModÃ¨le prÃ©dit toujours la classe majoritaire |\n",
    "\n",
    "### 5.2 Great Expectations pour ML Data\n",
    "\n",
    "```python\n",
    "import great_expectations as gx\n",
    "\n",
    "# CrÃ©er le contexte\n",
    "context = gx.get_context()\n",
    "\n",
    "# CrÃ©er une expectation suite pour les features ML\n",
    "suite = context.add_expectation_suite(\"ml_features_validation\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ge_expectations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# VALIDATIONS ML avec Great Expectations (conceptuel)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ml_feature_expectations = {\n",
    "    \"expectations\": [\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 1. ComplÃ©tude : pas de valeurs manquantes sur les features critiques\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": \"customer_id\"}\n",
    "        },\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n",
    "            \"kwargs\": {\"column\": \"total_transactions\"}\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 2. Plage de valeurs : dÃ©tecter les outliers\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"total_spent\",\n",
    "                \"min_value\": 0,\n",
    "                \"max_value\": 100000  # Alerter si > 100k\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"recency_days\",\n",
    "                \"min_value\": 0,\n",
    "                \"max_value\": 365\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 3. Distribution : moyennes et Ã©carts-types attendus\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_mean_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"avg_transaction_amount\",\n",
    "                \"min_value\": 50,\n",
    "                \"max_value\": 500\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_stdev_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"total_spent\",\n",
    "                \"min_value\": 10,\n",
    "                \"max_value\": 5000\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 4. CardinalitÃ© : vÃ©rifier les catÃ©gories\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_in_set\",\n",
    "            \"kwargs\": {\n",
    "                \"column\": \"favorite_category\",\n",
    "                \"value_set\": [\"Electronics\", \"Clothing\", \"Food\", \"Other\", None]\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 5. UnicitÃ© : pas de doublons\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_column_values_to_be_unique\",\n",
    "            \"kwargs\": {\"column\": \"customer_id\"}\n",
    "        },\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # 6. Volume : nombre de lignes attendu\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        {\n",
    "            \"expectation_type\": \"expect_table_row_count_to_be_between\",\n",
    "            \"kwargs\": {\n",
    "                \"min_value\": 1000,\n",
    "                \"max_value\": 1000000\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"âœ… Expectations ML dÃ©finies :\")\n",
    "for exp in ml_feature_expectations[\"expectations\"]:\n",
    "    print(f\"  - {exp['expectation_type']} sur {exp['kwargs'].get('column', 'table')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "label_validation",
   "metadata": {},
   "source": [
    "### 5.3 Validations SpÃ©cifiques ML\n",
    "\n",
    "```python\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Validation du label (target)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def validate_classification_labels(df, label_col, expected_classes):\n",
    "    \"\"\"\n",
    "    Valider les labels pour un problÃ¨me de classification.\n",
    "    \"\"\"\n",
    "    # 1. Pas de nulls dans le label\n",
    "    null_count = df.filter(F.col(label_col).isNull()).count()\n",
    "    assert null_count == 0, f\"Found {null_count} null labels!\"\n",
    "    \n",
    "    # 2. Labels dans les classes attendues\n",
    "    actual_classes = set(df.select(label_col).distinct().toPandas()[label_col].tolist())\n",
    "    unexpected = actual_classes - set(expected_classes)\n",
    "    assert len(unexpected) == 0, f\"Unexpected labels: {unexpected}\"\n",
    "    \n",
    "    # 3. VÃ©rifier le class imbalance\n",
    "    class_counts = df.groupBy(label_col).count().toPandas()\n",
    "    min_count = class_counts['count'].min()\n",
    "    max_count = class_counts['count'].max()\n",
    "    imbalance_ratio = max_count / min_count\n",
    "    \n",
    "    if imbalance_ratio > 10:\n",
    "        print(f\"âš ï¸ WARNING: Class imbalance ratio = {imbalance_ratio:.1f}\")\n",
    "        print(f\"   Consider using class weights or resampling.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Validation anti-leakage\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def validate_no_leakage(df, event_date_col, feature_date_col):\n",
    "    \"\"\"\n",
    "    VÃ©rifier qu'aucune feature n'est du futur par rapport Ã  l'event.\n",
    "    \"\"\"\n",
    "    leakage_count = df.filter(\n",
    "        F.col(feature_date_col) >= F.col(event_date_col)\n",
    "    ).count()\n",
    "    \n",
    "    if leakage_count > 0:\n",
    "        raise ValueError(f\"ğŸš¨ DATA LEAKAGE DETECTED! {leakage_count} rows have future features!\")\n",
    "    \n",
    "    print(\"âœ… No data leakage detected\")\n",
    "    return True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_monitoring",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Serving Data Infrastructure\n",
    "\n",
    "### 6.1 Patterns de Serving\n",
    "\n",
    "| Pattern | Latence | Use Case |\n",
    "|---------|---------|----------|\n",
    "| **Batch precompute** | Minutes-Hours | Scoring quotidien, recommendations |\n",
    "| **Online store lookup** | <10ms | Personnalisation temps rÃ©el |\n",
    "| **On-demand compute** | 100ms+ | Features complexes Ã  la demande |\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     SERVING DATA PATTERNS                                   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  BATCH PRECOMPUTE                                                   â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  Features â”€â”€â–¶ Batch Scoring â”€â”€â–¶ Predictions Table â”€â”€â–¶ Application  â”‚  â”‚\n",
    "â”‚   â”‚  (Spark)       (Spark MLlib)     (Delta/Postgres)      (lookup)    â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  â±ï¸ Latency: Minutes/Hours    ğŸ‘ Simple, scalable                   â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  ONLINE STORE LOOKUP                                                â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  Request â”€â”€â–¶ Feature Store â”€â”€â–¶ ML Model â”€â”€â–¶ Response               â”‚  â”‚\n",
    "â”‚   â”‚              (Redis <10ms)     (API)        (real-time)            â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  â±ï¸ Latency: <50ms total      ğŸ‘ Real-time personalization         â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚  ON-DEMAND COMPUTE                                                  â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  Request â”€â”€â–¶ Compute Features â”€â”€â–¶ ML Model â”€â”€â–¶ Response            â”‚  â”‚\n",
    "â”‚   â”‚              (on-the-fly)         (API)        (computed)          â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚  â±ï¸ Latency: 100ms+           ğŸ‘ Always fresh, complex features    â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 6.2 Batch Scoring Pipeline\n",
    "\n",
    "```python\n",
    "# batch_scoring_dag.py (Airflow)\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n",
    "\n",
    "with DAG('batch_scoring', schedule_interval='@daily') as dag:\n",
    "    \n",
    "    # 1. RÃ©cupÃ©rer les features du jour\n",
    "    prepare_features = SparkSubmitOperator(\n",
    "        task_id='prepare_scoring_features',\n",
    "        application='/jobs/prepare_features.py',\n",
    "    )\n",
    "    \n",
    "    # 2. Scorer avec le modÃ¨le\n",
    "    score = SparkSubmitOperator(\n",
    "        task_id='batch_score',\n",
    "        application='/jobs/batch_score.py',\n",
    "        application_args=['--model-uri', 'models:/churn_model/Production'],\n",
    "    )\n",
    "    \n",
    "    # 3. Ã‰crire les prÃ©dictions\n",
    "    write_predictions = SparkSubmitOperator(\n",
    "        task_id='write_predictions',\n",
    "        application='/jobs/write_predictions.py',\n",
    "    )\n",
    "    \n",
    "    prepare_features >> score >> write_predictions\n",
    "```\n",
    "\n",
    "### 6.3 Online Store avec Redis\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import json\n",
    "\n",
    "# Connexion Redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Ã‰criture : MatÃ©rialisation des features\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def materialize_to_redis(features_df):\n",
    "    \"\"\"Ã‰crire les features dans Redis pour serving temps rÃ©el.\"\"\"\n",
    "    \n",
    "    for row in features_df.collect():\n",
    "        customer_id = row['customer_id']\n",
    "        features = {\n",
    "            'total_transactions': row['total_transactions'],\n",
    "            'total_spent': row['total_spent'],\n",
    "            'recency_days': row['recency_days'],\n",
    "            'is_active_30d': row['is_active_30d'],\n",
    "        }\n",
    "        \n",
    "        # ClÃ© : customer_features:{customer_id}\n",
    "        r.hset(f\"customer_features:{customer_id}\", mapping=features)\n",
    "        \n",
    "        # TTL : 24 heures\n",
    "        r.expire(f\"customer_features:{customer_id}\", 86400)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Lecture : Serving temps rÃ©el\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def get_features_for_scoring(customer_id):\n",
    "    \"\"\"RÃ©cupÃ©rer les features pour le scoring temps rÃ©el.\"\"\"\n",
    "    \n",
    "    features = r.hgetall(f\"customer_features:{customer_id}\")\n",
    "    \n",
    "    if not features:\n",
    "        return None\n",
    "    \n",
    "    # Convertir bytes â†’ types Python\n",
    "    return {\n",
    "        k.decode(): float(v.decode())\n",
    "        for k, v in features.items()\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "# features = get_features_for_scoring(\"C001\")\n",
    "# prediction = model.predict([features])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Data Monitoring pour ML\n",
    "\n",
    "### 7.1 Types de Drift\n",
    "\n",
    "| Type | Description | Ce que le DE monitore |\n",
    "|------|-------------|----------------------|\n",
    "| **Data Drift** | Distribution des inputs change | âœ… Features distributions |\n",
    "| **Concept Drift** | Relation inputâ†’output change | âš ï¸ Alerter le DS |\n",
    "| **Prediction Drift** | Distribution des outputs change | âš ï¸ Alerter le DS |\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         DATA DRIFT DETECTION                                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Training Data Distribution          Production Data Distribution          â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   amount:                             amount:                               â”‚\n",
    "â”‚   mean = 150                          mean = 280  â† DRIFT!                 â”‚\n",
    "â”‚   std = 50                            std = 120   â† DRIFT!                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚        â–²                                    â–²                               â”‚\n",
    "â”‚       â•±â•²                                  â•±    â•²                            â”‚\n",
    "â”‚      â•±  â•²                               â•±      â•²                            â”‚\n",
    "â”‚     â•±    â•²                            â•±         â•²                           â”‚\n",
    "â”‚   â”€â•±â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â–¶                    â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â–¶                       â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Si drift dÃ©tectÃ© â†’ Alerter â†’ Potentiellement retrainer le modÃ¨le         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 7.2 Evidently AI pour Data Monitoring\n",
    "\n",
    "```python\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import (\n",
    "    DataDriftTable,\n",
    "    DatasetDriftMetric,\n",
    "    ColumnDriftMetric,\n",
    ")\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.tests import (\n",
    "    TestColumnDrift,\n",
    "    TestShareOfDriftedColumns,\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RAPPORT DE DRIFT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# reference_data = donnÃ©es de training\n",
    "# current_data = donnÃ©es de production rÃ©centes\n",
    "\n",
    "column_mapping = ColumnMapping(\n",
    "    numerical_features=['total_transactions', 'total_spent', 'avg_transaction_amount'],\n",
    "    categorical_features=['favorite_category'],\n",
    ")\n",
    "\n",
    "# CrÃ©er le rapport de drift\n",
    "drift_report = Report(metrics=[\n",
    "    DatasetDriftMetric(),\n",
    "    DataDriftTable(),\n",
    "])\n",
    "\n",
    "drift_report.run(\n",
    "    reference_data=reference_df,\n",
    "    current_data=current_df,\n",
    "    column_mapping=column_mapping\n",
    ")\n",
    "\n",
    "# Sauvegarder en HTML\n",
    "drift_report.save_html(\"reports/drift_report.html\")\n",
    "\n",
    "# Ou obtenir les rÃ©sultats en dict\n",
    "results = drift_report.as_dict()\n",
    "dataset_drift = results['metrics'][0]['result']['dataset_drift']\n",
    "print(f\"Dataset drift detected: {dataset_drift}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# TESTS AUTOMATISÃ‰S (pour CI/CD)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "drift_tests = TestSuite(tests=[\n",
    "    TestShareOfDriftedColumns(lt=0.3),  # Moins de 30% de colonnes en drift\n",
    "    TestColumnDrift(column_name='total_spent'),\n",
    "    TestColumnDrift(column_name='recency_days'),\n",
    "])\n",
    "\n",
    "drift_tests.run(\n",
    "    reference_data=reference_df,\n",
    "    current_data=current_df,\n",
    "    column_mapping=column_mapping\n",
    ")\n",
    "\n",
    "# VÃ©rifier si les tests passent\n",
    "test_results = drift_tests.as_dict()\n",
    "all_passed = all(t['status'] == 'SUCCESS' for t in test_results['tests'])\n",
    "\n",
    "if not all_passed:\n",
    "    print(\"ğŸš¨ DRIFT ALERT: Some tests failed!\")\n",
    "    # Envoyer une alerte (Slack, PagerDuty, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitoring_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MONITORING PIPELINE SIMPLIFIÃ‰ (sans Evidently)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compute_feature_stats(df, feature_cols):\n",
    "    \"\"\"\n",
    "    Calculer les statistiques de base pour le monitoring.\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        col_stats = df.select(\n",
    "            F.mean(col).alias(\"mean\"),\n",
    "            F.stddev(col).alias(\"std\"),\n",
    "            F.min(col).alias(\"min\"),\n",
    "            F.max(col).alias(\"max\"),\n",
    "            F.expr(f\"percentile_approx({col}, 0.5)\").alias(\"median\"),\n",
    "            (F.count(F.when(F.col(col).isNull(), 1)) / F.count(\"*\")).alias(\"null_rate\"),\n",
    "        ).collect()[0]\n",
    "        \n",
    "        stats[col] = {\n",
    "            \"mean\": col_stats[\"mean\"],\n",
    "            \"std\": col_stats[\"std\"],\n",
    "            \"min\": col_stats[\"min\"],\n",
    "            \"max\": col_stats[\"max\"],\n",
    "            \"median\": col_stats[\"median\"],\n",
    "            \"null_rate\": col_stats[\"null_rate\"],\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def detect_drift(reference_stats, current_stats, threshold=0.2):\n",
    "    \"\"\"\n",
    "    DÃ©tecter le drift en comparant les statistiques.\n",
    "    \n",
    "    MÃ©thode simple : alerte si la moyenne change de plus de threshold%.\n",
    "    \"\"\"\n",
    "    alerts = []\n",
    "    \n",
    "    for col in reference_stats:\n",
    "        ref_mean = reference_stats[col][\"mean\"]\n",
    "        cur_mean = current_stats[col][\"mean\"]\n",
    "        \n",
    "        if ref_mean != 0:\n",
    "            pct_change = abs(cur_mean - ref_mean) / abs(ref_mean)\n",
    "            \n",
    "            if pct_change > threshold:\n",
    "                alerts.append({\n",
    "                    \"column\": col,\n",
    "                    \"reference_mean\": ref_mean,\n",
    "                    \"current_mean\": cur_mean,\n",
    "                    \"pct_change\": pct_change * 100,\n",
    "                })\n",
    "    \n",
    "    return alerts\n",
    "\n",
    "# Exemple d'utilisation\n",
    "feature_cols = [\"total_transactions\", \"total_spent\", \"avg_transaction_amount\"]\n",
    "\n",
    "# Simuler des donnÃ©es de rÃ©fÃ©rence et actuelles\n",
    "reference_stats = compute_feature_stats(customer_features, feature_cols)\n",
    "\n",
    "# Simuler un drift (en production, ce serait les nouvelles donnÃ©es)\n",
    "drifted_data = customer_features.withColumn(\n",
    "    \"total_spent\", F.col(\"total_spent\") * 1.5  # +50% drift\n",
    ")\n",
    "current_stats = compute_feature_stats(drifted_data, feature_cols)\n",
    "\n",
    "# DÃ©tecter le drift\n",
    "alerts = detect_drift(reference_stats, current_stats, threshold=0.2)\n",
    "\n",
    "print(\"ğŸ“Š Monitoring Results:\")\n",
    "if alerts:\n",
    "    print(\"ğŸš¨ DRIFT DETECTED:\")\n",
    "    for alert in alerts:\n",
    "        print(f\"   - {alert['column']}: {alert['pct_change']:.1f}% change\")\n",
    "        print(f\"     (reference: {alert['reference_mean']:.2f} â†’ current: {alert['current_mean']:.2f})\")\n",
    "else:\n",
    "    print(\"âœ… No significant drift detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. MLflow pour Data Engineers\n",
    "\n",
    "En tant que Data Engineer, tu n'as pas besoin de tout connaÃ®tre de MLflow. Voici ce qui te concerne.\n",
    "\n",
    "### 8.1 Ce qu'un DE doit savoir\n",
    "\n",
    "| Composant MLflow | ResponsabilitÃ© DE | ResponsabilitÃ© DS/MLE |\n",
    "|------------------|-------------------|----------------------|\n",
    "| **Tracking** | Logger les datasets utilisÃ©s | Logger les mÃ©triques, paramÃ¨tres |\n",
    "| **Projects** | â€” | Packager le code |\n",
    "| **Models** | â€” | Sauvegarder les modÃ¨les |\n",
    "| **Registry** | Savoir quel modÃ¨le est en Production | Promouvoir les modÃ¨les |\n",
    "\n",
    "### 8.2 Logger les Datasets avec MLflow\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Dans ton pipeline de feature engineering\n",
    "def log_dataset_to_mlflow(df, dataset_name, run_id=None):\n",
    "    \"\"\"\n",
    "    Logger les mÃ©tadonnÃ©es du dataset pour traÃ§abilitÃ©.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        # Logger les infos du dataset\n",
    "        mlflow.log_param(f\"{dataset_name}_rows\", df.count())\n",
    "        mlflow.log_param(f\"{dataset_name}_cols\", len(df.columns))\n",
    "        mlflow.log_param(f\"{dataset_name}_columns\", \",\".join(df.columns))\n",
    "        \n",
    "        # Logger les statistiques\n",
    "        stats = df.describe().toPandas()\n",
    "        stats.to_csv(f\"/tmp/{dataset_name}_stats.csv\")\n",
    "        mlflow.log_artifact(f\"/tmp/{dataset_name}_stats.csv\")\n",
    "        \n",
    "        # Logger le chemin du dataset\n",
    "        mlflow.log_param(f\"{dataset_name}_path\", f\"s3://data/features/{dataset_name}\")\n",
    "```\n",
    "\n",
    "### 8.3 RÃ©cupÃ©rer le modÃ¨le en Production\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "\n",
    "# Pour le batch scoring, rÃ©cupÃ©rer le modÃ¨le en Production\n",
    "model_name = \"churn_model\"\n",
    "\n",
    "# MÃ©thode 1 : par stage\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n",
    "\n",
    "# MÃ©thode 2 : par version\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/3\")\n",
    "\n",
    "# Scorer\n",
    "predictions = model.predict(features_df.toPandas())\n",
    "```\n",
    "\n",
    "### 8.4 MLflow avec Spark\n",
    "\n",
    "```python\n",
    "import mlflow.spark\n",
    "\n",
    "# Logger un modÃ¨le Spark MLlib\n",
    "mlflow.spark.log_model(spark_model, \"model\")\n",
    "\n",
    "# Charger pour batch scoring\n",
    "loaded_model = mlflow.spark.load_model(\"models:/my_spark_model/Production\")\n",
    "\n",
    "# Scorer directement sur un DataFrame Spark\n",
    "predictions_df = loaded_model.transform(features_df)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Feature Pipeline avec Window Functions\n",
    "\n",
    "CrÃ©er un pipeline de features pour un modÃ¨le de dÃ©tection de fraude avec :\n",
    "- Montant moyen des 5 derniÃ¨res transactions\n",
    "- Nombre de transactions dans l'heure prÃ©cÃ©dente\n",
    "- Ã‰cart par rapport au montant moyen habituel\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 2 : Setup Feast Local\n",
    "\n",
    "1. Installer Feast\n",
    "2. CrÃ©er un projet avec les features customer\n",
    "3. MatÃ©rialiser les features\n",
    "4. RÃ©cupÃ©rer des features historiques\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 3 : Training Dataset Point-in-Time\n",
    "\n",
    "CrÃ©er un training dataset pour prÃ©dire le churn avec :\n",
    "- Events : clients qui ont churnÃ© (label=1) ou non (label=0)\n",
    "- Features : rÃ©cupÃ©rÃ©es 7 jours AVANT l'event\n",
    "- Validation : vÃ©rifier qu'il n'y a pas de leakage\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 4 : Data Validation Pipeline\n",
    "\n",
    "ImplÃ©menter un pipeline de validation avec :\n",
    "- VÃ©rification des nulls\n",
    "- VÃ©rification des plages de valeurs\n",
    "- DÃ©tection d'outliers\n",
    "- IntÃ©gration Airflow\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 5 : Data Drift Detection\n",
    "\n",
    "1. CrÃ©er un dataset de rÃ©fÃ©rence\n",
    "2. Simuler un drift sur certaines features\n",
    "3. ImplÃ©menter la dÃ©tection automatique\n",
    "4. GÃ©nÃ©rer une alerte si drift > 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Mini-Projet : ML Data Platform\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Construire une plateforme data complÃ¨te pour alimenter un modÃ¨le de prÃ©diction de churn.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      MINI-PROJET : ML DATA PLATFORM                         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Raw Data (CSV)                                                            â”‚\n",
    "â”‚        â”‚                                                                    â”‚\n",
    "â”‚        â–¼                                                                    â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ Data Ingestion  â”‚  Bronze Layer (Delta)                                 â”‚\n",
    "â”‚   â”‚   (Spark)       â”‚                                                       â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ Data Validation â”‚  Great Expectations                                   â”‚\n",
    "â”‚   â”‚                 â”‚                                                       â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ Feature Pipelineâ”‚  Silver Layer (Features)                              â”‚\n",
    "â”‚   â”‚   (Spark)       â”‚                                                       â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\n",
    "â”‚   â”‚  Feature Store  â”‚â”€â”€â”€â”€â–¶â”‚ Training Datasetâ”‚  Point-in-time correct       â”‚\n",
    "â”‚   â”‚    (Feast)      â”‚     â”‚   Generator     â”‚                               â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\n",
    "â”‚            â”‚                                                                â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ Data Monitoring â”‚  Drift detection                                      â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Livrables\n",
    "\n",
    "1. **Data Ingestion** : Script Spark pour ingÃ©rer les CSV â†’ Delta\n",
    "2. **Validation** : Suite Great Expectations pour les donnÃ©es brutes\n",
    "3. **Feature Pipeline** : Job Spark complet avec toutes les features\n",
    "4. **Feature Store** : Configuration Feast + matÃ©rialisation\n",
    "5. **Training Dataset** : GÃ©nÃ©rateur avec point-in-time join\n",
    "6. **Monitoring** : Script de dÃ©tection de drift\n",
    "7. **Orchestration** : DAG Airflow qui orchestre le tout\n",
    "\n",
    "### DonnÃ©es\n",
    "\n",
    "Utiliser les transactions crÃ©Ã©es dans ce notebook + gÃ©nÃ©rer des events de churn.\n",
    "\n",
    "### CritÃ¨res de succÃ¨s\n",
    "\n",
    "- [ ] Pipeline end-to-end fonctionnel\n",
    "- [ ] Pas de data leakage dans le training dataset\n",
    "- [ ] Validation automatique des donnÃ©es\n",
    "- [ ] Drift detection opÃ©rationnel\n",
    "- [ ] Documentation claire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "### Documentation\n",
    "- [Feast Documentation](https://docs.feast.dev/) â€” Feature Store\n",
    "- [Great Expectations](https://docs.greatexpectations.io/) â€” Data Validation\n",
    "- [Evidently AI](https://docs.evidentlyai.com/) â€” ML Monitoring\n",
    "- [MLflow](https://mlflow.org/docs/latest/index.html) â€” ML Lifecycle\n",
    "\n",
    "### Articles\n",
    "- [Google's ML Technical Debt Paper](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)\n",
    "- [Feature Store: The Missing Data Layer](https://www.tecton.ai/blog/what-is-a-feature-store/)\n",
    "- [MLOps Maturity Model](https://docs.microsoft.com/en-us/azure/architecture/example-scenario/mlops/mlops-maturity-model)\n",
    "\n",
    "### Outils\n",
    "- [Feast](https://feast.dev/) â€” Feature Store open-source\n",
    "- [Tecton](https://www.tecton.ai/) â€” Feature Store managed\n",
    "- [DVC](https://dvc.org/) â€” Data versioning\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `32_data_mesh_contracts`** â€” Data Mesh & Data Contracts\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu maÃ®trises maintenant l'infrastructure data pour le ML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
