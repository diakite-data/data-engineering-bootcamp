{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers\n",
    "\n",
    "Bienvenue dans ce module avancÃ© oÃ¹ tu vas **plonger dans les entrailles de Kubernetes**. Tu dÃ©couvriras comment fonctionne rÃ©ellement un cluster K8s, comment packager tes dÃ©ploiements avec Helm, automatiser le dÃ©ploiement continu avec ArgoCD, et mettre en place un monitoring professionnel avec Prometheus et Grafana â€” des compÃ©tences indispensables pour un Data Engineer Senior !\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Avoir suivi les modules `14_docker_for_data_engineers` et `15_kubernetes_fundamentals` |\n",
    "| âœ… Requis | Avoir suivi le module `16_k8s_for_data_workloads` |\n",
    "| âœ… Requis | MaÃ®triser kubectl, Pods, Deployments, Services, ConfigMaps, Secrets |\n",
    "| âœ… Requis | Connaissances solides en YAML |\n",
    "| ğŸ’¡ RecommandÃ© | Un cluster K8s accessible (Minikube, kind, ou cloud) |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Comprendre l'architecture interne de Kubernetes (etcd, Scheduler, Controllers)\n",
    "- Sauvegarder et restaurer etcd\n",
    "- Configurer le scheduling avancÃ© (affinity, taints, tolerations, priority classes)\n",
    "- Packager et dÃ©ployer des applications avec Helm\n",
    "- Mettre en place un workflow GitOps avec ArgoCD\n",
    "- DÃ©ployer et configurer Prometheus et Grafana pour monitorer tes pipelines data\n",
    "- CrÃ©er des dashboards et des alertes personnalisÃ©es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rappel : Ce qu'on a vu vs Ce qu'on va approfondir\n",
    "\n",
    "| Module | Ce qu'on a couvert | Ce module approfondit |\n",
    "|--------|-------------------|----------------------|\n",
    "| **M15 - Fundamentals** | Architecture simplifiÃ©e, Pods, Deployments, Services | Architecture interne complÃ¨te, etcd, Scheduler |\n",
    "| **M16 - Data Workloads** | Spark/Airflow sur K8s, HPA, Helm basics | Helm avancÃ©, GitOps, Monitoring pro |\n",
    "\n",
    "### SchÃ©ma : Du Fundamentals au Deep Dive\n",
    "\n",
    "```text\n",
    "M15 Fundamentals          M16 Data Workloads         M27 Deep Dive (ce module)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ â€¢ Pods          â”‚       â”‚ â€¢ Spark on K8s  â”‚       â”‚ â€¢ etcd internals        â”‚\n",
    "â”‚ â€¢ Deployments   â”‚  â”€â”€â”€â–¶ â”‚ â€¢ Airflow on K8sâ”‚  â”€â”€â”€â–¶ â”‚ â€¢ Scheduler avancÃ©      â”‚\n",
    "â”‚ â€¢ Services      â”‚       â”‚ â€¢ HPA           â”‚       â”‚ â€¢ Helm charts custom    â”‚\n",
    "â”‚ â€¢ kubectl       â”‚       â”‚ â€¢ Helm intro    â”‚       â”‚ â€¢ ArgoCD / GitOps       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â€¢ Prometheus / Grafana  â”‚\n",
    "                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Ce module est orientÃ© \"comprendre et opÃ©rer\"** â€” tu vas apprendre ce qui se passe sous le capot pour mieux diagnostiquer, optimiser et industrialiser tes dÃ©ploiements data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k8s_info",
   "metadata": {},
   "source": [
    "> â„¹ï¸ **Le savais-tu ?**\n",
    ">\n",
    "> Google exÃ©cute **des milliards de containers par semaine** sur son systÃ¨me interne Borg, l'ancÃªtre de Kubernetes.\n",
    ">\n",
    "> Le cluster Kubernetes le plus grand documentÃ© publiquement compte plus de **15 000 nodes** et gÃ¨re des centaines de milliers de pods simultanÃ©ment.\n",
    ">\n",
    "> etcd, le cerveau de Kubernetes, utilise l'algorithme de consensus **Raft** â€” le mÃªme algorithme utilisÃ© par des systÃ¨mes critiques comme CockroachDB et Consul.\n",
    ">\n",
    "> ğŸ“– [Borg: The Predecessor to Kubernetes](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture_interne",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Architecture Interne de Kubernetes\n",
    "\n",
    "Dans M15, on a vu l'architecture simplifiÃ©e. Maintenant, plongeons dans **tous les composants** du Control Plane.\n",
    "\n",
    "### Vue complÃ¨te du Control Plane\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                            CONTROL PLANE                                    â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚             â”‚    â”‚                     â”‚    â”‚                      â”‚    â”‚\n",
    "â”‚  â”‚    etcd     â”‚â—€â”€â”€â–¶â”‚    API Server       â”‚â—€â”€â”€â–¶â”‚  Controller Manager  â”‚    â”‚\n",
    "â”‚  â”‚  (database) â”‚    â”‚  (point d'entrÃ©e)   â”‚    â”‚  (boucles de contrÃ´le)â”‚   â”‚\n",
    "â”‚  â”‚             â”‚    â”‚                     â”‚    â”‚                      â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â”‚                                â”‚                                            â”‚\n",
    "â”‚                                â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n",
    "â”‚                                â”‚              â”‚                      â”‚      â”‚\n",
    "â”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Scheduler        â”‚      â”‚\n",
    "â”‚                                               â”‚  (placement pods)    â”‚      â”‚\n",
    "â”‚                                               â”‚                      â”‚      â”‚\n",
    "â”‚                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚                    Admission Controllers                             â”‚   â”‚\n",
    "â”‚  â”‚  (validation, mutation, policies)                                    â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                    â”‚\n",
    "                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                            WORKER NODES                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "â”‚  â”‚  Node 1         â”‚  â”‚  Node 2         â”‚  â”‚  Node 3         â”‚             â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚\n",
    "â”‚  â”‚  â”‚  kubelet  â”‚  â”‚  â”‚  â”‚  kubelet  â”‚  â”‚  â”‚  â”‚  kubelet  â”‚  â”‚             â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚\n",
    "â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚\n",
    "â”‚  â”‚  â”‚kube-proxy â”‚  â”‚  â”‚  â”‚kube-proxy â”‚  â”‚  â”‚  â”‚kube-proxy â”‚  â”‚             â”‚\n",
    "â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Composants du Control Plane\n",
    "\n",
    "| Composant | RÃ´le | Analogie |\n",
    "|-----------|------|----------|\n",
    "| **etcd** | Base de donnÃ©es clÃ©-valeur distribuÃ©e, stocke tout l'Ã©tat du cluster | Le **disque dur** du cerveau |\n",
    "| **API Server** | Point d'entrÃ©e unique, valide et persiste les objets | La **rÃ©ception** de l'entreprise |\n",
    "| **Controller Manager** | ExÃ©cute les boucles de contrÃ´le (Deployment, ReplicaSet...) | Les **managers** qui vÃ©rifient que tout tourne |\n",
    "| **Scheduler** | Assigne les pods aux nodes | Le **RH** qui place les employÃ©s |\n",
    "| **Admission Controllers** | Interceptent les requÃªtes pour valider/muter | Les **vigiles** Ã  l'entrÃ©e |\n",
    "\n",
    "### Flux d'une requÃªte kubectl\n",
    "\n",
    "Quand tu fais `kubectl apply -f deployment.yaml`, voici ce qui se passe :\n",
    "\n",
    "```text\n",
    "kubectl apply â”€â”€â”€â–¶ API Server â”€â”€â”€â–¶ Admission Controllers â”€â”€â”€â–¶ etcd (persist)\n",
    "                                                                    â”‚\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â–¼\n",
    "              Controller Manager (voit le nouveau Deployment)\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "              CrÃ©e un ReplicaSet â”€â”€â”€â–¶ API Server â”€â”€â”€â–¶ etcd\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "              ReplicaSet Controller crÃ©e des Pods â”€â”€â”€â–¶ etcd\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "              Scheduler assigne les Pods aux Nodes â”€â”€â”€â–¶ etcd\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "              kubelet (sur chaque node) dÃ©marre les containers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "etcd_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. etcd Deep Dive\n",
    "\n",
    "> **etcd** est une base de donnÃ©es clÃ©-valeur distribuÃ©e, fortement consistante, qui stocke **tout l'Ã©tat** de ton cluster Kubernetes.\n",
    "\n",
    "### Pourquoi etcd est critique\n",
    "\n",
    "| Aspect | Impact |\n",
    "|--------|--------|\n",
    "| **Tout est dedans** | Pods, Services, Secrets, ConfigMaps, Ã©tat des Deployments... |\n",
    "| **Single source of truth** | Si etcd meurt sans backup = cluster perdu |\n",
    "| **Performance** | Latence etcd = latence de tout le cluster |\n",
    "\n",
    "### Algorithme de consensus Raft\n",
    "\n",
    "etcd utilise **Raft** pour garantir la consistance entre plusieurs instances :\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CLUSTER etcd (3 nodes)                   â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚   â”‚ etcd-1  â”‚       â”‚ etcd-2  â”‚       â”‚ etcd-3  â”‚          â”‚\n",
    "â”‚   â”‚ LEADER  â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚FOLLOWER â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚FOLLOWER â”‚          â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚        â”‚                                                    â”‚\n",
    "â”‚        â”‚  Toutes les Ã©critures passent par le Leader       â”‚\n",
    "â”‚        â”‚  puis sont rÃ©pliquÃ©es aux Followers                â”‚\n",
    "â”‚        â–¼                                                    â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
    "â”‚   â”‚ Quorum = majoritÃ© (2/3 nodes OK = OK)   â”‚              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### RÃ¨gles de dimensionnement etcd\n",
    "\n",
    "| Nombre de nodes | TolÃ©rance aux pannes | Recommandation |\n",
    "|-----------------|---------------------|----------------|\n",
    "| 1 | 0 (aucune) | Dev/test uniquement |\n",
    "| 3 | 1 node | Production standard |\n",
    "| 5 | 2 nodes | Production critique |\n",
    "| 7 | 3 nodes | TrÃ¨s rare, overhead important |\n",
    "\n",
    "> âš ï¸ **Toujours un nombre impair** pour Ã©viter le split-brain !\n",
    "\n",
    "### Commandes etcd essentielles\n",
    "\n",
    "```bash\n",
    "# VÃ©rifier la santÃ© du cluster etcd\n",
    "etcdctl endpoint health --cluster\n",
    "\n",
    "# Lister les membres du cluster\n",
    "etcdctl member list\n",
    "\n",
    "# Voir le leader actuel\n",
    "etcdctl endpoint status --cluster -w table\n",
    "\n",
    "# Obtenir une clÃ© (exemple: voir les namespaces)\n",
    "etcdctl get /registry/namespaces --prefix --keys-only\n",
    "```\n",
    "\n",
    "### Backup et Restore etcd\n",
    "\n",
    "**C'est LA compÃ©tence critique** pour un cluster de production.\n",
    "\n",
    "```bash\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# BACKUP etcd\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Variables (adapter selon ton cluster)\n",
    "ETCD_ENDPOINTS=\"https://127.0.0.1:2379\"\n",
    "ETCD_CACERT=\"/etc/kubernetes/pki/etcd/ca.crt\"\n",
    "ETCD_CERT=\"/etc/kubernetes/pki/etcd/server.crt\"\n",
    "ETCD_KEY=\"/etc/kubernetes/pki/etcd/server.key\"\n",
    "\n",
    "# CrÃ©er un snapshot\n",
    "etcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d).db \\\n",
    "  --endpoints=$ETCD_ENDPOINTS \\\n",
    "  --cacert=$ETCD_CACERT \\\n",
    "  --cert=$ETCD_CERT \\\n",
    "  --key=$ETCD_KEY\n",
    "\n",
    "# VÃ©rifier le snapshot\n",
    "etcdctl snapshot status /backup/etcd-snapshot-20240115.db -w table\n",
    "```\n",
    "\n",
    "```bash\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# RESTORE etcd (âš ï¸ OpÃ©ration critique !)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# 1. ArrÃªter le kubelet et etcd\n",
    "systemctl stop kubelet\n",
    "systemctl stop etcd\n",
    "\n",
    "# 2. Sauvegarder l'ancien data-dir\n",
    "mv /var/lib/etcd /var/lib/etcd.backup\n",
    "\n",
    "# 3. Restaurer depuis le snapshot\n",
    "etcdctl snapshot restore /backup/etcd-snapshot-20240115.db \\\n",
    "  --data-dir=/var/lib/etcd \\\n",
    "  --name=master-1 \\\n",
    "  --initial-cluster=master-1=https://192.168.1.10:2380 \\\n",
    "  --initial-advertise-peer-urls=https://192.168.1.10:2380\n",
    "\n",
    "# 4. RedÃ©marrer les services\n",
    "systemctl start etcd\n",
    "systemctl start kubelet\n",
    "\n",
    "# 5. VÃ©rifier\n",
    "kubectl get nodes\n",
    "kubectl get pods -A\n",
    "```\n",
    "\n",
    "### Bonnes pratiques etcd\n",
    "\n",
    "| Pratique | Pourquoi |\n",
    "|----------|----------|\n",
    "| Backups automatiques (toutes les heures) | Limiter la perte de donnÃ©es |\n",
    "| Stocker les backups hors du cluster | Si le cluster meurt, les backups survivent |\n",
    "| Tester les restore rÃ©guliÃ¨rement | Un backup non testÃ© = pas de backup |\n",
    "| Monitorer les mÃ©triques etcd | Latence, espace disque, leader elections |\n",
    "| SSD obligatoire | etcd est trÃ¨s sensible aux I/O disque |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduler_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Scheduler AvancÃ©\n",
    "\n",
    "Le **kube-scheduler** dÃ©cide sur quel node placer chaque pod. En niveau avancÃ©, tu dois savoir **influencer ces dÃ©cisions**.\n",
    "\n",
    "### Comment le Scheduler fonctionne\n",
    "\n",
    "```text\n",
    "Pod Ã  scheduler\n",
    "      â”‚\n",
    "      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              PHASE 1 : FILTERING                â”‚\n",
    "â”‚  Ã‰liminer les nodes qui ne conviennent pas      â”‚\n",
    "â”‚  â€¢ Ressources insuffisantes (CPU, RAM)          â”‚\n",
    "â”‚  â€¢ Taints non tolÃ©rÃ©es                          â”‚\n",
    "â”‚  â€¢ NodeSelector non matchÃ©                      â”‚\n",
    "â”‚  â€¢ Affinity rules non respectÃ©es                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "      â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚              PHASE 2 : SCORING                  â”‚\n",
    "â”‚  Classer les nodes restants par score           â”‚\n",
    "â”‚  â€¢ Spread (rÃ©partir les pods)                   â”‚\n",
    "â”‚  â€¢ Resource balancing                           â”‚\n",
    "â”‚  â€¢ Preferred affinity                           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "      â”‚\n",
    "      â–¼\n",
    "Node avec le meilleur score sÃ©lectionnÃ©\n",
    "```\n",
    "\n",
    "### Node Selector (basique)\n",
    "\n",
    "Le plus simple : matcher un label sur le node.\n",
    "\n",
    "```yaml\n",
    "# Labelliser un node\n",
    "# kubectl label nodes worker-1 disk=ssd\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: spark-driver\n",
    "spec:\n",
    "  nodeSelector:\n",
    "    disk: ssd          # Ce pod ira uniquement sur les nodes avec disk=ssd\n",
    "  containers:\n",
    "  - name: spark\n",
    "    image: apache/spark:3.5.0\n",
    "```\n",
    "\n",
    "### Node Affinity (avancÃ©)\n",
    "\n",
    "Plus de contrÃ´le que nodeSelector : rÃ¨gles required vs preferred.\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: spark-executor\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      # OBLIGATOIRE : le pod ne sera PAS schedulÃ© si non respectÃ©\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: node-type\n",
    "            operator: In\n",
    "            values:\n",
    "            - compute\n",
    "            - highmem\n",
    "      \n",
    "      # PRÃ‰FÃ‰RÃ‰ : le scheduler essaie, mais schedule quand mÃªme si impossible\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 80    # Poids de 1 Ã  100\n",
    "        preference:\n",
    "          matchExpressions:\n",
    "          - key: zone\n",
    "            operator: In\n",
    "            values:\n",
    "            - eu-west-1a\n",
    "  containers:\n",
    "  - name: spark\n",
    "    image: apache/spark:3.5.0\n",
    "```\n",
    "\n",
    "### Pod Affinity / Anti-Affinity\n",
    "\n",
    "Placer des pods **ensemble** ou **sÃ©parÃ©ment**.\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: spark-executors\n",
    "spec:\n",
    "  replicas: 5\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: spark-executor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: spark-executor\n",
    "    spec:\n",
    "      affinity:\n",
    "        # Pod Affinity : placer AVEC les autres spark-executors\n",
    "        podAffinity:\n",
    "          preferredDuringSchedulingIgnoredDuringExecution:\n",
    "          - weight: 100\n",
    "            podAffinityTerm:\n",
    "              labelSelector:\n",
    "                matchLabels:\n",
    "                  app: spark-driver    # Proche du driver\n",
    "              topologyKey: kubernetes.io/hostname\n",
    "        \n",
    "        # Pod Anti-Affinity : NE PAS placer sur le mÃªme node qu'un autre executor\n",
    "        podAntiAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "          - labelSelector:\n",
    "              matchLabels:\n",
    "                app: spark-executor\n",
    "            topologyKey: kubernetes.io/hostname\n",
    "      containers:\n",
    "      - name: executor\n",
    "        image: apache/spark:3.5.0\n",
    "```\n",
    "\n",
    "### Taints et Tolerations\n",
    "\n",
    "**Taints** = \"je repousse certains pods\" (sur le node)\n",
    "**Tolerations** = \"je tolÃ¨re cette taint\" (sur le pod)\n",
    "\n",
    "```bash\n",
    "# Ajouter une taint Ã  un node\n",
    "kubectl taint nodes gpu-node-1 gpu=true:NoSchedule\n",
    "\n",
    "# Effets possibles :\n",
    "# - NoSchedule : nouveaux pods sans toleration rejetÃ©s\n",
    "# - PreferNoSchedule : Ã©viter si possible\n",
    "# - NoExecute : pods existants sans toleration Ã©vincÃ©s\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# Pod qui tolÃ¨re la taint GPU\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: ml-training\n",
    "spec:\n",
    "  tolerations:\n",
    "  - key: \"gpu\"\n",
    "    operator: \"Equal\"\n",
    "    value: \"true\"\n",
    "    effect: \"NoSchedule\"\n",
    "  containers:\n",
    "  - name: training\n",
    "    image: pytorch/pytorch:2.0.0-cuda11.7\n",
    "    resources:\n",
    "      limits:\n",
    "        nvidia.com/gpu: 1\n",
    "```\n",
    "\n",
    "### Priority Classes\n",
    "\n",
    "DÃ©finir des **prioritÃ©s** entre pods â€” les plus prioritaires peuvent prÃ©empter les autres.\n",
    "\n",
    "```yaml\n",
    "# CrÃ©er une PriorityClass\n",
    "apiVersion: scheduling.k8s.io/v1\n",
    "kind: PriorityClass\n",
    "metadata:\n",
    "  name: data-pipeline-critical\n",
    "value: 1000000           # Plus c'est haut, plus c'est prioritaire\n",
    "globalDefault: false\n",
    "description: \"Pour les pipelines data critiques\"\n",
    "preemptionPolicy: PreemptLowerPriority  # Peut Ã©vincer des pods moins prioritaires\n",
    "---\n",
    "apiVersion: scheduling.k8s.io/v1\n",
    "kind: PriorityClass\n",
    "metadata:\n",
    "  name: data-pipeline-batch\n",
    "value: 100000\n",
    "globalDefault: false\n",
    "description: \"Pour les jobs batch non critiques\"\n",
    "preemptionPolicy: Never   # Ne prÃ©empte jamais\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# Utiliser la PriorityClass\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: etl-critical\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      priorityClassName: data-pipeline-critical\n",
    "      containers:\n",
    "      - name: etl\n",
    "        image: my-etl:1.0\n",
    "      restartPolicy: OnFailure\n",
    "```\n",
    "\n",
    "### Cas d'usage Data Engineering\n",
    "\n",
    "| ScÃ©nario | Solution |\n",
    "|----------|----------|\n",
    "| Spark executors sur nodes avec SSD | `nodeSelector: disk=ssd` |\n",
    "| RÃ©partir Kafka brokers sur diffÃ©rents nodes | Pod Anti-Affinity |\n",
    "| RÃ©server des nodes GPU pour le ML | Taints + Tolerations |\n",
    "| ETL critique doit toujours tourner | PriorityClass Ã©levÃ©e |\n",
    "| Spark driver proche des executors | Pod Affinity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helm_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Helm â€” Le Package Manager Kubernetes\n",
    "\n",
    "> **Helm** est le gestionnaire de packages pour Kubernetes. Il te permet de packager, versionner et dÃ©ployer des applications complexes en une seule commande.\n",
    "\n",
    "### Pourquoi Helm ?\n",
    "\n",
    "| Sans Helm | Avec Helm |\n",
    "|-----------|----------|\n",
    "| 10+ fichiers YAML Ã  maintenir | 1 chart versionnÃ© |\n",
    "| Copier/coller pour chaque environnement | Variables par environnement |\n",
    "| Pas de rollback facile | `helm rollback` en 1 commande |\n",
    "| Difficile de partager | Charts publics sur Artifact Hub |\n",
    "\n",
    "### Concepts Helm\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Chart** | Package Helm (dossier avec templates + values) |\n",
    "| **Release** | Instance dÃ©ployÃ©e d'un chart |\n",
    "| **Repository** | Serveur qui hÃ©berge des charts |\n",
    "| **Values** | Variables de configuration |\n",
    "\n",
    "### Structure d'un Chart\n",
    "\n",
    "```text\n",
    "my-data-pipeline/\n",
    "â”œâ”€â”€ Chart.yaml           # MÃ©tadonnÃ©es du chart\n",
    "â”œâ”€â”€ values.yaml          # Valeurs par dÃ©faut\n",
    "â”œâ”€â”€ values-dev.yaml      # Override pour dev\n",
    "â”œâ”€â”€ values-prod.yaml     # Override pour prod\n",
    "â”œâ”€â”€ templates/           # Templates Kubernetes\n",
    "â”‚   â”œâ”€â”€ _helpers.tpl     # Fonctions rÃ©utilisables\n",
    "â”‚   â”œâ”€â”€ deployment.yaml\n",
    "â”‚   â”œâ”€â”€ service.yaml\n",
    "â”‚   â”œâ”€â”€ configmap.yaml\n",
    "â”‚   â”œâ”€â”€ secret.yaml\n",
    "â”‚   â”œâ”€â”€ cronjob.yaml\n",
    "â”‚   â””â”€â”€ NOTES.txt        # Message post-install\n",
    "â””â”€â”€ charts/              # DÃ©pendances (sub-charts)\n",
    "```\n",
    "\n",
    "### CrÃ©er un Chart pour un Pipeline ETL\n",
    "\n",
    "```bash\n",
    "# CrÃ©er la structure de base\n",
    "helm create etl-pipeline\n",
    "cd etl-pipeline\n",
    "```\n",
    "\n",
    "**Chart.yaml**\n",
    "```yaml\n",
    "apiVersion: v2\n",
    "name: etl-pipeline\n",
    "description: Pipeline ETL pour Data Engineering\n",
    "type: application\n",
    "version: 1.0.0        # Version du chart\n",
    "appVersion: \"2.1.0\"   # Version de l'application\n",
    "maintainers:\n",
    "  - name: Data Team\n",
    "    email: data@company.com\n",
    "dependencies:\n",
    "  - name: postgresql\n",
    "    version: \"12.x.x\"\n",
    "    repository: \"https://charts.bitnami.com/bitnami\"\n",
    "    condition: postgresql.enabled\n",
    "```\n",
    "\n",
    "**values.yaml**\n",
    "```yaml\n",
    "# Configuration globale\n",
    "replicaCount: 1\n",
    "\n",
    "image:\n",
    "  repository: my-registry/etl-pipeline\n",
    "  tag: \"2.1.0\"\n",
    "  pullPolicy: IfNotPresent\n",
    "\n",
    "# Configuration ETL\n",
    "etl:\n",
    "  schedule: \"0 2 * * *\"    # Tous les jours Ã  2h\n",
    "  sourceBucket: \"raw-data\"\n",
    "  destBucket: \"processed-data\"\n",
    "  batchSize: 10000\n",
    "\n",
    "# Configuration base de donnÃ©es\n",
    "database:\n",
    "  host: \"postgres\"\n",
    "  port: 5432\n",
    "  name: \"analytics\"\n",
    "  user: \"etl_user\"\n",
    "  # Le password vient d'un secret existant\n",
    "  existingSecret: \"db-credentials\"\n",
    "  secretKey: \"password\"\n",
    "\n",
    "# Ressources\n",
    "resources:\n",
    "  requests:\n",
    "    cpu: \"500m\"\n",
    "    memory: \"512Mi\"\n",
    "  limits:\n",
    "    cpu: \"2000m\"\n",
    "    memory: \"2Gi\"\n",
    "\n",
    "# PostgreSQL subchart\n",
    "postgresql:\n",
    "  enabled: true\n",
    "  auth:\n",
    "    database: analytics\n",
    "    username: etl_user\n",
    "```\n",
    "\n",
    "**templates/cronjob.yaml**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: {{ include \"etl-pipeline.fullname\" . }}\n",
    "  labels:\n",
    "    {{- include \"etl-pipeline.labels\" . | nindent 4 }}\n",
    "spec:\n",
    "  schedule: {{ .Values.etl.schedule | quote }}\n",
    "  concurrencyPolicy: Forbid\n",
    "  successfulJobsHistoryLimit: 3\n",
    "  failedJobsHistoryLimit: 1\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        metadata:\n",
    "          labels:\n",
    "            {{- include \"etl-pipeline.selectorLabels\" . | nindent 12 }}\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: etl\n",
    "            image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n",
    "            imagePullPolicy: {{ .Values.image.pullPolicy }}\n",
    "            env:\n",
    "            - name: SOURCE_BUCKET\n",
    "              value: {{ .Values.etl.sourceBucket | quote }}\n",
    "            - name: DEST_BUCKET\n",
    "              value: {{ .Values.etl.destBucket | quote }}\n",
    "            - name: BATCH_SIZE\n",
    "              value: {{ .Values.etl.batchSize | quote }}\n",
    "            - name: DB_HOST\n",
    "              value: {{ .Values.database.host | quote }}\n",
    "            - name: DB_PORT\n",
    "              value: {{ .Values.database.port | quote }}\n",
    "            - name: DB_NAME\n",
    "              value: {{ .Values.database.name | quote }}\n",
    "            - name: DB_USER\n",
    "              value: {{ .Values.database.user | quote }}\n",
    "            - name: DB_PASSWORD\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: {{ .Values.database.existingSecret }}\n",
    "                  key: {{ .Values.database.secretKey }}\n",
    "            resources:\n",
    "              {{- toYaml .Values.resources | nindent 14 }}\n",
    "```\n",
    "\n",
    "**templates/_helpers.tpl**\n",
    "```yaml\n",
    "{{/*\n",
    "Nom complet de l'application\n",
    "*/}}\n",
    "{{- define \"etl-pipeline.fullname\" -}}\n",
    "{{- if .Values.fullnameOverride }}\n",
    "{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n",
    "{{- else }}\n",
    "{{- $name := default .Chart.Name .Values.nameOverride }}\n",
    "{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n",
    "{{- end }}\n",
    "{{- end }}\n",
    "\n",
    "{{/*\n",
    "Labels communs\n",
    "*/}}\n",
    "{{- define \"etl-pipeline.labels\" -}}\n",
    "helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}\n",
    "app.kubernetes.io/name: {{ .Chart.Name }}\n",
    "app.kubernetes.io/instance: {{ .Release.Name }}\n",
    "app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\n",
    "app.kubernetes.io/managed-by: {{ .Release.Service }}\n",
    "{{- end }}\n",
    "\n",
    "{{/*\n",
    "Selector labels\n",
    "*/}}\n",
    "{{- define \"etl-pipeline.selectorLabels\" -}}\n",
    "app.kubernetes.io/name: {{ .Chart.Name }}\n",
    "app.kubernetes.io/instance: {{ .Release.Name }}\n",
    "{{- end }}\n",
    "```\n",
    "\n",
    "### Commandes Helm essentielles\n",
    "\n",
    "```bash\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# GESTION DES REPOSITORIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Ajouter un repo\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n",
    "\n",
    "# Mettre Ã  jour les repos\n",
    "helm repo update\n",
    "\n",
    "# Chercher un chart\n",
    "helm search repo kafka\n",
    "helm search hub airflow    # Cherche sur Artifact Hub\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSTALLATION ET DÃ‰PLOIEMENT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Installer un chart depuis un repo\n",
    "helm install my-kafka bitnami/kafka -n data --create-namespace\n",
    "\n",
    "# Installer avec des values custom\n",
    "helm install my-etl ./etl-pipeline -f values-prod.yaml -n production\n",
    "\n",
    "# Dry-run : voir ce qui serait gÃ©nÃ©rÃ© sans installer\n",
    "helm install my-etl ./etl-pipeline --dry-run --debug\n",
    "\n",
    "# Template : gÃ©nÃ©rer les manifests YAML\n",
    "helm template my-etl ./etl-pipeline -f values-prod.yaml > manifests.yaml\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MISE Ã€ JOUR ET ROLLBACK\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Mettre Ã  jour une release\n",
    "helm upgrade my-etl ./etl-pipeline -f values-prod.yaml -n production\n",
    "\n",
    "# Install ou upgrade (idempotent)\n",
    "helm upgrade --install my-etl ./etl-pipeline -f values-prod.yaml -n production\n",
    "\n",
    "# Voir l'historique des releases\n",
    "helm history my-etl -n production\n",
    "\n",
    "# Rollback Ã  une version prÃ©cÃ©dente\n",
    "helm rollback my-etl 2 -n production\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# INSPECTION ET DEBUG\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Lister les releases\n",
    "helm list -A\n",
    "\n",
    "# Voir les values d'une release\n",
    "helm get values my-etl -n production\n",
    "\n",
    "# Voir tous les manifests dÃ©ployÃ©s\n",
    "helm get manifest my-etl -n production\n",
    "\n",
    "# Voir les notes post-install\n",
    "helm get notes my-etl -n production\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SUPPRESSION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# DÃ©sinstaller une release\n",
    "helm uninstall my-etl -n production\n",
    "\n",
    "# Garder l'historique (permet rollback aprÃ¨s uninstall)\n",
    "helm uninstall my-etl -n production --keep-history\n",
    "```\n",
    "\n",
    "### Helm vs Kustomize\n",
    "\n",
    "| Aspect | Helm | Kustomize |\n",
    "|--------|------|----------|\n",
    "| Approche | Templating | Patching/Overlay |\n",
    "| ComplexitÃ© | Plus riche | Plus simple |\n",
    "| DÃ©pendances | Sub-charts | Non natif |\n",
    "| Rollback | IntÃ©grÃ© | Via kubectl |\n",
    "| Cas d'usage | Apps complexes | Customisations simples |\n",
    "\n",
    "> ğŸ’¡ En pratique, **Helm est le standard** pour les charts communautaires et les applications complexes. Kustomize est souvent utilisÃ© **en complÃ©ment** pour des overlays simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "argocd_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. GitOps avec ArgoCD\n",
    "\n",
    "> **GitOps** est une pratique oÃ¹ **Git est la source de vÃ©ritÃ©** pour l'Ã©tat de ton infrastructure. ArgoCD surveille ton repo Git et synchronise automatiquement ton cluster Kubernetes.\n",
    "\n",
    "### Pourquoi GitOps ?\n",
    "\n",
    "| Approche Traditionnelle | GitOps |\n",
    "|------------------------|--------|\n",
    "| `kubectl apply` manuel | Git push â†’ dÃ©ploiement auto |\n",
    "| Qui a dÃ©ployÃ© quoi ? | Historique Git complet |\n",
    "| Rollback complexe | `git revert` |\n",
    "| Ã‰tat du cluster inconnu | Ã‰tat = ce qui est dans Git |\n",
    "\n",
    "### SchÃ©ma GitOps avec ArgoCD\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          WORKFLOW GITOPS                                â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚   DÃ©veloppeur                                                           â”‚\n",
    "â”‚       â”‚                                                                 â”‚\n",
    "â”‚       â”‚ 1. git push (manifests/charts)                                 â”‚\n",
    "â”‚       â–¼                                                                 â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
    "â”‚   â”‚   Git Repo      â”‚                                                  â”‚\n",
    "â”‚   â”‚ (GitHub/GitLab) â”‚                                                  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
    "â”‚            â”‚                                                            â”‚\n",
    "â”‚            â”‚ 2. ArgoCD dÃ©tecte le changement                           â”‚\n",
    "â”‚            â–¼                                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚     ArgoCD      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚         Cluster Kubernetes          â”‚  â”‚\n",
    "â”‚   â”‚  (dans le K8s)  â”‚ 3. Sync â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”         â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚         â”‚  â”‚\n",
    "â”‚            â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜         â”‚  â”‚\n",
    "â”‚            â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚            â”‚ 4. Compare Ã©tat rÃ©el vs Ã©tat Git                          â”‚\n",
    "â”‚            â”‚    (et corrige les drifts)                                â”‚\n",
    "â”‚            â–¼                                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
    "â”‚   â”‚  UI ArgoCD      â”‚  Visualisation, alertes, rollback                â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Installation d'ArgoCD\n",
    "\n",
    "```bash\n",
    "# CrÃ©er le namespace\n",
    "kubectl create namespace argocd\n",
    "\n",
    "# Installer ArgoCD\n",
    "kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n",
    "\n",
    "# Attendre que tous les pods soient prÃªts\n",
    "kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s\n",
    "\n",
    "# RÃ©cupÃ©rer le mot de passe admin initial\n",
    "kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n",
    "\n",
    "# Exposer l'UI (pour dev local)\n",
    "kubectl port-forward svc/argocd-server -n argocd 8080:443\n",
    "\n",
    "# AccÃ©der Ã  https://localhost:8080\n",
    "# User: admin, Password: (celui rÃ©cupÃ©rÃ© ci-dessus)\n",
    "```\n",
    "\n",
    "### Installer le CLI ArgoCD\n",
    "\n",
    "```bash\n",
    "# Linux\n",
    "curl -sSL -o argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\n",
    "chmod +x argocd\n",
    "sudo mv argocd /usr/local/bin/\n",
    "\n",
    "# macOS\n",
    "brew install argocd\n",
    "\n",
    "# Se connecter\n",
    "argocd login localhost:8080 --username admin --password <password> --insecure\n",
    "```\n",
    "\n",
    "### CrÃ©er une Application ArgoCD\n",
    "\n",
    "Une **Application** ArgoCD pointe vers un repo Git et un path contenant les manifests.\n",
    "\n",
    "```yaml\n",
    "# argocd-app-etl.yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: etl-pipeline\n",
    "  namespace: argocd\n",
    "  finalizers:\n",
    "    - resources-finalizer.argocd.argoproj.io\n",
    "spec:\n",
    "  project: default\n",
    "  \n",
    "  # Source : oÃ¹ sont les manifests\n",
    "  source:\n",
    "    repoURL: https://github.com/myorg/data-platform.git\n",
    "    targetRevision: main           # Branche Ã  suivre\n",
    "    path: kubernetes/etl-pipeline  # Dossier contenant les manifests ou chart Helm\n",
    "    \n",
    "    # Si c'est un chart Helm\n",
    "    helm:\n",
    "      valueFiles:\n",
    "        - values-prod.yaml\n",
    "  \n",
    "  # Destination : oÃ¹ dÃ©ployer\n",
    "  destination:\n",
    "    server: https://kubernetes.default.svc\n",
    "    namespace: production\n",
    "  \n",
    "  # Politique de synchronisation\n",
    "  syncPolicy:\n",
    "    automated:\n",
    "      prune: true        # Supprimer les ressources qui ne sont plus dans Git\n",
    "      selfHeal: true     # Corriger les drifts (si quelqu'un modifie manuellement)\n",
    "      allowEmpty: false\n",
    "    syncOptions:\n",
    "      - CreateNamespace=true\n",
    "      - PrunePropagationPolicy=foreground\n",
    "    retry:\n",
    "      limit: 5\n",
    "      backoff:\n",
    "        duration: 5s\n",
    "        factor: 2\n",
    "        maxDuration: 3m\n",
    "```\n",
    "\n",
    "```bash\n",
    "# CrÃ©er l'application\n",
    "kubectl apply -f argocd-app-etl.yaml\n",
    "\n",
    "# Ou via CLI\n",
    "argocd app create etl-pipeline \\\n",
    "  --repo https://github.com/myorg/data-platform.git \\\n",
    "  --path kubernetes/etl-pipeline \\\n",
    "  --dest-server https://kubernetes.default.svc \\\n",
    "  --dest-namespace production \\\n",
    "  --sync-policy automated \\\n",
    "  --auto-prune \\\n",
    "  --self-heal\n",
    "```\n",
    "\n",
    "### ApplicationSets\n",
    "\n",
    "DÃ©ployer la **mÃªme application dans plusieurs environnements** ou clusters.\n",
    "\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ApplicationSet\n",
    "metadata:\n",
    "  name: etl-pipeline-envs\n",
    "  namespace: argocd\n",
    "spec:\n",
    "  generators:\n",
    "  - list:\n",
    "      elements:\n",
    "      - env: dev\n",
    "        namespace: dev\n",
    "        valuesFile: values-dev.yaml\n",
    "      - env: staging\n",
    "        namespace: staging\n",
    "        valuesFile: values-staging.yaml\n",
    "      - env: prod\n",
    "        namespace: production\n",
    "        valuesFile: values-prod.yaml\n",
    "  \n",
    "  template:\n",
    "    metadata:\n",
    "      name: 'etl-pipeline-{{env}}'\n",
    "    spec:\n",
    "      project: default\n",
    "      source:\n",
    "        repoURL: https://github.com/myorg/data-platform.git\n",
    "        targetRevision: main\n",
    "        path: kubernetes/etl-pipeline\n",
    "        helm:\n",
    "          valueFiles:\n",
    "            - '{{valuesFile}}'\n",
    "      destination:\n",
    "        server: https://kubernetes.default.svc\n",
    "        namespace: '{{namespace}}'\n",
    "      syncPolicy:\n",
    "        automated:\n",
    "          prune: true\n",
    "          selfHeal: true\n",
    "```\n",
    "\n",
    "### Commandes ArgoCD essentielles\n",
    "\n",
    "```bash\n",
    "# Lister les applications\n",
    "argocd app list\n",
    "\n",
    "# Voir le statut dÃ©taillÃ©\n",
    "argocd app get etl-pipeline\n",
    "\n",
    "# Synchroniser manuellement\n",
    "argocd app sync etl-pipeline\n",
    "\n",
    "# Voir l'historique\n",
    "argocd app history etl-pipeline\n",
    "\n",
    "# Rollback\n",
    "argocd app rollback etl-pipeline <revision>\n",
    "\n",
    "# Voir les diffÃ©rences (ce qui va changer)\n",
    "argocd app diff etl-pipeline\n",
    "\n",
    "# Supprimer une application (et ses ressources)\n",
    "argocd app delete etl-pipeline --cascade\n",
    "```\n",
    "\n",
    "### Bonnes pratiques GitOps\n",
    "\n",
    "| Pratique | Pourquoi |\n",
    "|----------|----------|\n",
    "| Un repo dÃ©diÃ© pour les manifests K8s | SÃ©paration code applicatif / config infra |\n",
    "| Branches par environnement ou values files | Promotion claire dev â†’ staging â†’ prod |\n",
    "| PR obligatoires pour main/prod | Review avant dÃ©ploiement |\n",
    "| Secrets chiffrÃ©s (Sealed Secrets, SOPS) | Ne jamais commiter de secrets en clair |\n",
    "| Notifications (Slack, Teams) | Savoir quand un sync Ã©choue |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monitoring_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Monitoring avec Prometheus et Grafana\n",
    "\n",
    "> **Prometheus** collecte et stocke les mÃ©triques. **Grafana** les visualise. Ensemble, ils forment le stack de monitoring standard de Kubernetes.\n",
    "\n",
    "### Architecture du Stack Monitoring\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        STACK MONITORING K8S                                â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚                         PROMETHEUS                                   â”‚  â”‚\n",
    "â”‚  â”‚                                                                      â”‚  â”‚\n",
    "â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”‚\n",
    "â”‚  â”‚   â”‚   Scraper    â”‚â”€â”€â”€â”€ pull metrics â”€â”€â”€â–¶â”‚   Time Series DB  â”‚       â”‚  â”‚\n",
    "â”‚  â”‚   â”‚  (discovery) â”‚                      â”‚   (local storage) â”‚       â”‚  â”‚\n",
    "â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚\n",
    "â”‚  â”‚                                                   â”‚                  â”‚  â”‚\n",
    "â”‚  â”‚                                                   â”‚ PromQL queries   â”‚  â”‚\n",
    "â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â–¼                  â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ AlertManager â”‚â—€â”€â”€â”€â”€ alerting rules â”€â”€â”€ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ (Slack/PD)   â”‚                         â”‚   Rules   â”‚            â”‚  â”‚\n",
    "â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                    â”‚                                       â”‚\n",
    "â”‚                                    â”‚ PromQL                                â”‚\n",
    "â”‚                                    â–¼                                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚                          GRAFANA                                     â”‚  â”‚\n",
    "â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ Dashboard 1 â”‚  â”‚ Dashboard 2 â”‚  â”‚ Dashboard 3 â”‚                 â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ K8s Cluster â”‚  â”‚ Data Pipelinesâ”‚ â”‚    Spark   â”‚                 â”‚  â”‚\n",
    "â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚                     SOURCES DE MÃ‰TRIQUES                             â”‚  â”‚\n",
    "â”‚  â”‚                                                                      â”‚  â”‚\n",
    "â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ kubelet  â”‚  â”‚ node-    â”‚  â”‚ kube-    â”‚  â”‚ App      â”‚           â”‚  â”‚\n",
    "â”‚  â”‚   â”‚ /metrics â”‚  â”‚ exporter â”‚  â”‚ state    â”‚  â”‚ metrics  â”‚           â”‚  â”‚\n",
    "â”‚  â”‚   â”‚          â”‚  â”‚          â”‚  â”‚ metrics  â”‚  â”‚ (custom) â”‚           â”‚  â”‚\n",
    "â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Installation avec kube-prometheus-stack\n",
    "\n",
    "Le moyen le plus simple d'installer tout le stack :\n",
    "\n",
    "```bash\n",
    "# Ajouter le repo Helm\n",
    "helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n",
    "helm repo update\n",
    "\n",
    "# CrÃ©er le namespace\n",
    "kubectl create namespace monitoring\n",
    "\n",
    "# Installer le stack complet\n",
    "helm install prometheus prometheus-community/kube-prometheus-stack \\\n",
    "  --namespace monitoring \\\n",
    "  --set grafana.adminPassword=admin123 \\\n",
    "  --set prometheus.prometheusSpec.retention=15d \\\n",
    "  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi\n",
    "\n",
    "# VÃ©rifier l'installation\n",
    "kubectl get pods -n monitoring\n",
    "\n",
    "# AccÃ©der Ã  Grafana\n",
    "kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80\n",
    "# http://localhost:3000 (admin / admin123)\n",
    "\n",
    "# AccÃ©der Ã  Prometheus\n",
    "kubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090\n",
    "# http://localhost:9090\n",
    "```\n",
    "\n",
    "### Prometheus : Concepts ClÃ©s\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Metric** | DonnÃ©e numÃ©rique avec timestamp (ex: `cpu_usage`) |\n",
    "| **Label** | ClÃ©-valeur pour filtrer (ex: `pod=\"etl-123\"`) |\n",
    "| **Scrape** | Prometheus tire les mÃ©triques des targets |\n",
    "| **Target** | Endpoint exposant des mÃ©triques (`/metrics`) |\n",
    "| **PromQL** | Langage de requÃªte Prometheus |\n",
    "\n",
    "### Types de mÃ©triques\n",
    "\n",
    "| Type | Description | Exemple |\n",
    "|------|-------------|----------|\n",
    "| **Counter** | Valeur croissante uniquement | `http_requests_total` |\n",
    "| **Gauge** | Valeur qui monte et descend | `memory_usage_bytes` |\n",
    "| **Histogram** | Distribution de valeurs | `request_duration_seconds` |\n",
    "| **Summary** | Quantiles prÃ©-calculÃ©s | `request_latency_seconds` |\n",
    "\n",
    "### PromQL : RequÃªtes Essentielles\n",
    "\n",
    "```promql\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# REQUÃŠTES DE BASE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# CPU utilisÃ© par namespace\n",
    "sum(rate(container_cpu_usage_seconds_total{namespace=\"production\"}[5m])) by (pod)\n",
    "\n",
    "# MÃ©moire utilisÃ©e par pod\n",
    "container_memory_usage_bytes{namespace=\"production\", container!=\"\"}\n",
    "\n",
    "# Pods en Ã©tat non-Running\n",
    "kube_pod_status_phase{phase!=\"Running\", phase!=\"Succeeded\"}\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# MÃ‰TRIQUES DATA ENGINEERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Jobs CronJob Ã©chouÃ©s dans les derniÃ¨res 24h\n",
    "kube_job_status_failed{namespace=\"data-pipeline\"} > 0\n",
    "\n",
    "# DurÃ©e moyenne des jobs ETL\n",
    "avg(kube_job_status_completion_time - kube_job_status_start_time) by (job_name)\n",
    "\n",
    "# Taux de redÃ©marrage des pods (signe de problÃ¨me)\n",
    "rate(kube_pod_container_status_restarts_total{namespace=\"production\"}[1h]) > 0\n",
    "\n",
    "# Utilisation PVC (espace disque)\n",
    "(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# FONCTIONS UTILES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# rate() : taux de changement par seconde (pour counters)\n",
    "rate(http_requests_total[5m])\n",
    "\n",
    "# increase() : augmentation sur une pÃ©riode (pour counters)\n",
    "increase(etl_rows_processed_total[1h])\n",
    "\n",
    "# sum() by () : agrÃ©gation\n",
    "sum(rate(cpu_usage[5m])) by (namespace)\n",
    "\n",
    "# topk() : top N\n",
    "topk(5, sum(rate(container_cpu_usage_seconds_total[5m])) by (pod))\n",
    "\n",
    "# histogram_quantile() : percentiles\n",
    "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n",
    "```\n",
    "\n",
    "### ServiceMonitor : Scraper des Applications Custom\n",
    "\n",
    "Pour que Prometheus scrape automatiquement ton application :\n",
    "\n",
    "```yaml\n",
    "# 1. Ton application expose /metrics\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: etl-pipeline\n",
    "  namespace: production\n",
    "  labels:\n",
    "    app: etl-pipeline\n",
    "spec:\n",
    "  ports:\n",
    "  - name: metrics\n",
    "    port: 8080\n",
    "    targetPort: 8080\n",
    "  selector:\n",
    "    app: etl-pipeline\n",
    "---\n",
    "# 2. ServiceMonitor pour Prometheus\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: etl-pipeline\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    release: prometheus    # Important : matcher le label du Prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: etl-pipeline\n",
    "  namespaceSelector:\n",
    "    matchNames:\n",
    "    - production\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 30s\n",
    "    path: /metrics\n",
    "```\n",
    "\n",
    "### Alerting avec PrometheusRule\n",
    "\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: data-pipeline-alerts\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  groups:\n",
    "  - name: data-pipeline\n",
    "    rules:\n",
    "    # Alerte : Job ETL Ã©chouÃ©\n",
    "    - alert: ETLJobFailed\n",
    "      expr: kube_job_status_failed{namespace=\"production\", job_name=~\"etl-.*\"} > 0\n",
    "      for: 1m\n",
    "      labels:\n",
    "        severity: critical\n",
    "        team: data\n",
    "      annotations:\n",
    "        summary: \"ETL Job {{ $labels.job_name }} a Ã©chouÃ©\"\n",
    "        description: \"Le job {{ $labels.job_name }} dans {{ $labels.namespace }} est en Ã©chec depuis plus d'1 minute.\"\n",
    "    \n",
    "    # Alerte : Pod restart trop frÃ©quent\n",
    "    - alert: PodRestartingTooMuch\n",
    "      expr: rate(kube_pod_container_status_restarts_total{namespace=\"production\"}[15m]) > 0.1\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: warning\n",
    "        team: data\n",
    "      annotations:\n",
    "        summary: \"Pod {{ $labels.pod }} redÃ©marre trop souvent\"\n",
    "        description: \"Le pod {{ $labels.pod }} a redÃ©marrÃ© plus de 3 fois en 15 minutes.\"\n",
    "    \n",
    "    # Alerte : Espace disque PVC > 80%\n",
    "    - alert: PVCAlmostFull\n",
    "      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 > 80\n",
    "      for: 10m\n",
    "      labels:\n",
    "        severity: warning\n",
    "        team: data\n",
    "      annotations:\n",
    "        summary: \"PVC {{ $labels.persistentvolumeclaim }} presque plein\"\n",
    "        description: \"Le PVC {{ $labels.persistentvolumeclaim }} est utilisÃ© Ã  {{ $value }}%.\"\n",
    "```\n",
    "\n",
    "### Grafana : Dashboard pour Data Pipelines\n",
    "\n",
    "Voici les panels essentiels pour un dashboard Data Engineering :\n",
    "\n",
    "| Panel | PromQL | Type |\n",
    "|-------|--------|------|\n",
    "| Jobs actifs | `kube_job_status_active{namespace=\"production\"}` | Stat |\n",
    "| Jobs Ã©chouÃ©s (24h) | `sum(increase(kube_job_status_failed{namespace=\"production\"}[24h]))` | Stat |\n",
    "| CPU par pod | `sum(rate(container_cpu_usage_seconds_total{namespace=\"production\"}[5m])) by (pod)` | Time series |\n",
    "| MÃ©moire par pod | `container_memory_usage_bytes{namespace=\"production\"}` | Time series |\n",
    "| DurÃ©e des jobs | `kube_job_status_completion_time - kube_job_status_start_time` | Histogram |\n",
    "| Pods non-Ready | `kube_pod_status_ready{condition=\"false\", namespace=\"production\"}` | Table |\n",
    "\n",
    "> ğŸ’¡ **Dashboards prÃ©-faits** : Import ID `315` pour le dashboard \"Kubernetes cluster monitoring\" et `13770` pour \"Kubernetes All-in-one Cluster Monitoring\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Backup etcd\n",
    "\n",
    "**Objectif** : CrÃ©er un CronJob qui sauvegarde etcd toutes les heures.\n",
    "\n",
    "**Instructions** :\n",
    "1. CrÃ©er un CronJob qui exÃ©cute `etcdctl snapshot save`\n",
    "2. Stocker le backup dans un PVC\n",
    "3. Garder les 24 derniers backups (rotation)\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Indice</summary>\n",
    "\n",
    "Tu auras besoin :\n",
    "- D'un PVC pour stocker les backups\n",
    "- Des certificats etcd montÃ©s dans le pod\n",
    "- D'un script shell pour la rotation\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 2 : Scheduling Spark\n",
    "\n",
    "**Objectif** : Configurer le scheduling d'un job Spark avec les contraintes suivantes :\n",
    "\n",
    "- Le **driver** doit tourner sur un node avec le label `role=driver`\n",
    "- Les **executors** doivent Ãªtre rÃ©partis sur des nodes diffÃ©rents (anti-affinity)\n",
    "- Les executors doivent prÃ©fÃ©rer les nodes avec `disk=ssd`\n",
    "\n",
    "**Instructions** :\n",
    "1. Ã‰crire le manifest YAML du driver avec `nodeSelector`\n",
    "2. Ã‰crire le manifest des executors avec `podAntiAffinity` et `nodeAffinity`\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 3 : Chart Helm Kafka\n",
    "\n",
    "**Objectif** : CrÃ©er un chart Helm pour dÃ©ployer un cluster Kafka simple.\n",
    "\n",
    "**Le chart doit inclure** :\n",
    "- Un StatefulSet Kafka (3 replicas)\n",
    "- Un Service headless\n",
    "- Un ConfigMap pour la config Kafka\n",
    "- Des values pour : replicas, resources, retention.ms\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 4 : GitOps Pipeline\n",
    "\n",
    "**Objectif** : Mettre en place un workflow GitOps complet.\n",
    "\n",
    "**Instructions** :\n",
    "1. CrÃ©er un repo Git avec la structure :\n",
    "   ```\n",
    "   data-platform/\n",
    "   â”œâ”€â”€ apps/\n",
    "   â”‚   â””â”€â”€ etl-pipeline/\n",
    "   â”‚       â”œâ”€â”€ Chart.yaml\n",
    "   â”‚       â”œâ”€â”€ values.yaml\n",
    "   â”‚       â”œâ”€â”€ values-dev.yaml\n",
    "   â”‚       â””â”€â”€ values-prod.yaml\n",
    "   â””â”€â”€ argocd/\n",
    "       â””â”€â”€ applications.yaml\n",
    "   ```\n",
    "2. CrÃ©er un ApplicationSet ArgoCD qui dÃ©ploie en dev et prod\n",
    "3. Tester le workflow : modifier les values â†’ git push â†’ observer le sync\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 5 : Dashboard Grafana\n",
    "\n",
    "**Objectif** : CrÃ©er un dashboard Grafana pour monitorer un pipeline ETL.\n",
    "\n",
    "**Panels requis** :\n",
    "1. Nombre de jobs ETL en cours\n",
    "2. Taux de succÃ¨s/Ã©chec sur 24h\n",
    "3. DurÃ©e moyenne des jobs (gauge)\n",
    "4. Top 5 des jobs les plus lents\n",
    "5. Alerte visuelle si un job Ã©choue\n",
    "\n",
    "**Bonus** : Exporter le dashboard en JSON et le versionner dans Git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Mini-Projet : Pipeline Data avec GitOps & Monitoring\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Mettre en place un **pipeline ETL complet** dÃ©ployÃ© via GitOps avec monitoring professionnel.\n",
    "\n",
    "### Architecture cible\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          MINI-PROJET                                    â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚\n",
    "â”‚   â”‚   Git Repo   â”‚                                                     â”‚\n",
    "â”‚   â”‚ (manifests)  â”‚                                                     â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚\n",
    "â”‚          â”‚                                                              â”‚\n",
    "â”‚          â–¼                                                              â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚   â”‚   ArgoCD     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚        Namespace: data-pipeline      â”‚   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                                      â”‚   â”‚\n",
    "â”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚\n",
    "â”‚                            â”‚  â”‚  PostgreSQL â”‚  â”‚   CronJob   â”‚    â”‚   â”‚\n",
    "â”‚                            â”‚  â”‚  (Helm)     â”‚  â”‚    ETL      â”‚    â”‚   â”‚\n",
    "â”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚\n",
    "â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                                          â”‚                             â”‚\n",
    "â”‚                                          â”‚ mÃ©triques                   â”‚\n",
    "â”‚                                          â–¼                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚   â”‚                  Namespace: monitoring                        â”‚    â”‚\n",
    "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚\n",
    "â”‚   â”‚   â”‚ Prometheus â”‚â”€â”€â–¶â”‚  Grafana   â”‚   â”‚  ServiceMonitor    â”‚   â”‚    â”‚\n",
    "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ (dashboard)â”‚   â”‚  PrometheusRule    â”‚   â”‚    â”‚\n",
    "â”‚   â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**Ã‰tape 1 : PrÃ©parer le repo Git**\n",
    "\n",
    "Structure requise :\n",
    "```\n",
    "k8s-data-platform/\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ apps/\n",
    "â”‚   â”œâ”€â”€ etl-pipeline/           # Chart Helm custom\n",
    "â”‚   â”‚   â”œâ”€â”€ Chart.yaml\n",
    "â”‚   â”‚   â”œâ”€â”€ values.yaml\n",
    "â”‚   â”‚   â””â”€â”€ templates/\n",
    "â”‚   â”‚       â”œâ”€â”€ namespace.yaml\n",
    "â”‚   â”‚       â”œâ”€â”€ configmap.yaml\n",
    "â”‚   â”‚       â”œâ”€â”€ secret.yaml\n",
    "â”‚   â”‚       â””â”€â”€ cronjob.yaml\n",
    "â”‚   â””â”€â”€ monitoring/\n",
    "â”‚       â”œâ”€â”€ servicemonitor.yaml\n",
    "â”‚       â””â”€â”€ prometheusrule.yaml\n",
    "â””â”€â”€ argocd/\n",
    "    â””â”€â”€ applications.yaml       # ApplicationSet\n",
    "```\n",
    "\n",
    "**Ã‰tape 2 : CrÃ©er le Chart ETL**\n",
    "\n",
    "- CronJob qui s'exÃ©cute toutes les heures\n",
    "- Se connecte Ã  PostgreSQL (dependency Helm)\n",
    "- Expose des mÃ©triques sur `/metrics`\n",
    "\n",
    "**Ã‰tape 3 : Configurer ArgoCD**\n",
    "\n",
    "- CrÃ©er l'Application ArgoCD\n",
    "- Activer le sync automatique\n",
    "\n",
    "**Ã‰tape 4 : Monitoring**\n",
    "\n",
    "- ServiceMonitor pour scraper les mÃ©triques ETL\n",
    "- PrometheusRule avec alerte si job Ã©choue\n",
    "- Dashboard Grafana\n",
    "\n",
    "**Ã‰tape 5 : Tester le workflow**\n",
    "\n",
    "1. Modifier le schedule dans `values.yaml`\n",
    "2. Git commit & push\n",
    "3. Observer ArgoCD synchroniser\n",
    "4. VÃ©rifier dans Grafana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project_solution",
   "metadata": {},
   "source": [
    "### âœ… Solution du mini-projet\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ“¥ Afficher la solution complÃ¨te</summary>\n",
    "\n",
    "**1. `apps/etl-pipeline/Chart.yaml`**\n",
    "```yaml\n",
    "apiVersion: v2\n",
    "name: etl-pipeline\n",
    "description: Pipeline ETL avec monitoring\n",
    "type: application\n",
    "version: 1.0.0\n",
    "appVersion: \"1.0.0\"\n",
    "dependencies:\n",
    "  - name: postgresql\n",
    "    version: \"12.x.x\"\n",
    "    repository: \"https://charts.bitnami.com/bitnami\"\n",
    "```\n",
    "\n",
    "**2. `apps/etl-pipeline/values.yaml`**\n",
    "```yaml\n",
    "namespace: data-pipeline\n",
    "\n",
    "etl:\n",
    "  image: python:3.11-slim\n",
    "  schedule: \"0 * * * *\"\n",
    "  resources:\n",
    "    requests:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"256Mi\"\n",
    "    limits:\n",
    "      cpu: \"500m\"\n",
    "      memory: \"512Mi\"\n",
    "\n",
    "database:\n",
    "  host: \"postgresql\"\n",
    "  port: \"5432\"\n",
    "  name: \"analytics\"\n",
    "  user: \"etl_user\"\n",
    "\n",
    "postgresql:\n",
    "  enabled: true\n",
    "  auth:\n",
    "    username: etl_user\n",
    "    password: etl_password\n",
    "    database: analytics\n",
    "  primary:\n",
    "    persistence:\n",
    "      size: 5Gi\n",
    "```\n",
    "\n",
    "**3. `apps/etl-pipeline/templates/namespace.yaml`**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: {{ .Values.namespace }}\n",
    "  labels:\n",
    "    app.kubernetes.io/name: {{ .Chart.Name }}\n",
    "```\n",
    "\n",
    "**4. `apps/etl-pipeline/templates/configmap.yaml`**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: etl-config\n",
    "  namespace: {{ .Values.namespace }}\n",
    "data:\n",
    "  DB_HOST: {{ .Values.database.host | quote }}\n",
    "  DB_PORT: {{ .Values.database.port | quote }}\n",
    "  DB_NAME: {{ .Values.database.name | quote }}\n",
    "  DB_USER: {{ .Values.database.user | quote }}\n",
    "```\n",
    "\n",
    "**5. `apps/etl-pipeline/templates/secret.yaml`**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: etl-secret\n",
    "  namespace: {{ .Values.namespace }}\n",
    "type: Opaque\n",
    "stringData:\n",
    "  DB_PASSWORD: {{ .Values.postgresql.auth.password | quote }}\n",
    "```\n",
    "\n",
    "**6. `apps/etl-pipeline/templates/cronjob.yaml`**\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: etl-job\n",
    "  namespace: {{ .Values.namespace }}\n",
    "  labels:\n",
    "    app: etl-pipeline\n",
    "spec:\n",
    "  schedule: {{ .Values.etl.schedule | quote }}\n",
    "  concurrencyPolicy: Forbid\n",
    "  successfulJobsHistoryLimit: 5\n",
    "  failedJobsHistoryLimit: 3\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        metadata:\n",
    "          labels:\n",
    "            app: etl-job\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: etl\n",
    "            image: {{ .Values.etl.image }}\n",
    "            command: [\"python\", \"-c\"]\n",
    "            args:\n",
    "            - |\n",
    "              import os\n",
    "              import time\n",
    "              print(\"ğŸš€ Starting ETL job...\")\n",
    "              print(f\"DB_HOST: {os.environ.get('DB_HOST')}\")\n",
    "              print(f\"DB_NAME: {os.environ.get('DB_NAME')}\")\n",
    "              # Simulate ETL work\n",
    "              time.sleep(10)\n",
    "              print(\"âœ… ETL completed successfully!\")\n",
    "            envFrom:\n",
    "            - configMapRef:\n",
    "                name: etl-config\n",
    "            env:\n",
    "            - name: DB_PASSWORD\n",
    "              valueFrom:\n",
    "                secretKeyRef:\n",
    "                  name: etl-secret\n",
    "                  key: DB_PASSWORD\n",
    "            resources:\n",
    "              {{- toYaml .Values.etl.resources | nindent 14 }}\n",
    "```\n",
    "\n",
    "**7. `apps/monitoring/servicemonitor.yaml`**\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: etl-pipeline-monitor\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: etl-pipeline\n",
    "  namespaceSelector:\n",
    "    matchNames:\n",
    "    - data-pipeline\n",
    "  endpoints:\n",
    "  - port: metrics\n",
    "    interval: 30s\n",
    "```\n",
    "\n",
    "**8. `apps/monitoring/prometheusrule.yaml`**\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: etl-alerts\n",
    "  namespace: monitoring\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  groups:\n",
    "  - name: etl-pipeline\n",
    "    rules:\n",
    "    - alert: ETLJobFailed\n",
    "      expr: kube_job_status_failed{namespace=\"data-pipeline\", job_name=~\"etl-.*\"} > 0\n",
    "      for: 1m\n",
    "      labels:\n",
    "        severity: critical\n",
    "      annotations:\n",
    "        summary: \"ETL Job failed\"\n",
    "        description: \"Job {{ $labels.job_name }} has failed.\"\n",
    "    \n",
    "    - alert: ETLJobTooLong\n",
    "      expr: time() - kube_job_status_start_time{namespace=\"data-pipeline\", job_name=~\"etl-.*\"} > 3600\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"ETL Job running too long\"\n",
    "        description: \"Job {{ $labels.job_name }} is running for more than 1 hour.\"\n",
    "```\n",
    "\n",
    "**9. `argocd/applications.yaml`**\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: etl-pipeline\n",
    "  namespace: argocd\n",
    "  finalizers:\n",
    "    - resources-finalizer.argocd.argoproj.io\n",
    "spec:\n",
    "  project: default\n",
    "  source:\n",
    "    repoURL: https://github.com/YOUR_ORG/k8s-data-platform.git\n",
    "    targetRevision: main\n",
    "    path: apps/etl-pipeline\n",
    "    helm:\n",
    "      valueFiles:\n",
    "        - values.yaml\n",
    "  destination:\n",
    "    server: https://kubernetes.default.svc\n",
    "    namespace: data-pipeline\n",
    "  syncPolicy:\n",
    "    automated:\n",
    "      prune: true\n",
    "      selfHeal: true\n",
    "    syncOptions:\n",
    "      - CreateNamespace=true\n",
    "---\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: etl-monitoring\n",
    "  namespace: argocd\n",
    "spec:\n",
    "  project: default\n",
    "  source:\n",
    "    repoURL: https://github.com/YOUR_ORG/k8s-data-platform.git\n",
    "    targetRevision: main\n",
    "    path: apps/monitoring\n",
    "  destination:\n",
    "    server: https://kubernetes.default.svc\n",
    "    namespace: monitoring\n",
    "  syncPolicy:\n",
    "    automated:\n",
    "      prune: true\n",
    "      selfHeal: true\n",
    "```\n",
    "\n",
    "**10. Commandes de dÃ©ploiement**\n",
    "```bash\n",
    "# 1. S'assurer qu'ArgoCD est installÃ©\n",
    "kubectl get pods -n argocd\n",
    "\n",
    "# 2. S'assurer que le monitoring stack est installÃ©\n",
    "kubectl get pods -n monitoring\n",
    "\n",
    "# 3. Appliquer les applications ArgoCD\n",
    "kubectl apply -f argocd/applications.yaml\n",
    "\n",
    "# 4. VÃ©rifier le sync\n",
    "argocd app list\n",
    "argocd app get etl-pipeline\n",
    "\n",
    "# 5. Tester manuellement le job\n",
    "kubectl create job --from=cronjob/etl-job test-etl -n data-pipeline\n",
    "\n",
    "# 6. Voir les logs\n",
    "kubectl logs -f job/test-etl -n data-pipeline\n",
    "\n",
    "# 7. AccÃ©der Ã  Grafana\n",
    "kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸŒ Documentation officielle\n",
    "- [Kubernetes Docs - Scheduling](https://kubernetes.io/docs/concepts/scheduling-eviction/) â€” Scheduling avancÃ©\n",
    "- [etcd Docs](https://etcd.io/docs/) â€” Documentation etcd\n",
    "- [Helm Docs](https://helm.sh/docs/) â€” Documentation Helm\n",
    "- [ArgoCD Docs](https://argo-cd.readthedocs.io/) â€” Documentation ArgoCD\n",
    "- [Prometheus Docs](https://prometheus.io/docs/) â€” Documentation Prometheus\n",
    "- [Grafana Docs](https://grafana.com/docs/) â€” Documentation Grafana\n",
    "\n",
    "### ğŸ“ Certifications\n",
    "- [CKA - Certified Kubernetes Administrator](https://www.cncf.io/certification/cka/) â€” Certification officielle CNCF\n",
    "- [CKAD - Certified Kubernetes Application Developer](https://www.cncf.io/certification/ckad/) â€” Pour les dÃ©veloppeurs\n",
    "\n",
    "### ğŸ® Pratique\n",
    "- [Killercoda](https://killercoda.com/) â€” Labs Kubernetes interactifs\n",
    "- [Play with Kubernetes](https://labs.play-with-k8s.com/) â€” Cluster K8s gratuit\n",
    "- [Artifact Hub](https://artifacthub.io/) â€” Catalogue de charts Helm\n",
    "\n",
    "### ğŸ“– Livres\n",
    "- *Kubernetes Patterns* â€” Bilgin Ibryam & Roland HuÃŸ\n",
    "- *GitOps and Kubernetes* â€” Billy Yuen et al.\n",
    "- *Prometheus: Up & Running* â€” Brian Brazil\n",
    "\n",
    "### ğŸ”§ Outils\n",
    "- [k9s](https://k9scli.io/) â€” Terminal UI pour Kubernetes\n",
    "- [Lens](https://k8slens.dev/) â€” IDE Kubernetes\n",
    "- [Helm Dashboard](https://github.com/komodorio/helm-dashboard) â€” UI pour Helm\n",
    "- [Argo CD Autopilot](https://argocd-autopilot.readthedocs.io/) â€” Bootstrap GitOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu maÃ®trises Kubernetes en profondeur, passons Ã  **l'orchestration avancÃ©e des pipelines data** !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `28_advanced_orchestration`** â€” Orchestration avancÃ©e\n",
    "\n",
    "Tu vas apprendre :\n",
    "- Airflow sur Kubernetes (KubernetesExecutor)\n",
    "- TaskFlow API\n",
    "- Comparatif Airflow vs Dagster vs Prefect\n",
    "- OpenLineage pour le data lineage\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module Kubernetes Deep Dive pour Data Engineers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
