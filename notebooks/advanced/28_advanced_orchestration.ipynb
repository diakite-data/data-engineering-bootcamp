{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üéº Advanced Orchestration pour Data Engineers\n",
    "\n",
    "Bienvenue dans ce module avanc√© o√π tu vas ma√Ætriser **l'orchestration de pipelines data √† l'√©chelle**. Tu apprendras √† d√©ployer Airflow sur Kubernetes, √† exploiter la TaskFlow API, √† comparer les orchestrateurs modernes, et √† mettre en place le data lineage avec OpenLineage ‚Äî des comp√©tences essentielles pour un Data Engineer Senior !\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "| Niveau | Comp√©tence |\n",
    "|--------|------------|\n",
    "| ‚úÖ Requis | Avoir suivi le module `12_orchestration_pipelines` (Airflow basics) |\n",
    "| ‚úÖ Requis | Avoir suivi les modules `15_kubernetes_fundamentals` et `27_kubernetes_deep_dive` |\n",
    "| ‚úÖ Requis | Ma√Ætriser les DAGs, Operators, XCom dans Airflow |\n",
    "| ‚úÖ Requis | Connaissances solides en Python et Docker |\n",
    "| üí° Recommand√© | Un cluster K8s accessible (Minikube, kind, ou cloud) |\n",
    "\n",
    "## üéØ Objectifs du module\n",
    "\n",
    "√Ä la fin de ce module, tu seras capable de :\n",
    "\n",
    "- D√©ployer et configurer Airflow sur Kubernetes\n",
    "- Utiliser le KubernetesExecutor et le KubernetesPodOperator\n",
    "- √âcrire des DAGs modernes avec la TaskFlow API\n",
    "- Impl√©menter le Dynamic Task Mapping\n",
    "- Comparer et choisir entre Airflow, Dagster et Prefect\n",
    "- Mettre en place le data lineage avec OpenLineage\n",
    "- Utiliser Astronomer (Astro CLI et Astro SDK) pour industrialiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rappel : Ce qu'on a vu vs Ce qu'on va approfondir\n",
    "\n",
    "| Module M12 (Beginner) | Ce module M28 (Advanced) |\n",
    "|-----------------------|--------------------------|\n",
    "| Architecture Airflow basique | Airflow sur Kubernetes |\n",
    "| DAGs, Operators simples | TaskFlow API, Dynamic Mapping |\n",
    "| XCom manuel | XCom automatique avec TaskFlow |\n",
    "| Mention des alternatives | Comparatif d√©taill√© + exemples de code |\n",
    "| ‚Äî | OpenLineage (data lineage) |\n",
    "| ‚Äî | Astronomer (plateforme enterprise) |\n",
    "\n",
    "### Sch√©ma : De l'orchestration basique √† l'orchestration avanc√©e\n",
    "\n",
    "```text\n",
    "M12 Orchestration Basics              M28 Advanced Orchestration (ce module)\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ ‚Ä¢ Cron / Task Scheduler ‚îÇ           ‚îÇ ‚Ä¢ Airflow sur Kubernetes            ‚îÇ\n",
    "‚îÇ ‚Ä¢ DAGs basics           ‚îÇ           ‚îÇ ‚Ä¢ KubernetesExecutor                ‚îÇ\n",
    "‚îÇ ‚Ä¢ Operators simples     ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂  ‚îÇ ‚Ä¢ TaskFlow API                      ‚îÇ\n",
    "‚îÇ ‚Ä¢ XCom manuel           ‚îÇ           ‚îÇ ‚Ä¢ Dynamic Task Mapping              ‚îÇ\n",
    "‚îÇ ‚Ä¢ SequentialExecutor    ‚îÇ           ‚îÇ ‚Ä¢ Dagster, Prefect (comparatif)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ ‚Ä¢ OpenLineage (lineage)             ‚îÇ\n",
    "                                      ‚îÇ ‚Ä¢ Astronomer (enterprise)           ‚îÇ\n",
    "                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "> üí° **Ce module est orient√© \"production √† l'√©chelle\"** ‚Äî tu vas apprendre √† faire tourner des centaines de DAGs avec des milliers de t√¢ches sur Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_info",
   "metadata": {},
   "source": [
    "> ‚ÑπÔ∏è **Le savais-tu ?**\n",
    ">\n",
    "> Apache Airflow a √©t√© cr√©√© par **Airbnb** en 2014 pour orchestrer leurs pipelines data. Il a √©t√© donn√© √† la **Apache Foundation** en 2016.\n",
    ">\n",
    "> Aujourd'hui, Airflow est utilis√© par des milliers d'entreprises dont **Uber**, **Lyft**, **Twitter**, **Slack**, **Adobe**, et bien d'autres.\n",
    ">\n",
    "> **Astronomer**, fond√© en 2018, est devenu le leader des solutions Airflow manag√©es, levant plus de **200 millions de dollars** et √©tant le principal contributeur au projet open-source Airflow.\n",
    ">\n",
    "> üìñ [History of Apache Airflow](https://airflow.apache.org/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_on_k8s",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Airflow sur Kubernetes\n",
    "\n",
    "D√©ployer Airflow sur Kubernetes permet de b√©n√©ficier de **l'√©lasticit√©**, de **l'isolation** et de la **scalabilit√©** native de K8s.\n",
    "\n",
    "### Pourquoi Airflow sur K8s ?\n",
    "\n",
    "| Aspect | Sans K8s (Celery/Local) | Avec K8s |\n",
    "|--------|------------------------|----------|\n",
    "| **Scaling** | Workers fixes | Pods √† la demande |\n",
    "| **Isolation** | D√©pendances partag√©es | Chaque t√¢che dans son pod |\n",
    "| **Ressources** | Allocation statique | Requests/Limits par t√¢che |\n",
    "| **Co√ªt** | Serveurs 24/7 | Pay-per-use (pods √©ph√©m√®res) |\n",
    "| **Maintenance** | G√©rer les workers | K8s g√®re tout |\n",
    "\n",
    "### Architecture Airflow sur Kubernetes\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        AIRFLOW ON KUBERNETES                                ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n",
    "‚îÇ  ‚îÇ                      Namespace: airflow                              ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ  Scheduler  ‚îÇ   ‚îÇ  Webserver  ‚îÇ   ‚îÇ   Triggerer (Airflow 2) ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   (Pod)     ‚îÇ   ‚îÇ   (Pod)     ‚îÇ   ‚îÇ       (Pod)             ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ          ‚îÇ                                                           ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ          ‚îÇ Cr√©e des pods pour chaque t√¢che                          ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ          ‚ñº                                                           ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ              Worker Pods (√©ph√©m√®res)                         ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   ‚îÇTask 1 ‚îÇ  ‚îÇTask 2 ‚îÇ  ‚îÇTask 3 ‚îÇ  ‚îÇTask 4 ‚îÇ  ‚îÇTask 5 ‚îÇ    ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   ‚îÇ Pod   ‚îÇ  ‚îÇ Pod   ‚îÇ  ‚îÇ Pod   ‚îÇ  ‚îÇ Pod   ‚îÇ  ‚îÇ Pod   ‚îÇ    ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                                      ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ   PostgreSQL    ‚îÇ   ‚îÇ            DAGs (PVC/Git-Sync)      ‚îÇ     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îÇ  (Metadata DB)  ‚îÇ   ‚îÇ                                     ‚îÇ     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Les Executors Airflow\n",
    "\n",
    "| Executor | Description | Quand l'utiliser |\n",
    "|----------|-------------|------------------|\n",
    "| **SequentialExecutor** | 1 t√¢che √† la fois | Dev/test uniquement |\n",
    "| **LocalExecutor** | Parall√®le sur 1 machine | Petite production |\n",
    "| **CeleryExecutor** | Workers Celery distribu√©s | Production classique |\n",
    "| **KubernetesExecutor** | 1 pod K8s par t√¢che | Production K8s |\n",
    "| **CeleryKubernetesExecutor** | Hybrid Celery + K8s | Workloads mixtes |\n",
    "\n",
    "### Installation avec le Helm Chart Officiel\n",
    "\n",
    "```bash\n",
    "# Ajouter le repo Helm officiel Airflow\n",
    "helm repo add apache-airflow https://airflow.apache.org\n",
    "helm repo update\n",
    "\n",
    "# Cr√©er le namespace\n",
    "kubectl create namespace airflow\n",
    "\n",
    "# Installer Airflow avec KubernetesExecutor\n",
    "helm install airflow apache-airflow/airflow \\\n",
    "  --namespace airflow \\\n",
    "  --set executor=KubernetesExecutor \\\n",
    "  --set webserver.defaultUser.password=admin \\\n",
    "  --set dags.persistence.enabled=true \\\n",
    "  --set dags.gitSync.enabled=true \\\n",
    "  --set dags.gitSync.repo=https://github.com/myorg/airflow-dags.git \\\n",
    "  --set dags.gitSync.branch=main \\\n",
    "  --set dags.gitSync.subPath=dags\n",
    "\n",
    "# V√©rifier l'installation\n",
    "kubectl get pods -n airflow\n",
    "\n",
    "# Acc√©der au webserver\n",
    "kubectl port-forward svc/airflow-webserver -n airflow 8080:8080\n",
    "# http://localhost:8080 (admin / admin)\n",
    "```\n",
    "\n",
    "### Configuration values.yaml avanc√©e\n",
    "\n",
    "```yaml\n",
    "# values-production.yaml\n",
    "executor: KubernetesExecutor\n",
    "\n",
    "# Webserver\n",
    "webserver:\n",
    "  replicas: 2\n",
    "  resources:\n",
    "    requests:\n",
    "      cpu: \"500m\"\n",
    "      memory: \"1Gi\"\n",
    "    limits:\n",
    "      cpu: \"1000m\"\n",
    "      memory: \"2Gi\"\n",
    "\n",
    "# Scheduler\n",
    "scheduler:\n",
    "  replicas: 2    # HA Scheduler (Airflow 2.0+)\n",
    "  resources:\n",
    "    requests:\n",
    "      cpu: \"500m\"\n",
    "      memory: \"1Gi\"\n",
    "\n",
    "# Configuration KubernetesExecutor\n",
    "config:\n",
    "  kubernetes:\n",
    "    # Namespace pour les worker pods\n",
    "    namespace: airflow\n",
    "    # Supprimer les pods apr√®s ex√©cution\n",
    "    delete_worker_pods: \"True\"\n",
    "    delete_worker_pods_on_failure: \"False\"  # Garder pour debug\n",
    "    # Image par d√©faut pour les workers\n",
    "    worker_container_repository: apache/airflow\n",
    "    worker_container_tag: 2.8.0-python3.11\n",
    "\n",
    "# Git-Sync pour les DAGs\n",
    "dags:\n",
    "  persistence:\n",
    "    enabled: false\n",
    "  gitSync:\n",
    "    enabled: true\n",
    "    repo: git@github.com:myorg/airflow-dags.git\n",
    "    branch: main\n",
    "    subPath: dags\n",
    "    sshKeySecret: airflow-git-ssh-key\n",
    "    wait: 60  # Sync toutes les 60 secondes\n",
    "\n",
    "# Logs dans un stockage externe\n",
    "logs:\n",
    "  persistence:\n",
    "    enabled: true\n",
    "    size: 10Gi\n",
    "\n",
    "# PostgreSQL (ou utiliser un service externe)\n",
    "postgresql:\n",
    "  enabled: true\n",
    "  auth:\n",
    "    postgresPassword: airflow\n",
    "    username: airflow\n",
    "    password: airflow\n",
    "    database: airflow\n",
    "```\n",
    "\n",
    "### KubernetesExecutor : Comment √ßa marche\n",
    "\n",
    "```text\n",
    "1. DAG est schedul√©\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "2. Scheduler parse le DAG et identifie les t√¢ches √† ex√©cuter\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "3. Pour chaque t√¢che, le Scheduler cr√©e un Pod K8s\n",
    "       ‚îÇ\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚îÇ  Pod Spec g√©n√©r√© automatiquement :        ‚îÇ\n",
    "   ‚îÇ  - Image: airflow (ou custom)             ‚îÇ\n",
    "   ‚îÇ  - Command: airflow tasks run ...         ‚îÇ\n",
    "   ‚îÇ  - Env: connexions, variables             ‚îÇ\n",
    "   ‚îÇ  - Resources: requests/limits             ‚îÇ\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "4. K8s schedule le pod sur un node\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "5. La t√¢che s'ex√©cute\n",
    "       ‚îÇ\n",
    "       ‚ñº\n",
    "6. Pod termin√© ‚Üí supprim√© (si delete_worker_pods=True)\n",
    "```\n",
    "\n",
    "### KubernetesPodOperator\n",
    "\n",
    "Pour ex√©cuter des t√¢ches avec des **images Docker personnalis√©es** :\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"etl_with_custom_image\",\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    \n",
    "    # T√¢che avec image Spark\n",
    "    spark_job = KubernetesPodOperator(\n",
    "        task_id=\"run_spark_etl\",\n",
    "        name=\"spark-etl-job\",\n",
    "        namespace=\"airflow\",\n",
    "        image=\"my-registry/spark-etl:1.0\",\n",
    "        cmds=[\"spark-submit\"],\n",
    "        arguments=[\n",
    "            \"--master\", \"k8s://https://kubernetes.default.svc\",\n",
    "            \"/app/etl_job.py\"\n",
    "        ],\n",
    "        # Ressources\n",
    "        container_resources={\n",
    "            \"requests\": {\"cpu\": \"1\", \"memory\": \"2Gi\"},\n",
    "            \"limits\": {\"cpu\": \"2\", \"memory\": \"4Gi\"},\n",
    "        },\n",
    "        # Variables d'environnement\n",
    "        env_vars={\n",
    "            \"SOURCE_PATH\": \"s3://bucket/raw/\",\n",
    "            \"DEST_PATH\": \"s3://bucket/processed/\",\n",
    "        },\n",
    "        # Secrets\n",
    "        secrets=[\n",
    "            {\"secret\": \"aws-credentials\", \"key\": \"AWS_ACCESS_KEY_ID\", \"env\": \"AWS_ACCESS_KEY_ID\"},\n",
    "            {\"secret\": \"aws-credentials\", \"key\": \"AWS_SECRET_ACCESS_KEY\", \"env\": \"AWS_SECRET_ACCESS_KEY\"},\n",
    "        ],\n",
    "        # Configuration K8s\n",
    "        is_delete_operator_pod=True,\n",
    "        get_logs=True,\n",
    "        startup_timeout_seconds=300,\n",
    "        # Affinity/Tolerations\n",
    "        affinity={\n",
    "            \"nodeAffinity\": {\n",
    "                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n",
    "                    \"nodeSelectorTerms\": [{\n",
    "                        \"matchExpressions\": [{\n",
    "                            \"key\": \"node-type\",\n",
    "                            \"operator\": \"In\",\n",
    "                            \"values\": [\"compute\"]\n",
    "                        }]\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # T√¢che avec image Python custom\n",
    "    python_job = KubernetesPodOperator(\n",
    "        task_id=\"run_python_transform\",\n",
    "        name=\"python-transform\",\n",
    "        namespace=\"airflow\",\n",
    "        image=\"my-registry/python-etl:2.0\",\n",
    "        cmds=[\"python\", \"/app/transform.py\"],\n",
    "        container_resources={\n",
    "            \"requests\": {\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n",
    "            \"limits\": {\"cpu\": \"1\", \"memory\": \"2Gi\"},\n",
    "        },\n",
    "        is_delete_operator_pod=True,\n",
    "        get_logs=True,\n",
    "    )\n",
    "    \n",
    "    spark_job >> python_job\n",
    "```\n",
    "\n",
    "### pod_template_file : Personnalisation avanc√©e\n",
    "\n",
    "Pour des configurations complexes, utiliser un template YAML :\n",
    "\n",
    "```yaml\n",
    "# pod_template.yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: airflow-worker\n",
    "  labels:\n",
    "    app: airflow-worker\n",
    "spec:\n",
    "  serviceAccountName: airflow-worker\n",
    "  containers:\n",
    "    - name: base\n",
    "      image: apache/airflow:2.8.0-python3.11\n",
    "      imagePullPolicy: IfNotPresent\n",
    "      env:\n",
    "        - name: AIRFLOW__CORE__EXECUTOR\n",
    "          value: \"LocalExecutor\"\n",
    "      resources:\n",
    "        requests:\n",
    "          cpu: \"500m\"\n",
    "          memory: \"512Mi\"\n",
    "        limits:\n",
    "          cpu: \"1000m\"\n",
    "          memory: \"1Gi\"\n",
    "      volumeMounts:\n",
    "        - name: dags\n",
    "          mountPath: /opt/airflow/dags\n",
    "          readOnly: true\n",
    "  volumes:\n",
    "    - name: dags\n",
    "      persistentVolumeClaim:\n",
    "        claimName: airflow-dags\n",
    "  restartPolicy: Never\n",
    "  securityContext:\n",
    "    runAsUser: 50000\n",
    "    fsGroup: 50000\n",
    "```\n",
    "\n",
    "```python\n",
    "# Dans le DAG, r√©f√©rencer le template\n",
    "from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n",
    "\n",
    "task = KubernetesPodOperator(\n",
    "    task_id=\"task_with_template\",\n",
    "    name=\"custom-task\",\n",
    "    namespace=\"airflow\",\n",
    "    pod_template_file=\"/opt/airflow/pod_templates/pod_template.yaml\",\n",
    "    # Override l'image du template\n",
    "    image=\"my-custom-image:1.0\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taskflow_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TaskFlow API\n",
    "\n",
    "> La **TaskFlow API** (introduite dans Airflow 2.0) permet d'√©crire des DAGs de mani√®re **Pythonic** avec des d√©corateurs `@dag` et `@task`, en g√©rant automatiquement les XComs.\n",
    "\n",
    "### Pourquoi TaskFlow ?\n",
    "\n",
    "| Approche Traditionnelle | TaskFlow API |\n",
    "|-----------------------|---------------|\n",
    "| `PythonOperator(python_callable=fn)` | `@task` sur la fonction |\n",
    "| `xcom_push` / `xcom_pull` manuels | Passage de donn√©es automatique |\n",
    "| Verbeux | Concis et lisible |\n",
    "| D√©pendances explicites `>>` | D√©pendances implicites par appel |\n",
    "\n",
    "### Exemple comparatif\n",
    "\n",
    "**Avant (approche traditionnelle) :**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "def extract(**context):\n",
    "    data = {\"users\": 100, \"orders\": 500}\n",
    "    context['ti'].xcom_push(key='extracted_data', value=data)\n",
    "    return data\n",
    "\n",
    "def transform(**context):\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract_task', key='extracted_data')\n",
    "    transformed = {\"total\": data['users'] + data['orders']}\n",
    "    ti.xcom_push(key='transformed_data', value=transformed)\n",
    "    return transformed\n",
    "\n",
    "def load(**context):\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='transform_task', key='transformed_data')\n",
    "    print(f\"Loading: {data}\")\n",
    "\n",
    "with DAG(\n",
    "    dag_id='traditional_etl',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    ") as dag:\n",
    "    \n",
    "    extract_task = PythonOperator(\n",
    "        task_id='extract_task',\n",
    "        python_callable=extract,\n",
    "    )\n",
    "    \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_task',\n",
    "        python_callable=transform,\n",
    "    )\n",
    "    \n",
    "    load_task = PythonOperator(\n",
    "        task_id='load_task',\n",
    "        python_callable=load,\n",
    "    )\n",
    "    \n",
    "    extract_task >> transform_task >> load_task\n",
    "```\n",
    "\n",
    "**Apr√®s (TaskFlow API) :**\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='taskflow_etl',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def taskflow_etl():\n",
    "    \n",
    "    @task\n",
    "    def extract() -> dict:\n",
    "        return {\"users\": 100, \"orders\": 500}\n",
    "    \n",
    "    @task\n",
    "    def transform(data: dict) -> dict:\n",
    "        return {\"total\": data['users'] + data['orders']}\n",
    "    \n",
    "    @task\n",
    "    def load(data: dict):\n",
    "        print(f\"Loading: {data}\")\n",
    "    \n",
    "    # D√©pendances implicites par appel de fonction !\n",
    "    raw_data = extract()\n",
    "    transformed_data = transform(raw_data)\n",
    "    load(transformed_data)\n",
    "\n",
    "# Instancier le DAG\n",
    "taskflow_etl()\n",
    "```\n",
    "\n",
    "### Avantages TaskFlow\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **Code Pythonic** | Ressemble √† du Python standard |\n",
    "| **XCom automatique** | Les retours de fonction sont automatiquement pass√©s |\n",
    "| **Type hints** | Support des annotations de type |\n",
    "| **Moins de boilerplate** | Pas de `PythonOperator` explicite |\n",
    "| **D√©pendances claires** | Le flux de donn√©es d√©finit les d√©pendances |\n",
    "\n",
    "### TaskFlow avec plusieurs outputs\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='taskflow_multiple_outputs',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def etl_pipeline():\n",
    "    \n",
    "    @task(multiple_outputs=True)\n",
    "    def extract() -> dict:\n",
    "        \"\"\"Retourne plusieurs valeurs nomm√©es\"\"\"\n",
    "        return {\n",
    "            \"users\": [{\"id\": 1, \"name\": \"Alice\"}],\n",
    "            \"orders\": [{\"id\": 101, \"amount\": 99.99}],\n",
    "            \"metadata\": {\"source\": \"api\", \"timestamp\": \"2024-01-01\"}\n",
    "        }\n",
    "    \n",
    "    @task\n",
    "    def process_users(users: list) -> list:\n",
    "        return [u['name'].upper() for u in users]\n",
    "    \n",
    "    @task\n",
    "    def process_orders(orders: list) -> float:\n",
    "        return sum(o['amount'] for o in orders)\n",
    "    \n",
    "    @task\n",
    "    def combine(users: list, total: float, metadata: dict):\n",
    "        print(f\"Users: {users}\")\n",
    "        print(f\"Total orders: {total}\")\n",
    "        print(f\"Source: {metadata['source']}\")\n",
    "    \n",
    "    # Extraire les donn√©es\n",
    "    data = extract()\n",
    "    \n",
    "    # Acc√©der aux outputs individuels\n",
    "    processed_users = process_users(data['users'])\n",
    "    orders_total = process_orders(data['orders'])\n",
    "    \n",
    "    # Combiner\n",
    "    combine(processed_users, orders_total, data['metadata'])\n",
    "\n",
    "etl_pipeline()\n",
    "```\n",
    "\n",
    "### Mixing TaskFlow avec des Operators classiques\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='mixed_taskflow',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def mixed_pipeline():\n",
    "    \n",
    "    # TaskFlow task\n",
    "    @task\n",
    "    def prepare_config() -> dict:\n",
    "        return {\"batch_size\": 1000, \"date\": \"2024-01-01\"}\n",
    "    \n",
    "    # Operator classique\n",
    "    run_spark = KubernetesPodOperator(\n",
    "        task_id=\"run_spark\",\n",
    "        name=\"spark-job\",\n",
    "        namespace=\"airflow\",\n",
    "        image=\"apache/spark:3.5.0\",\n",
    "        cmds=[\"spark-submit\", \"/app/job.py\"],\n",
    "    )\n",
    "    \n",
    "    # TaskFlow task qui d√©pend d'un Operator\n",
    "    @task\n",
    "    def validate_output():\n",
    "        print(\"Validating Spark output...\")\n",
    "        return True\n",
    "    \n",
    "    # Bash operator\n",
    "    notify = BashOperator(\n",
    "        task_id=\"notify\",\n",
    "        bash_command=\"echo 'Pipeline completed!'\",\n",
    "    )\n",
    "    \n",
    "    # D√©finir les d√©pendances\n",
    "    config = prepare_config()\n",
    "    config >> run_spark >> validate_output() >> notify\n",
    "\n",
    "mixed_pipeline()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic_mapping",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Dynamic Task Mapping\n",
    "\n",
    "> Le **Dynamic Task Mapping** (Airflow 2.3+) permet de cr√©er un nombre variable de t√¢ches **√† runtime**, bas√© sur les donn√©es.\n",
    "\n",
    "### Pourquoi Dynamic Mapping ?\n",
    "\n",
    "| Probl√®me | Solution |\n",
    "|----------|----------|\n",
    "| Nombre de fichiers inconnu √† l'avance | `.expand()` sur la liste de fichiers |\n",
    "| Traiter N partitions dynamiquement | Map sur les partitions |\n",
    "| Parall√©liser sur une liste variable | Cr√©er N t√¢ches automatiquement |\n",
    "\n",
    "### Sch√©ma Dynamic Mapping\n",
    "\n",
    "```text\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  list_files()   ‚îÇ\n",
    "                    ‚îÇ [f1, f2, f3, f4]‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                             ‚îÇ\n",
    "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "            ‚îÇ                ‚îÇ                ‚îÇ\n",
    "            ‚ñº                ‚ñº                ‚ñº\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚îÇ process(f1)   ‚îÇ ‚îÇ process(f2)   ‚îÇ ‚îÇ process(f3)   ‚îÇ ...\n",
    "    ‚îÇ   [mapped]    ‚îÇ ‚îÇ   [mapped]    ‚îÇ ‚îÇ   [mapped]    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "            ‚îÇ                 ‚îÇ                 ‚îÇ\n",
    "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                              ‚îÇ\n",
    "                              ‚ñº\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ   aggregate()   ‚îÇ\n",
    "                    ‚îÇ Combine results ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Exemple : Traiter des fichiers dynamiquement\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='dynamic_file_processing',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def process_files_dynamically():\n",
    "    \n",
    "    @task\n",
    "    def list_files() -> list[str]:\n",
    "        \"\"\"Liste les fichiers √† traiter (nombre variable)\"\"\"\n",
    "        # En r√©alit√© : lister depuis S3, GCS, etc.\n",
    "        return [\n",
    "            \"s3://bucket/data/file1.parquet\",\n",
    "            \"s3://bucket/data/file2.parquet\",\n",
    "            \"s3://bucket/data/file3.parquet\",\n",
    "            \"s3://bucket/data/file4.parquet\",\n",
    "        ]\n",
    "    \n",
    "    @task\n",
    "    def process_file(file_path: str) -> dict:\n",
    "        \"\"\"Traite UN fichier ‚Äî sera mapp√© dynamiquement\"\"\"\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        # Simuler le traitement\n",
    "        row_count = len(file_path) * 100  # Fake\n",
    "        return {\"file\": file_path, \"rows\": row_count}\n",
    "    \n",
    "    @task\n",
    "    def aggregate_results(results: list[dict]) -> dict:\n",
    "        \"\"\"Agr√®ge tous les r√©sultats\"\"\"\n",
    "        total_rows = sum(r['rows'] for r in results)\n",
    "        return {\n",
    "            \"files_processed\": len(results),\n",
    "            \"total_rows\": total_rows,\n",
    "        }\n",
    "    \n",
    "    # R√©cup√©rer la liste de fichiers\n",
    "    files = list_files()\n",
    "    \n",
    "    # üéØ DYNAMIC MAPPING : .expand() cr√©e N t√¢ches\n",
    "    processed = process_file.expand(file_path=files)\n",
    "    \n",
    "    # Agr√©ger les r√©sultats\n",
    "    aggregate_results(processed)\n",
    "\n",
    "process_files_dynamically()\n",
    "```\n",
    "\n",
    "### expand() avec plusieurs param√®tres\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='dynamic_multi_param',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def multi_param_mapping():\n",
    "    \n",
    "    @task\n",
    "    def get_partitions() -> list[dict]:\n",
    "        return [\n",
    "            {\"date\": \"2024-01-01\", \"region\": \"EU\"},\n",
    "            {\"date\": \"2024-01-01\", \"region\": \"US\"},\n",
    "            {\"date\": \"2024-01-02\", \"region\": \"EU\"},\n",
    "            {\"date\": \"2024-01-02\", \"region\": \"US\"},\n",
    "        ]\n",
    "    \n",
    "    @task\n",
    "    def process_partition(date: str, region: str) -> dict:\n",
    "        print(f\"Processing {region} for {date}\")\n",
    "        return {\"date\": date, \"region\": region, \"status\": \"done\"}\n",
    "    \n",
    "    partitions = get_partitions()\n",
    "    \n",
    "    # expand_kwargs pour mapper plusieurs param√®tres\n",
    "    process_partition.expand_kwargs(partitions)\n",
    "\n",
    "multi_param_mapping()\n",
    "```\n",
    "\n",
    "### Limiter le parall√©lisme\n",
    "\n",
    "```python\n",
    "@task(max_active_tis_per_dag=5)  # Max 5 instances en parall√®le\n",
    "def process_file(file_path: str) -> dict:\n",
    "    # ...\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Mapping sur un Operator (non-TaskFlow)\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id='dynamic_bash',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    ") as dag:\n",
    "    \n",
    "    # Liste statique (ou XCom d'une t√¢che pr√©c√©dente)\n",
    "    files = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n",
    "    \n",
    "    process = BashOperator.partial(\n",
    "        task_id=\"process_files\",\n",
    "    ).expand(\n",
    "        bash_command=[f\"python process.py {f}\" for f in files]\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparatif_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Airflow vs Dagster vs Prefect\n",
    "\n",
    "Il existe plusieurs orchestrateurs modernes. Voici une comparaison approfondie.\n",
    "\n",
    "### Vue d'ensemble\n",
    "\n",
    "| Crit√®re | Airflow | Dagster | Prefect |\n",
    "|---------|---------|---------|----------|\n",
    "| **Cr√©√© par** | Airbnb (2014) | Elementl (2018) | Prefect (2018) |\n",
    "| **Philosophie** | DAG-centric | Asset-centric | Flow-centric |\n",
    "| **Maturit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Communaut√©** | Tr√®s large | En croissance | En croissance |\n",
    "| **Adoption** | Standard industrie | Startups, ML | Startups, Data |\n",
    "| **Learning curve** | Moyenne | Plus raide | Plus douce |\n",
    "\n",
    "### Comparaison d√©taill√©e\n",
    "\n",
    "| Aspect | Airflow | Dagster | Prefect |\n",
    "|--------|---------|---------|----------|\n",
    "| **D√©finition** | DAGs Python | Assets + Ops | Flows + Tasks |\n",
    "| **Scheduling** | Cron-like | Cron + Sensors | Cron + Events |\n",
    "| **Data Lineage** | Via OpenLineage | Natif (Assets) | Via int√©grations |\n",
    "| **Testing** | Difficile | Excellent | Bon |\n",
    "| **Type checking** | Non natif | Natif (I/O types) | Pydantic |\n",
    "| **UI** | Fonctionnelle | Moderne | Moderne |\n",
    "| **Local dev** | Complexe | Excellent | Excellent |\n",
    "| **Cloud offering** | MWAA, Composer, Astronomer | Dagster Cloud | Prefect Cloud |\n",
    "\n",
    "### Exemples de code compar√©s\n",
    "\n",
    "**M√™me pipeline dans les 3 outils :**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "airflow_example",
   "metadata": {},
   "source": [
    "### Airflow (TaskFlow)\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='etl_pipeline',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def etl_pipeline():\n",
    "    \n",
    "    @task\n",
    "    def extract() -> dict:\n",
    "        return {\"data\": [1, 2, 3, 4, 5]}\n",
    "    \n",
    "    @task\n",
    "    def transform(raw: dict) -> dict:\n",
    "        return {\"data\": [x * 2 for x in raw['data']]}\n",
    "    \n",
    "    @task\n",
    "    def load(transformed: dict):\n",
    "        print(f\"Loading: {transformed}\")\n",
    "    \n",
    "    raw = extract()\n",
    "    transformed = transform(raw)\n",
    "    load(transformed)\n",
    "\n",
    "etl_pipeline()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dagster_example",
   "metadata": {},
   "source": [
    "### Dagster (Asset-based)\n",
    "\n",
    "```python\n",
    "from dagster import asset, Definitions, define_asset_job\n",
    "\n",
    "@asset\n",
    "def raw_data() -> dict:\n",
    "    \"\"\"Asset: donn√©es brutes\"\"\"\n",
    "    return {\"data\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "@asset\n",
    "def transformed_data(raw_data: dict) -> dict:\n",
    "    \"\"\"Asset: donn√©es transform√©es (d√©pend de raw_data)\"\"\"\n",
    "    return {\"data\": [x * 2 for x in raw_data['data']]}\n",
    "\n",
    "@asset\n",
    "def loaded_data(transformed_data: dict) -> None:\n",
    "    \"\"\"Asset: donn√©es charg√©es\"\"\"\n",
    "    print(f\"Loading: {transformed_data}\")\n",
    "\n",
    "# Job pour ex√©cuter tous les assets\n",
    "etl_job = define_asset_job(\"etl_job\", selection=\"*\")\n",
    "\n",
    "# D√©finitions Dagster\n",
    "defs = Definitions(\n",
    "    assets=[raw_data, transformed_data, loaded_data],\n",
    "    jobs=[etl_job],\n",
    ")\n",
    "```\n",
    "\n",
    "**Dagster : Concepts cl√©s**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Asset** | Objet de donn√©es persistant (table, fichier) |\n",
    "| **Op** | Unit√© de calcul (comme un Operator) |\n",
    "| **Graph** | Composition d'Ops |\n",
    "| **Job** | Graph ex√©cutable avec config |\n",
    "| **Resource** | Connexion externe (DB, S3) |\n",
    "| **I/O Manager** | G√®re la persistance des assets |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prefect_example",
   "metadata": {},
   "source": [
    "### Prefect (Flow-based)\n",
    "\n",
    "```python\n",
    "from prefect import flow, task\n",
    "\n",
    "@task\n",
    "def extract() -> dict:\n",
    "    return {\"data\": [1, 2, 3, 4, 5]}\n",
    "\n",
    "@task\n",
    "def transform(raw: dict) -> dict:\n",
    "    return {\"data\": [x * 2 for x in raw['data']]}\n",
    "\n",
    "@task\n",
    "def load(transformed: dict):\n",
    "    print(f\"Loading: {transformed}\")\n",
    "\n",
    "@flow(name=\"ETL Pipeline\")\n",
    "def etl_pipeline():\n",
    "    raw = extract()\n",
    "    transformed = transform(raw)\n",
    "    load(transformed)\n",
    "\n",
    "# Ex√©cuter\n",
    "if __name__ == \"__main__\":\n",
    "    etl_pipeline()\n",
    "```\n",
    "\n",
    "**Prefect : Concepts cl√©s**\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Flow** | Pipeline de t√¢ches |\n",
    "| **Task** | Unit√© de travail |\n",
    "| **Deployment** | Flow d√©ploy√© et schedulable |\n",
    "| **Work Pool** | Groupe de workers |\n",
    "| **Block** | Credentials et configs r√©utilisables |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quand_utiliser",
   "metadata": {},
   "source": [
    "### Quand utiliser quoi ?\n",
    "\n",
    "| Sc√©nario | Recommandation | Pourquoi |\n",
    "|----------|---------------|----------|\n",
    "| Grande entreprise, √©quipe mature | **Airflow** | Standard, tr√®s document√©, √©cosyst√®me large |\n",
    "| Pipelines ML avec assets | **Dagster** | Asset-centric, excellent testing, types |\n",
    "| Startup, it√©ration rapide | **Prefect** | Simple, moderne, local dev facile |\n",
    "| D√©j√† sur Airflow | **Rester sur Airflow** | Migration co√ªteuse, TaskFlow moderne |\n",
    "| Nouveau projet, √©quipe data | **Dagster** ou **Prefect** | Approches modernes |\n",
    "| K8s natif requis | **Airflow** ou **Dagster** | KubernetesExecutor mature |\n",
    "\n",
    "### Tableau de d√©cision\n",
    "\n",
    "```text\n",
    "                            Complexit√© du pipeline\n",
    "                     Faible ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ √âlev√©e\n",
    "                        ‚îÇ                          ‚îÇ\n",
    "    Taille √©quipe       ‚îÇ                          ‚îÇ\n",
    "         ‚îÇ              ‚îÇ                          ‚îÇ\n",
    "     Petite  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Prefect    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Dagster\n",
    "         ‚îÇ              ‚îÇ                          ‚îÇ\n",
    "         ‚îÇ              ‚îÇ                          ‚îÇ\n",
    "    Grande  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ Prefect    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Airflow\n",
    "         ‚îÇ              ‚îÇ    ou Dagster            ‚îÇ\n",
    "         ‚ñº              ‚îÇ                          ‚îÇ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "openlineage_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. OpenLineage ‚Äî Data Lineage\n",
    "\n",
    "> **OpenLineage** est un standard ouvert pour collecter et partager les m√©tadonn√©es de lineage des pipelines data. Il r√©pond √† la question : \"D'o√π viennent mes donn√©es et o√π vont-elles ?\"\n",
    "\n",
    "### Pourquoi le Data Lineage ?\n",
    "\n",
    "| Question | Lineage r√©pond |\n",
    "|----------|----------------|\n",
    "| D'o√π viennent les donn√©es de ce dashboard ? | ‚úÖ Tra√ßabilit√© amont |\n",
    "| Si cette table change, quoi d'autre est impact√© ? | ‚úÖ Impact analysis |\n",
    "| Ce job a √©chou√©, quelles donn√©es sont corrompues ? | ‚úÖ Root cause analysis |\n",
    "| Sommes-nous conformes RGPD ? | ‚úÖ Audit et gouvernance |\n",
    "\n",
    "### Sch√©ma OpenLineage\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         OPENLINEAGE ECOSYSTEM                               ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ   ‚îÇ   Airflow   ‚îÇ   ‚îÇ   Spark     ‚îÇ   ‚îÇ    dbt      ‚îÇ   ‚îÇ   Flink     ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îÇ  + OL Plugin‚îÇ   ‚îÇ  + OL Lib   ‚îÇ   ‚îÇ  + OL Plugin‚îÇ   ‚îÇ  + OL Lib   ‚îÇ    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "‚îÇ          ‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ            ‚îÇ\n",
    "‚îÇ          ‚îÇ    OpenLineage Events (JSON)      ‚îÇ                 ‚îÇ            ‚îÇ\n",
    "‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                                        ‚îÇ\n",
    "‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ\n",
    "‚îÇ                    ‚îÇ     OpenLineage Backend       ‚îÇ                       ‚îÇ\n",
    "‚îÇ                    ‚îÇ  (Marquez, Atlan, DataHub...) ‚îÇ                       ‚îÇ\n",
    "‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                                        ‚îÇ\n",
    "‚îÇ                                    ‚ñº                                        ‚îÇ\n",
    "‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ\n",
    "‚îÇ                    ‚îÇ    Lineage Visualization      ‚îÇ                       ‚îÇ\n",
    "‚îÇ                    ‚îÇ    Impact Analysis            ‚îÇ                       ‚îÇ\n",
    "‚îÇ                    ‚îÇ    Data Catalog               ‚îÇ                       ‚îÇ\n",
    "‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Concepts OpenLineage\n",
    "\n",
    "| Concept | Description | Exemple |\n",
    "|---------|-------------|----------|\n",
    "| **Job** | Pipeline ou transformation | DAG Airflow, job Spark |\n",
    "| **Run** | Ex√©cution d'un job | DAG run avec ID unique |\n",
    "| **Dataset** | Source ou destination de donn√©es | Table SQL, fichier S3 |\n",
    "| **Facet** | M√©tadonn√©es additionnelles | Schema, stats, owner |\n",
    "\n",
    "### Activer OpenLineage dans Airflow\n",
    "\n",
    "```bash\n",
    "# Installer le provider\n",
    "pip install apache-airflow-providers-openlineage\n",
    "```\n",
    "\n",
    "```python\n",
    "# airflow.cfg ou variables d'environnement\n",
    "[openlineage]\n",
    "transport = '{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\n",
    "namespace = \"my_airflow_instance\"\n",
    "```\n",
    "\n",
    "Ou via environnement :\n",
    "\n",
    "```bash\n",
    "export AIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\n",
    "export AIRFLOW__OPENLINEAGE__NAMESPACE=\"my_airflow_instance\"\n",
    "```\n",
    "\n",
    "### Marquez : Backend OpenLineage\n",
    "\n",
    "**Marquez** est le backend de r√©f√©rence pour OpenLineage (open-source par WeWork/Linux Foundation).\n",
    "\n",
    "```bash\n",
    "# D√©ployer Marquez avec Docker Compose\n",
    "git clone https://github.com/MarquezProject/marquez.git\n",
    "cd marquez\n",
    "docker-compose up -d\n",
    "\n",
    "# UI disponible sur http://localhost:3000\n",
    "# API sur http://localhost:5000\n",
    "```\n",
    "\n",
    "### DAG avec lineage explicite\n",
    "\n",
    "```python\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.lineage.entities import Table, File\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='etl_with_lineage',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def etl_with_lineage():\n",
    "    \n",
    "    @task(\n",
    "        inlets=[File(url=\"s3://bucket/raw/\")],\n",
    "        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"staging_orders\")],\n",
    "    )\n",
    "    def extract_and_stage():\n",
    "        \"\"\"Extract depuis S3 et charge dans staging\"\"\"\n",
    "        print(\"Extracting from S3 to staging...\")\n",
    "    \n",
    "    @task(\n",
    "        inlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"staging_orders\")],\n",
    "        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"fact_orders\")],\n",
    "    )\n",
    "    def transform_to_fact():\n",
    "        \"\"\"Transforme staging en fact table\"\"\"\n",
    "        print(\"Transforming to fact table...\")\n",
    "    \n",
    "    extract_and_stage() >> transform_to_fact()\n",
    "\n",
    "etl_with_lineage()\n",
    "```\n",
    "\n",
    "### Lineage automatique avec certains Operators\n",
    "\n",
    "Certains operators extraient automatiquement le lineage :\n",
    "\n",
    "| Operator | Lineage auto |\n",
    "|----------|-------------|\n",
    "| `SnowflakeOperator` | ‚úÖ (parse SQL) |\n",
    "| `BigQueryOperator` | ‚úÖ |\n",
    "| `PostgresOperator` | ‚úÖ |\n",
    "| `SparkSubmitOperator` | ‚úÖ (avec Spark OL) |\n",
    "| `PythonOperator` | ‚ùå (manuel) |\n",
    "\n",
    "### Alternatives √† Marquez\n",
    "\n",
    "| Outil | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| **Marquez** | Open-source | Backend de r√©f√©rence |\n",
    "| **DataHub** | Open-source | Catalogue + lineage (LinkedIn) |\n",
    "| **Atlan** | SaaS | Plateforme data catalog compl√®te |\n",
    "| **Collibra** | Enterprise | Gouvernance et lineage |\n",
    "| **Alation** | Enterprise | Data catalog |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "astronomer_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Astronomer ‚Äî Airflow Enterprise\n",
    "\n",
    "> **Astronomer** est la plateforme enterprise pour Apache Airflow. Elle fournit des outils, un hosting manag√©, et des SDKs pour industrialiser Airflow.\n",
    "\n",
    "### Pourquoi Astronomer ?\n",
    "\n",
    "| Probl√®me | Solution Astronomer |\n",
    "|----------|--------------------|\n",
    "| Installer/maintenir Airflow est complexe | Astro Cloud (fully managed) |\n",
    "| Dev local difficile | Astro CLI (environment identique) |\n",
    "| √âcrire du code Airflow verbeux | Astro SDK (API simplifi√©e) |\n",
    "| Pas de support | Support enterprise 24/7 |\n",
    "\n",
    "### √âcosyst√®me Astronomer\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                         ASTRONOMER ECOSYSTEM                                ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ   ‚îÇ                        Astro Cloud                                   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Airflow fully managed                                            ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Auto-scaling                                                      ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Observability int√©gr√©e                                           ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ CI/CD int√©gr√©                                                    ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ\n",
    "‚îÇ   ‚îÇ      Astro CLI        ‚îÇ   ‚îÇ      Astro SDK        ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Dev local          ‚îÇ   ‚îÇ  ‚Ä¢ API Pythonic       ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Deploy en 1 cmd    ‚îÇ   ‚îÇ  ‚Ä¢ SQL/Python tasks   ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Tests int√©gr√©s     ‚îÇ   ‚îÇ  ‚Ä¢ Data quality       ‚îÇ                    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ   ‚îÇ                    Astronomer Registry                               ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Providers certifi√©s                                               ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ DAGs de r√©f√©rence                                                 ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ  ‚Ä¢ Documentation enrichie                                            ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Astro CLI\n",
    "\n",
    "L'outil en ligne de commande pour d√©velopper et d√©ployer Airflow.\n",
    "\n",
    "```bash\n",
    "# Installation\n",
    "# macOS\n",
    "brew install astro\n",
    "\n",
    "# Linux\n",
    "curl -sSL install.astronomer.io | sudo bash -s\n",
    "\n",
    "# Windows (WSL)\n",
    "curl -sSL install.astronomer.io | sudo bash -s\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Cr√©er un nouveau projet Airflow\n",
    "mkdir my-airflow-project && cd my-airflow-project\n",
    "astro dev init\n",
    "\n",
    "# Structure g√©n√©r√©e :\n",
    "# .\n",
    "# ‚îú‚îÄ‚îÄ dags/                 # Tes DAGs\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ example_dag.py\n",
    "# ‚îú‚îÄ‚îÄ include/              # Fichiers additionnels\n",
    "# ‚îú‚îÄ‚îÄ plugins/              # Plugins custom\n",
    "# ‚îú‚îÄ‚îÄ tests/                # Tests\n",
    "# ‚îú‚îÄ‚îÄ Dockerfile            # Image Airflow custom\n",
    "# ‚îú‚îÄ‚îÄ packages.txt          # Packages syst√®me\n",
    "# ‚îú‚îÄ‚îÄ requirements.txt      # D√©pendances Python\n",
    "# ‚îî‚îÄ‚îÄ airflow_settings.yaml # Variables, connexions\n",
    "\n",
    "# D√©marrer l'environnement local\n",
    "astro dev start\n",
    "# UI sur http://localhost:8080 (admin/admin)\n",
    "\n",
    "# Voir les logs\n",
    "astro dev logs\n",
    "\n",
    "# Ex√©cuter les tests\n",
    "astro dev pytest\n",
    "\n",
    "# Parser les DAGs (v√©rifier les erreurs)\n",
    "astro dev parse\n",
    "\n",
    "# Arr√™ter\n",
    "astro dev stop\n",
    "\n",
    "# D√©ployer sur Astro Cloud\n",
    "astro deploy\n",
    "```\n",
    "\n",
    "### Astro SDK\n",
    "\n",
    "Une API Python simplifi√©e pour √©crire des DAGs, particuli√®rement pour les op√©rations SQL/dataframe.\n",
    "\n",
    "```bash\n",
    "pip install astro-sdk-python\n",
    "```\n",
    "\n",
    "**Exemple : ETL SQL simplifi√©**\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "from airflow.decorators import dag\n",
    "from astro import sql as aql\n",
    "from astro.files import File\n",
    "from astro.sql.table import Table\n",
    "\n",
    "@dag(\n",
    "    dag_id='astro_sdk_etl',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    ")\n",
    "def astro_sdk_etl():\n",
    "    \n",
    "    # Charger un CSV dans une table\n",
    "    raw_orders = aql.load_file(\n",
    "        input_file=File(\"s3://bucket/orders.csv\"),\n",
    "        output_table=Table(\n",
    "            name=\"raw_orders\",\n",
    "            conn_id=\"postgres_conn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Transformation SQL (inline)\n",
    "    @aql.transform\n",
    "    def transform_orders(input_table: Table):\n",
    "        return \"\"\"\n",
    "            SELECT \n",
    "                order_id,\n",
    "                customer_id,\n",
    "                order_date,\n",
    "                amount,\n",
    "                amount * 1.2 as amount_with_tax\n",
    "            FROM {{ input_table }}\n",
    "            WHERE amount > 0\n",
    "        \"\"\"\n",
    "    \n",
    "    # Appliquer la transformation\n",
    "    transformed = transform_orders(\n",
    "        input_table=raw_orders,\n",
    "        output_table=Table(\n",
    "            name=\"transformed_orders\",\n",
    "            conn_id=\"postgres_conn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Exporter vers S3\n",
    "    aql.export_to_file(\n",
    "        input_data=transformed,\n",
    "        output_file=File(\"s3://bucket/processed/orders.parquet\"),\n",
    "        if_exists=\"replace\",\n",
    "    )\n",
    "    \n",
    "    # Nettoyer les tables temporaires\n",
    "    aql.cleanup()\n",
    "\n",
    "astro_sdk_etl()\n",
    "```\n",
    "\n",
    "### Fonctionnalit√©s Astro SDK\n",
    "\n",
    "| Fonction | Description |\n",
    "|----------|-------------|\n",
    "| `aql.load_file()` | Charger CSV/Parquet/JSON dans une table |\n",
    "| `aql.transform()` | SQL transformation avec Jinja |\n",
    "| `aql.run_raw_sql()` | Ex√©cuter du SQL brut |\n",
    "| `aql.export_to_file()` | Exporter une table vers fichier |\n",
    "| `aql.merge()` | Merge/Upsert entre tables |\n",
    "| `aql.append()` | Append data to table |\n",
    "| `aql.dataframe()` | Transformer avec Pandas |\n",
    "| `aql.cleanup()` | Supprimer les tables temporaires |\n",
    "\n",
    "### Astro SDK : Transformation DataFrame\n",
    "\n",
    "```python\n",
    "from astro import sql as aql\n",
    "from astro.sql.table import Table\n",
    "import pandas as pd\n",
    "\n",
    "@aql.dataframe\n",
    "def process_with_pandas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Transformation avec Pandas\"\"\"\n",
    "    df['total'] = df['quantity'] * df['price']\n",
    "    df['processed_at'] = pd.Timestamp.now()\n",
    "    return df\n",
    "\n",
    "# Dans le DAG :\n",
    "processed = process_with_pandas(\n",
    "    df=some_table,\n",
    "    output_table=Table(name=\"processed\", conn_id=\"postgres\"),\n",
    ")\n",
    "```\n",
    "\n",
    "### Astro Cloud vs Self-Hosted\n",
    "\n",
    "| Aspect | Astro Cloud | Self-Hosted (Helm) |\n",
    "|--------|-------------|-------------------|\n",
    "| **Setup** | Minutes | Heures/jours |\n",
    "| **Maintenance** | Astronomer | Ton √©quipe |\n",
    "| **Scaling** | Automatique | Manuel |\n",
    "| **Updates** | Automatiques | Manuels |\n",
    "| **Co√ªt** | Par usage | Infrastructure |\n",
    "| **Contr√¥le** | Moyen | Total |\n",
    "| **S√©curit√©** | SOC 2, HIPAA | √Ä impl√©menter |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : DAG TaskFlow avec Dynamic Mapping\n",
    "\n",
    "**Objectif** : Cr√©er un DAG qui traite dynamiquement une liste de fichiers.\n",
    "\n",
    "**Instructions** :\n",
    "1. Cr√©er une t√¢che `list_files()` qui retourne une liste de chemins\n",
    "2. Cr√©er une t√¢che `process_file(path)` mapp√©e dynamiquement\n",
    "3. Cr√©er une t√¢che `report(results)` qui agr√®ge les r√©sultats\n",
    "4. Limiter √† 3 t√¢ches parall√®les maximum\n",
    "\n",
    "<details>\n",
    "<summary>üí° Indice</summary>\n",
    "\n",
    "Utilise `@task(max_active_tis_per_dag=3)` et `.expand()`\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 2 : KubernetesPodOperator avec Spark\n",
    "\n",
    "**Objectif** : Cr√©er un DAG qui ex√©cute un job Spark via KubernetesPodOperator.\n",
    "\n",
    "**Instructions** :\n",
    "1. Utiliser l'image `apache/spark:3.5.0`\n",
    "2. Configurer les ressources : 2 CPU, 4Gi RAM\n",
    "3. Passer des variables d'environnement pour la config\n",
    "4. Utiliser une node affinity pour cibler les nodes `compute`\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 3 : Comparatif Airflow/Dagster/Prefect\n",
    "\n",
    "**Objectif** : Impl√©menter le m√™me pipeline simple dans les 3 outils.\n",
    "\n",
    "**Pipeline** :\n",
    "1. Lire un fichier JSON\n",
    "2. Filtrer les enregistrements (amount > 100)\n",
    "3. Calculer une somme\n",
    "4. √âcrire le r√©sultat\n",
    "\n",
    "**Comparer** :\n",
    "- Nombre de lignes de code\n",
    "- Facilit√© de test local\n",
    "- Lisibilit√©\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 4 : OpenLineage avec Marquez\n",
    "\n",
    "**Objectif** : Configurer OpenLineage et visualiser le lineage.\n",
    "\n",
    "**Instructions** :\n",
    "1. D√©ployer Marquez en local (Docker Compose)\n",
    "2. Configurer Airflow pour envoyer les events √† Marquez\n",
    "3. Cr√©er un DAG avec `inlets` et `outlets` explicites\n",
    "4. Visualiser le lineage dans l'UI Marquez\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 5 : Astro CLI Project\n",
    "\n",
    "**Objectif** : Cr√©er un projet Airflow complet avec Astro CLI.\n",
    "\n",
    "**Instructions** :\n",
    "1. Initialiser un projet avec `astro dev init`\n",
    "2. Cr√©er un DAG utilisant Astro SDK\n",
    "3. Ajouter des tests dans `/tests`\n",
    "4. Valider avec `astro dev pytest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Mini-Projet : Pipeline Data Production-Ready\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Cr√©er un pipeline ETL **production-ready** d√©ploy√© sur Kubernetes avec monitoring et lineage.\n",
    "\n",
    "### Architecture cible\n",
    "\n",
    "```text\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                          MINI-PROJET M28                                    ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ   ‚îÇ                    Airflow sur Kubernetes                            ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ                                                                      ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ            DAG: data_pipeline_advanced                       ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ                                                              ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ Extract ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Transform  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ      Load       ‚îÇ     ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ (K8sPod)‚îÇ    ‚îÇ (TaskFlow)  ‚îÇ    ‚îÇ  (Astro SDK)    ‚îÇ     ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ                         ‚îÇ                                    ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ            Dynamic Mapping (N fichiers)                      ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ                                                                      ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îÇ  Scheduler  ‚îÇ   ‚îÇ  Webserver  ‚îÇ   ‚îÇ    Worker Pods (K8s)    ‚îÇ   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ                                    ‚îÇ                                        ‚îÇ\n",
    "‚îÇ                         OpenLineage Events                                  ‚îÇ\n",
    "‚îÇ                                    ‚ñº                                        ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ   ‚îÇ                         Marquez                                      ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ              (Data Lineage Visualization)                           ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Instructions\n",
    "\n",
    "**√âtape 1 : Setup Astro Project**\n",
    "\n",
    "```bash\n",
    "mkdir advanced-pipeline && cd advanced-pipeline\n",
    "astro dev init\n",
    "```\n",
    "\n",
    "**√âtape 2 : Cr√©er le DAG principal**\n",
    "\n",
    "Le DAG doit :\n",
    "- Lister des fichiers depuis S3 (simul√©)\n",
    "- Traiter chaque fichier avec Dynamic Mapping\n",
    "- Utiliser TaskFlow API\n",
    "- Avoir des inlets/outlets pour le lineage\n",
    "\n",
    "**√âtape 3 : Configurer OpenLineage**\n",
    "\n",
    "- Ajouter la config dans `airflow_settings.yaml`\n",
    "- V√©rifier que les events arrivent dans Marquez\n",
    "\n",
    "**√âtape 4 : Tests**\n",
    "\n",
    "- √âcrire des tests unitaires pour les t√¢ches\n",
    "- Valider avec `astro dev pytest`\n",
    "\n",
    "**√âtape 5 : D√©ployer (optionnel)**\n",
    "\n",
    "- D√©ployer sur un cluster K8s avec le Helm chart\n",
    "- Ou utiliser Astro Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project_solution",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution du mini-projet\n",
    "\n",
    "<details>\n",
    "<summary>üì• Afficher la solution compl√®te</summary>\n",
    "\n",
    "**1. Structure du projet Astro**\n",
    "```\n",
    "advanced-pipeline/\n",
    "‚îú‚îÄ‚îÄ dags/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ data_pipeline_advanced.py\n",
    "‚îú‚îÄ‚îÄ include/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ sql/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ transform.sql\n",
    "‚îú‚îÄ‚îÄ tests/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test_pipeline.py\n",
    "‚îú‚îÄ‚îÄ Dockerfile\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îî‚îÄ‚îÄ airflow_settings.yaml\n",
    "```\n",
    "\n",
    "**2. `requirements.txt`**\n",
    "```text\n",
    "astro-sdk-python>=1.5.0\n",
    "apache-airflow-providers-openlineage>=1.0.0\n",
    "apache-airflow-providers-cncf-kubernetes>=7.0.0\n",
    "pandas>=2.0.0\n",
    "```\n",
    "\n",
    "**3. `airflow_settings.yaml`**\n",
    "```yaml\n",
    "airflow:\n",
    "  connections:\n",
    "    - conn_id: postgres_conn\n",
    "      conn_type: postgres\n",
    "      host: postgres\n",
    "      login: airflow\n",
    "      password: airflow\n",
    "      schema: airflow\n",
    "      port: 5432\n",
    "\n",
    "  variables:\n",
    "    - variable_name: data_bucket\n",
    "      variable_value: s3://my-data-bucket\n",
    "\n",
    "  # OpenLineage config\n",
    "  openlineage:\n",
    "    transport: '{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\n",
    "    namespace: advanced-pipeline\n",
    "```\n",
    "\n",
    "**4. `dags/data_pipeline_advanced.py`**\n",
    "```python\n",
    "from datetime import datetime\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n",
    "from airflow.lineage.entities import File, Table\n",
    "from astro import sql as aql\n",
    "from astro.sql.table import Table as AstroTable\n",
    "import pandas as pd\n",
    "\n",
    "@dag(\n",
    "    dag_id='data_pipeline_advanced',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    schedule='@daily',\n",
    "    catchup=False,\n",
    "    tags=['production', 'etl'],\n",
    "    doc_md=\"\"\"## Pipeline Data Avanc√©\n",
    "    \n",
    "    Ce pipeline:\n",
    "    1. Liste les fichiers √† traiter\n",
    "    2. Extrait chaque fichier (Dynamic Mapping)\n",
    "    3. Transforme avec Astro SDK\n",
    "    4. Charge dans PostgreSQL\n",
    "    \"\"\",\n",
    ")\n",
    "def data_pipeline_advanced():\n",
    "    \n",
    "    # --- √âTAPE 1 : Lister les fichiers ---\n",
    "    @task\n",
    "    def list_files() -> list[str]:\n",
    "        \"\"\"Liste les fichiers √† traiter (simul√©)\"\"\"\n",
    "        # En production: lister depuis S3\n",
    "        return [\n",
    "            \"orders_2024_01.parquet\",\n",
    "            \"orders_2024_02.parquet\",\n",
    "            \"orders_2024_03.parquet\",\n",
    "        ]\n",
    "    \n",
    "    # --- √âTAPE 2 : Extraire chaque fichier ---\n",
    "    @task(\n",
    "        max_active_tis_per_dag=3,\n",
    "        inlets=[File(url=\"s3://bucket/raw/\")],\n",
    "    )\n",
    "    def extract_file(file_name: str) -> dict:\n",
    "        \"\"\"Extrait un fichier (simul√©)\"\"\"\n",
    "        print(f\"Extracting: {file_name}\")\n",
    "        # Simuler des donn√©es\n",
    "        return {\n",
    "            \"file\": file_name,\n",
    "            \"rows\": 1000,\n",
    "            \"data\": [\n",
    "                {\"order_id\": 1, \"amount\": 100},\n",
    "                {\"order_id\": 2, \"amount\": 200},\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    # --- √âTAPE 3 : Transformer avec Pandas ---\n",
    "    @task(\n",
    "        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"orders_processed\")],\n",
    "    )\n",
    "    def transform_data(extracted_list: list[dict]) -> list[dict]:\n",
    "        \"\"\"Agr√®ge et transforme les donn√©es\"\"\"\n",
    "        all_data = []\n",
    "        for extracted in extracted_list:\n",
    "            for record in extracted['data']:\n",
    "                record['source_file'] = extracted['file']\n",
    "                record['amount_with_tax'] = record['amount'] * 1.2\n",
    "                all_data.append(record)\n",
    "        \n",
    "        print(f\"Transformed {len(all_data)} records\")\n",
    "        return all_data\n",
    "    \n",
    "    # --- √âTAPE 4 : Charger dans PostgreSQL ---\n",
    "    @aql.dataframe\n",
    "    def load_to_postgres(data: list[dict]) -> pd.DataFrame:\n",
    "        \"\"\"Charge dans PostgreSQL via Astro SDK\"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        df['loaded_at'] = pd.Timestamp.now()\n",
    "        return df\n",
    "    \n",
    "    # --- √âTAPE 5 : Rapport final ---\n",
    "    @task\n",
    "    def generate_report(row_count: int):\n",
    "        \"\"\"G√©n√®re un rapport\"\"\"\n",
    "        print(f\"Pipeline completed. Total rows: {row_count}\")\n",
    "    \n",
    "    # --- ORCHESTRATION ---\n",
    "    files = list_files()\n",
    "    \n",
    "    # Dynamic mapping sur les fichiers\n",
    "    extracted = extract_file.expand(file_name=files)\n",
    "    \n",
    "    # Transformer\n",
    "    transformed = transform_data(extracted)\n",
    "    \n",
    "    # Charger\n",
    "    loaded = load_to_postgres(\n",
    "        data=transformed,\n",
    "        output_table=AstroTable(\n",
    "            name=\"orders_processed\",\n",
    "            conn_id=\"postgres_conn\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Rapport\n",
    "    generate_report(len(transformed))\n",
    "    \n",
    "    # Cleanup Astro SDK\n",
    "    aql.cleanup()\n",
    "\n",
    "# Instancier le DAG\n",
    "data_pipeline_advanced()\n",
    "```\n",
    "\n",
    "**5. `tests/test_pipeline.py`**\n",
    "```python\n",
    "import pytest\n",
    "from dags.data_pipeline_advanced import data_pipeline_advanced\n",
    "\n",
    "def test_dag_loads():\n",
    "    \"\"\"Test que le DAG se charge sans erreur\"\"\"\n",
    "    dag = data_pipeline_advanced()\n",
    "    assert dag is not None\n",
    "    assert dag.dag_id == \"data_pipeline_advanced\"\n",
    "\n",
    "def test_dag_has_expected_tasks():\n",
    "    \"\"\"Test que le DAG a les bonnes t√¢ches\"\"\"\n",
    "    dag = data_pipeline_advanced()\n",
    "    task_ids = [t.task_id for t in dag.tasks]\n",
    "    \n",
    "    assert \"list_files\" in task_ids\n",
    "    assert \"transform_data\" in task_ids\n",
    "\n",
    "def test_extract_file():\n",
    "    \"\"\"Test unitaire de la fonction extract\"\"\"\n",
    "    # Import direct de la fonction\n",
    "    from dags.data_pipeline_advanced import extract_file\n",
    "    \n",
    "    # Appeler la fonction wrapped\n",
    "    result = extract_file.function(\"test.parquet\")\n",
    "    \n",
    "    assert \"file\" in result\n",
    "    assert \"rows\" in result\n",
    "    assert result[\"file\"] == \"test.parquet\"\n",
    "```\n",
    "\n",
    "**6. Commandes pour tester**\n",
    "```bash\n",
    "# D√©marrer l'environnement\n",
    "astro dev start\n",
    "\n",
    "# Parser les DAGs\n",
    "astro dev parse\n",
    "\n",
    "# Lancer les tests\n",
    "astro dev pytest\n",
    "\n",
    "# Voir l'UI\n",
    "open http://localhost:8080\n",
    "\n",
    "# D√©clencher le DAG\n",
    "astro dev run dags trigger data_pipeline_advanced\n",
    "```\n",
    "\n",
    "**7. `docker-compose.override.yml` (pour Marquez)**\n",
    "```yaml\n",
    "version: \"3\"\n",
    "services:\n",
    "  marquez:\n",
    "    image: marquezproject/marquez:latest\n",
    "    ports:\n",
    "      - \"5000:5000\"\n",
    "      - \"5001:5001\"\n",
    "    environment:\n",
    "      - MARQUEZ_PORT=5000\n",
    "      - MARQUEZ_ADMIN_PORT=5001\n",
    "  \n",
    "  marquez-web:\n",
    "    image: marquezproject/marquez-web:latest\n",
    "    ports:\n",
    "      - \"3000:3000\"\n",
    "    environment:\n",
    "      - MARQUEZ_HOST=marquez\n",
    "      - MARQUEZ_PORT=5000\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Ressources pour aller plus loin\n",
    "\n",
    "### üåê Documentation officielle\n",
    "- [Apache Airflow Docs](https://airflow.apache.org/docs/) ‚Äî Documentation Airflow\n",
    "- [Airflow Helm Chart](https://airflow.apache.org/docs/helm-chart/stable/index.html) ‚Äî D√©ploiement K8s\n",
    "- [Dagster Docs](https://docs.dagster.io/) ‚Äî Documentation Dagster\n",
    "- [Prefect Docs](https://docs.prefect.io/) ‚Äî Documentation Prefect\n",
    "- [OpenLineage](https://openlineage.io/) ‚Äî Standard de lineage\n",
    "- [Astronomer Docs](https://docs.astronomer.io/) ‚Äî Documentation Astronomer\n",
    "\n",
    "### üéÆ Pratique\n",
    "- [Astronomer Academy](https://academy.astronomer.io/) ‚Äî Cours gratuits Airflow\n",
    "- [Astro CLI Quickstart](https://docs.astronomer.io/astro/cli/overview) ‚Äî D√©marrer avec Astro\n",
    "- [Marquez Demo](https://github.com/MarquezProject/marquez) ‚Äî Essayer OpenLineage\n",
    "\n",
    "### üìñ Livres & Articles\n",
    "- *Data Pipelines with Apache Airflow* ‚Äî Bas Harenslak & Julian de Ruiter\n",
    "- *Fundamentals of Data Engineering* ‚Äî Joe Reis & Matt Housley\n",
    "- [Astronomer Blog](https://www.astronomer.io/blog/) ‚Äî Best practices Airflow\n",
    "\n",
    "### üîß Outils\n",
    "- [Astronomer Registry](https://registry.astronomer.io/) ‚Äî Providers et DAGs\n",
    "- [Marquez](https://marquezproject.ai/) ‚Äî Backend OpenLineage\n",
    "- [DataHub](https://datahubproject.io/) ‚Äî Data catalog avec lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Prochaine √©tape\n",
    "\n",
    "Maintenant que tu ma√Ætrises l'orchestration avanc√©e, passons au **messaging distribu√©** !\n",
    "\n",
    "üëâ **Module suivant : `29_distributed_messaging`** ‚Äî Kafka avanc√©, RabbitMQ, Pulsar, Debezium\n",
    "\n",
    "Tu vas apprendre :\n",
    "- Kafka avanc√© (Quotas, Tiered Storage)\n",
    "- Alternatives : RabbitMQ, Apache Pulsar\n",
    "- Change Data Capture avec Debezium\n",
    "- Patterns de messaging distribu√©\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **F√©licitations !** Tu as termin√© le module Advanced Orchestration pour Data Engineers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
