{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸ“¨ Distributed Messaging pour Data Engineers\n",
    "\n",
    "Bienvenue dans ce module avancÃ© oÃ¹ tu vas maÃ®triser les **systÃ¨mes de messaging distribuÃ©s**. Tu apprendras les fonctionnalitÃ©s avancÃ©es de Kafka, les alternatives comme RabbitMQ et Pulsar, et le Change Data Capture avec Debezium â€” des compÃ©tences essentielles pour construire des architectures data temps rÃ©el !\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Avoir suivi le module `24_kafka_streaming` (Kafka, Spark SSS, Watermarks) |\n",
    "| âœ… Requis | MaÃ®triser topics, partitions, offsets, consumer groups |\n",
    "| âœ… Requis | ConnaÃ®tre kafka-python et confluent-kafka (producers/consumers) |\n",
    "| âœ… Requis | MaÃ®triser Spark Structured Streaming (readStream, writeStream, foreachBatch) |\n",
    "| âœ… Requis | Connaissances en Docker et Kubernetes (M14-M16, M27) |\n",
    "| âœ… Requis | Bases de donnÃ©es relationnelles et SQL (pour Debezium CDC) |\n",
    "| ğŸ’¡ RecommandÃ© | ExpÃ©rience avec des pipelines streaming en production |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Configurer les fonctionnalitÃ©s avancÃ©es de Kafka (Quotas, Tiered Storage, Transactions)\n",
    "- Comprendre et choisir entre Kafka, RabbitMQ et Pulsar\n",
    "- ImplÃ©menter le Change Data Capture complet avec Debezium\n",
    "- Concevoir des architectures de messaging robustes\n",
    "- GÃ©rer les patterns avancÃ©s : exactly-once, dead letter queues, event sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Rappel : Ce qu'on a vu en M24 vs Ce qu'on approfondit ici\n",
    "\n",
    "| Module M24 (Intermediate) | Ce module M29 (Advanced) |\n",
    "|---------------------------|-------------------------|\n",
    "| Architecture Lambda vs Kappa | â€” |\n",
    "| Topics, Partitions, Offsets, Consumer Groups | Tiered Storage, Quotas, Monitoring |\n",
    "| Producers / Consumers (kafka-python, confluent-kafka) | **Transactions Kafka, Exactly-Once (EOS)** |\n",
    "| Schema Registry basics (Avro) | **Schema Registry avancÃ©** (compatibilitÃ©, Ã©volution) |\n",
    "| Spark Structured Streaming complet | â€” |\n",
    "| Windowing, Watermarks, foreachBatch | â€” |\n",
    "| Faust (aperÃ§u) | â€” |\n",
    "| Debezium (mentionnÃ©) | **Debezium CDC en profondeur** |\n",
    "| â€” | **RabbitMQ** (alternative queue-based) |\n",
    "| â€” | **Apache Pulsar** (alternative multi-tenant) |\n",
    "| â€” | **Patterns** : DLQ, Saga, Event Sourcing, CQRS |\n",
    "\n",
    "### SchÃ©ma : Ã‰cosystÃ¨me Messaging\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    DISTRIBUTED MESSAGING LANDSCAPE                          â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\n",
    "â”‚   â”‚     KAFKA       â”‚   â”‚   RABBITMQ      â”‚   â”‚     PULSAR      â”‚          â”‚\n",
    "â”‚   â”‚  Log-based      â”‚   â”‚  Queue-based    â”‚   â”‚  Multi-tenant   â”‚          â”‚\n",
    "â”‚   â”‚  High throughputâ”‚   â”‚  Flexible routingâ”‚  â”‚  Geo-replicationâ”‚          â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\n",
    "â”‚            â”‚                    â”‚                     â”‚                     â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                                 â”‚                                           â”‚\n",
    "â”‚                                 â–¼                                           â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                        DEBEZIUM (CDC)                                â”‚  â”‚\n",
    "â”‚   â”‚   Capture changes from databases â†’ Stream to messaging systems      â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "messaging_info",
   "metadata": {},
   "source": [
    "> â„¹ï¸ **Le savais-tu ?**\n",
    ">\n",
    "> **Kafka** traite plus de **7 trillions de messages par jour** chez LinkedIn, son crÃ©ateur.\n",
    ">\n",
    "> **RabbitMQ** a Ã©tÃ© crÃ©Ã© en 2007 et implÃ©mente le protocole **AMQP**, un standard ouvert pour le messaging.\n",
    ">\n",
    "> **Apache Pulsar** a Ã©tÃ© dÃ©veloppÃ© par Yahoo! pour gÃ©rer leurs **100 milliards de messages quotidiens** avec une architecture multi-tenant native.\n",
    ">\n",
    "> **Debezium** (du latin \"from the beginning\") capture chaque changement depuis le dÃ©but du log de la base de donnÃ©es â€” c'est la base du **Change Data Capture**.\n",
    ">\n",
    "> ğŸ“– [Kafka at LinkedIn](https://engineering.linkedin.com/kafka)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kafka_advanced",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Kafka AvancÃ©\n",
    "\n",
    "Cette section couvre les fonctionnalitÃ©s avancÃ©es de Kafka pour la production Ã  grande Ã©chelle. Tu connais dÃ©jÃ  les bases (M24), on passe directement aux sujets avancÃ©s.\n",
    "\n",
    "### 1.1 Quotas et Throttling\n",
    "\n",
    "Les **quotas** permettent de limiter les ressources consommÃ©es par les clients pour Ã©viter qu'un client ne monopolise le cluster.\n",
    "\n",
    "#### Types de quotas\n",
    "\n",
    "| Quota | Description | UnitÃ© |\n",
    "|-------|-------------|-------|\n",
    "| **producer_byte_rate** | DÃ©bit max en Ã©criture | bytes/sec |\n",
    "| **consumer_byte_rate** | DÃ©bit max en lecture | bytes/sec |\n",
    "| **request_percentage** | % CPU du broker | % |\n",
    "| **controller_mutation_rate** | Taux de mutations (create/delete) | mutations/sec |\n",
    "\n",
    "#### Configurer les quotas\n",
    "\n",
    "```bash\n",
    "# Quota par user\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --alter --add-config 'producer_byte_rate=1048576,consumer_byte_rate=2097152' \\\n",
    "  --entity-type users --entity-name data-pipeline-user\n",
    "\n",
    "# Quota par client-id\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --alter --add-config 'producer_byte_rate=5242880' \\\n",
    "  --entity-type clients --entity-name etl-producer\n",
    "\n",
    "# Quota par user + client-id (plus spÃ©cifique)\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --alter --add-config 'producer_byte_rate=10485760' \\\n",
    "  --entity-type users --entity-name spark-user \\\n",
    "  --entity-type clients --entity-name spark-producer\n",
    "\n",
    "# Quota par dÃ©faut pour tous les users\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --alter --add-config 'producer_byte_rate=1048576' \\\n",
    "  --entity-type users --entity-default\n",
    "\n",
    "# Voir les quotas configurÃ©s\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --describe --entity-type users --entity-name data-pipeline-user\n",
    "```\n",
    "\n",
    "#### Quotas dans le code Python\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Le client doit spÃ©cifier son client.id pour Ãªtre identifiÃ© par les quotas\n",
    "producer = Producer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'client.id': 'etl-producer',  # Identifiant pour les quotas\n",
    "    'acks': 'all',\n",
    "})\n",
    "\n",
    "# Si le quota est dÃ©passÃ©, Kafka throttle automatiquement le client\n",
    "# Le producer recevra des dÃ©lais dans les rÃ©ponses\n",
    "```\n",
    "\n",
    "### 1.2 Tiered Storage (KIP-405)\n",
    "\n",
    "Le **Tiered Storage** permet de stocker les donnÃ©es anciennes sur un stockage moins cher (S3, GCS, Azure Blob) tout en gardant les donnÃ©es rÃ©centes sur disque local.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         TIERED STORAGE                                      â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                    LOCAL TIER (Hot Data)                             â”‚  â”‚\n",
    "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                           â”‚  â”‚\n",
    "â”‚   â”‚   â”‚Seg 5â”‚ â”‚Seg 6â”‚ â”‚Seg 7â”‚ â”‚Seg 8â”‚ â”‚Seg 9â”‚  â† DonnÃ©es rÃ©centes      â”‚  â”‚\n",
    "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    (SSD local, rapide)    â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                    â”‚                                        â”‚\n",
    "â”‚                        Offload automatique                                  â”‚\n",
    "â”‚                                    â–¼                                        â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                   REMOTE TIER (Cold Data)                            â”‚  â”‚\n",
    "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                                   â”‚  â”‚\n",
    "â”‚   â”‚   â”‚Seg 1â”‚ â”‚Seg 2â”‚ â”‚Seg 3â”‚ â”‚Seg 4â”‚  â† DonnÃ©es anciennes             â”‚  â”‚\n",
    "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    (S3/GCS/Azure, Ã©conomique)    â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### Configuration Tiered Storage\n",
    "\n",
    "```properties\n",
    "# server.properties (broker)\n",
    "\n",
    "# Activer le tiered storage\n",
    "remote.log.storage.system.enable=true\n",
    "\n",
    "# Plugin de stockage (exemple S3)\n",
    "remote.log.storage.manager.class.name=org.apache.kafka.tiered.storage.s3.S3RemoteStorageManager\n",
    "remote.log.storage.manager.class.path=/opt/kafka/plugins/tiered-storage-s3.jar\n",
    "\n",
    "# Configuration S3\n",
    "remote.log.storage.s3.bucket=my-kafka-tiered-storage\n",
    "remote.log.storage.s3.region=eu-west-1\n",
    "\n",
    "# RÃ©tention locale (donnÃ©es chaudes)\n",
    "local.retention.ms=86400000  # 1 jour en local\n",
    "\n",
    "# RÃ©tention totale (incluant remote)\n",
    "retention.ms=2592000000  # 30 jours au total\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Activer le tiered storage sur un topic existant\n",
    "kafka-configs.sh --bootstrap-server localhost:9092 \\\n",
    "  --alter --entity-type topics --entity-name events \\\n",
    "  --add-config 'remote.storage.enable=true,local.retention.ms=86400000,retention.ms=2592000000'\n",
    "```\n",
    "\n",
    "#### Avantages du Tiered Storage\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **CoÃ»t rÃ©duit** | Stockage S3 ~10x moins cher que SSD |\n",
    "| **RÃ©tention illimitÃ©e** | Garder des annÃ©es de donnÃ©es |\n",
    "| **Cluster plus petit** | Moins de disque local nÃ©cessaire |\n",
    "| **Replay facilitÃ©** | Relire des donnÃ©es anciennes pour reprocessing |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kafka_transactions",
   "metadata": {},
   "source": [
    "### 1.3 Transactions et Exactly-Once Semantics (EOS)\n",
    "\n",
    "En M24, tu as vu les **garanties de livraison** (at-most-once, at-least-once, exactly-once). Ici, on va **implÃ©menter** exactly-once avec les transactions Kafka.\n",
    "\n",
    "#### Rappel des garanties\n",
    "\n",
    "| Niveau | Description | Risque |\n",
    "|--------|-------------|--------|\n",
    "| **At-most-once** | Fire & forget | Perte de messages |\n",
    "| **At-least-once** | Retry jusqu'Ã  ACK | Doublons possibles |\n",
    "| **Exactly-once** | Transactions + idempotence | Aucun (mais plus complexe) |\n",
    "\n",
    "#### Producer Idempotent (pas de doublons)\n",
    "\n",
    "L'idempotence garantit qu'un message n'est Ã©crit qu'une seule fois mÃªme en cas de retry rÃ©seau.\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# Producer idempotent â€” PAS de duplicatas mÃªme avec retries\n",
    "producer = Producer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'enable.idempotence': True,  # â† Active l'idempotence\n",
    "    'acks': 'all',               # Requis pour idempotence\n",
    "    'retries': 2147483647,       # Retries infinis (best practice)\n",
    "    'max.in.flight.requests.per.connection': 5,  # Max 5 avec idempotence\n",
    "})\n",
    "\n",
    "# Comment Ã§a marche ?\n",
    "# 1. Le producer assigne un Producer ID (PID) et un Sequence Number Ã  chaque message\n",
    "# 2. Le broker dÃ©tecte les doublons en comparant (PID, Sequence)\n",
    "# 3. Si un retry envoie le mÃªme message, le broker le reconnaÃ®t et ignore le doublon\n",
    "```\n",
    "\n",
    "#### Transactions complÃ¨tes (multi-topics atomique)\n",
    "\n",
    "Les transactions permettent d'Ã©crire sur **plusieurs topics/partitions de maniÃ¨re atomique** : tout rÃ©ussit ou tout Ã©choue.\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Producer, KafkaException\n",
    "\n",
    "# Producer transactionnel\n",
    "producer = Producer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'transactional.id': 'etl-pipeline-001',  # ID unique et STABLE\n",
    "    'enable.idempotence': True,              # Implicitement activÃ©\n",
    "    'acks': 'all',\n",
    "})\n",
    "\n",
    "# Initialiser les transactions (une seule fois au dÃ©marrage)\n",
    "producer.init_transactions()\n",
    "\n",
    "try:\n",
    "    # DÃ©marrer une transaction\n",
    "    producer.begin_transaction()\n",
    "    \n",
    "    # Ã‰crire sur PLUSIEURS topics (atomique)\n",
    "    producer.produce('orders-processed', key='order-1', value='{\"status\": \"done\"}')\n",
    "    producer.produce('audit-log', key='order-1', value='{\"action\": \"order_processed\"}')\n",
    "    producer.produce('metrics', key='counter', value='{\"orders_processed\": 1}')\n",
    "    \n",
    "    # Commit la transaction â€” TOUT ou RIEN\n",
    "    producer.commit_transaction()\n",
    "    print(\"âœ… Transaction committed successfully\")\n",
    "    \n",
    "except KafkaException as e:\n",
    "    # Abort en cas d'erreur â€” aucun message n'est visible\n",
    "    producer.abort_transaction()\n",
    "    print(f\"âŒ Transaction aborted: {e}\")\n",
    "```\n",
    "\n",
    "#### Pattern Read-Process-Write (Exactly-Once complet)\n",
    "\n",
    "Le pattern le plus puissant : lire, traiter, Ã©crire, et commiter les offsets **dans la mÃªme transaction**.\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Consumer, Producer, KafkaException\n",
    "\n",
    "# Consumer avec isolation transactionnelle\n",
    "consumer = Consumer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'etl-group',\n",
    "    'isolation.level': 'read_committed',  # â† Ne lit que les messages committÃ©s\n",
    "    'enable.auto.commit': False,          # â† Commit manuel dans la transaction\n",
    "})\n",
    "\n",
    "producer = Producer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'transactional.id': 'etl-processor-001',\n",
    "    'enable.idempotence': True,\n",
    "})\n",
    "\n",
    "producer.init_transactions()\n",
    "consumer.subscribe(['raw-events'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # 1. DÃ©marrer la transaction\n",
    "        producer.begin_transaction()\n",
    "        \n",
    "        # 2. Traiter le message\n",
    "        processed = process_message(msg.value())\n",
    "        \n",
    "        # 3. Ã‰crire le rÃ©sultat\n",
    "        producer.produce('processed-events', value=processed)\n",
    "        \n",
    "        # 4. Commit les offsets DANS la transaction\n",
    "        producer.send_offsets_to_transaction(\n",
    "            consumer.position(consumer.assignment()),\n",
    "            consumer.consumer_group_metadata()\n",
    "        )\n",
    "        \n",
    "        # 5. Commit atomique : Ã©criture + offset ensemble\n",
    "        producer.commit_transaction()\n",
    "        \n",
    "    except Exception as e:\n",
    "        producer.abort_transaction()\n",
    "        print(f\"Transaction failed: {e}\")\n",
    "```\n",
    "\n",
    "### 1.4 Schema Registry AvancÃ©\n",
    "\n",
    "En M24, tu as vu les bases du Schema Registry avec Avro. Approfondissons les **modes de compatibilitÃ©** et l'**Ã©volution de schÃ©mas**.\n",
    "\n",
    "#### Modes de compatibilitÃ© dÃ©taillÃ©s\n",
    "\n",
    "| Mode | Nouveau consumer lit ancien | Ancien consumer lit nouveau | Changements autorisÃ©s |\n",
    "|------|---------------------------|---------------------------|----------------------|\n",
    "| **BACKWARD** | âœ… Oui | âŒ Non | Ajouter champs optionnels, supprimer champs |\n",
    "| **FORWARD** | âŒ Non | âœ… Oui | Ajouter champs, supprimer champs optionnels |\n",
    "| **FULL** | âœ… Oui | âœ… Oui | Ajouter/supprimer champs optionnels uniquement |\n",
    "| **NONE** | â€” | â€” | Tout (âš ï¸ dangereux en production) |\n",
    "\n",
    "```bash\n",
    "# Configurer la compatibilitÃ© globale\n",
    "curl -X PUT http://localhost:8081/config \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"compatibility\": \"BACKWARD\"}'\n",
    "\n",
    "# Configurer par sujet (override global)\n",
    "curl -X PUT http://localhost:8081/config/orders-value \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"compatibility\": \"FULL\"}'\n",
    "\n",
    "# Tester la compatibilitÃ© AVANT de publier un nouveau schÃ©ma\n",
    "curl -X POST http://localhost:8081/compatibility/subjects/orders-value/versions/latest \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"schema\": \"{...}\"}'\n",
    "# RÃ©ponse: {\"is_compatible\": true} ou {\"is_compatible\": false}\n",
    "```\n",
    "\n",
    "#### Ã‰volution de schÃ©ma â€” Exemple pratique\n",
    "\n",
    "```python\n",
    "# Version 1 du schÃ©ma\n",
    "schema_v1 = '''\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Order\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"order_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"amount\", \"type\": \"double\"}\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Version 2 â€” Ajouter un champ optionnel (BACKWARD compatible)\n",
    "schema_v2 = '''\n",
    "{\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"Order\",\n",
    "  \"fields\": [\n",
    "    {\"name\": \"order_id\", \"type\": \"string\"},\n",
    "    {\"name\": \"amount\", \"type\": \"double\"},\n",
    "    {\"name\": \"currency\", \"type\": \"string\", \"default\": \"EUR\"}\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "# âœ… Les nouveaux consumers peuvent lire les anciens messages (currency = \"EUR\" par dÃ©faut)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rabbitmq_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. RabbitMQ â€” L'Alternative Queue-Based\n",
    "\n",
    "> **RabbitMQ** est un message broker traditionnel implÃ©mentant le protocole AMQP. Contrairement Ã  Kafka (log-based), RabbitMQ est **queue-based** avec un routage flexible.\n",
    "\n",
    "### Kafka vs RabbitMQ â€” Comparaison dÃ©taillÃ©e\n",
    "\n",
    "| Aspect | Kafka | RabbitMQ |\n",
    "|--------|-------|----------|\n",
    "| **ModÃ¨le** | Log distribuÃ© (append-only) | Message queue (FIFO) |\n",
    "| **Persistance** | Toujours sur disque | Optionnelle (mÃ©moire ou disque) |\n",
    "| **Ordre** | Garanti par partition | Garanti par queue |\n",
    "| **Replay** | âœ… Natif (offsets) | âŒ Messages supprimÃ©s aprÃ¨s ACK |\n",
    "| **Routage** | Topics + Partitions | Exchanges (fanout, direct, topic, headers) |\n",
    "| **Throughput** | TrÃ¨s Ã©levÃ© (millions/sec) | Ã‰levÃ© (dizaines de milliers/sec) |\n",
    "| **Latence** | Millisecondes | Sub-milliseconde |\n",
    "| **Use case principal** | Event streaming, analytics | Task queues, RPC, notifications |\n",
    "\n",
    "### Architecture RabbitMQ\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         RABBITMQ ARCHITECTURE                               â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Producer â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚\n",
    "â”‚                    â”‚                                                        â”‚\n",
    "â”‚                    â–¼                                                        â”‚\n",
    "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚\n",
    "â”‚            â”‚   EXCHANGE    â”‚  â† Routing logic (type: fanout/direct/topic)  â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚\n",
    "â”‚                    â”‚ Bindings (routing rules)                               â”‚\n",
    "â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚        â”‚           â”‚           â”‚                                            â”‚\n",
    "â”‚        â–¼           â–¼           â–¼                                            â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
    "â”‚   â”‚ Queue 1 â”‚ â”‚ Queue 2 â”‚ â”‚ Queue 3 â”‚  â† Messages stockÃ©s ici              â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                       â”‚\n",
    "â”‚        â”‚           â”‚           â”‚                                            â”‚\n",
    "â”‚        â–¼           â–¼           â–¼                                            â”‚\n",
    "â”‚   Consumer 1   Consumer 2   Consumer 3                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Types d'Exchanges\n",
    "\n",
    "| Exchange | Routage | Use case |\n",
    "|----------|---------|----------|\n",
    "| **Direct** | routing_key exact match | Logs par niveau (errorâ†’error_queue) |\n",
    "| **Fanout** | Broadcast Ã  toutes les queues liÃ©es | Notifications, cache invalidation |\n",
    "| **Topic** | Pattern matching (*.error, logs.#) | Logs multi-critÃ¨res (app.module.level) |\n",
    "| **Headers** | Match sur headers du message | Routage complexe multi-attributs |\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Docker\n",
    "docker run -d --name rabbitmq \\\n",
    "  -p 5672:5672 \\\n",
    "  -p 15672:15672 \\\n",
    "  rabbitmq:3-management\n",
    "\n",
    "# Management UI: http://localhost:15672 (guest/guest)\n",
    "```\n",
    "\n",
    "### Producer Python (pika)\n",
    "\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Connexion\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# DÃ©clarer un exchange de type topic\n",
    "channel.exchange_declare(exchange='data_events', exchange_type='topic', durable=True)\n",
    "\n",
    "# Publier un message\n",
    "message = {'event_type': 'order_created', 'order_id': 'ORD-001', 'amount': 99.99}\n",
    "\n",
    "channel.basic_publish(\n",
    "    exchange='data_events',\n",
    "    routing_key='orders.created',\n",
    "    body=json.dumps(message),\n",
    "    properties=pika.BasicProperties(delivery_mode=2, content_type='application/json')\n",
    ")\n",
    "\n",
    "connection.close()\n",
    "```\n",
    "\n",
    "### Consumer Python\n",
    "\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "channel.queue_declare(queue='order_processor', durable=True)\n",
    "channel.queue_bind(exchange='data_events', queue='order_processor', routing_key='orders.*')\n",
    "\n",
    "def callback(ch, method, properties, body):\n",
    "    message = json.loads(body)\n",
    "    print(f\"Received: {message}\")\n",
    "    ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "\n",
    "channel.basic_qos(prefetch_count=1)\n",
    "channel.basic_consume(queue='order_processor', on_message_callback=callback)\n",
    "channel.start_consuming()\n",
    "```\n",
    "\n",
    "### Quand utiliser RabbitMQ vs Kafka ?\n",
    "\n",
    "| âœ… RabbitMQ | âœ… Kafka |\n",
    "|-------------|----------|\n",
    "| Task queues (jobs async) | Event streaming temps rÃ©el |\n",
    "| RPC (request/reply) | Log aggregation |\n",
    "| Routage complexe (exchanges) | Replay de donnÃ©es historiques |\n",
    "| Faible latence critique (<1ms) | TrÃ¨s haut throughput (millions/sec) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pulsar_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Apache Pulsar â€” Le Challenger Multi-Tenant\n",
    "\n",
    "> **Apache Pulsar** combine les avantages de Kafka (log-based) et RabbitMQ (queuing) avec une architecture cloud-native.\n",
    "\n",
    "### Kafka vs Pulsar\n",
    "\n",
    "| Aspect | Kafka | Pulsar |\n",
    "|--------|-------|--------|\n",
    "| **Architecture** | Brokers = Storage + Compute | SÃ©paration Brokers / BookKeeper |\n",
    "| **Multi-tenancy** | LimitÃ© | Natif (tenants, namespaces) |\n",
    "| **Geo-replication** | MirrorMaker (externe) | Natif et synchrone |\n",
    "| **Queuing** | Non natif | Natif (shared subscriptions) |\n",
    "\n",
    "### Types de Subscriptions Pulsar\n",
    "\n",
    "| Type | Description | Ã‰quivalent |\n",
    "|------|-------------|------------|\n",
    "| **Exclusive** | 1 seul consumer | Kafka standard |\n",
    "| **Failover** | Failover automatique | â€” |\n",
    "| **Shared** | Load balanced (round-robin) | RabbitMQ |\n",
    "| **Key_Shared** | Ordre par clÃ© | Kafka partitions |\n",
    "\n",
    "### Producer/Consumer Python\n",
    "\n",
    "```python\n",
    "import pulsar\n",
    "import json\n",
    "\n",
    "client = pulsar.Client('pulsar://localhost:6650')\n",
    "\n",
    "# Producer\n",
    "producer = client.create_producer('persistent://public/default/orders')\n",
    "producer.send(json.dumps({'order_id': 'ORD-001'}).encode('utf-8'))\n",
    "\n",
    "# Consumer (shared = load balanced)\n",
    "consumer = client.subscribe(\n",
    "    'persistent://public/default/orders',\n",
    "    subscription_name='order-processor',\n",
    "    consumer_type=pulsar.ConsumerType.Shared\n",
    ")\n",
    "\n",
    "msg = consumer.receive()\n",
    "print(json.loads(msg.data()))\n",
    "consumer.acknowledge(msg)\n",
    "client.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debezium_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Debezium â€” Change Data Capture en Profondeur\n",
    "\n",
    "> **Debezium** est une plateforme open-source de **Change Data Capture (CDC)** qui capture les changements dans les bases de donnÃ©es et les streame vers Kafka.\n",
    "\n",
    "En M24, Debezium Ã©tait mentionnÃ© dans le quiz. Ici, on l'implÃ©mente en profondeur.\n",
    "\n",
    "### Pourquoi le CDC ?\n",
    "\n",
    "| Approche traditionnelle | CDC avec Debezium |\n",
    "|------------------------|-------------------|\n",
    "| Batch ETL (SELECT * toutes les heures) | Streaming temps rÃ©el |\n",
    "| Query la DB source (charge CPU/IO) | Lit le transaction log (lÃ©ger) |\n",
    "| DÃ©tection des DELETEs difficile | Capture TOUS les changements |\n",
    "| Latence Ã©levÃ©e (heures) | Latence sub-seconde |\n",
    "\n",
    "### Architecture Debezium\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         DEBEZIUM ARCHITECTURE                               â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                           â”‚\n",
    "â”‚   â”‚  PostgreSQL â”‚     Transaction Log (WAL)                                 â”‚\n",
    "â”‚   â”‚   (source)  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                                        â”‚\n",
    "â”‚                                    â–¼                                        â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚   â”‚    MySQL    â”‚â”€â”€â”€â–¶â”‚   KAFKA CONNECT     â”‚â”€â”€â”€â–¶â”‚   KAFKA TOPICS      â”‚    â”‚\n",
    "â”‚   â”‚   (source)  â”‚    â”‚  + Debezium         â”‚    â”‚  (change events)    â”‚    â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    Connectors       â”‚    â”‚                     â”‚    â”‚\n",
    "â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ dbserver.schema  â”‚    â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚    .table           â”‚    â”‚\n",
    "â”‚   â”‚   MongoDB   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚   â”‚   (source)  â”‚                                         â”‚                â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â–¼                â”‚\n",
    "â”‚                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\n",
    "â”‚                                                â”‚    Consumers        â”‚     â”‚\n",
    "â”‚                                                â”‚  â€¢ Data Warehouse   â”‚     â”‚\n",
    "â”‚                                                â”‚  â€¢ Elasticsearch    â”‚     â”‚\n",
    "â”‚                                                â”‚  â€¢ Microservices    â”‚     â”‚\n",
    "â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Connecteurs Debezium\n",
    "\n",
    "| Base de donnÃ©es | MÃ©thode de capture | MaturitÃ© |\n",
    "|-----------------|--------------------|---------|\n",
    "| **PostgreSQL** | Logical replication (pgoutput) | â­â­â­â­â­ |\n",
    "| **MySQL/MariaDB** | Binary log (binlog) | â­â­â­â­â­ |\n",
    "| **MongoDB** | Oplog / Change Streams | â­â­â­â­â­ |\n",
    "| **SQL Server** | CDC tables | â­â­â­â­ |\n",
    "| **Oracle** | LogMiner | â­â­â­â­ |\n",
    "\n",
    "### Docker Compose complet\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    depends_on: [zookeeper]\n",
    "    ports: [\"9092:9092\"]\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n",
    "\n",
    "  postgres:\n",
    "    image: postgres:15\n",
    "    ports: [\"5432:5432\"]\n",
    "    environment:\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "      POSTGRES_DB: source_db\n",
    "    command: [\"postgres\", \"-c\", \"wal_level=logical\"]  # CRUCIAL\n",
    "\n",
    "  connect:\n",
    "    image: debezium/connect:2.5\n",
    "    depends_on: [kafka, postgres]\n",
    "    ports: [\"8083:8083\"]\n",
    "    environment:\n",
    "      BOOTSTRAP_SERVERS: kafka:29092\n",
    "      GROUP_ID: debezium-connect\n",
    "      CONFIG_STORAGE_TOPIC: connect_configs\n",
    "      OFFSET_STORAGE_TOPIC: connect_offsets\n",
    "      STATUS_STORAGE_TOPIC: connect_statuses\n",
    "```\n",
    "\n",
    "### Enregistrer le connecteur\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8083/connectors \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"name\": \"postgres-connector\",\n",
    "    \"config\": {\n",
    "      \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n",
    "      \"database.hostname\": \"postgres\",\n",
    "      \"database.port\": \"5432\",\n",
    "      \"database.user\": \"postgres\",\n",
    "      \"database.password\": \"postgres\",\n",
    "      \"database.dbname\": \"source_db\",\n",
    "      \"topic.prefix\": \"cdc\",\n",
    "      \"table.include.list\": \"public.orders\",\n",
    "      \"plugin.name\": \"pgoutput\",\n",
    "      \"slot.name\": \"debezium_slot\",\n",
    "      \"transforms\": \"unwrap\",\n",
    "      \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n",
    "      \"transforms.unwrap.drop.tombstones\": \"false\",\n",
    "      \"transforms.unwrap.delete.handling.mode\": \"rewrite\",\n",
    "      \"transforms.unwrap.add.fields\": \"op,source.ts_ms\"\n",
    "    }\n",
    "  }'\n",
    "```\n",
    "\n",
    "### Format des messages Debezium\n",
    "\n",
    "| Champ | Description |\n",
    "|-------|-------------|\n",
    "| **before** | Ã‰tat AVANT le changement (null pour INSERT) |\n",
    "| **after** | Ã‰tat APRÃˆS le changement (null pour DELETE) |\n",
    "| **op** | OpÃ©ration : `c`=create, `u`=update, `d`=delete, `r`=read (snapshot) |\n",
    "| **source** | MÃ©tadonnÃ©es (table, transaction ID, LSN) |\n",
    "\n",
    "### Consumer Python CDC\n",
    "\n",
    "```python\n",
    "from confluent_kafka import Consumer\n",
    "import json\n",
    "\n",
    "consumer = Consumer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'cdc-processor',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "})\n",
    "consumer.subscribe(['cdc.public.orders'])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    \n",
    "    event = json.loads(msg.value())\n",
    "    op = event.get('__op')\n",
    "    data = {k: v for k, v in event.items() if not k.startswith('__')}\n",
    "    \n",
    "    if op in ('c', 'r'):  # INSERT ou SNAPSHOT\n",
    "        print(f\"INSERT: {data}\")\n",
    "    elif op == 'u':  # UPDATE\n",
    "        print(f\"UPDATE: {data}\")\n",
    "    elif op == 'd':  # DELETE\n",
    "        print(f\"DELETE: id={data.get('id')}\")\n",
    "```\n",
    "\n",
    "### Outbox Pattern\n",
    "\n",
    "Le **Outbox Pattern** garantit la cohÃ©rence entre les mises Ã  jour DB et l'envoi d'Ã©vÃ©nements.\n",
    "\n",
    "```sql\n",
    "-- Table outbox\n",
    "CREATE TABLE outbox (\n",
    "    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "    aggregate_type VARCHAR(255) NOT NULL,\n",
    "    aggregate_id VARCHAR(255) NOT NULL,\n",
    "    event_type VARCHAR(255) NOT NULL,\n",
    "    payload JSONB NOT NULL,\n",
    "    created_at TIMESTAMP DEFAULT NOW()\n",
    ");\n",
    "\n",
    "-- Dans une TRANSACTION\n",
    "BEGIN;\n",
    "    UPDATE orders SET status = 'shipped' WHERE id = 1;\n",
    "    INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)\n",
    "    VALUES ('Order', '1', 'OrderShipped', '{\"order_id\": 1}');\n",
    "COMMIT;\n",
    "-- Debezium capture l'INSERT dans outbox â†’ Kafka\n",
    "```\n",
    "\n",
    "### Use Cases CDC\n",
    "\n",
    "| Use Case | Description |\n",
    "|----------|-------------|\n",
    "| **Sync Data Warehouse** | RÃ©plication temps rÃ©el vers Snowflake, BigQuery |\n",
    "| **Cache invalidation** | Invalider Redis quand la DB change |\n",
    "| **Search indexing** | Sync vers Elasticsearch |\n",
    "| **Microservices events** | Outbox pattern |\n",
    "| **Audit log** | Compliance, RGPD |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patterns_section",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Patterns de Messaging DistribuÃ©\n",
    "\n",
    "### 5.1 Dead Letter Queue (DLQ)\n",
    "\n",
    "```python\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    headers = dict(msg.headers() or [])\n",
    "    retry_count = int(headers.get('retry_count', b'0'))\n",
    "    \n",
    "    try:\n",
    "        process_message(msg.value())\n",
    "        consumer.commit(msg)\n",
    "    except Exception as e:\n",
    "        if retry_count >= MAX_RETRIES:\n",
    "            # Envoyer vers DLQ\n",
    "            producer.produce('orders-dlq', key=msg.key(), value=msg.value(),\n",
    "                           headers=[('error', str(e))])\n",
    "        else:\n",
    "            # Retry\n",
    "            producer.produce('orders-retry', key=msg.key(), value=msg.value(),\n",
    "                           headers=[('retry_count', str(retry_count + 1))])\n",
    "        consumer.commit(msg)\n",
    "```\n",
    "\n",
    "### 5.2 Saga Pattern\n",
    "\n",
    "```text\n",
    "Create Order â”€â”€â–¶ Reserve Stock â”€â”€â–¶ Process Payment â”€â”€â–¶ Ship Order\n",
    "      â”‚               â”‚                  â”‚\n",
    "      â–¼               â–¼                  â–¼\n",
    "Cancel Order â—€â”€â”€ Release Stock â—€â”€â”€ Refund Payment  (compensation)\n",
    "```\n",
    "\n",
    "### 5.3 Event Sourcing\n",
    "\n",
    "```python\n",
    "events = [\n",
    "    {'type': 'OrderCreated', 'order_id': 1, 'amount': 100},\n",
    "    {'type': 'PaymentReceived', 'order_id': 1},\n",
    "    {'type': 'OrderShipped', 'order_id': 1},\n",
    "]\n",
    "\n",
    "def rebuild_state(events):\n",
    "    state = {}\n",
    "    for e in events:\n",
    "        if e['type'] == 'OrderCreated':\n",
    "            state = {'id': e['order_id'], 'status': 'created'}\n",
    "        elif e['type'] == 'PaymentReceived':\n",
    "            state['status'] = 'paid'\n",
    "        elif e['type'] == 'OrderShipped':\n",
    "            state['status'] = 'shipped'\n",
    "    return state\n",
    "```\n",
    "\n",
    "### 5.4 CQRS\n",
    "\n",
    "```text\n",
    "Commands â”€â”€â–¶ Events (Kafka) â”€â”€â–¶ Projector â”€â”€â–¶ Read Model (optimized)\n",
    "   â”‚                                              â”‚\n",
    "   â–¼                                              â–¼\n",
    "Write DB                                      Query API\n",
    "(PostgreSQL)                                  (Elasticsearch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Kafka Transactions\n",
    "ImplÃ©menter un producer transactionnel qui Ã©crit sur 2 topics atomiquement.\n",
    "\n",
    "### Exercice 2 : RabbitMQ Task Queue\n",
    "CrÃ©er une task queue avec prioritÃ©s et DLQ.\n",
    "\n",
    "### Exercice 3 : Pipeline CDC Complet\n",
    "DÃ©ployer PostgreSQL + Kafka + Debezium et sync vers un data warehouse.\n",
    "\n",
    "### Exercice 4 : Comparatif Performance\n",
    "Comparer Kafka vs RabbitMQ (100K messages, throughput, latence).\n",
    "\n",
    "### Exercice 5 : Outbox Pattern\n",
    "ImplÃ©menter le pattern Outbox avec Debezium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "- [Apache Kafka Docs](https://kafka.apache.org/documentation/)\n",
    "- [RabbitMQ Docs](https://www.rabbitmq.com/documentation.html)\n",
    "- [Apache Pulsar Docs](https://pulsar.apache.org/docs/)\n",
    "- [Debezium Docs](https://debezium.io/documentation/)\n",
    "- *Kafka: The Definitive Guide* â€” Neha Narkhede\n",
    "- *Designing Data-Intensive Applications* â€” Martin Kleppmann\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `30_spark_scala_deep_dive`** â€” Spark & Scala Deep Dive\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le module Distributed Messaging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
