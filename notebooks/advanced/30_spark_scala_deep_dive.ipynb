{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ğŸš€ Spark & Scala Deep Dive\n",
    "\n",
    "Bienvenue dans ce module avancÃ© oÃ¹ tu vas maÃ®triser **Scala pour Spark** et comprendre les internals de Spark pour Ã©crire des jobs performants. Tu apprendras Ã  dÃ©velopper, builder et dÃ©ployer des applications Spark professionnelles.\n",
    "\n",
    "---\n",
    "\n",
    "## PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | PySpark DataFrame API (M19) |\n",
    "| âœ… Requis | Spark sur Kubernetes (M21) |\n",
    "| âœ… Requis | Notions de programmation fonctionnelle |\n",
    "| ğŸ’¡ RecommandÃ© | ExpÃ©rience avec un IDE (VS Code, PyCharm) |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce module, tu seras capable de :\n",
    "\n",
    "- Ã‰crire du code Scala idiomatique pour Spark\n",
    "- Utiliser les ADT (Algebraic Data Types) et Either/Try pour des pipelines robustes\n",
    "- Configurer un environnement de dÃ©veloppement complet (Notebook + IntelliJ)\n",
    "- Builder et dÃ©ployer des applications Spark avec sbt et spark-submit\n",
    "- Comprendre Catalyst, AQE et Tungsten pour optimiser tes jobs\n",
    "- Diagnostiquer et rÃ©soudre les problÃ¨mes de performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scala_intro",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Scala pour Data Engineers\n",
    "\n",
    "### 1.1 Pourquoi Scala pour Spark ?\n",
    "\n",
    "| Aspect | Scala | Python |\n",
    "|--------|-------|--------|\n",
    "| **Performance** | Natif JVM, pas de sÃ©rialisation | SÃ©rialisation Python â†” JVM |\n",
    "| **Typage** | Statique, erreurs Ã  la compilation | Dynamique, erreurs au runtime |\n",
    "| **API Spark** | API native, toutes les features | Wrapper, parfois en retard |\n",
    "| **Ã‰cosystÃ¨me** | Kafka, Flink, Akka | ML, Data Science |\n",
    "| **Courbe d'apprentissage** | Plus raide | Plus accessible |\n",
    "\n",
    "**RÃ¨gle pratique** :\n",
    "- **Python** : Exploration, prototypage, Data Science, petits pipelines\n",
    "- **Scala** : Production, gros volumes, performance critique, Ã©quipe backend Java/Scala\n",
    "\n",
    "### 1.2 Syntaxe Essentielle\n",
    "\n",
    "```scala\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Variables : val (immutable) vs var (mutable)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "val name: String = \"Spark\"     // Immutable (prÃ©fÃ©rÃ©)\n",
    "var counter: Int = 0           // Mutable (Ã  Ã©viter)\n",
    "val inferred = 42              // Type infÃ©rÃ© automatiquement\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Fonctions\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "// Fonction classique\n",
    "def add(a: Int, b: Int): Int = {\n",
    "  a + b\n",
    "}\n",
    "\n",
    "// Fonction one-liner (return implicite)\n",
    "def multiply(a: Int, b: Int): Int = a * b\n",
    "\n",
    "// Fonction anonyme (lambda)\n",
    "val double = (x: Int) => x * 2\n",
    "\n",
    "// ParamÃ¨tres par dÃ©faut\n",
    "def greet(name: String, greeting: String = \"Hello\"): String = \n",
    "  s\"$greeting, $name!\"\n",
    "\n",
    "greet(\"Alice\")              // \"Hello, Alice!\"\n",
    "greet(\"Bob\", \"Bonjour\")     // \"Bonjour, Bob!\"\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// String interpolation\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "val version = 3.5\n",
    "println(s\"Spark version: $version\")           // Simple\n",
    "println(s\"Next version: ${version + 0.1}\")    // Expression\n",
    "println(f\"Pi = ${math.Pi}%.2f\")               // FormatÃ©\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Conditions et boucles\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "// if est une expression (retourne une valeur)\n",
    "val status = if (counter > 0) \"positive\" else \"zero or negative\"\n",
    "\n",
    "// for-comprehension\n",
    "val squares = for (i <- 1 to 5) yield i * i  // Vector(1, 4, 9, 16, 25)\n",
    "\n",
    "// for avec filtres\n",
    "val evenSquares = for {\n",
    "  i <- 1 to 10\n",
    "  if i % 2 == 0\n",
    "} yield i * i  // Vector(4, 16, 36, 64, 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scala_collections",
   "metadata": {},
   "source": [
    "### 1.3 Collections Fonctionnelles\n",
    "\n",
    "Les collections Scala sont la base pour comprendre les transformations Spark (RDD, DataFrame).\n",
    "\n",
    "```scala\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Types de collections\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "val list = List(1, 2, 3, 4, 5)           // Immutable, linked list\n",
    "val vector = Vector(1, 2, 3, 4, 5)       // Immutable, indexed\n",
    "val set = Set(1, 2, 3, 3, 3)             // Immutable, unique: Set(1, 2, 3)\n",
    "val map = Map(\"a\" -> 1, \"b\" -> 2)        // Immutable, key-value\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Transformations (comme Spark !)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "val numbers = List(1, 2, 3, 4, 5)\n",
    "\n",
    "// map : transformer chaque Ã©lÃ©ment\n",
    "numbers.map(x => x * 2)              // List(2, 4, 6, 8, 10)\n",
    "numbers.map(_ * 2)                   // Syntaxe courte\n",
    "\n",
    "// filter : garder les Ã©lÃ©ments qui matchent\n",
    "numbers.filter(x => x > 2)           // List(3, 4, 5)\n",
    "numbers.filter(_ > 2)                // Syntaxe courte\n",
    "\n",
    "// flatMap : map + flatten\n",
    "val words = List(\"hello world\", \"scala spark\")\n",
    "words.flatMap(_.split(\" \"))          // List(\"hello\", \"world\", \"scala\", \"spark\")\n",
    "\n",
    "// reduce : agrÃ©ger en une valeur\n",
    "numbers.reduce((a, b) => a + b)      // 15\n",
    "numbers.reduce(_ + _)                // Syntaxe courte\n",
    "\n",
    "// fold : reduce avec valeur initiale\n",
    "numbers.fold(0)(_ + _)               // 15\n",
    "numbers.fold(10)(_ + _)              // 25 (commence Ã  10)\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// ChaÃ®nage (comme les pipelines Spark)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "val result = numbers\n",
    "  .filter(_ % 2 == 0)    // Garder les pairs\n",
    "  .map(_ * 10)           // Multiplier par 10\n",
    "  .sum                   // Sommer : 60 (20 + 40)\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// groupBy (comme Spark groupBy !)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "case class Sale(product: String, amount: Double)\n",
    "\n",
    "val sales = List(\n",
    "  Sale(\"laptop\", 1000),\n",
    "  Sale(\"phone\", 500),\n",
    "  Sale(\"laptop\", 1200),\n",
    "  Sale(\"phone\", 600)\n",
    ")\n",
    "\n",
    "val byProduct = sales.groupBy(_.product)\n",
    "// Map(\n",
    "//   \"laptop\" -> List(Sale(\"laptop\", 1000), Sale(\"laptop\", 1200)),\n",
    "//   \"phone\"  -> List(Sale(\"phone\", 500), Sale(\"phone\", 600))\n",
    "// )\n",
    "\n",
    "val totalByProduct = sales\n",
    "  .groupBy(_.product)\n",
    "  .map { case (product, sales) => \n",
    "    (product, sales.map(_.amount).sum) \n",
    "  }\n",
    "// Map(\"laptop\" -> 2200.0, \"phone\" -> 1100.0)\n",
    "```\n",
    "\n",
    "### 1.4 Option : GÃ©rer les valeurs absentes\n",
    "\n",
    "```scala\n",
    "// Option = Some(valeur) ou None (jamais null !)\n",
    "\n",
    "def findUser(id: Int): Option[String] = {\n",
    "  val users = Map(1 -> \"Alice\", 2 -> \"Bob\")\n",
    "  users.get(id)  // Retourne Option[String]\n",
    "}\n",
    "\n",
    "findUser(1)  // Some(\"Alice\")\n",
    "findUser(99) // None\n",
    "\n",
    "// Pattern matching\n",
    "findUser(1) match {\n",
    "  case Some(name) => println(s\"Found: $name\")\n",
    "  case None       => println(\"User not found\")\n",
    "}\n",
    "\n",
    "// getOrElse : valeur par dÃ©faut\n",
    "val user = findUser(99).getOrElse(\"Unknown\")\n",
    "\n",
    "// map sur Option (sÃ»r, pas d'exception)\n",
    "val upperName = findUser(1).map(_.toUpperCase)  // Some(\"ALICE\")\n",
    "val noName = findUser(99).map(_.toUpperCase)    // None (pas d'erreur !)\n",
    "\n",
    "// flatMap pour chaÃ®ner\n",
    "def findEmail(name: String): Option[String] = {\n",
    "  if (name == \"Alice\") Some(\"alice@example.com\") else None\n",
    "}\n",
    "\n",
    "val email = findUser(1).flatMap(findEmail)  // Some(\"alice@example.com\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scala_case_classes",
   "metadata": {},
   "source": [
    "### 1.5 Case Classes et Pattern Matching\n",
    "\n",
    "```scala\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Case Class = classe de donnÃ©es immuable\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "case class Customer(\n",
    "  id: Long,\n",
    "  name: String,\n",
    "  email: String,\n",
    "  country: String = \"France\"  // Valeur par dÃ©faut\n",
    ")\n",
    "\n",
    "// CrÃ©ation (pas besoin de \"new\")\n",
    "val alice = Customer(1, \"Alice\", \"alice@example.com\")\n",
    "val bob = Customer(2, \"Bob\", \"bob@example.com\", \"USA\")\n",
    "\n",
    "// Accesseurs automatiques\n",
    "println(alice.name)    // \"Alice\"\n",
    "println(alice.country) // \"France\"\n",
    "\n",
    "// equals automatique (compare les valeurs)\n",
    "val alice2 = Customer(1, \"Alice\", \"alice@example.com\")\n",
    "println(alice == alice2)  // true !\n",
    "\n",
    "// copy : crÃ©er une copie modifiÃ©e (immutabilitÃ©)\n",
    "val aliceUSA = alice.copy(country = \"USA\")\n",
    "\n",
    "// toString automatique\n",
    "println(alice)  // Customer(1,Alice,alice@example.com,France)\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Pattern Matching (switch++ ultra puissant)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def describeCustomer(c: Customer): String = c match {\n",
    "  case Customer(_, \"Alice\", _, _)     => \"C'est Alice !\"\n",
    "  case Customer(id, _, _, \"France\")   => s\"Client franÃ§ais #$id\"\n",
    "  case Customer(_, name, _, country)  => s\"$name de $country\"\n",
    "}\n",
    "\n",
    "// Pattern matching avec guards\n",
    "def customerTier(c: Customer, totalSpent: Double): String = c match {\n",
    "  case Customer(_, _, _, \"France\") if totalSpent > 10000 => \"VIP France\"\n",
    "  case _ if totalSpent > 5000                             => \"Gold\"\n",
    "  case _                                                  => \"Standard\"\n",
    "}\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Case classes et Spark\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "// Spark utilise les case classes pour crÃ©er des Datasets typÃ©s !\n",
    "import spark.implicits._\n",
    "\n",
    "case class Sale(product: String, amount: Double, date: String)\n",
    "\n",
    "val sales = Seq(\n",
    "  Sale(\"laptop\", 1000, \"2024-01-15\"),\n",
    "  Sale(\"phone\", 500, \"2024-01-16\")\n",
    ").toDS()  // Dataset[Sale] - typÃ© !\n",
    "\n",
    "// AccÃ¨s typÃ©\n",
    "sales.filter(_.amount > 600).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scala_adt",
   "metadata": {},
   "source": [
    "### 1.6 Sealed Traits et ADT (Algebraic Data Types) ğŸ”¥\n",
    "\n",
    "Les **ADT** permettent de modÃ©liser tous les Ã©tats possibles d'un systÃ¨me. Le compilateur garantit l'exhaustivitÃ© !\n",
    "\n",
    "```scala\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// sealed trait = toutes les sous-classes dans le mÃªme fichier\n",
    "// Le compilateur CONNAÃT tous les cas possibles\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "// ModÃ©liser le rÃ©sultat d'un job ETL\n",
    "sealed trait JobResult\n",
    "case class Success(recordsProcessed: Long, durationMs: Long) extends JobResult\n",
    "case class Failure(error: String, stage: String) extends JobResult\n",
    "case object Skipped extends JobResult  // object = singleton\n",
    "\n",
    "def handleResult(result: JobResult): Unit = result match {\n",
    "  case Success(n, d) => println(s\"âœ… $n records traitÃ©s en ${d}ms\")\n",
    "  case Failure(e, s) => println(s\"âŒ Erreur Ã  l'Ã©tape $s: $e\")\n",
    "  case Skipped       => println(s\"â­ï¸ Job ignorÃ©\")\n",
    "}\n",
    "// Si tu oublies un cas, le compilateur te PRÃ‰VIENT !\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Exemple : Sources de donnÃ©es\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sealed trait DataSource\n",
    "case class JdbcSource(url: String, table: String, user: String) extends DataSource\n",
    "case class S3Source(bucket: String, path: String, format: String) extends DataSource\n",
    "case class KafkaSource(brokers: String, topic: String) extends DataSource\n",
    "case class LocalSource(path: String) extends DataSource\n",
    "\n",
    "def readData(source: DataSource)(implicit spark: SparkSession): DataFrame = source match {\n",
    "  case JdbcSource(url, table, user) =>\n",
    "    spark.read\n",
    "      .format(\"jdbc\")\n",
    "      .option(\"url\", url)\n",
    "      .option(\"dbtable\", table)\n",
    "      .option(\"user\", user)\n",
    "      .load()\n",
    "      \n",
    "  case S3Source(bucket, path, format) =>\n",
    "    spark.read\n",
    "      .format(format)\n",
    "      .load(s\"s3a://$bucket/$path\")\n",
    "      \n",
    "  case KafkaSource(brokers, topic) =>\n",
    "    spark.read\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", brokers)\n",
    "      .option(\"subscribe\", topic)\n",
    "      .load()\n",
    "      \n",
    "  case LocalSource(path) =>\n",
    "    spark.read.parquet(path)\n",
    "}\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Exemple : Modes de traitement\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sealed trait WriteMode\n",
    "case object Overwrite extends WriteMode\n",
    "case object Append extends WriteMode\n",
    "case class MergeByKey(keys: Seq[String]) extends WriteMode\n",
    "\n",
    "def writeData(df: DataFrame, path: String, mode: WriteMode): Unit = mode match {\n",
    "  case Overwrite       => df.write.mode(\"overwrite\").parquet(path)\n",
    "  case Append          => df.write.mode(\"append\").parquet(path)\n",
    "  case MergeByKey(keys) => \n",
    "    // Logique Delta Lake MERGE\n",
    "    println(s\"Merge on keys: ${keys.mkString(\", \")}\")\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scala_either_try",
   "metadata": {},
   "source": [
    "### 1.7 Gestion d'Erreurs : Either et Try ğŸ”¥\n",
    "\n",
    "Fini les `try/catch` partout ! Scala offre des types pour gÃ©rer les erreurs proprement.\n",
    "\n",
    "```scala\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Try[T] : capturer les exceptions (code legacy, I/O)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import scala.util.{Try, Success, Failure}\n",
    "\n",
    "def parseNumber(s: String): Try[Int] = Try {\n",
    "  s.toInt  // Peut lancer NumberFormatException\n",
    "}\n",
    "\n",
    "parseNumber(\"42\")   // Success(42)\n",
    "parseNumber(\"abc\")  // Failure(NumberFormatException)\n",
    "\n",
    "// Pattern matching\n",
    "parseNumber(\"42\") match {\n",
    "  case Success(n)  => println(s\"Nombre: $n\")\n",
    "  case Failure(ex) => println(s\"Erreur: ${ex.getMessage}\")\n",
    "}\n",
    "\n",
    "// getOrElse\n",
    "val num = parseNumber(\"abc\").getOrElse(0)  // 0\n",
    "\n",
    "// map / flatMap (chaÃ®nage sÃ»r)\n",
    "val doubled = parseNumber(\"21\").map(_ * 2)  // Success(42)\n",
    "\n",
    "// recover : transformer l'erreur\n",
    "val safe = parseNumber(\"abc\").recover {\n",
    "  case _: NumberFormatException => -1\n",
    "}  // Success(-1)\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Either[L, R] : erreurs mÃ©tier typÃ©es (pas d'exceptions)\n",
    "// Left = erreur, Right = succÃ¨s\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "// DÃ©finir les types d'erreurs\n",
    "sealed trait ETLError\n",
    "case class SourceNotFound(path: String) extends ETLError\n",
    "case class SchemaError(expected: String, actual: String) extends ETLError\n",
    "case class WriteError(message: String) extends ETLError\n",
    "case class ConfigError(param: String) extends ETLError\n",
    "\n",
    "// Fonctions qui retournent Either\n",
    "def readSource(path: String): Either[ETLError, DataFrame] = {\n",
    "  if (!new java.io.File(path).exists())\n",
    "    Left(SourceNotFound(path))\n",
    "  else\n",
    "    Right(spark.read.parquet(path))\n",
    "}\n",
    "\n",
    "def validateSchema(df: DataFrame, expected: Seq[String]): Either[ETLError, DataFrame] = {\n",
    "  val actual = df.columns.toSeq\n",
    "  if (expected.forall(actual.contains))\n",
    "    Right(df)\n",
    "  else\n",
    "    Left(SchemaError(expected.mkString(\",\"), actual.mkString(\",\")))\n",
    "}\n",
    "\n",
    "def writeOutput(df: DataFrame, path: String): Either[ETLError, Long] = {\n",
    "  try {\n",
    "    df.write.parquet(path)\n",
    "    Right(df.count())\n",
    "  } catch {\n",
    "    case e: Exception => Left(WriteError(e.getMessage))\n",
    "  }\n",
    "}\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// ChaÃ®nage avec for-comprehension (le pattern ultime !)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def runETL(inputPath: String, outputPath: String): Either[ETLError, Long] = {\n",
    "  for {\n",
    "    rawData      <- readSource(inputPath)\n",
    "    validated    <- validateSchema(rawData, Seq(\"id\", \"name\", \"amount\"))\n",
    "    transformed  = validated.filter(col(\"amount\") > 0)  // = pour les transformations pures\n",
    "    recordCount  <- writeOutput(transformed, outputPath)\n",
    "  } yield recordCount\n",
    "}\n",
    "\n",
    "// Utilisation\n",
    "runETL(\"data/input\", \"data/output\") match {\n",
    "  case Right(count) => println(s\"âœ… ETL terminÃ© : $count records\")\n",
    "  case Left(SourceNotFound(p)) => println(s\"âŒ Source introuvable : $p\")\n",
    "  case Left(SchemaError(e, a)) => println(s\"âŒ SchÃ©ma invalide. Attendu: $e, ReÃ§u: $a\")\n",
    "  case Left(WriteError(msg))   => println(s\"âŒ Erreur d'Ã©criture : $msg\")\n",
    "  case Left(ConfigError(p))    => println(s\"âŒ Config manquante : $p\")\n",
    "}\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// Quand utiliser quoi ?\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "| Outil | Quand l'utiliser | Exemple |\n",
    "|-------|-----------------|--------|\n",
    "| **Option[T]** | Valeur absente (pas une erreur) | `findUser(id)` |\n",
    "| **Try[T]** | Exceptions Java/legacy, I/O | `Try { file.read() }` |\n",
    "| **Either[E, T]** | Erreurs mÃ©tier typÃ©es | Pipeline ETL, validation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env_notebook",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Environnements de DÃ©veloppement\n",
    "\n",
    "### 2.1 Scala dans Jupyter Notebook avec Almond\n",
    "\n",
    "**Almond** est un kernel Scala pour Jupyter, idÃ©al pour l'exploration et la formation.\n",
    "\n",
    "#### Installation\n",
    "\n",
    "```bash\n",
    "# 1. Installer Coursier (gestionnaire de packages Scala)\n",
    "curl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz | gzip -d > cs\n",
    "chmod +x cs\n",
    "./cs setup  # Ajoute au PATH\n",
    "\n",
    "# 2. Installer Almond\n",
    "cs launch --fork almond -- --install\n",
    "\n",
    "# 3. VÃ©rifier\n",
    "jupyter kernelspec list\n",
    "# Devrait afficher : scala  /home/user/.local/share/jupyter/kernels/scala\n",
    "\n",
    "# 4. Lancer Jupyter\n",
    "jupyter notebook\n",
    "# â†’ Nouveau notebook â†’ Kernel \"Scala\"\n",
    "```\n",
    "\n",
    "#### Configuration Spark dans Almond\n",
    "\n",
    "```scala\n",
    "// Dans une cellule du notebook Scala\n",
    "\n",
    "// Importer les dÃ©pendances avec Ammonite\n",
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\n",
    "import $ivy.`io.delta::delta-spark:3.1.0`\n",
    "\n",
    "// CrÃ©er la SparkSession\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Almond Spark\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Test\n",
    "val df = Seq((1, \"Alice\"), (2, \"Bob\")).toDF(\"id\", \"name\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "#### Alternatives\n",
    "\n",
    "| Kernel | Avantages | InconvÃ©nients |\n",
    "|--------|-----------|---------------|\n",
    "| **Almond** | Moderne, Ammonite, bien maintenu | Setup manuel |\n",
    "| **spylon-kernel** | Simple, pip install | Moins de features |\n",
    "| **Apache Toree** | Officiel Apache Spark | Plus lourd, moins actif |\n",
    "\n",
    "### 2.2 IntelliJ IDEA\n",
    "\n",
    "Pour les projets de production, **IntelliJ IDEA** est l'IDE de rÃ©fÃ©rence pour Scala.\n",
    "\n",
    "#### Installation\n",
    "\n",
    "```bash\n",
    "# 1. TÃ©lÃ©charger IntelliJ IDEA Community Edition (gratuit)\n",
    "# https://www.jetbrains.com/idea/download/\n",
    "\n",
    "# 2. Linux : extraire et lancer\n",
    "tar -xzf ideaIC-*.tar.gz\n",
    "cd idea-IC-*/bin\n",
    "./idea.sh\n",
    "\n",
    "# 3. Installer le plugin Scala\n",
    "# File â†’ Settings â†’ Plugins â†’ Marketplace â†’ Rechercher \"Scala\" â†’ Install\n",
    "\n",
    "# 4. Configurer le JDK\n",
    "# File â†’ Project Structure â†’ SDKs â†’ + â†’ Download JDK â†’ Version 11 ou 17\n",
    "```\n",
    "\n",
    "### 2.3 Quand utiliser quoi ?\n",
    "\n",
    "| Environnement | Use case |\n",
    "|---------------|----------|\n",
    "| **Notebook (Almond)** | Exploration, prototypage, formation, dÃ©mos |\n",
    "| **IntelliJ + sbt** | DÃ©veloppement, tests, refactoring, projets production |\n",
    "| **spark-submit** | ExÃ©cution sur cluster (YARN, K8s) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellij_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Projet Complet avec IntelliJ (Step-by-Step)\n",
    "\n",
    "On va crÃ©er un projet Spark/Scala complet de A Ã  Z.\n",
    "\n",
    "### 3.1 CrÃ©er le projet\n",
    "\n",
    "1. **File â†’ New â†’ Project**\n",
    "2. SÃ©lectionner **sbt** (Ã  gauche)\n",
    "3. Configurer :\n",
    "   - Name : `spark-etl-project`\n",
    "   - Location : `/home/user/projects/spark-etl-project`\n",
    "   - JDK : 11 ou 17\n",
    "   - sbt version : 1.9.x\n",
    "   - Scala version : **2.12.18** (compatible Spark 3.5)\n",
    "4. **Create**\n",
    "\n",
    "### 3.2 Structure du projet\n",
    "\n",
    "```text\n",
    "spark-etl-project/\n",
    "â”œâ”€â”€ build.sbt                      # DÃ©pendances et config\n",
    "â”œâ”€â”€ project/\n",
    "â”‚   â”œâ”€â”€ build.properties           # Version sbt\n",
    "â”‚   â””â”€â”€ plugins.sbt                # Plugins (sbt-assembly)\n",
    "â”œâ”€â”€ src/\n",
    "â”‚   â”œâ”€â”€ main/\n",
    "â”‚   â”‚   â”œâ”€â”€ scala/\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ com/\n",
    "â”‚   â”‚   â”‚       â””â”€â”€ example/\n",
    "â”‚   â”‚   â”‚           â”œâ”€â”€ Main.scala\n",
    "â”‚   â”‚   â”‚           â”œâ”€â”€ config/\n",
    "â”‚   â”‚   â”‚           â”‚   â””â”€â”€ AppConfig.scala\n",
    "â”‚   â”‚   â”‚           â”œâ”€â”€ jobs/\n",
    "â”‚   â”‚   â”‚           â”‚   â””â”€â”€ SalesETL.scala\n",
    "â”‚   â”‚   â”‚           â”œâ”€â”€ models/\n",
    "â”‚   â”‚   â”‚           â”‚   â””â”€â”€ Models.scala\n",
    "â”‚   â”‚   â”‚           â””â”€â”€ utils/\n",
    "â”‚   â”‚   â”‚               â””â”€â”€ SparkSessionWrapper.scala\n",
    "â”‚   â”‚   â””â”€â”€ resources/\n",
    "â”‚   â”‚       â”œâ”€â”€ application.conf\n",
    "â”‚   â”‚       â””â”€â”€ log4j2.properties\n",
    "â”‚   â””â”€â”€ test/\n",
    "â”‚       â””â”€â”€ scala/\n",
    "â”‚           â””â”€â”€ com/\n",
    "â”‚               â””â”€â”€ example/\n",
    "â”‚                   â””â”€â”€ jobs/\n",
    "â”‚                       â””â”€â”€ SalesETLSpec.scala\n",
    "â”œâ”€â”€ data/\n",
    "â”‚   â””â”€â”€ input/\n",
    "â”‚       â””â”€â”€ sales.csv\n",
    "â””â”€â”€ output/                        # GÃ©nÃ©rÃ©\n",
    "```\n",
    "\n",
    "### 3.3 Fichiers de configuration\n",
    "\n",
    "**`build.sbt`** :\n",
    "```scala\n",
    "name := \"spark-etl-project\"\n",
    "version := \"1.0.0\"\n",
    "scalaVersion := \"2.12.18\"\n",
    "\n",
    "// Spark 3.5 (provided = dÃ©jÃ  sur le cluster)\n",
    "val sparkVersion = \"3.5.0\"\n",
    "\n",
    "libraryDependencies ++= Seq(\n",
    "  // Spark (provided pour le cluster, compile pour local)\n",
    "  \"org.apache.spark\" %% \"spark-sql\"  % sparkVersion % \"provided\",\n",
    "  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % \"provided\",\n",
    "  \n",
    "  // Delta Lake\n",
    "  \"io.delta\" %% \"delta-spark\" % \"3.1.0\",\n",
    "  \n",
    "  // Config\n",
    "  \"com.typesafe\" % \"config\" % \"1.4.3\",\n",
    "  \n",
    "  // Tests\n",
    "  \"org.scalatest\" %% \"scalatest\" % \"3.2.17\" % Test\n",
    ")\n",
    "\n",
    "// Pour exÃ©cuter en local dans IntelliJ (override provided)\n",
    "Compile / run := Defaults.runTask(\n",
    "  Compile / fullClasspath,\n",
    "  Compile / run / mainClass,\n",
    "  Compile / run / runner\n",
    ").evaluated\n",
    "\n",
    "// Assembly config (fat JAR)\n",
    "assembly / assemblyMergeStrategy := {\n",
    "  case PathList(\"META-INF\", xs @ _*) => MergeStrategy.discard\n",
    "  case \"reference.conf\"              => MergeStrategy.concat\n",
    "  case x                              => MergeStrategy.first\n",
    "}\n",
    "\n",
    "assembly / assemblyJarName := s\"${name.value}-${version.value}.jar\"\n",
    "```\n",
    "\n",
    "**`project/build.properties`** :\n",
    "```properties\n",
    "sbt.version=1.9.8\n",
    "```\n",
    "\n",
    "**`project/plugins.sbt`** :\n",
    "```scala\n",
    "addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"2.1.5\")\n",
    "```\n",
    "\n",
    "### 3.4 Code Scala\n",
    "\n",
    "**`src/main/scala/com/example/utils/SparkSessionWrapper.scala`** :\n",
    "```scala\n",
    "package com.example.utils\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "trait SparkSessionWrapper {\n",
    "  \n",
    "  lazy val spark: SparkSession = SparkSession.builder()\n",
    "    .appName(\"SparkETL\")\n",
    "    .master(sys.env.getOrElse(\"SPARK_MASTER\", \"local[*]\"))\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \n",
    "            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    "    \n",
    "  // RÃ©duire les logs Spark\n",
    "  spark.sparkContext.setLogLevel(\"WARN\")\n",
    "}\n",
    "```\n",
    "\n",
    "**`src/main/scala/com/example/models/Models.scala`** :\n",
    "```scala\n",
    "package com.example.models\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// ModÃ¨les de donnÃ©es (case classes)\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "case class Sale(\n",
    "  transactionId: String,\n",
    "  productCategory: String,\n",
    "  amount: Double,\n",
    "  quantity: Int,\n",
    "  date: String,\n",
    "  customerId: String\n",
    ")\n",
    "\n",
    "case class SalesSummary(\n",
    "  productCategory: String,\n",
    "  totalSales: Double,\n",
    "  totalQuantity: Long,\n",
    "  avgSale: Double,\n",
    "  transactionCount: Long\n",
    ")\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// ADT pour les rÃ©sultats de job\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sealed trait JobResult\n",
    "case class JobSuccess(\n",
    "  recordsRead: Long,\n",
    "  recordsWritten: Long,\n",
    "  durationMs: Long\n",
    ") extends JobResult\n",
    "\n",
    "case class JobFailure(\n",
    "  stage: String,\n",
    "  error: String\n",
    ") extends JobResult\n",
    "\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "// ADT pour les erreurs ETL\n",
    "// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "sealed trait ETLError\n",
    "case class SourceNotFound(path: String) extends ETLError\n",
    "case class InvalidSchema(message: String) extends ETLError\n",
    "case class TransformError(message: String) extends ETLError\n",
    "case class WriteError(message: String) extends ETLError\n",
    "```\n",
    "\n",
    "**`src/main/scala/com/example/jobs/SalesETL.scala`** :\n",
    "```scala\n",
    "package com.example.jobs\n",
    "\n",
    "import com.example.models._\n",
    "import com.example.utils.SparkSessionWrapper\n",
    "import org.apache.spark.sql.{DataFrame, Dataset}\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "object SalesETL extends SparkSessionWrapper {\n",
    "  \n",
    "  import spark.implicits._\n",
    "  \n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  // EXTRACT\n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  \n",
    "  def extract(inputPath: String): Either[ETLError, Dataset[Sale]] = {\n",
    "    try {\n",
    "      val df = spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .csv(inputPath)\n",
    "        \n",
    "      // Renommer les colonnes pour matcher la case class\n",
    "      val cleaned = df\n",
    "        .withColumnRenamed(\"transaction_id\", \"transactionId\")\n",
    "        .withColumnRenamed(\"product_category\", \"productCategory\")\n",
    "        .withColumnRenamed(\"customer_id\", \"customerId\")\n",
    "        \n",
    "      Right(cleaned.as[Sale])\n",
    "    } catch {\n",
    "      case e: org.apache.spark.sql.AnalysisException =>\n",
    "        Left(SourceNotFound(inputPath))\n",
    "      case e: Exception =>\n",
    "        Left(InvalidSchema(e.getMessage))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  // TRANSFORM\n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  \n",
    "  def transform(sales: Dataset[Sale]): Either[ETLError, Dataset[SalesSummary]] = {\n",
    "    try {\n",
    "      val summary = sales\n",
    "        .filter(_.amount > 0)\n",
    "        .groupByKey(_.productCategory)\n",
    "        .agg(\n",
    "          sum($\"amount\").as[Double],\n",
    "          sum($\"quantity\").as[Long],\n",
    "          avg($\"amount\").as[Double],\n",
    "          count(\"*\").as[Long]\n",
    "        )\n",
    "        .map { case (category, total, qty, avgSale, count) =>\n",
    "          SalesSummary(category, total, qty, avgSale, count)\n",
    "        }\n",
    "        \n",
    "      Right(summary)\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        Left(TransformError(e.getMessage))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  // LOAD\n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  \n",
    "  def load(data: Dataset[SalesSummary], outputPath: String): Either[ETLError, Long] = {\n",
    "    try {\n",
    "      data.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .save(outputPath)\n",
    "        \n",
    "      Right(data.count())\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        Left(WriteError(e.getMessage))\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  // RUN (orchestration avec for-comprehension)\n",
    "  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "  \n",
    "  def run(inputPath: String, outputPath: String): Either[ETLError, JobSuccess] = {\n",
    "    val startTime = System.currentTimeMillis()\n",
    "    \n",
    "    for {\n",
    "      sales   <- extract(inputPath)\n",
    "      _       = println(s\"ğŸ“¥ Extracted ${sales.count()} records\")\n",
    "      \n",
    "      summary <- transform(sales)\n",
    "      _       = println(s\"ğŸ”„ Transformed to ${summary.count()} categories\")\n",
    "      \n",
    "      count   <- load(summary, outputPath)\n",
    "      _       = println(s\"ğŸ“¤ Loaded $count records to $outputPath\")\n",
    "      \n",
    "    } yield JobSuccess(\n",
    "      recordsRead = sales.count(),\n",
    "      recordsWritten = count,\n",
    "      durationMs = System.currentTimeMillis() - startTime\n",
    "    )\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**`src/main/scala/com/example/Main.scala`** :\n",
    "```scala\n",
    "package com.example\n",
    "\n",
    "import com.example.jobs.SalesETL\n",
    "import com.example.models._\n",
    "\n",
    "object Main {\n",
    "  \n",
    "  def main(args: Array[String]): Unit = {\n",
    "    \n",
    "    val inputPath = args.lift(0).getOrElse(\"data/input/sales.csv\")\n",
    "    val outputPath = args.lift(1).getOrElse(\"output/sales_summary\")\n",
    "    \n",
    "    println(s\"ğŸš€ Starting ETL Job\")\n",
    "    println(s\"   Input:  $inputPath\")\n",
    "    println(s\"   Output: $outputPath\")\n",
    "    println()\n",
    "    \n",
    "    SalesETL.run(inputPath, outputPath) match {\n",
    "      case Right(JobSuccess(read, written, duration)) =>\n",
    "        println()\n",
    "        println(s\"âœ… Job completed successfully!\")\n",
    "        println(s\"   Records read:    $read\")\n",
    "        println(s\"   Records written: $written\")\n",
    "        println(s\"   Duration:        ${duration}ms\")\n",
    "        \n",
    "      case Left(SourceNotFound(path)) =>\n",
    "        System.err.println(s\"âŒ Source not found: $path\")\n",
    "        System.exit(1)\n",
    "        \n",
    "      case Left(InvalidSchema(msg)) =>\n",
    "        System.err.println(s\"âŒ Invalid schema: $msg\")\n",
    "        System.exit(1)\n",
    "        \n",
    "      case Left(TransformError(msg)) =>\n",
    "        System.err.println(s\"âŒ Transform error: $msg\")\n",
    "        System.exit(1)\n",
    "        \n",
    "      case Left(WriteError(msg)) =>\n",
    "        System.err.println(s\"âŒ Write error: $msg\")\n",
    "        System.exit(1)\n",
    "    }\n",
    "    \n",
    "    // ArrÃªter Spark proprement\n",
    "    SalesETL.spark.stop()\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 3.5 DonnÃ©es de test\n",
    "\n",
    "**`data/input/sales.csv`** :\n",
    "```csv\n",
    "transaction_id,product_category,amount,quantity,date,customer_id\n",
    "TXN001,Electronics,1299.99,1,2024-01-15,CUST001\n",
    "TXN002,Clothing,89.99,3,2024-01-15,CUST002\n",
    "TXN003,Electronics,599.99,1,2024-01-16,CUST001\n",
    "TXN004,Food,45.50,10,2024-01-16,CUST003\n",
    "TXN005,Clothing,129.99,2,2024-01-17,CUST002\n",
    "TXN006,Electronics,199.99,2,2024-01-17,CUST004\n",
    "TXN007,Food,78.25,5,2024-01-18,CUST001\n",
    "TXN008,Clothing,299.99,1,2024-01-18,CUST005\n",
    "TXN009,Electronics,899.99,1,2024-01-19,CUST003\n",
    "TXN010,Food,156.00,8,2024-01-19,CUST002\n",
    "```\n",
    "\n",
    "### 3.6 ExÃ©cuter dans IntelliJ\n",
    "\n",
    "1. **Clic droit sur `Main.scala`** â†’ **Run 'Main'**\n",
    "2. Ou crÃ©er une **Run Configuration** :\n",
    "   - Run â†’ Edit Configurations â†’ + â†’ Application\n",
    "   - Main class : `com.example.Main`\n",
    "   - Program arguments : `data/input/sales.csv output/sales_summary`\n",
    "   - VM options : `--add-opens java.base/sun.nio.ch=ALL-UNNAMED` (Java 17)\n",
    "\n",
    "### 3.7 Builder le JAR\n",
    "\n",
    "```bash\n",
    "# Dans le terminal IntelliJ (View â†’ Tool Windows â†’ Terminal)\n",
    "\n",
    "# Compiler\n",
    "sbt compile\n",
    "\n",
    "# CrÃ©er le fat JAR (inclut toutes les dÃ©pendances sauf Spark)\n",
    "sbt assembly\n",
    "\n",
    "# Le JAR est crÃ©Ã© dans :\n",
    "# target/scala-2.12/spark-etl-project-1.0.0.jar\n",
    "```\n",
    "\n",
    "### 3.8 ExÃ©cuter avec spark-submit (local)\n",
    "\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --master local[*] \\\n",
    "  --driver-memory 2g \\\n",
    "  --class com.example.Main \\\n",
    "  --packages io.delta:delta-spark_2.12:3.1.0 \\\n",
    "  target/scala-2.12/spark-etl-project-1.0.0.jar \\\n",
    "  data/input/sales.csv \\\n",
    "  output/sales_summary\n",
    "```\n",
    "\n",
    "### 3.9 ExÃ©cuter sur cluster (YARN/K8s)\n",
    "\n",
    "```bash\n",
    "# YARN\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --driver-memory 4g \\\n",
    "  --executor-memory 8g \\\n",
    "  --executor-cores 4 \\\n",
    "  --num-executors 10 \\\n",
    "  --class com.example.Main \\\n",
    "  --packages io.delta:delta-spark_2.12:3.1.0 \\\n",
    "  spark-etl-project-1.0.0.jar \\\n",
    "  hdfs:///data/input/sales.csv \\\n",
    "  hdfs:///data/output/sales_summary\n",
    "\n",
    "# Kubernetes (voir M27)\n",
    "spark-submit \\\n",
    "  --master k8s://https://k8s-api:6443 \\\n",
    "  --deploy-mode cluster \\\n",
    "  --conf spark.kubernetes.container.image=my-spark:3.5.0 \\\n",
    "  --class com.example.Main \\\n",
    "  local:///opt/spark/jars/spark-etl-project-1.0.0.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connectors",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Connecteurs : JARs et Configuration\n",
    "\n",
    "### 4.1 JARs requis par source de donnÃ©es\n",
    "\n",
    "| Source | JAR | Maven coordinates |\n",
    "|--------|-----|-------------------|\n",
    "| **PostgreSQL** | postgresql-42.7.0.jar | `org.postgresql:postgresql:42.7.0` |\n",
    "| **MySQL** | mysql-connector-j-8.0.33.jar | `com.mysql:mysql-connector-j:8.0.33` |\n",
    "| **SQL Server** | mssql-jdbc-12.4.0.jar | `com.microsoft.sqlserver:mssql-jdbc:12.4.0` |\n",
    "| **Oracle** | ojdbc11-23.3.0.0.jar | `com.oracle.database.jdbc:ojdbc11:23.3.0.0` |\n",
    "| **Kafka** | spark-sql-kafka-0-10_2.12-3.5.0.jar | `org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0` |\n",
    "| **Delta Lake** | delta-spark_2.12-3.1.0.jar | `io.delta:delta-spark_2.12:3.1.0` |\n",
    "| **Iceberg** | iceberg-spark-runtime-3.5_2.12-1.4.0.jar | `org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0` |\n",
    "| **AWS S3** | hadoop-aws-3.3.4.jar + aws-java-sdk-bundle | `org.apache.hadoop:hadoop-aws:3.3.4` |\n",
    "| **GCS** | gcs-connector-hadoop3-2.2.17.jar | â€” (tÃ©lÃ©chargement manuel) |\n",
    "\n",
    "### 4.2 Ajouter dans build.sbt\n",
    "\n",
    "```scala\n",
    "libraryDependencies ++= Seq(\n",
    "  // Spark (provided = dÃ©jÃ  sur le cluster)\n",
    "  \"org.apache.spark\" %% \"spark-sql\" % \"3.5.0\" % \"provided\",\n",
    "  \n",
    "  // Connecteurs (compile = inclus dans le JAR)\n",
    "  \"org.postgresql\" % \"postgresql\" % \"42.7.0\",\n",
    "  \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.5.0\",\n",
    "  \"io.delta\" %% \"delta-spark\" % \"3.1.0\",\n",
    ")\n",
    "```\n",
    "\n",
    "### 4.3 spark-submit : --jars vs --packages\n",
    "\n",
    "| Option | Usage |\n",
    "|--------|-------|\n",
    "| `--jars` | Chemins locaux ou HDFS vers des JARs |\n",
    "| `--packages` | CoordonnÃ©es Maven (tÃ©lÃ©chargement automatique) |\n",
    "| `--repositories` | Repos Maven custom |\n",
    "| `--driver-class-path` | JARs pour le driver uniquement |\n",
    "\n",
    "```bash\n",
    "# Avec JARs locaux\n",
    "spark-submit \\\n",
    "  --jars /path/to/postgresql-42.7.0.jar,/path/to/delta-spark_2.12-3.1.0.jar \\\n",
    "  my-app.jar\n",
    "\n",
    "# Avec tÃ©lÃ©chargement Maven\n",
    "spark-submit \\\n",
    "  --packages org.postgresql:postgresql:42.7.0,io.delta:delta-spark_2.12:3.1.0 \\\n",
    "  my-app.jar\n",
    "\n",
    "# Repo custom\n",
    "spark-submit \\\n",
    "  --repositories https://my-company.jfrog.io/artifactory/libs-release \\\n",
    "  --packages com.mycompany:my-lib:1.0.0 \\\n",
    "  my-app.jar\n",
    "```\n",
    "\n",
    "### 4.4 Exemples de connexion\n",
    "\n",
    "> â„¹ï¸ **Note** : Ces exemples supposent que vous avez accÃ¨s aux services correspondants.\n",
    "\n",
    "**PostgreSQL** :\n",
    "```scala\n",
    "val df = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\")\n",
    "  .option(\"dbtable\", \"customers\")\n",
    "  .option(\"user\", \"postgres\")\n",
    "  .option(\"password\", \"secret\")\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\n",
    "  .load()\n",
    "```\n",
    "\n",
    "**Kafka** :\n",
    "```scala\n",
    "val df = spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "  .option(\"subscribe\", \"events\")\n",
    "  .option(\"startingOffsets\", \"earliest\")\n",
    "  .load()\n",
    "```\n",
    "\n",
    "**Delta Lake** :\n",
    "```scala\n",
    "// Lecture\n",
    "val df = spark.read.format(\"delta\").load(\"/path/to/delta\")\n",
    "\n",
    "// Ã‰criture\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/delta\")\n",
    "\n",
    "// Time travel\n",
    "val dfV2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"/path/to/delta\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catalyst",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Catalyst Optimizer\n",
    "\n",
    "**Catalyst** est le moteur d'optimisation de Spark SQL. Il transforme ton code en un plan d'exÃ©cution optimal.\n",
    "\n",
    "### 5.1 Phases d'optimisation\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         CATALYST OPTIMIZER                                  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   Code SQL/DataFrame                                                        â”‚\n",
    "â”‚         â”‚                                                                   â”‚\n",
    "â”‚         â–¼                                                                   â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚    ANALYSIS     â”‚  RÃ©soudre les noms de colonnes, tables               â”‚\n",
    "â”‚   â”‚                 â”‚  VÃ©rifier les types                                   â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ LOGICAL OPTIM   â”‚  Predicate pushdown, Column pruning                  â”‚\n",
    "â”‚   â”‚                 â”‚  Constant folding, Filter reordering                 â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ PHYSICAL PLAN   â”‚  Choisir les algorithmes (Sort-Merge vs Broadcast)   â”‚\n",
    "â”‚   â”‚                 â”‚  Cost-Based Optimization                             â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
    "â”‚   â”‚ CODE GENERATION â”‚  GÃ©nÃ©rer du bytecode Java optimisÃ©                   â”‚\n",
    "â”‚   â”‚   (Tungsten)    â”‚  Whole-Stage Code Generation                         â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
    "â”‚            â–¼                                                                â”‚\n",
    "â”‚        EXECUTION                                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 5.2 Lire un plan d'exÃ©cution\n",
    "\n",
    "```scala\n",
    "val df = spark.read.parquet(\"data/sales\")\n",
    "  .filter($\"amount\" > 100)\n",
    "  .groupBy(\"category\")\n",
    "  .agg(sum(\"amount\"))\n",
    "\n",
    "// Plan simple\n",
    "df.explain()\n",
    "\n",
    "// Plan dÃ©taillÃ© (Parsed â†’ Analyzed â†’ Optimized â†’ Physical)\n",
    "df.explain(true)\n",
    "\n",
    "// Plan formatÃ© (Spark 3.0+)\n",
    "df.explain(\"formatted\")\n",
    "\n",
    "// Plan avec coÃ»ts (si CBO activÃ©)\n",
    "df.explain(\"cost\")\n",
    "```\n",
    "\n",
    "### 5.3 Exemple de plan\n",
    "\n",
    "```text\n",
    "== Physical Plan ==\n",
    "AdaptiveSparkPlan isFinalPlan=false\n",
    "+- HashAggregate(keys=[category#12], functions=[sum(amount#14)])\n",
    "   +- Exchange hashpartitioning(category#12, 200)        â† SHUFFLE\n",
    "      +- HashAggregate(keys=[category#12], functions=[partial_sum(amount#14)])\n",
    "         +- Project [category#12, amount#14]             â† Colonnes sÃ©lectionnÃ©es\n",
    "            +- Filter (amount#14 > 100)                  â† Filtre pushdown\n",
    "               +- FileScan parquet [category#12,amount#14]  â† Lecture\n",
    "```\n",
    "\n",
    "**Ce qu'il faut regarder** :\n",
    "- **Exchange** = Shuffle (coÃ»teux !)\n",
    "- **BroadcastHashJoin** vs **SortMergeJoin** (broadcast = plus rapide si petite table)\n",
    "- **FileScan** : colonnes pruned, partitions pruned\n",
    "- **Filter** : poussÃ© au plus prÃ¨s de la source\n",
    "\n",
    "### 5.4 Cost-Based Optimization (CBO)\n",
    "\n",
    "```scala\n",
    "// Activer CBO\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", \"true\")\n",
    "\n",
    "// Collecter les statistiques (important !)\n",
    "spark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS amount, category\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aqe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Adaptive Query Execution (AQE)\n",
    "\n",
    "**AQE** (Spark 3.0+) optimise le plan d'exÃ©cution **pendant** l'exÃ©cution, en se basant sur les statistiques rÃ©elles.\n",
    "\n",
    "### 6.1 FonctionnalitÃ©s\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Coalesce Partitions** | Fusionner les petites partitions aprÃ¨s shuffle |\n",
    "| **Broadcast Join Conversion** | Convertir Sort-Merge â†’ Broadcast si table petite |\n",
    "| **Skew Join Optimization** | Splitter les partitions dÃ©sÃ©quilibrÃ©es |\n",
    "\n",
    "### 6.2 Configuration\n",
    "\n",
    "```scala\n",
    "// Activer AQE (activÃ© par dÃ©faut depuis Spark 3.2)\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "// Coalesce automatique\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\")\n",
    "\n",
    "// Broadcast dynamique\n",
    "spark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "\n",
    "// Skew join\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n",
    "```\n",
    "\n",
    "### 6.3 Voir AQE en action\n",
    "\n",
    "```scala\n",
    "// Le plan montre \"AdaptiveSparkPlan\"\n",
    "df.explain()\n",
    "\n",
    "// == Physical Plan ==\n",
    "// AdaptiveSparkPlan isFinalPlan=true    â† AQE activÃ© !\n",
    "// +- ...\n",
    "\n",
    "// AprÃ¨s exÃ©cution, voir le plan final\n",
    "df.collect()  // ExÃ©cuter d'abord\n",
    "df.explain()  // Maintenant isFinalPlan=true\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tungsten",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Tungsten Engine\n",
    "\n",
    "**Tungsten** est le moteur d'exÃ©cution bas-niveau de Spark, optimisÃ© pour le CPU et la mÃ©moire.\n",
    "\n",
    "### 7.1 Optimisations Tungsten\n",
    "\n",
    "| Optimisation | Description |\n",
    "|--------------|-------------|\n",
    "| **Off-heap Memory** | Stockage hors JVM heap (Ã©vite GC) |\n",
    "| **Binary Format** | DonnÃ©es en format binaire compact |\n",
    "| **Cache-aware** | Algorithmes optimisÃ©s pour le cache CPU |\n",
    "| **Whole-Stage CodeGen** | Compile le plan en bytecode Java |\n",
    "\n",
    "### 7.2 Whole-Stage Code Generation\n",
    "\n",
    "Au lieu d'appeler des fonctions virtuelles pour chaque row, Tungsten gÃ©nÃ¨re du code spÃ©cialisÃ©.\n",
    "\n",
    "```text\n",
    "SANS CODEGEN                          AVEC CODEGEN\n",
    "                                      \n",
    "for (row in data) {                   // Code gÃ©nÃ©rÃ© automatiquement\n",
    "  filter.eval(row)   â† virtual call   for (row in data) {\n",
    "  project.eval(row)  â† virtual call     if (row.amount > 100) {\n",
    "  agg.update(row)    â† virtual call       sum += row.amount\n",
    "}                                       }\n",
    "                                      }\n",
    "```\n",
    "\n",
    "### 7.3 Voir le code gÃ©nÃ©rÃ©\n",
    "\n",
    "```scala\n",
    "// Activer le debug\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n",
    "\n",
    "// Voir le code gÃ©nÃ©rÃ©\n",
    "df.queryExecution.debug.codegen()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Memory Management & Tuning\n",
    "\n",
    "### 8.1 Architecture MÃ©moire Spark\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     EXECUTOR MEMORY (spark.executor.memory)                 â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                SPARK MEMORY (spark.memory.fraction = 0.6)           â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚   STORAGE MEMORY    â”‚   â”‚      EXECUTION MEMORY           â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚                     â”‚   â”‚                                 â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚   Cache, Broadcast  â”‚ âŸ· â”‚   Shuffle, Join, Sort, Agg     â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚                     â”‚   â”‚                                 â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚   (storageFraction  â”‚   â”‚   (1 - storageFraction          â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â”‚    = 0.5)           â”‚   â”‚    = 0.5)                       â”‚    â”‚  â”‚\n",
    "â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚\n",
    "â”‚   â”‚                                                                     â”‚  â”‚\n",
    "â”‚   â”‚   â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Unified Memory (frontiÃ¨re flexible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                    USER MEMORY (0.4)                                â”‚  â”‚\n",
    "â”‚   â”‚   Structures internes, UDFs, mÃ©tadonnÃ©es                            â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                    RESERVED MEMORY (300MB fixe)                     â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 8.2 Configurations MÃ©moire\n",
    "\n",
    "```scala\n",
    "// MÃ©moire totale executor\n",
    "spark.conf.set(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "// Fraction pour Spark (vs User)\n",
    "spark.conf.set(\"spark.memory.fraction\", \"0.6\")  // 60% pour Spark\n",
    "\n",
    "// Fraction Storage dans Spark Memory\n",
    "spark.conf.set(\"spark.memory.storageFraction\", \"0.5\")  // 50% Storage, 50% Execution\n",
    "\n",
    "// Off-heap (Tungsten)\n",
    "spark.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.memory.offHeap.size\", \"4g\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "slow_jobs",
   "metadata": {},
   "source": [
    "### 8.3 Pourquoi 80% des Jobs Spark sont Lents ğŸ”¥\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚            LES 5 CAUSES DE 80% DES JOBS SPARK LENTS                         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   1ï¸âƒ£ DATA SKEW (40%)                                                        â”‚\n",
    "â”‚      â””â”€ 1 partition avec 10M rows, les autres avec 1K                       â”‚\n",
    "â”‚      â””â”€ SymptÃ´me : 199 tasks Ã  2s, 1 task Ã  45min                          â”‚\n",
    "â”‚      â””â”€ Fix : salting, repartition par clÃ©, AQE skew join                  â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   2ï¸âƒ£ SHUFFLE EXCESSIF (25%)                                                 â”‚\n",
    "â”‚      â””â”€ Trop de groupBy/join, pas de broadcast                              â”‚\n",
    "â”‚      â””â”€ SymptÃ´me : \"Shuffle Write\" Ã©norme dans Spark UI                    â”‚\n",
    "â”‚      â””â”€ Fix : broadcast join, rÃ©duire colonnes avant shuffle               â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   3ï¸âƒ£ MAUVAIS PARTITIONING (15%)                                             â”‚\n",
    "â”‚      â””â”€ 200 partitions par dÃ©faut, fichiers trop petits/gros                â”‚\n",
    "â”‚      â””â”€ SymptÃ´me : 10000 tasks de 100KB ou 2 tasks de 50GB                 â”‚\n",
    "â”‚      â””â”€ Fix : repartition/coalesce, AQE coalesce, tuning shuffle.partitionsâ”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   4ï¸âƒ£ SPILL TO DISK (10%)                                                    â”‚\n",
    "â”‚      â””â”€ MÃ©moire insuffisante, donnÃ©es Ã©crites sur disque                    â”‚\n",
    "â”‚      â””â”€ SymptÃ´me : \"Spill (Memory)\" dans Stage details                     â”‚\n",
    "â”‚      â””â”€ Fix : augmenter executor memory, rÃ©duire taille partitions         â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   5ï¸âƒ£ SMALL FILES PROBLEM (10%)                                              â”‚\n",
    "â”‚      â””â”€ Lire 10000 fichiers de 1MB au lieu de 100 de 100MB                  â”‚\n",
    "â”‚      â””â”€ SymptÃ´me : temps de listing S3 trÃ¨s long, driver OOM               â”‚\n",
    "â”‚      â””â”€ Fix : compaction Delta, maxPartitionBytes, bin-packing             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 8.4 Diagnostic Toolkit\n",
    "\n",
    "```scala\n",
    "// 1. VÃ©rifier le plan d'exÃ©cution\n",
    "df.explain(true)          // Plan complet\n",
    "df.explain(\"cost\")        // Avec coÃ»ts estimÃ©s\n",
    "\n",
    "// 2. Voir le nombre de partitions\n",
    "println(s\"Partitions: ${df.rdd.getNumPartitions}\")\n",
    "\n",
    "// 3. DÃ©tecter le skew (distribution par partition)\n",
    "import org.apache.spark.sql.functions._\n",
    "df.withColumn(\"partition_id\", spark_partition_id())\n",
    "  .groupBy(\"partition_id\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "  .show()\n",
    "\n",
    "// 4. Voir la taille des partitions\n",
    "df.rdd.mapPartitionsWithIndex { case (idx, iter) =>\n",
    "  Iterator((idx, iter.size))\n",
    "}.toDF(\"partition\", \"rows\").show()\n",
    "\n",
    "// 5. MÃ©triques pendant l'exÃ©cution\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### 8.5 Spark UI : Ce qu'il faut regarder\n",
    "\n",
    "| Onglet | Ce qu'on cherche |\n",
    "|--------|------------------|\n",
    "| **Jobs** | DurÃ©e totale, jobs qui traÃ®nent |\n",
    "| **Stages** | Shuffle Read/Write, Spill, tÃ¢ches en retard |\n",
    "| **Tasks** | Distribution (min/median/max), stragglers |\n",
    "| **Storage** | Cache utilisÃ©, mÃ©moire disponible |\n",
    "| **SQL** | Plan physique, mÃ©triques par opÃ©rateur |\n",
    "| **Executors** | MÃ©moire, GC time, shuffle read/write |\n",
    "\n",
    "### 8.6 Debugging OOM\n",
    "\n",
    "```text\n",
    "java.lang.OutOfMemoryError: Java heap space\n",
    "\n",
    "CAUSES PROBABLES :\n",
    "â”œâ”€â”€ collect() sur un gros DataFrame\n",
    "â”œâ”€â”€ broadcast() d'une table trop grande\n",
    "â”œâ”€â”€ Trop de partitions small â†’ overhead\n",
    "â”œâ”€â”€ Skew â†’ 1 executor avec trop de donnÃ©es\n",
    "â””â”€â”€ Driver OOM â†’ trop de metadata / rÃ©sultats\n",
    "\n",
    "SOLUTIONS :\n",
    "â”œâ”€â”€ Augmenter spark.executor.memory\n",
    "â”œâ”€â”€ Augmenter spark.driver.memory (si driver OOM)\n",
    "â”œâ”€â”€ Repartitionner les donnÃ©es\n",
    "â”œâ”€â”€ Ã‰viter collect(), utiliser take() ou write()\n",
    "â””â”€â”€ RÃ©duire spark.sql.autoBroadcastJoinThreshold\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercices",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Pratique Scala â€” ADT Pipeline\n",
    "\n",
    "ModÃ©liser un pipeline de traitement avec des ADT.\n",
    "\n",
    "```scala\n",
    "// TODO: CrÃ©er un sealed trait pour les Ã©tapes du pipeline\n",
    "// Ã‰tapes : Extract, Validate, Transform, Load\n",
    "// Chaque Ã©tape peut Success ou Fail\n",
    "\n",
    "// TODO: ImplÃ©menter une fonction qui exÃ©cute le pipeline\n",
    "// et retourne un rapport dÃ©taillÃ© de chaque Ã©tape\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "```scala\n",
    "sealed trait PipelineStep\n",
    "case object Extract extends PipelineStep\n",
    "case object Validate extends PipelineStep\n",
    "case object Transform extends PipelineStep\n",
    "case object Load extends PipelineStep\n",
    "\n",
    "sealed trait StepResult\n",
    "case class StepSuccess(step: PipelineStep, durationMs: Long, records: Long) extends StepResult\n",
    "case class StepFailure(step: PipelineStep, error: String) extends StepResult\n",
    "\n",
    "case class PipelineReport(results: List[StepResult]) {\n",
    "  def isSuccess: Boolean = results.forall(_.isInstanceOf[StepSuccess])\n",
    "  def failedStep: Option[StepFailure] = results.collectFirst { case f: StepFailure => f }\n",
    "}\n",
    "```\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 2 : Either pour ETL\n",
    "\n",
    "ImplÃ©menter un mini-ETL avec gestion d'erreurs typÃ©e.\n",
    "\n",
    "```scala\n",
    "// TODO: CrÃ©er une fonction qui :\n",
    "// 1. Lit un fichier CSV (peut Ã©chouer : FileNotFound)\n",
    "// 2. Valide que la colonne \"amount\" existe (peut Ã©chouer : SchemaError)\n",
    "// 3. Filtre les montants > 0 (peut Ã©chouer : DataError si 0 records)\n",
    "// 4. Ã‰crit en Parquet\n",
    "//\n",
    "// Utiliser Either[ETLError, DataFrame] Ã  chaque Ã©tape\n",
    "// ChaÃ®ner avec for-comprehension\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 3 : Setup Almond + Spark\n",
    "\n",
    "1. Installer Almond (suivre les instructions section 2.1)\n",
    "2. CrÃ©er un notebook Scala\n",
    "3. Importer Spark et Delta Lake\n",
    "4. CrÃ©er un DataFrame, le transformer, l'Ã©crire en Delta\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 4 : Projet IntelliJ Complet\n",
    "\n",
    "1. CrÃ©er le projet selon la structure section 3\n",
    "2. Ajouter les fichiers de config\n",
    "3. ImplÃ©menter le code\n",
    "4. CrÃ©er le fichier CSV de test\n",
    "5. ExÃ©cuter dans IntelliJ\n",
    "6. Builder avec `sbt assembly`\n",
    "7. Lancer avec `spark-submit`\n",
    "\n",
    "---\n",
    "\n",
    "### Exercice 5 : Diagnostiquer un Job Lent\n",
    "\n",
    "```scala\n",
    "// Ce code est intentionnellement lent. Pourquoi ?\n",
    "val df1 = spark.read.parquet(\"big_table\")  // 100M rows\n",
    "val df2 = spark.read.parquet(\"small_table\")  // 1000 rows\n",
    "\n",
    "val result = df1\n",
    "  .join(df2, \"key\")  // Quel type de join ?\n",
    "  .groupBy(\"category\")\n",
    "  .agg(sum(\"amount\"))\n",
    "  .collect()  // ProblÃ¨me ?\n",
    "\n",
    "// TODO: \n",
    "// 1. Identifier les problÃ¨mes\n",
    "// 2. Proposer des optimisations\n",
    "// 3. RÃ©Ã©crire le code optimisÃ©\n",
    "```\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Solution</summary>\n",
    "\n",
    "**ProblÃ¨mes** :\n",
    "1. Join sans broadcast â†’ Sort-Merge Join (shuffle de 100M rows)\n",
    "2. collect() sur un rÃ©sultat potentiellement gros\n",
    "\n",
    "**Solution** :\n",
    "```scala\n",
    "import org.apache.spark.sql.functions.broadcast\n",
    "\n",
    "val result = df1\n",
    "  .join(broadcast(df2), \"key\")  // Broadcast join !\n",
    "  .groupBy(\"category\")\n",
    "  .agg(sum(\"amount\"))\n",
    "  .write.parquet(\"output\")  // Ã‰crire au lieu de collect\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mini_project",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Mini-Projet : ETL Scala Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Construire un pipeline ETL complet en Scala avec architecture Medallion.\n",
    "\n",
    "```text\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         MEDALLION ARCHITECTURE                              â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\n",
    "â”‚   â”‚   BRONZE    â”‚ â”€â”€â”€â–¶ â”‚   SILVER    â”‚ â”€â”€â”€â–¶ â”‚    GOLD     â”‚                â”‚\n",
    "â”‚   â”‚   (Raw)     â”‚      â”‚  (Cleaned)  â”‚      â”‚ (Aggregated)â”‚                â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   CSV/JSON           Delta Lake           Delta Lake                        â”‚\n",
    "â”‚   + metadata         + validation         + business KPIs                   â”‚\n",
    "â”‚   + ingestion_ts     + dedup              + reporting ready                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Livrables\n",
    "\n",
    "1. Projet IntelliJ complet avec structure\n",
    "2. ADT pour les erreurs et rÃ©sultats\n",
    "3. 3 jobs : BronzeJob, SilverJob, GoldJob\n",
    "4. Main qui orchestre avec Either\n",
    "5. Tests avec ScalaTest\n",
    "6. Fat JAR et script spark-submit\n",
    "\n",
    "### DonnÃ©es\n",
    "\n",
    "**`data/raw/transactions_*.json`** (plusieurs fichiers) :\n",
    "```json\n",
    "{\"transaction_id\": \"TXN001\", \"customer_id\": \"C001\", \"amount\": 150.0, \"category\": \"Electronics\", \"timestamp\": \"2024-01-15T10:30:00Z\"}\n",
    "```\n",
    "\n",
    "### Jobs Ã  implÃ©menter\n",
    "\n",
    "**BronzeJob** :\n",
    "- Lire les JSON\n",
    "- Ajouter `_ingestion_timestamp`, `_source_file`\n",
    "- Ã‰crire en Delta (append)\n",
    "\n",
    "**SilverJob** :\n",
    "- Lire Bronze\n",
    "- Valider schÃ©ma\n",
    "- DÃ©dupliquer par `transaction_id`\n",
    "- Filtrer `amount > 0`\n",
    "- Ã‰crire en Delta avec MERGE\n",
    "\n",
    "**GoldJob** :\n",
    "- Lire Silver\n",
    "- AgrÃ©gations : ventes par catÃ©gorie, par jour\n",
    "- Top customers\n",
    "- Ã‰crire plusieurs tables Gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "### Documentation\n",
    "- [Scala Documentation](https://docs.scala-lang.org/)\n",
    "- [Spark Scala API](https://spark.apache.org/docs/latest/api/scala/)\n",
    "- [sbt Documentation](https://www.scala-sbt.org/1.x/docs/)\n",
    "- [Delta Lake Scala API](https://docs.delta.io/latest/api/scala/)\n",
    "\n",
    "### Livres\n",
    "- *Programming in Scala* â€” Martin Odersky\n",
    "- *Functional Programming in Scala* â€” Paul Chiusano, RÃºnar Bjarnason\n",
    "- *Spark: The Definitive Guide* â€” Bill Chambers, Matei Zaharia\n",
    "\n",
    "### Outils\n",
    "- [IntelliJ IDEA](https://www.jetbrains.com/idea/) â€” IDE\n",
    "- [Almond](https://almond.sh/) â€” Kernel Scala pour Jupyter\n",
    "- [Metals](https://scalameta.org/metals/) â€” LSP pour VS Code\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `31_ml_engineering`** â€” ML Engineering\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu maÃ®trises maintenant Scala pour Spark et les optimisations avancÃ©es."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
