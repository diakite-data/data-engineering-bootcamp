{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e26c6a",
   "metadata": {},
   "source": [
    "# ğŸ§  Introduction au Data Engineering\n",
    "\n",
    "Bienvenue dans cette premiÃ¨re leÃ§on du parcours **Data Engineering â€” From Zero to Hero**.\n",
    "\n",
    "Dans ce notebook, nous allons dÃ©couvrir :\n",
    "\n",
    "- Ce qu'est le Data Engineering\n",
    "- Les diffÃ©rences entre Data Engineer, Data Scientist et Data Analyst\n",
    "- L'architecture d'un systÃ¨me de donnÃ©es moderne \n",
    "    - ETL vs ELT â€” Quelle est la diffÃ©rence ?\n",
    "- Les types de pipelines (batch vs streaming)\n",
    "    - Fondations des pipelines de donnÃ©es\n",
    "- Panorama des outils du Data Engineer\n",
    "- Soft Skills & Mindset du Data Engineer\n",
    "- Quiz de fin de module\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "Avant de commencer ce parcours, il est recommandÃ© d'avoir :\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Basique | Savoir utiliser un ordinateur et naviguer dans des fichiers |\n",
    "| âœ… Basique | ConnaÃ®tre les concepts de base d'internet (API, serveurs, clients) |\n",
    "| ğŸŸ¡ Optionnel | Avoir dÃ©jÃ  Ã©crit quelques lignes de code (Python, JavaScript, etc.) |\n",
    "| ğŸŸ¡ Optionnel | ConnaÃ®tre les bases de SQL (SELECT, WHERE, JOIN) |\n",
    "\n",
    "> ğŸ’¡ **Pas de panique !** Ce parcours est conÃ§u pour les dÃ©butants. Nous couvrirons tout ce dont tu as besoin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7438137",
   "metadata": {},
   "source": [
    "## ğŸ“Œ 1. C'est quoi le Data Engineering ?\n",
    "\n",
    "Le **Data Engineering** (ou ingÃ©nierie des donnÃ©es) est une discipline qui consiste Ã  **concevoir, construire, maintenir et optimiser** les systÃ¨mes de traitement de donnÃ©es. \n",
    "\n",
    "Le Data Engineer, spÃ©cialiste en gestion des donnÃ©es, conÃ§oit et maintient l'infrastructure data (bases de donnÃ©es, entrepÃ´ts de donnÃ©es, lacs de donnÃ©es) et dÃ©veloppe des pipelines automatisÃ©s qui extraient, transforment et chargent les donnÃ©es dans des systÃ¨mes adaptÃ©s. \n",
    "\n",
    "C'est le socle technique qui garantit la qualitÃ©, la disponibilitÃ© et la sÃ©curitÃ© des donnÃ©es utilisÃ©es par les Data Analysts et Data Scientists pour gÃ©nÃ©rer des insights et orienter les stratÃ©gies d'entreprise.\n",
    "\n",
    "### ğŸ¢ Exemples concrets en entreprise\n",
    "\n",
    "| Entreprise | Cas d'usage Data Engineering |\n",
    "|------------|-----------------------------|\n",
    "| **Netflix** | Pipeline qui collecte les donnÃ©es de visionnage de millions d'utilisateurs pour alimenter le systÃ¨me de recommandation |\n",
    "| **Uber** | Traitement en temps rÃ©el des donnÃ©es GPS de milliers de chauffeurs pour optimiser les trajets et les prix |\n",
    "| **Spotify** | AgrÃ©gation des donnÃ©es d'Ã©coute pour gÃ©nÃ©rer les playlists \"Discover Weekly\" personnalisÃ©es |\n",
    "| **Airbnb** | Pipeline de donnÃ©es pour analyser les prix du marchÃ© et suggÃ©rer des tarifs aux hÃ´tes |\n",
    "| **E-commerce** | Synchronisation des stocks entre le site web, l'ERP et les entrepÃ´ts en temps rÃ©el |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18400450",
   "metadata": {},
   "source": [
    "## ğŸ‘¨ğŸ½â€ğŸ’» 2. DiffÃ©rences entre mÃ©tiers\n",
    "\n",
    "| MÃ©tier           | RÃ´le Principal                                      | Focus                  | Outils ClÃ©s                                               |\n",
    "|------------------|----------------------------------------------------|------------------------|----------------------------------------------------------|\n",
    "| ğŸ”§ Data Engineer  | Construire et maintenir l'infrastructure de donnÃ©es| Infrastructure & Pipelines | Apache Airflow, Apache Spark, Kafka, Snowflake, dbt, Python, SQL, Prefect, Docker, Kubernetes |\n",
    "| ğŸ”¬ Data Scientist | Extraire des insights et crÃ©er des modÃ¨les prÃ©dictifs | ModÃ©lisation & ML      | Python, R, scikit-learn, TensorFlow, PyTorch, Jupyter, MLflow, Pandas, XGBoost |\n",
    "| ğŸ“ˆ Data Analyst   | Transformer les donnÃ©es en insights actionnables   | Business Intelligence   | SQL, Excel, Power BI, Tableau, Looker, Google Analytics, Python (basique)          |\n",
    "\n",
    "### ğŸ”„ Comment ces rÃ´les collaborent ?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Data Engineer  â”‚ â”€â”€â–¶ â”‚  Data Scientist â”‚ â”€â”€â–¶ â”‚  Data Analyst   â”‚\n",
    "â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚\n",
    "â”‚ â€¢ Collecte      â”‚     â”‚ â€¢ ModÃ©lisation  â”‚     â”‚ â€¢ Visualisation â”‚\n",
    "â”‚ â€¢ Pipelines     â”‚     â”‚ â€¢ PrÃ©dictions   â”‚     â”‚ â€¢ Reporting     â”‚\n",
    "â”‚ â€¢ Infrastructureâ”‚     â”‚ â€¢ ML/AI         â”‚     â”‚ â€¢ Insights      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â”‚                                               â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback & Besoins â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61392f6",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ 3. Architecture typique d'un pipeline de donnÃ©es moderne\n",
    "\n",
    "Un pipeline de donnÃ©es moderne suit gÃ©nÃ©ralement ce flux :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mermaid_diagram",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Sources[\"ğŸ“¥ Sources\"]\n",
    "        A1[API REST]\n",
    "        A2[Bases de donnÃ©es]\n",
    "        A3[Fichiers CSV/JSON]\n",
    "        A4[Ã‰vÃ©nements/Logs]\n",
    "    end\n",
    "    \n",
    "    subgraph Ingestion[\"ğŸ”„ Ingestion\"]\n",
    "        B1[ETL / ELT]\n",
    "        B2[Streaming]\n",
    "    end\n",
    "    \n",
    "    subgraph Storage[\"ğŸ’¾ Stockage\"]\n",
    "        C1[Data Lake]\n",
    "        C2[Data Warehouse]\n",
    "    end\n",
    "    \n",
    "    subgraph Transform[\"âš™ï¸ Transformation\"]\n",
    "        D1[Nettoyage]\n",
    "        D2[AgrÃ©gation]\n",
    "        D3[Enrichissement]\n",
    "    end\n",
    "    \n",
    "    subgraph Exposition[\"ğŸ“Š Exposition\"]\n",
    "        E1[Dashboards BI]\n",
    "        E2[ModÃ¨les ML]\n",
    "        E3[APIs]\n",
    "    end\n",
    "    \n",
    "    Sources --> Ingestion --> Storage --> Transform --> Exposition\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a54b6c",
   "metadata": {},
   "source": [
    "**ReprÃ©sentation simplifiÃ©e :**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“¥ SOURCES      â”‚  API, CSV, Bases de donnÃ©es, Logs, IoT\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ”„ INGESTION    â”‚  ETL / ELT (Airbyte, Fivetran, Scripts)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ’¾ STOCKAGE     â”‚  Data Lake (S3) / Data Warehouse (Snowflake, BigQuery)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  âš™ï¸ TRANSFORMATIONâ”‚  dbt, Spark, SQL, Pandas\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ğŸ“Š EXPOSITION   â”‚  Dashboards (Tableau, Power BI), ML, APIs\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ff348",
   "metadata": {},
   "source": [
    "### ğŸ” ETL vs ELT â€” Quelle est la diffÃ©rence ?\n",
    "\n",
    "| CritÃ¨re        | ETL (Extract â†’ Transform â†’ Load) | ELT (Extract â†’ Load â†’ Transform) |\n",
    "|----------------|----------------------------------|----------------------------------|\n",
    "| ğŸ”„ Ordre       | Extraction â†’ Transformation â†’ Chargement | Extraction â†’ Chargement â†’ Transformation |\n",
    "| ğŸ“ Lieu de la transformation | En dehors du stockage (dans un script ou un outil) | Directement dans le data warehouse |\n",
    "| âœ… Avantages | Plus de contrÃ´le sur la transformation | Plus rapide sur des gros volumes |\n",
    "| âš ï¸ InconvÃ©nients | Peut surcharger les outils intermÃ©diaires | Besoin d'un entrepÃ´t puissant (coÃ»ts) |\n",
    "| ğŸ› ï¸ Outils typiques | Informatica, Talend, scripts Python | dbt, Snowflake, BigQuery |\n",
    "| ğŸ“… Cas d'usage | DonnÃ©es sensibles nÃ©cessitant un prÃ©-traitement | Analytics modernes sur le cloud |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f90d35",
   "metadata": {},
   "source": [
    "## ğŸ•˜ 4. Pipelines batch vs streaming\n",
    "\n",
    "| Mode | DÃ©finition | Latence | Exemples |\n",
    "|------|------------|---------|----------|\n",
    "| **Batch** | Traitement pÃ©riodique (toutes les heures, tous les joursâ€¦) | Minutes Ã  heures | Rapport quotidien, import CSV, agrÃ©gations nocturnes |\n",
    "| **Streaming** | Traitement en temps rÃ©el, Ã©vÃ©nement par Ã©vÃ©nement | Millisecondes Ã  secondes | Logs serveurs, capteurs IoT, transactions bancaires, dÃ©tection de fraude |\n",
    "\n",
    "### ğŸ¤” Comment choisir ?\n",
    "\n",
    "| Question | Si oui â†’ |\n",
    "|----------|----------|\n",
    "| Les donnÃ©es doivent-elles Ãªtre traitÃ©es immÃ©diatement ? | **Streaming** |\n",
    "| Le volume est-il trÃ¨s Ã©levÃ© mais la latence peu critique ? | **Batch** |\n",
    "| Avez-vous besoin de dÃ©tecter des anomalies en temps rÃ©el ? | **Streaming** |\n",
    "| S'agit-il de rapports quotidiens/hebdomadaires ? | **Batch** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77109d5d",
   "metadata": {},
   "source": [
    "### ğŸ§± Fondations des pipelines de donnÃ©es\n",
    "\n",
    "Tout pipeline de donnÃ©es repose sur plusieurs piliers fondamentaux :\n",
    "\n",
    "| ğŸ”¢ Pilier                          | ğŸ’¡ Description | ğŸ“š Module associÃ© |\n",
    "|-----------------------------------|----------------|-------------------|\n",
    "| **1. Data Collecting**            | Comment collecter les donnÃ©es brutes (fichiers, API, capteursâ€¦) | Python, APIs |\n",
    "| **2. Data Ingestion**             | Comment les intÃ©grer dans un systÃ¨me (DB, data lakeâ€¦) | ETL, Airbyte |\n",
    "| **3. Data Storage**               | Comment et oÃ¹ les stocker (SQL, NoSQL, S3â€¦) | Databases, Cloud |\n",
    "| **4. Data Processing**            | Comment les transformer, nettoyer, agrÃ©ger | Python, Spark, dbt |\n",
    "| **5. Data Modeling**              | Comment organiser les donnÃ©es pour l'analyse | SQL, dbt |\n",
    "| **6. Data Quality & Governance**  | Comment garantir la fiabilitÃ© et la traÃ§abilitÃ© | Great Expectations |\n",
    "| **7. Data Orchestration**         | Comment automatiser les tÃ¢ches et gÃ©rer les dÃ©pendances | Airflow, Prefect |\n",
    "| **8. ScalabilitÃ© & Performance**  | Comment faire face Ã  de gros volumes ou Ã  la charge | Spark, Kubernetes |\n",
    "| **9. SÃ©curitÃ© des donnÃ©es**       | Chiffrement, contrÃ´le d'accÃ¨s, audit | IAM, Vault |\n",
    "| **10. DevOps pour la Data**       | Conteneurisation, CI/CD, monitoring | Docker, GitHub Actions |\n",
    "\n",
    "> ğŸ“˜ Ces concepts seront abordÃ©s progressivement dans le parcours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3e117",
   "metadata": {},
   "source": [
    "## ğŸ§° 5. Panorama des outils du Data Engineer\n",
    "\n",
    "| Domaine                | Outils populaires                  | Niveau |\n",
    "|------------------------|------------------------------------|---------|\n",
    "| ğŸ“¥ Collecte            | Python, API REST, Scrapy, Kafka    | DÃ©butant â†’ IntermÃ©diaire |\n",
    "| ğŸ”„ Ingestion           | Airbyte, Fivetran, Python scripts  | DÃ©butant |\n",
    "| ğŸ’¾ Stockage            | PostgreSQL, Snowflake, S3, Delta Lake | DÃ©butant â†’ AvancÃ© |\n",
    "| âš™ï¸ Traitement (Batch)  | Pandas, Spark, dbt, SQL            | DÃ©butant â†’ AvancÃ© |\n",
    "| âš¡ Traitement (Streaming) | Kafka, Spark Streaming, Flink    | IntermÃ©diaire â†’ AvancÃ© |\n",
    "| ğŸ¼ Orchestration       | Apache Airflow, Prefect, Dagster   | IntermÃ©diaire |\n",
    "| ğŸ³ DevOps & CI/CD      | Docker, GitHub Actions, Terraform  | IntermÃ©diaire |\n",
    "| ğŸ“Š Monitoring          | Grafana, Prometheus, ELK Stack     | IntermÃ©diaire |\n",
    "\n",
    "### ğŸ—ºï¸ Stack moderne typique (2024)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STACK DATA MODERNE                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Orchestration    â”‚  Airflow / Prefect / Dagster            â”‚\n",
    "â”‚  Transformation   â”‚  dbt / Spark / Python                   â”‚\n",
    "â”‚  Warehouse        â”‚  Snowflake / BigQuery / Redshift        â”‚\n",
    "â”‚  Ingestion        â”‚  Airbyte / Fivetran / Stitch            â”‚\n",
    "â”‚  Sources          â”‚  APIs / Databases / SaaS / Files        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebf0b1",
   "metadata": {},
   "source": [
    "## ğŸ§  6. Soft Skills & Mindset du Data Engineer\n",
    "\n",
    "Le mÃ©tier de Data Engineer n'est pas uniquement technique. Pour rÃ©ussir dans ce domaine, il faut aussi dÃ©velopper des compÃ©tences humaines et professionnelles essentielles :\n",
    "\n",
    "| CompÃ©tence | Description | Pourquoi c'est important |\n",
    "|------------|-------------|-------------------------|\n",
    "| ğŸ“ **Documenter** | Ã‰crire une documentation claire pour son code et ses pipelines | Facilite la maintenance et l'onboarding des nouveaux membres |\n",
    "| ğŸ¤ **Collaborer** | Travailler avec les Ã©quipes Data Science, BI, Produit, DevOps | Les donnÃ©es traversent toute l'organisation |\n",
    "| ğŸ¯ **ÃŠtre rigoureux** | Garantir la qualitÃ©, la fiabilitÃ© et la traÃ§abilitÃ© des donnÃ©es | Une erreur de donnÃ©es peut avoir des consÃ©quences business majeures |\n",
    "| ğŸ•µğŸ½â€â™‚ï¸ **Investiguer** | Savoir dÃ©bugger des anomalies, logs ou Ã©checs de pipeline | Les pipelines cassent, il faut savoir pourquoi rapidement |\n",
    "| ğŸ“š **Apprendre en continu** | Se tenir Ã  jour sur les nouveaux outils et pratiques | Le domaine Ã©volue trÃ¨s rapidement |\n",
    "| ğŸ’¬ **Communiquer** | Expliquer des concepts techniques Ã  des non-techniques | Alignement avec les Ã©quipes mÃ©tier |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89240eb",
   "metadata": {},
   "source": [
    "## ğŸ§ª Quiz de fin de module \n",
    "\n",
    "RÃ©ponds aux questions suivantes pour valider tes acquis ğŸ‘‡ğŸ½\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q1. Quel est le rÃ´le principal d'un Data Engineer ?\n",
    "a) CrÃ©er des modÃ¨les prÃ©dictifs  \n",
    "b) Visualiser les donnÃ©es dans Power BI  \n",
    "c) Concevoir et maintenir des pipelines de donnÃ©es  \n",
    "d) Faire des analyses statistiques dans Excel\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Le Data Engineer conÃ§oit et maintient des pipelines de donnÃ©es.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Dans un pipeline **ETL**, que signifie le \"T\" ?\n",
    "a) Transfer  \n",
    "b) Trigger  \n",
    "c) Transform  \n",
    "d) Transport\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” \"T\" signifie *Transform*, c'est l'Ã©tape de transformation des donnÃ©es.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Quelle est la principale diffÃ©rence entre ETL et ELT ?\n",
    "a) ELT ne fait pas de transformation  \n",
    "b) ELT transforme les donnÃ©es **aprÃ¨s** les avoir chargÃ©es  \n",
    "c) ETL est utilisÃ© uniquement pour les fichiers CSV  \n",
    "d) ELT est un outil comme Apache Airflow\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” ELT charge d'abord les donnÃ©es, puis les transforme dans le data warehouse.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Lequel de ces outils est utilisÃ© pour orchestrer des pipelines ?\n",
    "a) Apache Kafka  \n",
    "b) Apache Airflow  \n",
    "c) Tableau  \n",
    "d) PostgreSQL\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” Apache Airflow est un outil d'orchestration de pipelines.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Le traitement **batch** consiste Ã  :\n",
    "a) Traiter les donnÃ©es en continu  \n",
    "b) Traiter les donnÃ©es en petits lots Ã  la volÃ©e  \n",
    "c) Traiter les donnÃ©es par groupe, Ã  intervalle rÃ©gulier  \n",
    "d) Traiter uniquement les donnÃ©es texte\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Le batch consiste Ã  traiter les donnÃ©es par lots Ã  intervalles dÃ©finis.\n",
    "\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763f3cc",
   "metadata": {},
   "source": [
    "## ğŸ§  Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Que signifie **ETLt** ?\n",
    "a) Une erreur dans la chaÃ®ne ETL  \n",
    "b) Une combinaison hybride entre ETL et ELT  \n",
    "c) Une technique de transfert via email  \n",
    "d) Une transformation uniquement aprÃ¨s chargement\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : b** â€” ETLt correspond Ã  : **Extract â†’ Transform (1) â†’ Load â†’ Transform (2)**.  \n",
    "C'est une approche hybride oÃ¹ une partie des transformations est faite **avant** le chargement, et une autre **aprÃ¨s**.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Le **Reverse ETL** est utilisÃ© pour :\n",
    "a) Recharger les donnÃ©es sources depuis le warehouse  \n",
    "b) Supprimer les donnÃ©es invalides dans un lac de donnÃ©es  \n",
    "c) Pousser les donnÃ©es du data warehouse vers les outils mÃ©tiers  \n",
    "d) Transformer les donnÃ©es en reverse-engineering\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "\n",
    "âœ… **RÃ©ponse : c** â€” Reverse ETL consiste Ã  **extraire les donnÃ©es d'un data warehouse** (ex. BigQuery) pour les charger dans des **outils mÃ©tiers** comme Salesforce, Notion, Slack, etc.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resources",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources pour aller plus loin\n",
    "\n",
    "### ğŸ“– Lectures recommandÃ©es\n",
    "- [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/) â€” Joe Reis & Matt Housley\n",
    "- [The Data Warehouse Toolkit](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/books/) â€” Ralph Kimball\n",
    "- [Designing Data-Intensive Applications](https://dataintensive.net/) â€” Martin Kleppmann\n",
    "\n",
    "### ğŸŒ Sites & Blogs\n",
    "- [Data Engineering Weekly](https://www.dataengineeringweekly.com/) â€” Newsletter hebdomadaire\n",
    "- [Seattle Data Guy](https://www.youtube.com/@SeattleDataGuy) â€” ChaÃ®ne YouTube\n",
    "- [Start Data Engineering](https://www.startdataengineering.com/) â€” Tutoriels pratiques\n",
    "\n",
    "### ğŸ“ Certifications\n",
    "- Google Cloud Professional Data Engineer\n",
    "- AWS Certified Data Engineer\n",
    "- Databricks Certified Data Engineer\n",
    "\n",
    "### ğŸ› ï¸ Outils Ã  explorer\n",
    "- [dbt](https://www.getdbt.com/) â€” Transformation de donnÃ©es\n",
    "- [Airbyte](https://airbyte.com/) â€” Ingestion de donnÃ©es (open source)\n",
    "- [Apache Airflow](https://airflow.apache.org/) â€” Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next_steps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape\n",
    "\n",
    "Maintenant que tu as une vue d'ensemble du Data Engineering, passons Ã  la pratique !\n",
    "\n",
    "ğŸ‘‰ **Module suivant : `02_bash_for_data_engineers.ipynb`** â€” MaÃ®triser la ligne de commande\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **FÃ©licitations !** Tu as terminÃ© le premier module du parcours Data Engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
