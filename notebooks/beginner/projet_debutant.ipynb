{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üéÆ Projet Int√©grateur : Video Games Analytics Platform\n",
    "\n",
    "## Du CSV au Dashboard ‚Äî Ton Premier Pipeline Data Complet\n",
    "\n",
    "---\n",
    "\n",
    "Bienvenue dans ce **projet int√©grateur** ! Tu vas construire une plateforme d'analyse de jeux vid√©o **de A √† Z**, en mobilisant toutes les comp√©tences acquises dans les modules pr√©c√©dents.\n",
    "\n",
    "### üéì Approche P√©dagogique\n",
    "\n",
    "Ce projet est structur√© en **d√©fis**. Pour chaque √©tape :\n",
    "\n",
    "1. üìã **Lis le d√©fi** et les consignes\n",
    "2. ü§î **R√©fl√©chis** et essaie de coder toi-m√™me\n",
    "3. üí° **Consulte les indices** si tu bloques\n",
    "4. ‚úÖ **V√©rifie** ta solution en d√©roulant les r√©ponses\n",
    "\n",
    "> ‚ö†Ô∏è **Important** : Ne regarde pas les solutions avant d'avoir essay√© ! C'est en pratiquant qu'on apprend.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Ce que tu vas construire\n",
    "\n",
    "```{mermaid}\n",
    "flowchart LR\n",
    "    subgraph SOURCES[\"üì• SOURCES\"]\n",
    "        K[\"Kaggle<br/>Video Games Sales\"]\n",
    "        S[\"Web Scraping<br/>RAWG API\"]\n",
    "    end\n",
    "    \n",
    "    subgraph PROCESSING[\"‚öôÔ∏è PROCESSING\"]\n",
    "        PD[\"Pandas<br/>Nettoyage\"]\n",
    "        SP[\"PySpark<br/>Agr√©gations\"]\n",
    "    end\n",
    "    \n",
    "    subgraph STORAGE[\"üíæ STOCKAGE\"]\n",
    "        DUCK[(\"DuckDB<br/>SQL Analytics\")]\n",
    "        ES[(\"Elasticsearch<br/>Recherche\")]\n",
    "    end\n",
    "    \n",
    "    subgraph SERVING[\"üìä EXPOSITION\"]\n",
    "        API[\"FastAPI<br/>REST API\"]\n",
    "        DASH[\"Streamlit<br/>Dashboard\"]\n",
    "    end\n",
    "    \n",
    "    K --> PD\n",
    "    S --> PD\n",
    "    PD --> DUCK\n",
    "    PD --> ES\n",
    "    DUCK --> SP\n",
    "    SP --> API\n",
    "    ES --> API\n",
    "    API --> DASH\n",
    "    DUCK --> DASH\n",
    "    \n",
    "    style SOURCES fill:#e74c3c,color:#fff\n",
    "    style PROCESSING fill:#3498db,color:#fff\n",
    "    style STORAGE fill:#2ecc71,color:#fff\n",
    "    style SERVING fill:#9b59b6,color:#fff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competences",
   "metadata": {},
   "source": [
    "## üìö Comp√©tences Mobilis√©es\n",
    "\n",
    "| Module | Comp√©tence | Application dans le projet |\n",
    "|--------|------------|---------------------------|\n",
    "| M01 | Concepts Data Engineering | Architecture du pipeline |\n",
    "| M02 | Bash & Linux | Scripts d'automatisation |\n",
    "| M03 | Git | Versioning du projet |\n",
    "| M04-05 | Python & Pandas | Traitement de donn√©es |\n",
    "| M06-07 | SQL & Databases | Requ√™tes analytiques avec DuckDB |\n",
    "| M08 | Big Data Concepts | Pens√©e distribu√©e |\n",
    "| M10 | Elasticsearch | Recherche full-text |\n",
    "| M11 | PySpark | Traitement √† l'√©chelle |\n",
    "| M12 | Orchestration | Pipeline automatis√© |\n",
    "| M13 | FastAPI | API REST |\n",
    "| **NEW** | **Web Scraping** | BeautifulSoup, Requests |\n",
    "| **NEW** | **Streamlit** | Dashboard interactif |\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Le Dataset\n",
    "\n",
    "**Kaggle - Video Game Sales with Ratings**\n",
    "\n",
    "üîó https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings\n",
    "\n",
    "| Colonne | Description |\n",
    "|---------|-------------|\n",
    "| `Name` | Nom du jeu |\n",
    "| `Platform` | Console (PS4, Xbox, PC...) |\n",
    "| `Year_of_Release` | Ann√©e de sortie |\n",
    "| `Genre` | Genre (Action, Sports, RPG...) |\n",
    "| `Publisher` | √âditeur |\n",
    "| `NA_Sales`, `EU_Sales`, `JP_Sales`, `Other_Sales` | Ventes par r√©gion (millions) |\n",
    "| `Global_Sales` | Ventes mondiales |\n",
    "| `Critic_Score` | Note Metacritic (0-100) |\n",
    "| `User_Score` | Note utilisateurs (0-10) |\n",
    "| `Rating` | Classification ESRB (E, T, M...) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase0_title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üöÄ Phase 0 : Setup du Projet\n",
    "\n",
    "Avant de coder, il faut pr√©parer l'environnement de travail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi0_1",
   "metadata": {},
   "source": [
    "## üß© D√©fi 0.1 : Cr√©er la structure du projet\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Cr√©e une structure de dossiers pour le projet `videogames-analytics` avec :\n",
    "- Un dossier `data/` avec des sous-dossiers `raw/`, `processed/`, `enriched/`\n",
    "- Un dossier `scripts/` pour les scripts Python\n",
    "- Un dossier `api/` pour FastAPI\n",
    "- Un dossier `dashboard/` pour Streamlit\n",
    "- Un dossier `notebooks/` pour l'exploration\n",
    "- Les fichiers `requirements.txt`, `.gitignore`, `README.md`\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "1. Quelle commande Bash permet de cr√©er plusieurs dossiers en une seule ligne ?\n",
    "2. Comment cr√©er des sous-dossiers imbriqu√©s qui n'existent pas encore ?\n",
    "3. Quels fichiers/dossiers doit-on ignorer dans Git pour un projet Python data ?\n",
    "\n",
    "---\n",
    "\n",
    "*Prends le temps de r√©fl√©chir et d'essayer avant de regarder les indices ou la solution* ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code0_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# üìù TON CODE ICI\n",
    "# Cr√©e la structure du projet videogames-analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution0_1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "- Utilise `mkdir -p` pour cr√©er des dossiers imbriqu√©s (l'option `-p` cr√©e les parents si n√©cessaire)\n",
    "- Tu peux utiliser les accolades `{}` pour cr√©er plusieurs dossiers : `mkdir -p projet/{dossier1,dossier2}`\n",
    "- Pour le `.gitignore`, pense √† : `__pycache__/`, `.env`, `data/raw/*`, `*.pyc`, `.venv/`, `*.db`\n",
    "- Utilise `cat << 'EOF' > fichier` pour cr√©er un fichier multi-lignes\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution compl√®te</b></summary>\n",
    "\n",
    "```bash\n",
    "# Cr√©ation de la structure en une commande\n",
    "mkdir -p videogames-analytics/{data/{raw,processed,enriched},scripts,api,dashboard,notebooks,tests}\n",
    "\n",
    "cd videogames-analytics\n",
    "\n",
    "# Cr√©er requirements.txt\n",
    "cat << 'EOF' > requirements.txt\n",
    "# Data Processing\n",
    "pandas>=2.0.0\n",
    "numpy>=1.24.0\n",
    "pyarrow>=12.0.0\n",
    "\n",
    "# Web Scraping\n",
    "requests>=2.31.0\n",
    "beautifulsoup4>=4.12.0\n",
    "lxml>=4.9.0\n",
    "\n",
    "# Databases\n",
    "duckdb>=0.9.0\n",
    "elasticsearch>=8.0.0\n",
    "\n",
    "# Big Data\n",
    "pyspark>=3.5.0\n",
    "\n",
    "# API\n",
    "fastapi>=0.104.0\n",
    "uvicorn>=0.24.0\n",
    "\n",
    "# Dashboard\n",
    "streamlit>=1.28.0\n",
    "plotly>=5.18.0\n",
    "\n",
    "# Utilities\n",
    "python-dotenv>=1.0.0\n",
    "tqdm>=4.66.0\n",
    "EOF\n",
    "\n",
    "# Cr√©er .gitignore\n",
    "cat << 'EOF' > .gitignore\n",
    "# Data (on ne versionne pas les donn√©es)\n",
    "data/raw/*\n",
    "data/processed/*\n",
    "data/enriched/*\n",
    "!data/*/.gitkeep\n",
    "*.db\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".venv/\n",
    "venv/\n",
    "\n",
    "# Environment\n",
    ".env\n",
    "*.log\n",
    "\n",
    "# IDE\n",
    ".idea/\n",
    ".vscode/\n",
    "\n",
    "# Jupyter\n",
    ".ipynb_checkpoints/\n",
    "EOF\n",
    "\n",
    "# Cr√©er les .gitkeep pour garder les dossiers vides dans Git\n",
    "touch data/raw/.gitkeep data/processed/.gitkeep data/enriched/.gitkeep\n",
    "\n",
    "echo \"‚úÖ Structure cr√©√©e !\"\n",
    "```\n",
    "\n",
    "**Explications :**\n",
    "- `mkdir -p` cr√©e tous les dossiers parents manquants\n",
    "- Les accolades `{a,b,c}` cr√©ent plusieurs dossiers en une commande\n",
    "- `cat << 'EOF' > fichier` permet d'√©crire plusieurs lignes dans un fichier\n",
    "- `.gitkeep` est une convention pour garder les dossiers vides dans Git\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi0_2",
   "metadata": {},
   "source": [
    "## üß© D√©fi 0.2 : Initialiser Git\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "1. Initialise un d√©p√¥t Git dans le dossier `videogames-analytics`\n",
    "2. Ajoute tous les fichiers\n",
    "3. Fais un premier commit avec le message : `\"üéÆ Initial commit: project structure\"`\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "1. Quelle commande initialise un nouveau d√©p√¥t Git ?\n",
    "2. Comment ajouter tous les fichiers d'un coup au staging ?\n",
    "3. Quelle est la syntaxe pour cr√©er un commit avec un message ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code0_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# üìù TON CODE ICI\n",
    "# Initialise Git et fais le premier commit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution0_2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "Les 3 commandes Git essentielles :\n",
    "- `git init` : initialise un nouveau d√©p√¥t\n",
    "- `git add .` : ajoute tous les fichiers au staging\n",
    "- `git commit -m \"message\"` : cr√©e un commit\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```bash\n",
    "cd videogames-analytics\n",
    "\n",
    "# Initialiser le d√©p√¥t Git\n",
    "git init\n",
    "\n",
    "# Ajouter tous les fichiers au staging\n",
    "git add .\n",
    "\n",
    "# Cr√©er le premier commit\n",
    "git commit -m \"üéÆ Initial commit: project structure\"\n",
    "\n",
    "# V√©rifier l'historique\n",
    "git log --oneline\n",
    "```\n",
    "\n",
    "**R√©sultat attendu :**\n",
    "```\n",
    "abc1234 üéÆ Initial commit: project structure\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1_title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üì• Phase 1 : Ingestion des Donn√©es\n",
    "\n",
    "## T√©l√©chargement du Dataset\n",
    "\n",
    "T√©l√©charge le dataset depuis Kaggle :\n",
    "1. Va sur https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings\n",
    "2. T√©l√©charge le ZIP\n",
    "3. Extrait le CSV dans `data/raw/`\n",
    "\n",
    "> üí° Si tu n'as pas de compte Kaggle, utilise le code ci-dessous pour g√©n√©rer des donn√©es d'exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©ration de donn√©es d'exemple (ex√©cute cette cellule si tu n'as pas Kaggle)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"videogames-analytics\")\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_games = 5000\n",
    "\n",
    "platforms = ['PS4', 'XOne', 'PC', 'WiiU', 'PS3', 'X360', 'Wii', 'PSV', '3DS', 'PS2']\n",
    "genres = ['Action', 'Sports', 'Shooter', 'Role-Playing', 'Racing', 'Platform', \n",
    "          'Fighting', 'Simulation', 'Adventure', 'Strategy', 'Puzzle', 'Misc']\n",
    "publishers = ['Electronic Arts', 'Activision', 'Ubisoft', 'Nintendo', 'Sony', \n",
    "              'Take-Two', 'Sega', 'Capcom', 'Konami', 'Bandai Namco', 'Square Enix']\n",
    "ratings = ['E', 'E10+', 'T', 'M', 'RP', None]\n",
    "\n",
    "game_prefixes = ['Super', 'Ultimate', 'Call of', 'Legend of', 'Final', 'Grand', 'Dark']\n",
    "game_suffixes = ['Warriors', 'Quest', 'Adventure', 'Legends', 'Chronicles', 'Heroes']\n",
    "game_names = [f\"{np.random.choice(game_prefixes)} {np.random.choice(game_suffixes)} {i}\" \n",
    "              for i in range(n_games)]\n",
    "\n",
    "games_df = pd.DataFrame({\n",
    "    'Name': game_names,\n",
    "    'Platform': np.random.choice(platforms, n_games),\n",
    "    'Year_of_Release': np.random.choice(range(2000, 2024), n_games),\n",
    "    'Genre': np.random.choice(genres, n_games),\n",
    "    'Publisher': np.random.choice(publishers, n_games),\n",
    "    'NA_Sales': np.round(np.random.exponential(0.5, n_games), 2),\n",
    "    'EU_Sales': np.round(np.random.exponential(0.3, n_games), 2),\n",
    "    'JP_Sales': np.round(np.random.exponential(0.2, n_games), 2),\n",
    "    'Other_Sales': np.round(np.random.exponential(0.1, n_games), 2),\n",
    "    'Critic_Score': np.where(np.random.random(n_games) > 0.2, \n",
    "                             np.random.randint(40, 100, n_games), np.nan),\n",
    "    'User_Score': np.where(np.random.random(n_games) > 0.3,\n",
    "                           np.round(np.random.uniform(3, 10, n_games), 1), np.nan),\n",
    "    'Rating': np.random.choice(ratings, n_games)\n",
    "})\n",
    "\n",
    "games_df['Global_Sales'] = (games_df['NA_Sales'] + games_df['EU_Sales'] + \n",
    "                            games_df['JP_Sales'] + games_df['Other_Sales']).round(2)\n",
    "\n",
    "games_df.to_csv(RAW_DIR / 'Video_Games_Sales.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Dataset cr√©√© : {len(games_df):,} jeux\")\n",
    "print(f\"üìÅ Fichier : {RAW_DIR / 'Video_Games_Sales.csv'}\")\n",
    "games_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi1_1",
   "metadata": {},
   "source": [
    "## üß© D√©fi 1.1 : Explorer les donn√©es\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Charge le CSV et r√©ponds √† ces questions :\n",
    "\n",
    "1. **Combien** de jeux contient le dataset ?\n",
    "2. **Quelles colonnes** ont des valeurs manquantes ? Quel pourcentage ?\n",
    "3. **Quel est le jeu** le plus vendu ?\n",
    "4. **Quels sont les 5 genres** les plus repr√©sent√©s ?\n",
    "5. **Quelle est la plage d'ann√©es** couverte par le dataset ?\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "- Quelle m√©thode Pandas donne les dimensions d'un DataFrame ?\n",
    "- Comment compter les valeurs manquantes par colonne ?\n",
    "- Comment trouver la ligne avec la valeur maximale d'une colonne ?\n",
    "- Comment compter les occurrences de chaque valeur d'une colonne ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code1_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TON CODE ICI\n",
    "# Explore les donn√©es et r√©ponds aux 5 questions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Charge le CSV\n",
    "df = pd.read_csv('videogames-analytics/data/raw/Video_Games_Sales.csv')\n",
    "\n",
    "# Question 1 : Combien de jeux ?\n",
    "\n",
    "\n",
    "# Question 2 : Valeurs manquantes ?\n",
    "\n",
    "\n",
    "# Question 3 : Jeu le plus vendu ?\n",
    "\n",
    "\n",
    "# Question 4 : Top 5 genres ?\n",
    "\n",
    "\n",
    "# Question 5 : Plage d'ann√©es ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution1_1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "- Nombre de lignes : `len(df)` ou `df.shape[0]`\n",
    "- Valeurs manquantes : `df.isnull().sum()`\n",
    "- Pourcentage : `df.isnull().sum() / len(df) * 100`\n",
    "- Ligne avec max : `df.loc[df['colonne'].idxmax()]`\n",
    "- Compter par cat√©gorie : `df['colonne'].value_counts()`\n",
    "- Min/Max : `df['colonne'].min()`, `df['colonne'].max()`\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Charger les donn√©es\n",
    "df = pd.read_csv('videogames-analytics/data/raw/Video_Games_Sales.csv')\n",
    "\n",
    "# 1. Nombre de jeux\n",
    "print(f\"üìä Nombre de jeux : {len(df):,}\")\n",
    "print(f\"   (ou avec shape : {df.shape[0]:,} lignes, {df.shape[1]} colonnes)\")\n",
    "\n",
    "# 2. Valeurs manquantes\n",
    "print(\"\\n‚ùì Valeurs manquantes :\")\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(1)\n",
    "missing_df = pd.DataFrame({'Manquantes': missing, '%': missing_pct})\n",
    "print(missing_df[missing_df['Manquantes'] > 0])\n",
    "\n",
    "# 3. Jeu le plus vendu\n",
    "top_game = df.loc[df['Global_Sales'].idxmax()]\n",
    "print(f\"\\nüèÜ Jeu le plus vendu : {top_game['Name']}\")\n",
    "print(f\"   Ventes : {top_game['Global_Sales']}M$ | Plateforme : {top_game['Platform']}\")\n",
    "\n",
    "# 4. Top 5 genres\n",
    "print(\"\\nüéØ Top 5 genres :\")\n",
    "print(df['Genre'].value_counts().head())\n",
    "\n",
    "# 5. Plage d'ann√©es\n",
    "min_year = df['Year_of_Release'].min()\n",
    "max_year = df['Year_of_Release'].max()\n",
    "print(f\"\\nüìÖ Ann√©es : {min_year:.0f} - {max_year:.0f}\")\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi1_2",
   "metadata": {},
   "source": [
    "## üß© D√©fi 1.2 : Nettoyer les donn√©es\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Cr√©e une fonction `clean_videogames_data(input_path, output_path)` qui :\n",
    "\n",
    "1. **Supprime les doublons** sur les colonnes (Name, Platform, Year_of_Release)\n",
    "2. **Convertit `Year_of_Release`** en entier (en g√©rant les NaN)\n",
    "3. **Remplit les ventes manquantes** par 0\n",
    "4. **Cr√©e une colonne `Decade`** : la d√©cennie (ex: 2010 pour l'ann√©e 2015)\n",
    "5. **Cr√©e une colonne `Sales_Category`** bas√©e sur Global_Sales :\n",
    "   - `< 0.1` ‚Üí \"Flop\"\n",
    "   - `0.1 - 1` ‚Üí \"Niche\"\n",
    "   - `1 - 5` ‚Üí \"Hit\"\n",
    "   - `> 5` ‚Üí \"Blockbuster\"\n",
    "6. **Sauvegarde le r√©sultat en Parquet** dans `data/processed/`\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "- Comment supprimer les doublons sur certaines colonnes seulement ?\n",
    "- Comment cr√©er une colonne bas√©e sur des conditions multiples (bins) ?\n",
    "- Pourquoi choisir Parquet plut√¥t que CSV pour les donn√©es nettoy√©es ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code1_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TON CODE ICI\n",
    "# Cr√©e la fonction de nettoyage\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_videogames_data(input_path: Path, output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nettoie le dataset de jeux vid√©o.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Chemin vers le CSV brut\n",
    "        output_path: Chemin pour sauvegarder le Parquet nettoy√©\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame nettoy√©\n",
    "    \"\"\"\n",
    "    # TON CODE ICI\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test ta fonction\n",
    "# PROJECT_ROOT = Path('videogames-analytics')\n",
    "# cleaned_df = clean_videogames_data(\n",
    "#     input_path=PROJECT_ROOT / 'data' / 'raw' / 'Video_Games_Sales.csv',\n",
    "#     output_path=PROJECT_ROOT / 'data' / 'processed' / 'games_cleaned.parquet'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution1_2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "- Doublons sur colonnes sp√©cifiques : `df.drop_duplicates(subset=['col1', 'col2'])`\n",
    "- Conversion avec NaN : `pd.to_numeric(df['col'], errors='coerce')` puis `.astype('Int64')` (nullable integer)\n",
    "- D√©cennie : `df['Year'] // 10 * 10` (division enti√®re puis multiplication)\n",
    "- Cat√©gories avec bins : `pd.cut(df['col'], bins=[...], labels=[...])`\n",
    "- Parquet : `df.to_parquet('fichier.parquet', index=False)`\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_videogames_data(input_path: Path, output_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Nettoie le dataset de jeux vid√©o.\n",
    "    \"\"\"\n",
    "    print(\"üßπ Nettoyage des donn√©es...\")\n",
    "    \n",
    "    # Charger\n",
    "    df = pd.read_csv(input_path)\n",
    "    initial_count = len(df)\n",
    "    print(f\"   Lignes initiales : {initial_count:,}\")\n",
    "    \n",
    "    # 1. Supprimer les doublons\n",
    "    df = df.drop_duplicates(subset=['Name', 'Platform', 'Year_of_Release'])\n",
    "    print(f\"   Apr√®s d√©duplication : {len(df):,} (-{initial_count - len(df)})\")\n",
    "    \n",
    "    # 2. Convertir Year_of_Release en entier nullable\n",
    "    df['Year_of_Release'] = pd.to_numeric(df['Year_of_Release'], errors='coerce')\n",
    "    df['Year_of_Release'] = df['Year_of_Release'].astype('Int64')\n",
    "    \n",
    "    # 3. Remplir les ventes manquantes par 0\n",
    "    sales_cols = ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales']\n",
    "    df[sales_cols] = df[sales_cols].fillna(0)\n",
    "    \n",
    "    # 4. Cr√©er la d√©cennie\n",
    "    df['Decade'] = (df['Year_of_Release'] // 10 * 10).astype('Int64')\n",
    "    \n",
    "    # 5. Cr√©er la cat√©gorie de ventes\n",
    "    df['Sales_Category'] = pd.cut(\n",
    "        df['Global_Sales'],\n",
    "        bins=[-np.inf, 0.1, 1, 5, np.inf],\n",
    "        labels=['Flop', 'Niche', 'Hit', 'Blockbuster']\n",
    "    )\n",
    "    \n",
    "    # 6. Sauvegarder en Parquet\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    print(f\"   ‚úÖ Sauvegard√© : {output_path}\")\n",
    "    print(f\"   Lignes finales : {len(df):,}\")\n",
    "    print(f\"   Nouvelles colonnes : Decade, Sales_Category\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Ex√©cution\n",
    "PROJECT_ROOT = Path('videogames-analytics')\n",
    "cleaned_df = clean_videogames_data(\n",
    "    input_path=PROJECT_ROOT / 'data' / 'raw' / 'Video_Games_Sales.csv',\n",
    "    output_path=PROJECT_ROOT / 'data' / 'processed' / 'games_cleaned.parquet'\n",
    ")\n",
    "\n",
    "# V√©rification\n",
    "print(\"\\nüìä Aper√ßu :\")\n",
    "print(cleaned_df[['Name', 'Year_of_Release', 'Decade', 'Global_Sales', 'Sales_Category']].head())\n",
    "```\n",
    "\n",
    "**Pourquoi Parquet ?**\n",
    "- Compression : fichier 5-10x plus petit que CSV\n",
    "- Types pr√©serv√©s : pas de perte des types (dates, entiers, cat√©gories)\n",
    "- Lecture rapide : format colonne optimis√© pour l'analytique\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üï∑Ô∏è Phase 2 : Web Scraping\n",
    "\n",
    "Le **Web Scraping** consiste √† extraire des donn√©es depuis des pages web automatiquement.\n",
    "\n",
    "### Outils Python\n",
    "\n",
    "| Outil | Usage |\n",
    "|-------|-------|\n",
    "| `requests` | Envoyer des requ√™tes HTTP et r√©cup√©rer le HTML |\n",
    "| `BeautifulSoup` | Parser le HTML et extraire les √©l√©ments |\n",
    "| `lxml` | Parser HTML/XML (plus rapide) |\n",
    "\n",
    "### ‚ö†Ô∏è R√®gles d'√âthique du Scraping\n",
    "\n",
    "1. **Respecter le `robots.txt`** ‚Äî v√©rifie ce que le site autorise\n",
    "2. **Ajouter des d√©lais** ‚Äî `time.sleep()` entre les requ√™tes\n",
    "3. **S'identifier** ‚Äî utilise un User-Agent descriptif\n",
    "4. **Ne pas surcharger** ‚Äî limite le nombre de requ√™tes\n",
    "5. **V√©rifier les CGU** ‚Äî certains sites interdisent le scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi2_1",
   "metadata": {},
   "source": [
    "## üß© D√©fi 2.1 : Scraper Wikipedia\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "√âcris une fonction `scrape_bestselling_games()` qui :\n",
    "\n",
    "1. R√©cup√®re la page : https://en.wikipedia.org/wiki/List_of_best-selling_video_games\n",
    "2. Trouve le premier tableau avec la classe `wikitable`\n",
    "3. Extrait les **10 premiers jeux** avec : Nom, Ventes, Plateforme(s)\n",
    "4. Retourne un DataFrame pandas\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "- Comment envoyer une requ√™te HTTP GET en Python ?\n",
    "- Pourquoi faut-il sp√©cifier un User-Agent ?\n",
    "- Comment trouver un √©l√©ment HTML par sa classe avec BeautifulSoup ?\n",
    "- Comment extraire le texte d'une balise HTML ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code2_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TON CODE ICI\n",
    "# Cr√©e la fonction de scraping Wikipedia\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_bestselling_games() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape la liste des jeux les plus vendus depuis Wikipedia.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame avec colonnes: name, sales, platform\n",
    "    \"\"\"\n",
    "    # TON CODE ICI\n",
    "    pass\n",
    "\n",
    "\n",
    "# Test ta fonction\n",
    "# df_wiki = scrape_bestselling_games()\n",
    "# print(df_wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution2_1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. Requ√™te HTTP avec User-Agent\n",
    "headers = {'User-Agent': 'MonBot/1.0 (contact@example.com)'}\n",
    "response = requests.get(url, headers=headers, timeout=10)\n",
    "response.raise_for_status()  # L√®ve une exception si erreur HTTP\n",
    "\n",
    "# 2. Parser le HTML\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "# 3. Trouver un √©l√©ment par classe\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# 4. Trouver toutes les lignes\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# 5. Extraire le texte d'une cellule\n",
    "cell.get_text(strip=True)  # strip=True enl√®ve les espaces\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_bestselling_games() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape la liste des jeux les plus vendus depuis Wikipedia.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_best-selling_video_games\"\n",
    "    \n",
    "    # Headers pour s'identifier (bonne pratique)\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Educational Bot - Data Engineering Bootcamp)'\n",
    "    }\n",
    "    \n",
    "    # Requ√™te HTTP\n",
    "    print(f\"üåê R√©cup√©ration de {url}...\")\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.raise_for_status()  # Erreur si status != 200\n",
    "    \n",
    "    # Parser le HTML\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Trouver le premier tableau wikitable\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    \n",
    "    if not table:\n",
    "        raise ValueError(\"‚ùå Tableau non trouv√© sur la page\")\n",
    "    \n",
    "    # Extraire les donn√©es\n",
    "    games = []\n",
    "    rows = table.find_all('tr')[1:11]  # Skip header, prendre 10 lignes\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if len(cells) >= 3:\n",
    "            games.append({\n",
    "                'name': cells[0].get_text(strip=True),\n",
    "                'sales': cells[1].get_text(strip=True),\n",
    "                'platform': cells[2].get_text(strip=True) if len(cells) > 2 else 'N/A'\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ {len(games)} jeux extraits\")\n",
    "    return pd.DataFrame(games)\n",
    "\n",
    "\n",
    "# Test\n",
    "try:\n",
    "    df_wiki = scrape_bestselling_games()\n",
    "    print(\"\\nüéÆ Top 10 jeux les plus vendus (Wikipedia) :\")\n",
    "    print(df_wiki.to_string(index=False))\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur : {e}\")\n",
    "```\n",
    "\n",
    "**Explications :**\n",
    "- `raise_for_status()` l√®ve une exception si le serveur retourne une erreur (404, 500, etc.)\n",
    "- `find_all('tr')[1:11]` : on ignore la premi√®re ligne (header) et on prend les 10 suivantes\n",
    "- `get_text(strip=True)` extrait le texte en enlevant les espaces superflus\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üíæ Phase 3 : Stockage des Donn√©es\n",
    "\n",
    "On va stocker nos donn√©es dans deux syst√®mes compl√©mentaires :\n",
    "\n",
    "| Syst√®me | Usage | Avantage |\n",
    "|---------|-------|----------|\n",
    "| **DuckDB** | Requ√™tes SQL analytiques | Ultra-rapide, zero config |\n",
    "| **Elasticsearch** | Recherche full-text | Recherche fuzzy, suggestions |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi3_1",
   "metadata": {},
   "source": [
    "## üß© D√©fi 3.1 : Charger dans DuckDB\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "1. Cr√©e une base DuckDB `data/videogames.db`\n",
    "2. Charge le fichier Parquet nettoy√© dans une table `games`\n",
    "3. √âcris et ex√©cute les requ√™tes SQL suivantes :\n",
    "   - **Top 10 jeux** les plus vendus\n",
    "   - **Ventes totales par genre** (tri√©es par ventes d√©croissantes)\n",
    "   - **Top 3 jeux par genre** (utilise une Window Function)\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "- Comment DuckDB peut lire directement un fichier Parquet sans le charger en m√©moire ?\n",
    "- Quelle est la diff√©rence entre `GROUP BY` et `PARTITION BY` ?\n",
    "- Quelle fonction SQL permet de num√©roter les lignes dans chaque groupe ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code3_1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TON CODE ICI\n",
    "# Charge les donn√©es dans DuckDB et ex√©cute les requ√™tes\n",
    "\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('videogames-analytics')\n",
    "DB_PATH = PROJECT_ROOT / 'data' / 'videogames.db'\n",
    "\n",
    "# 1. Connexion et cr√©ation de la table\n",
    "\n",
    "\n",
    "# 2. Top 10 jeux les plus vendus\n",
    "\n",
    "\n",
    "# 3. Ventes totales par genre\n",
    "\n",
    "\n",
    "# 4. Top 3 jeux par genre (Window Function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution3_1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "```python\n",
    "# Connexion DuckDB\n",
    "conn = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "# Cr√©er une table depuis un Parquet\n",
    "conn.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE games AS \n",
    "    SELECT * FROM read_parquet('chemin/fichier.parquet')\n",
    "\"\"\")\n",
    "\n",
    "# Window Function pour classement par groupe\n",
    "ROW_NUMBER() OVER (PARTITION BY genre ORDER BY sales DESC) as rank\n",
    "\n",
    "# Puis filtrer avec WHERE rank <= 3\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```python\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('videogames-analytics')\n",
    "DB_PATH = PROJECT_ROOT / 'data' / 'videogames.db'\n",
    "PARQUET_PATH = PROJECT_ROOT / 'data' / 'processed' / 'games_cleaned.parquet'\n",
    "\n",
    "# Connexion\n",
    "conn = duckdb.connect(str(DB_PATH))\n",
    "\n",
    "# 1. Cr√©er la table depuis le Parquet\n",
    "conn.execute(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE games AS \n",
    "    SELECT * FROM read_parquet('{PARQUET_PATH}')\n",
    "\"\"\")\n",
    "print(f\"‚úÖ Table 'games' cr√©√©e avec {conn.execute('SELECT COUNT(*) FROM games').fetchone()[0]:,} lignes\")\n",
    "\n",
    "# 2. Top 10 jeux les plus vendus\n",
    "print(\"\\nüèÜ Top 10 jeux les plus vendus :\")\n",
    "print(conn.execute(\"\"\"\n",
    "    SELECT Name, Platform, Genre, Global_Sales\n",
    "    FROM games\n",
    "    ORDER BY Global_Sales DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "# 3. Ventes totales par genre\n",
    "print(\"\\nüìä Ventes par genre :\")\n",
    "print(conn.execute(\"\"\"\n",
    "    SELECT \n",
    "        Genre,\n",
    "        COUNT(*) as nb_games,\n",
    "        ROUND(SUM(Global_Sales), 2) as total_sales,\n",
    "        ROUND(AVG(Global_Sales), 2) as avg_sales\n",
    "    FROM games\n",
    "    GROUP BY Genre\n",
    "    ORDER BY total_sales DESC\n",
    "\"\"\").fetchdf())\n",
    "\n",
    "# 4. Top 3 jeux par genre (Window Function)\n",
    "print(\"\\nüéØ Top 3 par genre :\")\n",
    "print(conn.execute(\"\"\"\n",
    "    WITH ranked AS (\n",
    "        SELECT \n",
    "            Genre,\n",
    "            Name,\n",
    "            Global_Sales,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY Genre \n",
    "                ORDER BY Global_Sales DESC\n",
    "            ) as rank\n",
    "        FROM games\n",
    "    )\n",
    "    SELECT Genre, rank, Name, Global_Sales\n",
    "    FROM ranked\n",
    "    WHERE rank <= 3\n",
    "    ORDER BY Genre, rank\n",
    "\"\"\").fetchdf().head(20))\n",
    "\n",
    "conn.close()\n",
    "print(f\"\\n‚úÖ Base sauvegard√©e : {DB_PATH}\")\n",
    "```\n",
    "\n",
    "**Explication Window Function :**\n",
    "- `PARTITION BY Genre` : cr√©e des \"fen√™tres\" par genre\n",
    "- `ORDER BY Global_Sales DESC` : ordonne dans chaque fen√™tre\n",
    "- `ROW_NUMBER()` : num√©rote de 1 √† N dans chaque fen√™tre\n",
    "- On filtre ensuite `WHERE rank <= 3` pour garder le top 3\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defi3_2",
   "metadata": {},
   "source": [
    "## üß© D√©fi 3.2 : Indexer dans Elasticsearch\n",
    "\n",
    "### üìã Pr√©requis\n",
    "\n",
    "Lance Elasticsearch en local (comme vu dans le M10) :\n",
    "```bash\n",
    "cd elasticsearch-8.x.x\n",
    "./bin/elasticsearch\n",
    "```\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "1. Cr√©e une fonction `index_games_to_es(df, index_name)` qui :\n",
    "   - Se connecte √† Elasticsearch local (http://localhost:9200)\n",
    "   - Supprime l'index s'il existe d√©j√†\n",
    "   - Cr√©e un index `videogames` avec un mapping appropri√©\n",
    "   - Indexe tous les jeux en utilisant le bulk API\n",
    "\n",
    "2. Cr√©e une fonction `search_games(query)` qui :\n",
    "   - Fait une recherche fuzzy sur le champ `Name`\n",
    "   - Retourne les 10 meilleurs r√©sultats\n",
    "\n",
    "### ü§î Questions pour r√©fl√©chir\n",
    "\n",
    "- Pourquoi utiliser le bulk API plut√¥t que des insertions une par une ?\n",
    "- Quelle est la diff√©rence entre les types `text` et `keyword` dans ES ?\n",
    "- Comment fonctionne la recherche fuzzy ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code3_2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù TON CODE ICI\n",
    "# Indexe les jeux dans Elasticsearch\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "def index_games_to_es(df: pd.DataFrame, index_name: str = \"videogames\") -> int:\n",
    "    \"\"\"\n",
    "    Indexe les jeux dans Elasticsearch.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame des jeux\n",
    "        index_name: Nom de l'index ES\n",
    "    \n",
    "    Returns:\n",
    "        Nombre de documents index√©s\n",
    "    \"\"\"\n",
    "    # TON CODE ICI\n",
    "    pass\n",
    "\n",
    "\n",
    "def search_games(query: str, index_name: str = \"videogames\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recherche des jeux par nom (fuzzy search).\n",
    "    \n",
    "    Args:\n",
    "        query: Terme de recherche\n",
    "        index_name: Nom de l'index ES\n",
    "    \n",
    "    Returns:\n",
    "        Liste des r√©sultats\n",
    "    \"\"\"\n",
    "    # TON CODE ICI\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solution3_2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Cliquer pour voir les indices</b></summary>\n",
    "\n",
    "```python\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Connexion\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "es.ping()  # V√©rifie la connexion\n",
    "\n",
    "# Supprimer un index\n",
    "es.indices.delete(index=\"videogames\", ignore=[404])\n",
    "\n",
    "# Cr√©er avec mapping\n",
    "es.indices.create(index=\"videogames\", body={\"mappings\": {...}})\n",
    "\n",
    "# Bulk indexation\n",
    "actions = [{\"_index\": \"videogames\", \"_source\": doc} for doc in docs]\n",
    "helpers.bulk(es, actions)\n",
    "\n",
    "# Recherche fuzzy\n",
    "es.search(index=\"videogames\", query={\n",
    "    \"match\": {\"Name\": {\"query\": \"...\", \"fuzziness\": \"AUTO\"}}\n",
    "})\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "<summary>‚úÖ <b>Cliquer pour voir la solution</b></summary>\n",
    "\n",
    "```python\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "def index_games_to_es(df: pd.DataFrame, index_name: str = \"videogames\") -> int:\n",
    "    \"\"\"\n",
    "    Indexe les jeux dans Elasticsearch.\n",
    "    \"\"\"\n",
    "    # Connexion\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    \n",
    "    if not es.ping():\n",
    "        raise ConnectionError(\"‚ùå Elasticsearch non disponible sur localhost:9200\")\n",
    "    \n",
    "    print(f\"‚úÖ Connect√© √† Elasticsearch\")\n",
    "    \n",
    "    # Supprimer l'index s'il existe\n",
    "    if es.indices.exists(index=index_name):\n",
    "        es.indices.delete(index=index_name)\n",
    "        print(f\"   Index '{index_name}' supprim√©\")\n",
    "    \n",
    "    # Cr√©er l'index avec mapping\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"Name\": {\"type\": \"text\", \"analyzer\": \"standard\"},\n",
    "                \"Platform\": {\"type\": \"keyword\"},\n",
    "                \"Genre\": {\"type\": \"keyword\"},\n",
    "                \"Publisher\": {\"type\": \"keyword\"},\n",
    "                \"Year_of_Release\": {\"type\": \"integer\"},\n",
    "                \"Global_Sales\": {\"type\": \"float\"},\n",
    "                \"Critic_Score\": {\"type\": \"float\"},\n",
    "                \"Sales_Category\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    es.indices.create(index=index_name, body=mapping)\n",
    "    print(f\"   Index '{index_name}' cr√©√©\")\n",
    "    \n",
    "    # Pr√©parer les documents (remplacer NaN par None)\n",
    "    records = df.where(pd.notnull(df), None).to_dict('records')\n",
    "    \n",
    "    # Bulk indexation\n",
    "    actions = [\n",
    "        {\"_index\": index_name, \"_source\": record}\n",
    "        for record in records\n",
    "    ]\n",
    "    \n",
    "    success, errors = helpers.bulk(es, actions, raise_on_error=False)\n",
    "    \n",
    "    print(f\"‚úÖ {success} documents index√©s\")\n",
    "    if errors:\n",
    "        print(f\"‚ö†Ô∏è {len(errors)} erreurs\")\n",
    "    \n",
    "    return success\n",
    "\n",
    "\n",
    "def search_games(query: str, index_name: str = \"videogames\") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Recherche des jeux par nom (fuzzy search).\n",
    "    \"\"\"\n",
    "    es = Elasticsearch(\"http://localhost:9200\")\n",
    "    \n",
    "    response = es.search(\n",
    "        index=index_name,\n",
    "        query={\n",
    "            \"match\": {\n",
    "                \"Name\": {\n",
    "                    \"query\": query,\n",
    "                    \"fuzziness\": \"AUTO\"  # Tol√®re les fautes de frappe\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        size=10\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        results.append({\n",
    "            'score': hit['_score'],\n",
    "            **hit['_source']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Test\n",
    "try:\n",
    "    # Charger et indexer\n",
    "    df = pd.read_parquet('videogames-analytics/data/processed/games_cleaned.parquet')\n",
    "    index_games_to_es(df)\n",
    "    \n",
    "    # Rechercher\n",
    "    print(\"\\nüîç Recherche 'Final Fantasi' (avec faute) :\")\n",
    "    results = search_games(\"Final Fantasi\")\n",
    "    for r in results[:5]:\n",
    "        print(f\"   {r['Name']} ({r['Platform']}) - Score: {r['score']:.2f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erreur : {e}\")\n",
    "    print(\"   Assure-toi qu'Elasticsearch est lanc√© sur localhost:9200\")\n",
    "```\n",
    "\n",
    "**Explications :**\n",
    "- `text` : le champ est analys√© (tokenis√©, stemming) ‚Üí pour la recherche full-text\n",
    "- `keyword` : valeur exacte ‚Üí pour les filtres et agr√©gations\n",
    "- `fuzziness: AUTO` : tol√®re 1-2 caract√®res d'erreur selon la longueur du mot\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remaining_phases",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ‚ö° Phase 4 : Traitement PySpark\n",
    "\n",
    "## üß© D√©fi 4.1 : Analyses avec Window Functions\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Utilise PySpark pour :\n",
    "1. Calculer les **statistiques par Publisher** (nb jeux, ventes totales, score moyen)\n",
    "2. Utiliser `ROW_NUMBER()` pour classer les jeux **par ventes dans chaque genre**\n",
    "3. Sauvegarder les r√©sultats en Parquet\n",
    "\n",
    "---\n",
    "\n",
    "# üîå Phase 5 : API REST avec FastAPI\n",
    "\n",
    "## üß© D√©fi 5.1 : Cr√©er les endpoints\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Cr√©e `api/main.py` avec :\n",
    "- `GET /games` : liste avec filtres (genre, platform, min_sales)\n",
    "- `GET /games/{name}` : d√©tails d'un jeu\n",
    "- `GET /stats/genres` : stats par genre\n",
    "- `GET /search?q=...` : recherche Elasticsearch\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Phase 6 : Dashboard Streamlit\n",
    "\n",
    "## üß© D√©fi 6.1 : Dashboard interactif\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Cr√©e `dashboard/app.py` avec :\n",
    "- **Sidebar** : filtres (genre, plateforme, ann√©es)\n",
    "- **KPIs** : nombre de jeux, ventes totales, score moyen\n",
    "- **Graphiques** : bar chart par genre, line chart √©volution, pie chart r√©gions\n",
    "- **Tableau** : top 10 jeux\n",
    "\n",
    "---\n",
    "\n",
    "# ü§ñ Phase 7 : Automatisation\n",
    "\n",
    "## üß© D√©fi 7.1 : Script de pipeline\n",
    "\n",
    "### üìã Consigne\n",
    "\n",
    "Cr√©e `scripts/run_pipeline.sh` qui ex√©cute tout le pipeline avec logging.\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Note** : Les solutions compl√®tes pour les phases 4-7 suivent le m√™me format que les phases pr√©c√©dentes. Essaie d'abord par toi-m√™me !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_recap",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ F√©licitations !\n",
    "\n",
    "Tu as termin√© le **Projet Int√©grateur D√©butant** !\n",
    "\n",
    "## ‚úÖ Ce que tu as construit\n",
    "\n",
    "```{mermaid}\n",
    "mindmap\n",
    "  root((Video Games<br/>Analytics))\n",
    "    Ingestion\n",
    "      Kaggle CSV\n",
    "      Web Scraping\n",
    "      BeautifulSoup\n",
    "    Processing\n",
    "      Pandas\n",
    "      PySpark\n",
    "      Window Functions\n",
    "    Storage\n",
    "      DuckDB SQL\n",
    "      Elasticsearch\n",
    "      Parquet\n",
    "    Serving\n",
    "      FastAPI\n",
    "      Streamlit\n",
    "      REST API\n",
    "    DevOps\n",
    "      Git\n",
    "      Bash\n",
    "      Automation\n",
    "```\n",
    "\n",
    "## üìö Comp√©tences Valid√©es\n",
    "\n",
    "| Domaine | Comp√©tences |\n",
    "|---------|-------------|\n",
    "| **Ingestion** | CSV, Web Scraping, APIs |\n",
    "| **Processing** | Pandas, PySpark, SQL |\n",
    "| **Storage** | DuckDB, Elasticsearch, Parquet |\n",
    "| **Serving** | FastAPI, Streamlit |\n",
    "| **DevOps** | Git, Bash, Automation |\n",
    "\n",
    "## üöÄ Prochaine √©tape\n",
    "\n",
    "üëâ **Niveau Interm√©diaire** : Docker, Kubernetes, Kafka, Delta Lake, dbt, Airflow...\n",
    "\n",
    "---\n",
    "\n",
    "*üéÆ Video Games Analytics Platform ‚Äî Data Engineering Bootcamp*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
