{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Processing \n",
    "\n",
    "Ce module couvre le **traitement de donn√©es avanc√©** avec Python : Pandas, visualisation, APIs, et pipelines ETL.\n",
    "\n",
    "---\n",
    "\n",
    "## Pr√©requis\n",
    "\n",
    "| Niveau | Comp√©tence |\n",
    "|--------|------------|\n",
    "| ‚úÖ Requis | Avoir suivi le module `04_python_basics_for_data_engineers` |\n",
    "| ‚úÖ Requis | Ma√Ætriser les bases de Python (variables, fonctions, boucles) |\n",
    "| ‚úÖ Requis | Savoir utiliser pip et les environnements virtuels |\n",
    "\n",
    "## Objectifs du module\n",
    "\n",
    "√Ä la fin de ce notebook, tu seras capable de :\n",
    "\n",
    "- Manipuler des donn√©es avec **Pandas** (DataFrames, nettoyage, agr√©gations)\n",
    "- Visualiser des donn√©es avec **Matplotlib**\n",
    "- Cr√©er des graphiques statistiques avec **Seaborn**\n",
    "- Traiter du texte et utiliser les **regex**\n",
    "- Consommer des **APIs REST**\n",
    "- Valider la qualit√© des donn√©es\n",
    "- Construire un **pipeline ETL** complet\n",
    "- G√©rer les configurations et secrets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation des d√©pendances\n",
    "\n",
    "Avant de commencer, assurons-nous d'avoir toutes les librairies n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des packages (√† ex√©cuter une seule fois)\n",
    "!pip install pandas numpy requests python-dotenv pytest pandera pyarrow openpyxl matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports de base\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration de l'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis !\")\n",
    "print(f\"Version Pandas : {pd.__version__}\")\n",
    "print(f\"Version NumPy : {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Pandas ‚Äî Le c≈ìur du Data Processing\n",
    "\n",
    "Pandas est LA librairie incontournable pour manipuler des donn√©es tabulaires en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cr√©er et lire des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame simple\n",
    "data = {\n",
    "    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [25, 30, 35, None, 28],\n",
    "    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon'],\n",
    "    'salaire': [45000, 55000, 60000, 50000, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"üìä DataFrame cr√©√© :\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder en CSV\n",
    "df.to_csv('exemple_employes.csv', index=False)\n",
    "print(\"‚úÖ Fichier CSV sauvegard√©\")\n",
    "\n",
    "# Lire depuis CSV\n",
    "df_from_csv = pd.read_csv('exemple_employes.csv')\n",
    "print(\"\\nüìÇ Lecture depuis CSV :\")\n",
    "print(df_from_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Exploration des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations g√©n√©rales\n",
    "print(\"üìã Informations du DataFrame :\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nüìä Statistiques descriptives :\")\n",
    "print(df.describe())\n",
    "\n",
    "# Premi√®res lignes\n",
    "print(\"\\nüîù Premi√®res lignes :\")\n",
    "print(df.head(3))\n",
    "\n",
    "# Derni√®res lignes\n",
    "print(\"\\nüîö Derni√®res lignes :\")\n",
    "print(df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Nettoyage des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©tecter les valeurs manquantes\n",
    "print(\"‚ùì Valeurs manquantes par colonne :\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valeurs manquantes : {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Visualiser les lignes avec des valeurs manquantes\n",
    "print(\"\\nüîç Lignes avec des NaN :\")\n",
    "print(df[df.isnull().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strat√©gies de gestion des valeurs manquantes\n",
    "\n",
    "# 1. Supprimer les lignes avec des NaN\n",
    "df_drop = df.dropna()\n",
    "print(\"üóëÔ∏è Apr√®s suppression des lignes avec NaN :\")\n",
    "print(df_drop)\n",
    "\n",
    "# 2. Remplir avec une valeur par d√©faut\n",
    "df_fill = df.fillna({\n",
    "    'age': df['age'].median(),\n",
    "    'salaire': df['salaire'].mean()\n",
    "})\n",
    "print(\"\\n‚ú® Apr√®s remplissage des NaN :\")\n",
    "print(df_fill)\n",
    "\n",
    "# 3. Forward fill (propager la valeur pr√©c√©dente)\n",
    "df_ffill = df.fillna(method='ffill')\n",
    "print(\"\\n‚û°Ô∏è Apr√®s forward fill :\")\n",
    "print(df_ffill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les doublons\n",
    "df_with_duplicates = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 2, 4],\n",
    "    'nom': ['Alice', 'Bob', 'Charlie', 'Bob', 'David']\n",
    "})\n",
    "\n",
    "print(\"Avant suppression des doublons :\")\n",
    "print(df_with_duplicates)\n",
    "\n",
    "df_no_duplicates = df_with_duplicates.drop_duplicates()\n",
    "print(\"\\nApr√®s suppression :\")\n",
    "print(df_no_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 S√©lection et filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisons le DataFrame nettoy√©\n",
    "df_clean = df_fill.copy()\n",
    "\n",
    "# S√©lectionner une colonne\n",
    "print(\"üìå Colonne 'nom' :\")\n",
    "print(df_clean['nom'])\n",
    "\n",
    "# S√©lectionner plusieurs colonnes\n",
    "print(\"\\nüìå Colonnes 'nom' et 'ville' :\")\n",
    "print(df_clean[['nom', 'ville']])\n",
    "\n",
    "# Filtrer les lignes\n",
    "print(\"\\nüîç Employ√©s de Paris :\")\n",
    "print(df_clean[df_clean['ville'] == 'Paris'])\n",
    "\n",
    "# Filtres multiples\n",
    "print(\"\\nüîç Employ√©s de Paris avec salaire > 50000 :\")\n",
    "print(df_clean[(df_clean['ville'] == 'Paris') & (df_clean['salaire'] > 50000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexation avanc√©e avec loc et iloc\n",
    "\n",
    "# loc : par label/nom\n",
    "print(\"üìç loc[0, 'nom'] :\")\n",
    "print(df_clean.loc[0, 'nom'])\n",
    "\n",
    "# iloc : par position num√©rique\n",
    "print(\"\\nüìç iloc[0, 0] (premi√®re ligne, premi√®re colonne) :\")\n",
    "print(df_clean.iloc[0, 0])\n",
    "\n",
    "# S√©lection de plages\n",
    "print(\"\\nüìç loc[0:2, ['nom', 'age']] :\")\n",
    "print(df_clean.loc[0:2, ['nom', 'age']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 GroupBy et agr√©gations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouper par ville et calculer des statistiques\n",
    "print(\"üìä Statistiques par ville :\")\n",
    "grouped = df_clean.groupby('ville').agg({\n",
    "    'nom': 'count',\n",
    "    'age': ['mean', 'min', 'max'],\n",
    "    'salaire': ['mean', 'sum']\n",
    "})\n",
    "print(grouped)\n",
    "\n",
    "# Renommer les colonnes pour plus de clart√©\n",
    "print(\"\\nüìä Salaire moyen par ville :\")\n",
    "salaire_moyen = df_clean.groupby('ville')['salaire'].mean().round(2)\n",
    "print(salaire_moyen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Apply vs Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une colonne calcul√©e\n",
    "\n",
    "# M√©thode 1 : Apply (plus lent mais flexible)\n",
    "def categoriser_age(age):\n",
    "    if age < 30:\n",
    "        return 'Junior'\n",
    "    elif age < 40:\n",
    "        return 'Senior'\n",
    "    else:\n",
    "        return 'Expert'\n",
    "\n",
    "df_clean['categorie_apply'] = df_clean['age'].apply(categoriser_age)\n",
    "\n",
    "# M√©thode 2 : Vectorisation (plus rapide)\n",
    "df_clean['categorie_vect'] = pd.cut(\n",
    "    df_clean['age'],\n",
    "    bins=[0, 30, 40, 100],\n",
    "    labels=['Junior', 'Senior', 'Expert']\n",
    ")\n",
    "\n",
    "print(\"üîß Colonnes calcul√©es :\")\n",
    "print(df_clean[['nom', 'age', 'categorie_apply', 'categorie_vect']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de performance (sur un grand dataset)\n",
    "import time\n",
    "\n",
    "# Cr√©er un grand DataFrame\n",
    "big_df = pd.DataFrame({\n",
    "    'valeur': np.random.randint(1, 100, 100000)\n",
    "})\n",
    "\n",
    "# M√©thode Apply\n",
    "start = time.time()\n",
    "big_df['double_apply'] = big_df['valeur'].apply(lambda x: x * 2)\n",
    "time_apply = time.time() - start\n",
    "\n",
    "# M√©thode Vectoris√©e\n",
    "start = time.time()\n",
    "big_df['double_vect'] = big_df['valeur'] * 2\n",
    "time_vect = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è Temps Apply : {time_apply:.4f}s\")\n",
    "print(f\"‚è±Ô∏è Temps Vectorisation : {time_vect:.4f}s\")\n",
    "print(f\"üöÄ Vectorisation est {time_apply/time_vect:.1f}x plus rapide !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Gestion de la m√©moire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier l'utilisation m√©moire\n",
    "print(\" Utilisation m√©moire par colonne :\")\n",
    "print(df_clean.memory_usage(deep=True))\n",
    "print(f\"\\nTotal : {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimiser les types de donn√©es\n",
    "df_optimized = df_clean.copy()\n",
    "\n",
    "# Avant optimisation\n",
    "print(\"Avant optimisation :\")\n",
    "print(df_optimized.dtypes)\n",
    "print(f\"M√©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Convertir en types plus efficaces\n",
    "df_optimized['age'] = df_optimized['age'].astype('int8')\n",
    "df_optimized['salaire'] = df_optimized['salaire'].astype('int32')\n",
    "df_optimized['ville'] = df_optimized['ville'].astype('category')\n",
    "\n",
    "print(\"\\nApr√®s optimisation :\")\n",
    "print(df_optimized.dtypes)\n",
    "print(f\"M√©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Manipulation de dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame avec des dates\n",
    "df_dates = pd.DataFrame({\n",
    "    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],\n",
    "    'montant': [1000, 1500, 1200, 1800]\n",
    "})\n",
    "\n",
    "# Convertir en datetime\n",
    "df_dates['date'] = pd.to_datetime(df_dates['date_str'])\n",
    "\n",
    "# Extraire des composantes\n",
    "df_dates['annee'] = df_dates['date'].dt.year\n",
    "df_dates['mois'] = df_dates['date'].dt.month\n",
    "df_dates['jour'] = df_dates['date'].dt.day\n",
    "df_dates['nom_mois'] = df_dates['date'].dt.month_name()\n",
    "df_dates['jour_semaine'] = df_dates['date'].dt.day_name()\n",
    "\n",
    "print(\"üìÖ DataFrame avec dates extraites :\")\n",
    "print(df_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculs avec les dates\n",
    "df_dates['jours_depuis_debut'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n",
    "\n",
    "# Ajouter/soustraire des p√©riodes\n",
    "df_dates['date_plus_30j'] = df_dates['date'] + pd.Timedelta(days=30)\n",
    "df_dates['date_moins_1mois'] = df_dates['date'] - pd.DateOffset(months=1)\n",
    "\n",
    "print(\" Calculs de dates :\")\n",
    "print(df_dates[['date', 'jours_depuis_debut', 'date_plus_30j', 'date_moins_1mois']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Export de donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSV\n",
    "df_clean.to_csv('employes_clean.csv', index=False)\n",
    "print(\"‚úÖ Export CSV r√©ussi\")\n",
    "\n",
    "# Export JSON\n",
    "df_clean.to_json('employes_clean.json', orient='records', indent=2)\n",
    "print(\"‚úÖ Export JSON r√©ussi\")\n",
    "\n",
    "# Export Parquet (format columnar, tr√®s efficace)\n",
    "df_clean.to_parquet('employes_clean.parquet', index=False)\n",
    "print(\"‚úÖ Export Parquet r√©ussi\")\n",
    "\n",
    "# Export Excel\n",
    "df_clean.to_excel('employes_clean.xlsx', index=False, sheet_name='Employ√©s')\n",
    "print(\"‚úÖ Export Excel r√©ussi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique 1 : Pandas\n",
    "\n",
    "**Objectif** : Analyser un fichier de ventes\n",
    "\n",
    "1. Cr√©er un DataFrame avec des donn√©es de ventes (produit, quantit√©, prix, date)\n",
    "2. Calculer le chiffre d'affaires total\n",
    "3. Trouver le produit le plus vendu\n",
    "4. Calculer les ventes mensuelles\n",
    "5. Exporter le r√©sultat en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 1. Cr√©er un DataFrame avec des donn√©es de ventes\nnp.random.seed(42)\ndates = pd.date_range(start='2024-01-01', periods=100, freq='D')\nproduits = ['Laptop', 'T√©l√©phone', 'Tablette', 'Casque', 'Souris']\n\nventes_data = {\n    'date': np.random.choice(dates, 200),\n    'produit': np.random.choice(produits, 200),\n    'quantite': np.random.randint(1, 20, 200),\n    'prix_unitaire': np.random.choice([999, 599, 449, 79, 29], 200)\n}\n\ndf_ventes = pd.DataFrame(ventes_data)\ndf_ventes['montant'] = df_ventes['quantite'] * df_ventes['prix_unitaire']\n\n# 2. Chiffre d'affaires total\nca_total = df_ventes['montant'].sum()\nprint(f\"üí∞ CA total : {ca_total:,.0f} ‚Ç¨\")\n\n# 3. Produit le plus vendu\nproduit_top = df_ventes.groupby('produit')['quantite'].sum().idxmax()\nprint(f\"üèÜ Produit top : {produit_top}\")\n\n# 4. Ventes mensuelles\ndf_ventes['mois'] = pd.to_datetime(df_ventes['date']).dt.to_period('M')\nprint(df_ventes.groupby('mois')['montant'].sum())\n\n# 5. Export CSV\ndf_ventes.to_csv('ventes_analyse.csv', index=False)\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Outils Modernes d'Exploration Automatique (EDA)\n",
    "\n",
    "En 2024-2025, de nombreux outils permettent d'**automatiser l'exploration des donn√©es** et de g√©n√©rer des rapports complets en quelques lignes de code. Ces outils sont un **gain de temps √©norme** pour les Data Engineers.\n",
    "\n",
    "| Outil | Type | Points forts | Quand l'utiliser |\n",
    "|-------|------|--------------|------------------|\n",
    "| **Julius.ai** | IA Cloud | Analyse en langage naturel, pas de code | Exploration rapide, non-techniques |\n",
    "| **ydata-profiling** | Librairie | Rapport HTML complet, alertes | Premier aper√ßu d'un dataset |\n",
    "| **sweetviz** | Librairie | Comparaison train/test, beau design | Comparer deux datasets |\n",
    "| **D-Tale** | App Web | Interface interactive type Excel | Exploration interactive |\n",
    "| **Pygwalker** | Librairie | Interface Tableau dans Jupyter | Visualisation drag & drop |\n",
    "\n",
    "> üí° Ces outils ne remplacent pas Pandas, mais **acc√©l√®rent** la phase d'exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julius.ai ‚Äî L'IA pour analyser tes donn√©es\n",
    "\n",
    "**Julius.ai** est une plateforme d'IA qui permet d'analyser des donn√©es **en langage naturel**, sans √©crire de code.\n",
    "\n",
    "### Acc√®s\n",
    "\n",
    "üëâ **[julius.ai](https://julius.ai)** ‚Äî Gratuit avec limitations, plans payants disponibles\n",
    "\n",
    "### Fonctionnalit√©s\n",
    "\n",
    "| Fonctionnalit√© | Description |\n",
    "|----------------|-------------|\n",
    "| **Upload de fichiers** | CSV, Excel, JSON, bases de donn√©es |\n",
    "| **Questions en fran√ßais** | \"Quelle est la moyenne des salaires par ville ?\" |\n",
    "| **G√©n√©ration de code** | Python/Pandas g√©n√©r√© automatiquement |\n",
    "| **Visualisations** | Graphiques cr√©√©s √† la demande |\n",
    "| **Export** | Code Python, graphiques, rapports |\n",
    "\n",
    "### Exemples de questions √† poser\n",
    "\n",
    "```\n",
    "- \"Montre-moi les 10 premi√®res lignes\"\n",
    "- \"Combien de valeurs manquantes par colonne ?\"\n",
    "- \"Cr√©e un graphique des ventes par mois\"\n",
    "- \"Quelle est la corr√©lation entre age et salaire ?\"\n",
    "- \"Nettoie les doublons et les valeurs aberrantes\"\n",
    "- \"G√©n√®re un rapport de qualit√© des donn√©es\"\n",
    "```\n",
    "\n",
    "### üí° Cas d'usage Data Engineering\n",
    "\n",
    "| Situation | Comment Julius aide |\n",
    "|-----------|--------------------|\n",
    "| Nouveau dataset inconnu | Exploration rapide sans code |\n",
    "| R√©union avec non-techniques | D√©mo interactive |\n",
    "| Prototypage rapide | G√©n√©rer du code Pandas √† r√©utiliser |\n",
    "| Debugging | \"Pourquoi j'ai des NaN dans cette colonne ?\" |\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "- Donn√©es envoy√©es dans le cloud (attention aux donn√©es sensibles)\n",
    "- Gratuit limit√© en nombre de requ√™tes\n",
    "- Pas adapt√© pour la production (utiliser le code g√©n√©r√© plut√¥t)\n",
    "\n",
    "> üí° **Astuce** : Utilise Julius pour explorer, puis **copie le code Python g√©n√©r√©** dans ton pipeline !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ydata-profiling ‚Äî Rapport complet en 1 ligne\n",
    "\n",
    "**ydata-profiling** (anciennement `pandas-profiling`) g√©n√®re un **rapport HTML interactif** complet sur ton DataFrame.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install ydata-profiling\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (d√©commenter si n√©cessaire)\n",
    "# !pip install ydata-profiling\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Cr√©er un dataset d'exemple\n",
    "df_exemple = pd.DataFrame({\n",
    "    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'age': [25, 30, 35, None, 28, 45, 32, 29],\n",
    "    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris', 'Lyon', 'Paris'],\n",
    "    'salaire': [45000, 55000, 60000, 50000, None, 75000, 52000, 48000],\n",
    "    'experience': [2, 5, 8, 3, 4, 15, 7, 3],\n",
    "    'date_embauche': pd.to_datetime(['2022-01-15', '2019-06-20', '2016-03-10', \n",
    "                                      '2021-09-01', '2020-04-15', '2010-01-01',\n",
    "                                      '2017-08-20', '2021-11-30'])\n",
    "})\n",
    "\n",
    "# G√©n√©rer le rapport (mode minimal pour rapidit√©)\n",
    "profile = ProfileReport(\n",
    "    df_exemple, \n",
    "    title=\"Rapport Employ√©s\",\n",
    "    minimal=True,  # Mode rapide\n",
    "    explorative=True\n",
    ")\n",
    "\n",
    "# Afficher dans le notebook\n",
    "profile.to_notebook_iframe()\n",
    "\n",
    "# Ou sauvegarder en HTML\n",
    "# profile.to_file(\"rapport_employes.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ce que contient le rapport\n",
    "\n",
    "| Section | Contenu |\n",
    "|---------|--------|\n",
    "| **Overview** | Nombre de lignes, colonnes, types, taille m√©moire |\n",
    "| **Variables** | Stats par colonne (min, max, mean, distribution) |\n",
    "| **Interactions** | Corr√©lations entre variables |\n",
    "| **Correlations** | Matrices de corr√©lation (Pearson, Spearman) |\n",
    "| **Missing values** | Visualisation des valeurs manquantes |\n",
    "| **Duplicates** | D√©tection des doublons |\n",
    "| **Alerts** | ‚ö†Ô∏è Alertes automatiques (haute cardinalit√©, skewness, etc.) |\n",
    "\n",
    "### Options utiles\n",
    "\n",
    "```python\n",
    "# Rapport complet (plus lent)\n",
    "profile = ProfileReport(df, minimal=False)\n",
    "\n",
    "# Comparer deux datasets\n",
    "profile_train = ProfileReport(df_train, title=\"Train\")\n",
    "profile_test = ProfileReport(df_test, title=\"Test\")\n",
    "comparison = profile_train.compare(profile_test)\n",
    "comparison.to_file(\"comparison.html\")\n",
    "\n",
    "# Exclure certaines analyses (plus rapide)\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    correlations=None,  # D√©sactiver les corr√©lations\n",
    "    interactions=None   # D√©sactiver les interactions\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweetviz ‚Äî Comparaison visuelle de datasets\n",
    "\n",
    "**Sweetviz** est sp√©cialis√© dans la **comparaison** de datasets (train vs test, avant vs apr√®s nettoyage).\n",
    "\n",
    "### üì¶ Installation\n",
    "\n",
    "```bash\n",
    "pip install sweetviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (d√©commenter si n√©cessaire)\n",
    "# !pip install sweetviz\n",
    "\n",
    "import sweetviz as sv\n",
    "\n",
    "# Rapport simple\n",
    "report = sv.analyze(df_exemple)\n",
    "report.show_notebook()  # Afficher dans le notebook\n",
    "# report.show_html(\"sweetviz_report.html\")  # Ou sauvegarder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de deux datasets (ex: train vs test)\n",
    "df_train = df_exemple.iloc[:5]\n",
    "df_test = df_exemple.iloc[5:]\n",
    "\n",
    "# G√©n√©rer le rapport de comparaison\n",
    "comparison_report = sv.compare([df_train, \"Train\"], [df_test, \"Test\"])\n",
    "comparison_report.show_notebook()\n",
    "\n",
    "# Analyse avec variable cible (pour ML)\n",
    "# report = sv.analyze(df, target_feat=\"salaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points forts de Sweetviz\n",
    "\n",
    "| Fonctionnalit√© | Description |\n",
    "|----------------|-------------|\n",
    "| **Comparaison c√¥te √† c√¥te** | Voir les diff√©rences entre 2 datasets |\n",
    "| **Variable cible** | Analyse par rapport √† une target (ML) |\n",
    "| **Design moderne** | Rapports visuellement attractifs |\n",
    "| **Rapide** | Plus l√©ger que ydata-profiling |\n",
    "\n",
    "### ydata-profiling vs Sweetviz\n",
    "\n",
    "| Crit√®re | ydata-profiling | Sweetviz |\n",
    "|---------|-----------------|----------|\n",
    "| **Profondeur d'analyse** | ‚≠ê‚≠ê‚≠ê Tr√®s d√©taill√© | ‚≠ê‚≠ê Essentiel |\n",
    "| **Vitesse** | üê¢ Plus lent | üêá Plus rapide |\n",
    "| **Comparaison** | ‚úÖ Possible | ‚≠ê‚≠ê‚≠ê Excellent |\n",
    "| **Design** | Classique | Moderne |\n",
    "| **Alertes** | ‚úÖ Oui | ‚ùå Non |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-Tale ‚Äî Exploration interactive (comme Excel)\n",
    "\n",
    "**D-Tale** lance une **interface web interactive** pour explorer tes donn√©es comme dans Excel/Google Sheets, mais avec la puissance de Python derri√®re.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install dtale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (d√©commenter si n√©cessaire)\n",
    "# !pip install dtale\n",
    "\n",
    "import dtale\n",
    "\n",
    "# Lancer D-Tale\n",
    "d = dtale.show(df_exemple)\n",
    "\n",
    "# Afficher dans le notebook (ou ouvre un nouvel onglet)\n",
    "d.notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctionnalit√©s D-Tale\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  D-Tale                                         [Export] [Code]‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  [Filters] [Sort] [Charts] [Correlations] [Describe] [Missing] ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ    nom    ‚îÇ  age  ‚îÇ  ville   ‚îÇ salaire ‚îÇ experience ‚îÇ          ‚îÇ\n",
    "‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ          ‚îÇ\n",
    "‚îÇ  Alice    ‚îÇ  25   ‚îÇ  Paris   ‚îÇ  45000  ‚îÇ     2      ‚îÇ          ‚îÇ\n",
    "‚îÇ  Bob      ‚îÇ  30   ‚îÇ  Lyon    ‚îÇ  55000  ‚îÇ     5      ‚îÇ          ‚îÇ\n",
    "‚îÇ  Charlie  ‚îÇ  35   ‚îÇ  Paris   ‚îÇ  60000  ‚îÇ     8      ‚îÇ          ‚îÇ\n",
    "‚îÇ  ...      ‚îÇ  ...  ‚îÇ  ...     ‚îÇ  ...    ‚îÇ    ...     ‚îÇ          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "| Action | Comment |\n",
    "|--------|--------|\n",
    "| **Filtrer** | Cliquer sur une colonne ‚Üí Filter |\n",
    "| **Trier** | Cliquer sur l'en-t√™te de colonne |\n",
    "| **Graphiques** | Menu Charts ‚Üí choisir le type |\n",
    "| **Stats** | Menu Describe ‚Üí stats par colonne |\n",
    "| **Exporter le code** | Bouton \"Code Export\" ‚Üí copier le Pandas g√©n√©r√© |\n",
    "\n",
    "> üí° **Killer feature** : D-Tale g√©n√®re le code Pandas de toutes tes manipulations !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pygwalker ‚Äî Interface Tableau dans Jupyter\n",
    "\n",
    "**Pygwalker** transforme ton DataFrame en une interface **drag & drop** comme Tableau/Power BI, directement dans Jupyter.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "pip install pygwalker\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (d√©commenter si n√©cessaire)\n",
    "# !pip install pygwalker\n",
    "\n",
    "import pygwalker as pyg\n",
    "\n",
    "# Lancer l'interface interactive\n",
    "walker = pyg.walk(df_exemple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment utiliser Pygwalker\n",
    "\n",
    "1. **Glisser-d√©poser** les colonnes sur les axes X, Y, Color, Size\n",
    "2. **Choisir le type** de graphique (bar, line, scatter, heatmap...)\n",
    "3. **Filtrer** les donn√©es visuellement\n",
    "4. **Exporter** la configuration pour la r√©utiliser\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Pygwalker                                                      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ   FIELDS         ‚îÇ                                              ‚îÇ\n",
    "‚îÇ                  ‚îÇ         [Graphique interactif]               ‚îÇ\n",
    "‚îÇ      nom         ‚îÇ                                              ‚îÇ\n",
    "‚îÇ      age         ‚îÇ              ‚ñà‚ñà‚ñà‚ñà                            ‚îÇ\n",
    "‚îÇ      ville       ‚îÇ         ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                         ‚îÇ\n",
    "‚îÇ      salaire     ‚îÇ    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                      ‚îÇ\n",
    "‚îÇ      experience  ‚îÇ                                              ‚îÇ\n",
    "‚îÇ                  ‚îÇ                                              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  X: ville        ‚îÇ  Y: salaire    Color: experience             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### üí° Cas d'usage\n",
    "\n",
    "- **Exploration visuelle** rapide sans √©crire de code matplotlib\n",
    "- **Pr√©sentation** √† des non-techniques\n",
    "- **Prototypage** de dashboards avant de coder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©capitulatif ‚Äî Quel outil choisir ?\n",
    "\n",
    "| Situation | Outil recommand√© |\n",
    "|-----------|------------------|\n",
    "| Premier aper√ßu rapide d'un dataset | **ydata-profiling** (minimal=True) |\n",
    "| Comparer data1 vs data2 | **Sweetviz** |\n",
    "| Exploration interactive (comme Excel) | **D-Tale** |\n",
    "| Cr√©er des graphiques sans code | **Pygwalker** |\n",
    "| Poser des questions en fran√ßais | **Julius.ai** |\n",
    "| Donn√©es sensibles (pas de cloud) | **D-Tale** ou **ydata-profiling** (tout local) |\n",
    "| G√©n√©rer du code Pandas | **Julius.ai** ou **D-Tale** |\n",
    "\n",
    "### Installation compl√®te\n",
    "\n",
    "```bash\n",
    "pip install ydata-profiling sweetviz dtale pygwalker\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Bonnes pratiques\n",
    "\n",
    "| ‚úÖ Faire | ‚ùå √âviter |\n",
    "|---------|----------|\n",
    "| Utiliser ces outils pour **explorer** | Les utiliser en **production** |\n",
    "| Copier le code g√©n√©r√© dans ton pipeline | D√©pendre de l'interface pour le traitement |\n",
    "| Partager les rapports HTML avec l'√©quipe | Envoyer des donn√©es sensibles sur Julius.ai |\n",
    "| Combiner plusieurs outils | Se limiter √† un seul |\n",
    "\n",
    "> üí° **Workflow recommand√©** :\n",
    "> 1. **Julius.ai** pour les premi√®res questions\n",
    "> 2. **ydata-profiling** pour un rapport complet\n",
    "> 3. **D-Tale** pour explorer interactivement\n",
    "> 4. **Copier le code** dans ton pipeline Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Matplotlib ‚Äî Visualisation de donn√©es\n",
    "\n",
    "Matplotlib est la biblioth√®que de visualisation de base en Python. Elle permet de cr√©er des graphiques de haute qualit√© et est la fondation de nombreuses autres biblioth√®ques de visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de Matplotlib (si n√©cessaire)\n",
    "!pip install matplotlib\n",
    "\n",
    "# Import de Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration pour afficher les graphiques dans le notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration du style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')  # Style plus moderne\n",
    "plt.rcParams['figure.figsize'] = [10, 6]  # Taille par d√©faut\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Matplotlib import√© avec succ√®s !\")\n",
    "print(f\"Version Matplotlib : {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphiques lin√©aires (Line Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es pour un graphique lin√©aire\n",
    "x = np.linspace(0, 10, 100)\n",
    "y1 = np.sin(x)\n",
    "y2 = np.cos(x)\n",
    "\n",
    "# Cr√©ation du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, y1, label='Sin(x)', color='blue', linewidth=2)\n",
    "plt.plot(x, y2, label='Cos(x)', color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Personnalisation\n",
    "plt.title('Fonctions trigonom√©triques', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphiques √† barres (Bar Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es de ventes par mois\n",
    "mois = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Juin']\n",
    "ventes_2023 = [1200, 1500, 1800, 1600, 2000, 2200]\n",
    "ventes_2024 = [1400, 1700, 1900, 1800, 2300, 2500]\n",
    "\n",
    "x = np.arange(len(mois))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barres group√©es\n",
    "bars1 = ax.bar(x - width/2, ventes_2023, width, label='2023', color='steelblue')\n",
    "bars2 = ax.bar(x + width/2, ventes_2024, width, label='2024', color='coral')\n",
    "\n",
    "# Personnalisation\n",
    "ax.set_title('Comparaison des ventes 2023 vs 2024', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Mois', fontsize=12)\n",
    "ax.set_ylabel('Ventes (‚Ç¨)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(mois)\n",
    "ax.legend()\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuages de points (Scatter Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es al√©atoires avec corr√©lation\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(100)\n",
    "y = 2 * x + np.random.randn(100) * 0.5\n",
    "colors = np.random.rand(100)\n",
    "sizes = np.random.rand(100) * 200\n",
    "\n",
    "# Scatter plot avec couleurs et tailles variables\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(x, y, c=colors, s=sizes, alpha=0.6, cmap='viridis')\n",
    "\n",
    "# Ajouter une ligne de tendance\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(x, p(x), 'r--', linewidth=2, label=f'Tendance: y = {z[0]:.2f}x + {z[1]:.2f}')\n",
    "\n",
    "plt.colorbar(scatter, label='Valeur')\n",
    "plt.title('Nuage de points avec ligne de tendance', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Variable X')\n",
    "plt.ylabel('Variable Y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogrammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es de distribution\n",
    "np.random.seed(42)\n",
    "data_normal = np.random.normal(loc=50, scale=10, size=1000)\n",
    "data_skewed = np.random.exponential(scale=10, size=1000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme distribution normale\n",
    "axes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\n",
    "axes[0].set_title('Distribution Normale', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Valeur')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogramme distribution exponentielle\n",
    "axes[1].hist(data_skewed, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(data_skewed.mean(), color='red', linestyle='--', label=f'Moyenne: {data_skewed.mean():.1f}')\n",
    "axes[1].set_title('Distribution Exponentielle', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Valeur')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphiques circulaires (Pie Charts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es de r√©partition\n",
    "categories = ['Produit A', 'Produit B', 'Produit C', 'Produit D', 'Autres']\n",
    "parts = [35, 25, 20, 15, 5]\n",
    "colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\n",
    "explode = (0.05, 0, 0, 0, 0)  # Mettre en √©vidence le premier segment\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "wedges, texts, autotexts = plt.pie(parts, labels=categories, colors=colors, explode=explode,\n",
    "                                    autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "\n",
    "# Am√©liorer l'apparence du texte\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontsize(11)\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.title('R√©partition des ventes par produit', fontsize=16, fontweight='bold')\n",
    "plt.axis('equal')  # Assure que le cercle est bien rond\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sous-graphiques (Subplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une grille de sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Donn√©es\n",
    "x = np.linspace(0, 10, 50)\n",
    "\n",
    "# Graphique 1: Ligne\n",
    "axes[0, 0].plot(x, np.sin(x), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Graphique lin√©aire')\n",
    "axes[0, 0].set_xlabel('X')\n",
    "axes[0, 0].set_ylabel('Sin(X)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique 2: Barres\n",
    "categories = ['A', 'B', 'C', 'D']\n",
    "values = [23, 45, 56, 78]\n",
    "axes[0, 1].bar(categories, values, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\n",
    "axes[0, 1].set_title('Graphique √† barres')\n",
    "\n",
    "# Graphique 3: Scatter\n",
    "x_scatter = np.random.rand(50)\n",
    "y_scatter = np.random.rand(50)\n",
    "axes[1, 0].scatter(x_scatter, y_scatter, c='purple', alpha=0.6, s=100)\n",
    "axes[1, 0].set_title('Nuage de points')\n",
    "\n",
    "# Graphique 4: Histogramme\n",
    "data = np.random.randn(1000)\n",
    "axes[1, 1].hist(data, bins=30, color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Histogramme')\n",
    "\n",
    "plt.suptitle('Tableau de bord - Vue d\\'ensemble', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarder des graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un graphique √† sauvegarder\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.linspace(0, 10, 100)\n",
    "ax.plot(x, np.sin(x), 'b-', linewidth=2, label='Sin(x)')\n",
    "ax.set_title('Graphique √† exporter', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Sauvegarder dans diff√©rents formats\n",
    "fig.savefig('graphique.png', dpi=300, bbox_inches='tight')\n",
    "fig.savefig('graphique.pdf', bbox_inches='tight')\n",
    "fig.savefig('graphique.svg', bbox_inches='tight')\n",
    "\n",
    "print(\"‚úÖ Graphiques sauvegard√©s en PNG, PDF et SVG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique : Matplotlib\n",
    "\n",
    "**Objectif** : Cr√©er un tableau de bord de visualisation\n",
    "\n",
    "1. Cr√©er un DataFrame avec des donn√©es de ventes (produit, mois, ventes, profit)\n",
    "2. Cr√©er 4 sous-graphiques montrant :\n",
    "   - √âvolution des ventes mensuelles (ligne)\n",
    "   - Ventes par produit (barres)\n",
    "   - Relation ventes/profit (scatter)\n",
    "   - Distribution des profits (histogramme)\n",
    "3. Personnaliser les couleurs et ajouter des titres\n",
    "4. Sauvegarder le r√©sultat en PNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nmois = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Juin']\ndata = {'mois': mois, 'ventes': np.random.randint(100, 300, 6), \n        'profit': np.random.randint(20, 80, 6)}\ndf = pd.DataFrame(data)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Ligne\naxes[0,0].plot(df['mois'], df['ventes'], marker='o')\naxes[0,0].set_title('üìà √âvolution mensuelle')\n\n# Barres\naxes[0,1].bar(df['mois'], df['ventes'], color='steelblue')\naxes[0,1].set_title('üìä Ventes par mois')\n\n# Scatter\naxes[1,0].scatter(df['ventes'], df['profit'], c='coral', s=100)\naxes[1,0].set_title('üìç Ventes vs Profit')\n\n# Histogramme\naxes[1,1].hist(df['profit'], bins=5, color='green', alpha=0.7)\naxes[1,1].set_title('üìä Distribution profits')\n\nplt.tight_layout()\nplt.savefig('dashboard.png', dpi=150)\nplt.show()\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Seaborn ‚Äî Visualisation statistique avanc√©e\n",
    "\n",
    "Seaborn est une biblioth√®que de visualisation bas√©e sur Matplotlib qui offre une interface de haut niveau pour cr√©er des graphiques statistiques attrayants. Elle est particuli√®rement utile pour l'exploration de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de Seaborn (si n√©cessaire)\n",
    "!pip install seaborn\n",
    "\n",
    "# Import de Seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration du style\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "\n",
    "print(\"‚úÖ Seaborn import√© avec succ√®s !\")\n",
    "print(f\"Version Seaborn : {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charger des jeux de donn√©es int√©gr√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn propose des jeux de donn√©es pour s'entra√Æner\n",
    "tips = sns.load_dataset('tips')\n",
    "print(\"üìä Dataset 'tips' :\")\n",
    "print(tips.head())\n",
    "print(f\"\\nDimensions : {tips.shape}\")\n",
    "print(f\"\\nColonnes : {list(tips.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme avec KDE (Kernel Density Estimation)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogramme simple\n",
    "sns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Distribution du montant total', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Histogramme avec hue (groupement)\n",
    "sns.histplot(data=tips, x='total_bill', hue='time', kde=True, ax=axes[1])\n",
    "axes[1].set_title('Distribution par moment de la journ√©e', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot (densit√© de probabilit√©)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=tips, x='total_bill', hue='day', fill=True, alpha=0.5)\n",
    "plt.title('Densit√© du montant total par jour', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des donn√©es cat√©gorielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot - Distribution par cat√©gorie\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Box plot simple\n",
    "sns.boxplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Set2')\n",
    "axes[0].set_title('Montant total par jour', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Box plot avec hue\n",
    "sns.boxplot(data=tips, x='day', y='total_bill', hue='sex', ax=axes[1], palette='Set1')\n",
    "axes[1].set_title('Montant total par jour et sexe', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot - Combine box plot et KDE\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=tips, x='day', y='total_bill', hue='sex', split=True, palette='muted')\n",
    "plt.title('Distribution du montant par jour et sexe (Violin Plot)', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot avec estimation statistique\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar plot avec intervalle de confiance\n",
    "sns.barplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Blues_d', errorbar='ci')\n",
    "axes[0].set_title('Montant moyen par jour (avec IC 95%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Count plot (compte les occurrences)\n",
    "sns.countplot(data=tips, x='day', hue='time', ax=axes[1], palette='Set2')\n",
    "axes[1].set_title('Nombre de repas par jour', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des relations entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot avec r√©gression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot simple avec r√©gression\n",
    "sns.regplot(data=tips, x='total_bill', y='tip', ax=axes[0], color='coral')\n",
    "axes[0].set_title('Relation montant/pourboire avec r√©gression', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Scatter plot avec hue et style\n",
    "sns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', style='sex', \n",
    "                size='size', sizes=(50, 200), ax=axes[1], palette='Set1')\n",
    "axes[1].set_title('Relation multidimensionnelle', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmplot - R√©gression avec facettes\n",
    "g = sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', col='time', \n",
    "               height=5, aspect=1.2, palette='Set1')\n",
    "g.fig.suptitle('R√©gression par moment et statut fumeur', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps et matrices de corr√©lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "# S√©lectionner uniquement les colonnes num√©riques\n",
    "numeric_cols = tips.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_cols.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.2f', linewidths=0.5, square=True)\n",
    "plt.title('Matrice de corr√©lation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap de donn√©es pivot√©es\n",
    "pivot_data = tips.pivot_table(values='tip', index='day', columns='time', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)\n",
    "plt.title('Pourboire moyen par jour et moment', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair plots (visualisation multivari√©e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot - Toutes les combinaisons de variables\n",
    "g = sns.pairplot(tips, hue='time', palette='Set1', diag_kind='kde', \n",
    "                 plot_kws={'alpha': 0.6}, height=2.5)\n",
    "g.fig.suptitle('Pair Plot - Dataset Tips', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FacetGrid - Graphiques multi-facettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une grille de facettes\n",
    "g = sns.FacetGrid(tips, col='time', row='smoker', height=4, aspect=1.2)\n",
    "g.map_dataframe(sns.histplot, x='total_bill', kde=True)\n",
    "g.add_legend()\n",
    "g.fig.suptitle('Distribution du montant par temps et statut fumeur', y=1.02, \n",
    "               fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint plots - Distributions jointes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint plot avec distributions marginales\n",
    "g = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', \n",
    "                  height=8, ratio=4, color='coral')\n",
    "g.fig.suptitle('Distribution jointe montant/pourboire', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint plot avec hexbin (pour grandes quantit√©s de donn√©es)\n",
    "g = sns.jointplot(data=tips, x='total_bill', y='tip', kind='hex', \n",
    "                  height=8, ratio=4, cmap='Blues')\n",
    "g.fig.suptitle('Distribution jointe (Hexbin)', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personnalisation des styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer diff√©rents styles\n",
    "styles = ['white', 'dark', 'whitegrid', 'darkgrid', 'ticks']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for ax, style in zip(axes, styles):\n",
    "    with sns.axes_style(style):\n",
    "        sns.histplot(tips['total_bill'], ax=ax, color='steelblue')\n",
    "        ax.set_title(f\"Style: {style}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palettes de couleurs\n",
    "palettes = ['deep', 'muted', 'bright', 'pastel', 'dark', 'colorblind']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, palette in zip(axes, palettes):\n",
    "    sns.barplot(data=tips, x='day', y='total_bill', palette=palette, ax=ax)\n",
    "    ax.set_title(f\"Palette: {palette}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple : Tableau de bord complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un tableau de bord d'analyse complet\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Cr√©er une grille personnalis√©e\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Distribution des montants\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "sns.histplot(data=tips, x='total_bill', kde=True, ax=ax1, color='steelblue')\n",
    "ax1.set_title('Distribution des montants', fontweight='bold')\n",
    "\n",
    "# 2. Distribution des pourboires\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "sns.histplot(data=tips, x='tip', kde=True, ax=ax2, color='coral')\n",
    "ax2.set_title('Distribution des pourboires', fontweight='bold')\n",
    "\n",
    "# 3. Relation montant/pourboire\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "sns.regplot(data=tips, x='total_bill', y='tip', ax=ax3, color='purple', scatter_kws={'alpha':0.5})\n",
    "ax3.set_title('Montant vs Pourboire', fontweight='bold')\n",
    "\n",
    "# 4. Boxplot par jour\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "sns.boxplot(data=tips, x='day', y='total_bill', ax=ax4, palette='Set2')\n",
    "ax4.set_title('Montants par jour', fontweight='bold')\n",
    "\n",
    "# 5. Violin plot par temps\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "sns.violinplot(data=tips, x='time', y='total_bill', hue='sex', split=True, ax=ax5, palette='muted')\n",
    "ax5.set_title('Distribution par temps et sexe', fontweight='bold')\n",
    "\n",
    "# 6. Count plot\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "sns.countplot(data=tips, x='day', hue='time', ax=ax6, palette='Set1')\n",
    "ax6.set_title('Nombre de repas', fontweight='bold')\n",
    "\n",
    "# 7. Heatmap de corr√©lation (grande)\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "pivot = tips.pivot_table(values='tip', index='day', columns='size', aggfunc='mean')\n",
    "sns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax7, linewidths=0.5)\n",
    "ax7.set_title('Pourboire moyen par jour et taille de groupe', fontweight='bold')\n",
    "\n",
    "plt.suptitle('üìä Tableau de bord - Analyse des pourboires', fontsize=18, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('dashboard_seaborn.png', dpi=300, bbox_inches='tight')\n",
    "print(\"‚úÖ Dashboard sauvegard√© en PNG\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique : Seaborn\n",
    "\n",
    "**Objectif** : Analyser le dataset 'titanic' de Seaborn\n",
    "\n",
    "1. Charger le dataset avec `sns.load_dataset('titanic')`\n",
    "2. Cr√©er un tableau de bord avec :\n",
    "   - Distribution des √¢ges par classe (violin plot)\n",
    "   - Taux de survie par sexe et classe (bar plot)\n",
    "   - Matrice de corr√©lation des variables num√©riques (heatmap)\n",
    "   - Relation √¢ge/tarif avec survie en couleur (scatter plot)\n",
    "3. Utiliser FacetGrid pour analyser les survivants par sexe et classe\n",
    "4. Sauvegarder votre tableau de bord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Charger le dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "print(titanic.head())\n",
    "print(f\"\\nDimensions : {titanic.shape}\")\n",
    "\n",
    "# Votre code de visualisation ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntitanic = sns.load_dataset('titanic')\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Violin plot\nsns.violinplot(data=titanic, x='class', y='age', hue='survived', \n               split=True, ax=axes[0,0])\naxes[0,0].set_title('√Çge par classe')\n\n# Bar plot survie\ntitanic.groupby(['sex','class'])['survived'].mean().unstack().plot(\n    kind='bar', ax=axes[0,1])\naxes[0,1].set_title('Survie par sexe/classe')\n\n# Heatmap\nsns.heatmap(titanic.select_dtypes(include=[np.number]).corr(), \n            annot=True, cmap='coolwarm', ax=axes[1,0])\n\n# Scatter\nsns.scatterplot(data=titanic, x='age', y='fare', hue='survived', ax=axes[1,1])\n\nplt.tight_layout()\nplt.savefig('titanic_dashboard.png')\nplt.show()\n\n# FacetGrid\ng = sns.FacetGrid(titanic, col='sex', row='class', hue='survived')\ng.map(sns.histplot, 'age')\ng.add_legend()\nplt.show()\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Manipulation de donn√©es textuelles\n",
    "\n",
    "Le traitement de texte est essentiel en Data Engineering (logs, parsing, normalisation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Nettoyage de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es textuelles brutes\n",
    "textes = pd.DataFrame({\n",
    "    'texte': [\n",
    "        '  BONJOUR   ',\n",
    "        'Salut tout le monde!',\n",
    "        'Python_est_g√©nial',\n",
    "        'Data-Engineering-2024'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Nettoyage basique\n",
    "textes['clean'] = textes['texte'].str.strip()  # Supprimer espaces\n",
    "textes['lower'] = textes['texte'].str.lower()  # Minuscules\n",
    "textes['upper'] = textes['texte'].str.upper()  # Majuscules\n",
    "textes['replace'] = textes['texte'].str.replace('_', ' ')  # Remplacer\n",
    "\n",
    "print(\"üßπ Nettoyage de texte :\")\n",
    "print(textes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 M√©thodes Pandas string (`.str` accessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es d'exemple\n",
    "df_text = pd.DataFrame({\n",
    "    'email': ['alice@example.com', 'bob@test.org', 'charlie@mail.fr'],\n",
    "    'nom_complet': ['Jean Dupont', 'Marie Martin', 'Pierre Durand'],\n",
    "    'telephone': ['0612345678', '06-98-76-54-32', '06 11 22 33 44']\n",
    "})\n",
    "\n",
    "# V√©rifier si contient\n",
    "df_text['email_gmail'] = df_text['email'].str.contains('gmail')\n",
    "\n",
    "# Commencer/finir par\n",
    "df_text['email_com'] = df_text['email'].str.endswith('.com')\n",
    "\n",
    "# Extraire le domaine\n",
    "df_text['domaine'] = df_text['email'].str.split('@').str[1]\n",
    "\n",
    "# S√©parer nom et pr√©nom\n",
    "df_text[['prenom', 'nom']] = df_text['nom_complet'].str.split(' ', expand=True)\n",
    "\n",
    "# Longueur\n",
    "df_text['longueur_nom'] = df_text['nom_complet'].str.len()\n",
    "\n",
    "print(\"üî§ M√©thodes string :\")\n",
    "print(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Expressions r√©guli√®res (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Exemples de regex courantes\n",
    "texte_test = \"\"\"\n",
    "Contact: alice@example.com ou bob@test.org\n",
    "T√©l√©phones: 06.12.34.56.78, 01-23-45-67-89\n",
    "URL: https://www.example.com\n",
    "Prix: 29.99‚Ç¨, 15.50‚Ç¨, 100‚Ç¨\n",
    "\"\"\"\n",
    "\n",
    "# Extraire les emails\n",
    "emails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', texte_test)\n",
    "print(\"üìß Emails trouv√©s :\")\n",
    "print(emails)\n",
    "\n",
    "# Extraire les t√©l√©phones\n",
    "telephones = re.findall(r'\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}', texte_test)\n",
    "print(\"\\nüìû T√©l√©phones trouv√©s :\")\n",
    "print(telephones)\n",
    "\n",
    "# Extraire les URLs\n",
    "urls = re.findall(r'https?://[^\\s]+', texte_test)\n",
    "print(\"\\nüîó URLs trouv√©es :\")\n",
    "print(urls)\n",
    "\n",
    "# Extraire les prix\n",
    "prix = re.findall(r'\\d+\\.?\\d*‚Ç¨', texte_test)\n",
    "print(\"\\nüí∞ Prix trouv√©s :\")\n",
    "print(prix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation avec regex\n",
    "def valider_email(email):\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "# Test\n",
    "emails_test = ['alice@example.com', 'bob@invalid', 'charlie.fr', 'david@test.org']\n",
    "for email in emails_test:\n",
    "    valide = \"‚úÖ\" if valider_email(email) else \"‚ùå\"\n",
    "    print(f\"{valide} {email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Cas d'usage r√©els : Parsing de logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de logs Apache/Nginx\n",
    "logs = \"\"\"\n",
    "192.168.1.1 - - [01/Dec/2024:10:15:30 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n",
    "192.168.1.2 - - [01/Dec/2024:10:16:45 +0000] \"POST /api/login HTTP/1.1\" 401 567\n",
    "192.168.1.3 - - [01/Dec/2024:10:17:20 +0000] \"GET /api/products HTTP/1.1\" 200 8901\n",
    "\"\"\"\n",
    "\n",
    "# Pattern pour parser les logs\n",
    "pattern = r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)'\n",
    "\n",
    "# Extraire les informations\n",
    "matches = re.findall(pattern, logs)\n",
    "\n",
    "# Cr√©er un DataFrame\n",
    "df_logs = pd.DataFrame(matches, columns=[\n",
    "    'ip', 'timestamp', 'methode', 'endpoint', 'protocole', 'status', 'bytes'\n",
    "])\n",
    "\n",
    "# Convertir les types\n",
    "df_logs['status'] = df_logs['status'].astype(int)\n",
    "df_logs['bytes'] = df_logs['bytes'].astype(int)\n",
    "\n",
    "print(\"üìã Logs pars√©s :\")\n",
    "print(df_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Gestion de l'encodage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un fichier avec encodage sp√©cifique\n",
    "texte_accentue = \"Voici du texte avec des accents : √©√†√π√¥ √ß√±\"\n",
    "\n",
    "# Sauvegarder en UTF-8\n",
    "with open('test_utf8.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(texte_accentue)\n",
    "\n",
    "# Sauvegarder en Latin-1\n",
    "with open('test_latin1.txt', 'w', encoding='latin-1') as f:\n",
    "    f.write(texte_accentue)\n",
    "\n",
    "# Lire avec le bon encodage\n",
    "print(\"‚úÖ Lecture UTF-8 :\")\n",
    "with open('test_utf8.txt', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\n‚úÖ Lecture Latin-1 :\")\n",
    "with open('test_latin1.txt', 'r', encoding='latin-1') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©tecter l'encodage automatiquement\n",
    "!pip install chardet\n",
    "\n",
    "import chardet\n",
    "\n",
    "# D√©tecter l'encodage d'un fichier\n",
    "with open('test_utf8.txt', 'rb') as f:\n",
    "    result = chardet.detect(f.read())\n",
    "    print(f\"Encodage d√©tect√© : {result['encoding']} (confiance: {result['confidence']*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique 2 : Texte et Regex\n",
    "\n",
    "**Objectif** : Nettoyer et valider des donn√©es clients\n",
    "\n",
    "1. Cr√©er un DataFrame avec nom, email, t√©l√©phone\n",
    "2. Nettoyer les noms (trim, capitaliser)\n",
    "3. Valider les emails avec regex\n",
    "4. Normaliser les num√©ros de t√©l√©phone (format uniforme)\n",
    "5. Exporter les donn√©es valides uniquement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({\n    'nom': ['  alice DUPONT  ', 'BOB martin', 'Charlie Brown'],\n    'email': ['alice@gmail.com', 'bob@invalid', 'charlie@test.fr'],\n    'telephone': ['06 12 34 56 78', '+33698765432', '06-11-22-33-44']\n})\n\n# Nettoyer noms\ndf['nom_clean'] = df['nom'].str.strip().str.title()\n\n# Valider emails\nemail_re = r'^[\\w.+-]+@[\\w-]+\\.[a-z]{2,}$'\ndf['email_ok'] = df['email'].apply(lambda x: bool(re.match(email_re, x)))\n\n# Normaliser t√©l√©phones\ndef norm_tel(t):\n    d = re.sub(r'\\D', '', t)\n    if d.startswith('33'): d = '0' + d[2:]\n    return ' '.join([d[i:i+2] for i in range(0,10,2)]) if len(d)==10 else None\n\ndf['tel_clean'] = df['telephone'].apply(norm_tel)\n\n# Export valides\ndf[df['email_ok'] & df['tel_clean'].notna()].to_csv('clients_ok.csv', index=False)\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ JSON et APIs REST\n",
    "\n",
    "Les APIs sont une source de donn√©es majeure en Data Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Manipulation de JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Cr√©er un dictionnaire Python\n",
    "data = {\n",
    "    \"nom\": \"Alice\",\n",
    "    \"age\": 30,\n",
    "    \"competences\": [\"Python\", \"SQL\", \"Pandas\"],\n",
    "    \"actif\": True\n",
    "}\n",
    "\n",
    "# Convertir en JSON\n",
    "json_str = json.dumps(data, indent=2)\n",
    "print(\"JSON format√© :\")\n",
    "print(json_str)\n",
    "\n",
    "# Reconvertir en dictionnaire\n",
    "data_reloaded = json.loads(json_str)\n",
    "print(\"\\n Recharg√© :\")\n",
    "print(data_reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder et lire des fichiers JSON\n",
    "\n",
    "# Sauvegarder\n",
    "with open('data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "print(\"‚úÖ JSON sauvegard√©\")\n",
    "\n",
    "# Lire\n",
    "with open('data.json', 'r', encoding='utf-8') as f:\n",
    "    data_loaded = json.load(f)\n",
    "print(\"\\n JSON charg√© :\")\n",
    "print(data_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Appels API avec `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# API publique gratuite : JSONPlaceholder\n",
    "url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "\n",
    "# GET Request\n",
    "response = requests.get(url)\n",
    "\n",
    "# V√©rifier le statut\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    users = response.json()\n",
    "    print(f\"\\n‚úÖ {len(users)} utilisateurs r√©cup√©r√©s\")\n",
    "    print(\"\\nPremier utilisateur :\")\n",
    "    print(json.dumps(users[0], indent=2))\n",
    "else:\n",
    "    print(\"‚ùå Erreur lors de la requ√™te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en DataFrame\n",
    "df_users = pd.json_normalize(users)\n",
    "print(\"üë• DataFrame des utilisateurs :\")\n",
    "print(df_users.head())\n",
    "print(f\"\\nColonnes : {df_users.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Gestion des erreurs HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_safe(url):\n",
    "    \"\"\"R√©cup√®re des donn√©es avec gestion d'erreurs\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # L√®ve une exception si statut >= 400\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"‚è±Ô∏è Timeout : le serveur met trop de temps √† r√©pondre\")\n",
    "        return None\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"‚ùå Erreur HTTP : {e}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Erreur de connexion : {e}\")\n",
    "        return None\n",
    "\n",
    "# Test avec une URL valide\n",
    "data = fetch_data_safe(\"https://jsonplaceholder.typicode.com/users/1\")\n",
    "if data:\n",
    "    print(\"‚úÖ Donn√©es r√©cup√©r√©es :\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "\n",
    "# Test avec une URL invalide\n",
    "data = fetch_data_safe(\"https://jsonplaceholder.typicode.com/invalid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Authentification API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple 1 : API Key dans les headers\n",
    "headers = {\n",
    "    \"Authorization\": \"Bearer YOUR_API_KEY_HERE\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# response = requests.get(url, headers=headers)\n",
    "\n",
    "# Exemple 2 : API Key dans les param√®tres\n",
    "params = {\n",
    "    \"api_key\": \"YOUR_API_KEY_HERE\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "# response = requests.get(url, params=params)\n",
    "\n",
    "# Exemple 3 : Basic Auth\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# response = requests.get(url, auth=HTTPBasicAuth('username', 'password'))\n",
    "\n",
    "print(\"üí° Les exemples ci-dessus montrent diff√©rentes m√©thodes d'authentification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Pagination d'APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_pages(base_url, max_pages=5):\n",
    "    \"\"\"R√©cup√®re toutes les pages d'une API pagin√©e\"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}?_page={page}&_limit=10\"\n",
    "        print(f\" R√©cup√©ration page {page}...\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if not data:  # Plus de donn√©es\n",
    "                break\n",
    "            all_data.extend(data)\n",
    "        else:\n",
    "            print(f\"‚ùå Erreur page {page}\")\n",
    "            break\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "# Test avec JSONPlaceholder\n",
    "posts = fetch_all_pages(\"https://jsonplaceholder.typicode.com/posts\", max_pages=3)\n",
    "print(f\"\\n‚úÖ Total r√©cup√©r√© : {len(posts)} posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Rate Limiting et Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_with_retry(url, max_retries=3, delay=2):\n",
    "    \"\"\"R√©cup√®re des donn√©es avec retry et backoff exponentiel\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Tentative {attempt + 1}/{max_retries}\")\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ùå Erreur : {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = delay * (2 ** attempt)  # Backoff exponentiel\n",
    "                print(f\"‚è≥ Attente de {wait_time}s avant nouvelle tentative...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(\"‚ùå √âchec apr√®s toutes les tentatives\")\n",
    "                return None\n",
    "\n",
    "# Test\n",
    "data = fetch_with_retry(\"https://jsonplaceholder.typicode.com/users/1\")\n",
    "if data:\n",
    "    print(\"\\n‚úÖ Succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate limiting simple\n",
    "def fetch_with_rate_limit(urls, requests_per_second=2):\n",
    "    \"\"\"R√©cup√®re plusieurs URLs en respectant un rate limit\"\"\"\n",
    "    delay = 1.0 / requests_per_second\n",
    "    results = []\n",
    "    \n",
    "    for url in urls:\n",
    "        start = time.time()\n",
    "        print(f\"‚è¨ R√©cup√©ration : {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            results.append(response.json())\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        sleep_time = max(0, delay - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test\n",
    "urls = [\n",
    "    \"https://jsonplaceholder.typicode.com/users/1\",\n",
    "    \"https://jsonplaceholder.typicode.com/users/2\",\n",
    "    \"https://jsonplaceholder.typicode.com/users/3\"\n",
    "]\n",
    "\n",
    "start_time = time.time()\n",
    "results = fetch_with_rate_limit(urls, requests_per_second=1)\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ {len(results)} URLs r√©cup√©r√©es en {total_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 JSON imbriqu√© complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON complexe imbriqu√©\n",
    "complex_json = {\n",
    "    \"id\": 1,\n",
    "    \"nom\": \"Entreprise A\",\n",
    "    \"employes\": [\n",
    "        {\n",
    "            \"id\": 101,\n",
    "            \"nom\": \"Alice\",\n",
    "            \"competences\": [\"Python\", \"SQL\"],\n",
    "            \"adresse\": {\"ville\": \"Paris\", \"code_postal\": \"75001\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": 102,\n",
    "            \"nom\": \"Bob\",\n",
    "            \"competences\": [\"Java\", \"Docker\"],\n",
    "            \"adresse\": {\"ville\": \"Lyon\", \"code_postal\": \"69001\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Normaliser avec json_normalize\n",
    "df_complex = pd.json_normalize(\n",
    "    complex_json,\n",
    "    record_path='employes',\n",
    "    meta=['nom'],\n",
    "    meta_prefix='entreprise_'\n",
    ")\n",
    "\n",
    "print(\"üîÑ JSON normalis√© :\")\n",
    "print(df_complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique 3 : APIs\n",
    "\n",
    "**Objectif** : R√©cup√©rer et analyser des donn√©es d'une API publique\n",
    "\n",
    "1. Utiliser l'API JSONPlaceholder pour r√©cup√©rer les posts\n",
    "2. Convertir en DataFrame\n",
    "3. Compter le nombre de posts par utilisateur\n",
    "4. R√©cup√©rer les d√©tails des 5 utilisateurs les plus actifs\n",
    "5. Exporter le r√©sultat en JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport requests\nimport pandas as pd\n\n# 1-2. R√©cup√©rer les posts\nposts = requests.get(\"https://jsonplaceholder.typicode.com/posts\").json()\ndf_posts = pd.DataFrame(posts)\n\n# 3. Posts par utilisateur\nposts_count = df_posts.groupby('userId').size().reset_index(name='nb_posts')\nposts_count = posts_count.sort_values('nb_posts', ascending=False)\n\n# 4. D√©tails des 5 top users\ntop_5 = posts_count.head(5)['userId'].tolist()\nusers = []\nfor uid in top_5:\n    u = requests.get(f\"https://jsonplaceholder.typicode.com/users/{uid}\").json()\n    users.append({'userId': u['id'], 'name': u['name'], 'email': u['email']})\n\ndf_result = pd.DataFrame(users).merge(posts_count, on='userId')\nprint(df_result)\n\n# 5. Export JSON\ndf_result.to_json('top_users.json', orient='records', indent=2)\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Data Validation\n",
    "\n",
    "La validation des donn√©es est cruciale pour garantir leur qualit√©."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 V√©rifications basiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es de test\n",
    "test_data = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 2, 5],\n",
    "    'email': ['alice@test.com', 'bob@test', None, 'bob@test', 'eve@test.com'],\n",
    "    'age': [25, 150, -5, 30, 28],\n",
    "    'salaire': [45000, 55000, 60000, 55000, None]\n",
    "})\n",
    "\n",
    "print(\"Donn√©es de test :\")\n",
    "print(test_data)\n",
    "\n",
    "# V√©rifications\n",
    "print(\"\\n V√©rifications :\")\n",
    "print(f\"Colonnes manquantes : {set(['user_id', 'email', 'age', 'salaire']) - set(test_data.columns)}\")\n",
    "print(f\"Valeurs nulles : {test_data.isnull().sum().sum()}\")\n",
    "print(f\"Doublons : {test_data.duplicated().sum()}\")\n",
    "print(f\"Types : \\n{test_data.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Classe de validation compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidator:\n",
    "    \"\"\"Validateur simple pour DataFrames\"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.errors = []\n",
    "    \n",
    "    def check_columns(self, required_columns):\n",
    "        \"\"\"V√©rifie pr√©sence des colonnes requises\"\"\"\n",
    "        missing = set(required_columns) - set(self.df.columns)\n",
    "        if missing:\n",
    "            self.errors.append(f\"Colonnes manquantes: {missing}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_nulls(self, max_null_pct=10):\n",
    "        \"\"\"V√©rifie le pourcentage de valeurs nulles\"\"\"\n",
    "        null_pct = (self.df.isnull().sum() / len(self.df)) * 100\n",
    "        violations = null_pct[null_pct > max_null_pct]\n",
    "        if not violations.empty:\n",
    "            self.errors.append(f\"Trop de nulls: {violations.to_dict()}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_range(self, column, min_val, max_val):\n",
    "        \"\"\"V√©rifie que les valeurs sont dans une plage\"\"\"\n",
    "        if column in self.df.columns:\n",
    "            violations = self.df[(self.df[column] < min_val) | (self.df[column] > max_val)]\n",
    "            if len(violations) > 0:\n",
    "                self.errors.append(f\"{column}: {len(violations)} valeurs hors plage [{min_val}, {max_val}]\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def check_duplicates(self, subset=None):\n",
    "        \"\"\"V√©rifie les doublons\"\"\"\n",
    "        duplicates = self.df.duplicated(subset=subset).sum()\n",
    "        if duplicates > 0:\n",
    "            self.errors.append(f\"{duplicates} doublons trouv√©s\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_types(self, column, expected_type):\n",
    "        \"\"\"V√©rifie le type d'une colonne\"\"\"\n",
    "        if column in self.df.columns:\n",
    "            if self.df[column].dtype != expected_type:\n",
    "                self.errors.append(f\"{column}: type attendu {expected_type}, obtenu {self.df[column].dtype}\")\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Retourne True si valide, False sinon\"\"\"\n",
    "        return len(self.errors) == 0\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"G√©n√®re un rapport de validation\"\"\"\n",
    "        return {\n",
    "            'is_valid': self.validate(),\n",
    "            'total_errors': len(self.errors),\n",
    "            'errors': self.errors\n",
    "        }\n",
    "\n",
    "# Utilisation\n",
    "validator = DataValidator(test_data)\n",
    "validator.check_columns(['user_id', 'email', 'age'])\n",
    "validator.check_nulls(max_null_pct=15)\n",
    "validator.check_range('age', 0, 120)\n",
    "validator.check_duplicates(subset=['user_id', 'email'])\n",
    "\n",
    "report = validator.report()\n",
    "print(\"\\n Rapport de validation:\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Validation avec sch√©ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finir un sch√©ma de validation\n",
    "schema = {\n",
    "    'user_id': {'type': 'int64', 'nullable': False, 'unique': True},\n",
    "    'email': {'type': 'object', 'nullable': False, 'pattern': r'.+@.+\\..+'},\n",
    "    'age': {'type': 'int64', 'nullable': False, 'min': 0, 'max': 120},\n",
    "    'salaire': {'type': 'int64', 'nullable': True, 'min': 0}\n",
    "}\n",
    "\n",
    "def validate_schema(df, schema):\n",
    "    \"\"\"Valide un DataFrame contre un sch√©ma\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    for column, rules in schema.items():\n",
    "        # V√©rifier si la colonne existe\n",
    "        if column not in df.columns:\n",
    "            errors.append(f\"Colonne manquante: {column}\")\n",
    "            continue\n",
    "        \n",
    "        # V√©rifier les nulls\n",
    "        if not rules.get('nullable', True) and df[column].isnull().any():\n",
    "            errors.append(f\"{column}: contient des valeurs nulles\")\n",
    "        \n",
    "        # V√©rifier l'unicit√©\n",
    "        if rules.get('unique', False) and df[column].duplicated().any():\n",
    "            errors.append(f\"{column}: contient des doublons\")\n",
    "        \n",
    "        # V√©rifier la plage\n",
    "        if 'min' in rules:\n",
    "            violations = df[df[column] < rules['min']]\n",
    "            if len(violations) > 0:\n",
    "                errors.append(f\"{column}: {len(violations)} valeurs < {rules['min']}\")\n",
    "        \n",
    "        if 'max' in rules:\n",
    "            violations = df[df[column] > rules['max']]\n",
    "            if len(violations) > 0:\n",
    "                errors.append(f\"{column}: {len(violations)} valeurs > {rules['max']}\")\n",
    "        \n",
    "        # V√©rifier le pattern (pour les strings)\n",
    "        if 'pattern' in rules:\n",
    "            pattern = rules['pattern']\n",
    "            invalid = df[column].dropna()[~df[column].dropna().str.match(pattern)]\n",
    "            if len(invalid) > 0:\n",
    "                errors.append(f\"{column}: {len(invalid)} valeurs ne matchent pas le pattern\")\n",
    "    \n",
    "    return {\n",
    "        'is_valid': len(errors) == 0,\n",
    "        'errors': errors\n",
    "    }\n",
    "\n",
    "# Test\n",
    "result = validate_schema(test_data, schema)\n",
    "print(\"\\nüìã Validation avec sch√©ma :\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Pratique 4 : Validation\n",
    "\n",
    "**Objectif** : Cr√©er un validateur pour des transactions\n",
    "\n",
    "1. Cr√©er un DataFrame de transactions (id, date, montant, type)\n",
    "2. D√©finir un sch√©ma de validation\n",
    "3. Valider que toutes les transactions ont un montant positif\n",
    "4. V√©rifier qu'il n'y a pas de doublons d'ID\n",
    "5. G√©n√©rer un rapport de qualit√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Votre code ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 1. Cr√©er transactions (avec erreurs)\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'id': list(range(1,51)) + [25, 30],  # doublons\n    'date': pd.date_range('2024-01-01', periods=52),\n    'montant': list(np.random.uniform(10, 500, 50)) + [-50, 0],  # n√©gatifs\n    'type': np.random.choice(['achat', 'remboursement'], 52)\n})\n\n# 2-4. Validation\nerreurs = []\nif (df['montant'] <= 0).any():\n    erreurs.append(f\"‚ùå {(df['montant']<=0).sum()} montants invalides\")\nif df['id'].duplicated().any():\n    erreurs.append(f\"‚ùå {df['id'].duplicated().sum()} doublons\")\n\n# 5. Rapport\nprint(\"üìã RAPPORT\")\nprint(f\"Valide: {len(erreurs)==0}\")\nfor e in erreurs: print(e)\n\n# Nettoyage\ndf_clean = df[(df['montant']>0) & ~df['id'].duplicated(keep='first')]\nprint(f\"‚úÖ {len(df_clean)}/{len(df)} lignes valides\")\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Mini-Pipeline Complet\n",
    "\n",
    "Construisons un pipeline ETL complet en int√©grant tous les concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Architecture du pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er la structure de dossiers\n",
    "from pathlib import Path\n",
    "\n",
    "dirs = ['data/raw', 'data/processed', 'data/output', 'logs']\n",
    "for dir_path in dirs:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Structure de dossiers cr√©√©e\")\n",
    "print(\"\\nüìÅ Structure :\")\n",
    "print(\"\"\"\n",
    "project/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ output/\n",
    "‚îî‚îÄ‚îÄ logs/\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Configuration et Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration du logging\n",
    "log_file = f\"logs/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('ETL_Pipeline')\n",
    "logger.info(\"üöÄ Pipeline d√©marr√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration centralis√©e\n",
    "class Config:\n",
    "    \"\"\"Configuration du pipeline\"\"\"\n",
    "    # Chemins\n",
    "    RAW_DATA_DIR = 'data/raw'\n",
    "    PROCESSED_DATA_DIR = 'data/processed'\n",
    "    OUTPUT_DIR = 'data/output'\n",
    "    \n",
    "    # API\n",
    "    API_URL = 'https://jsonplaceholder.typicode.com'\n",
    "    API_TIMEOUT = 10\n",
    "    API_MAX_RETRIES = 3\n",
    "    \n",
    "    # Validation\n",
    "    MAX_NULL_PCT = 10\n",
    "    \n",
    "    # Export\n",
    "    EXPORT_FORMATS = ['csv', 'parquet', 'json']\n",
    "\n",
    "config = Config()\n",
    "logger.info(\"‚öôÔ∏è Configuration charg√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 √âtape 1 : Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_api(url, max_retries=3):\n",
    "    \"\"\"Extrait des donn√©es depuis une API\"\"\"\n",
    "    logger.info(f\"üì• Extraction depuis {url}\")\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=config.API_TIMEOUT)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            logger.info(f\"‚úÖ {len(data)} enregistrements extraits\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Tentative {attempt + 1}/{max_retries} √©chou√©e: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                logger.error(\"‚ùå Extraction √©chou√©e\")\n",
    "                raise\n",
    "            time.sleep(2 ** attempt)\n",
    "\n",
    "# Test extraction\n",
    "users_data = extract_from_api(f\"{config.API_URL}/users\")\n",
    "df_raw = pd.DataFrame(users_data)\n",
    "\n",
    "# Sauvegarder les donn√©es brutes\n",
    "raw_file = f\"{config.RAW_DATA_DIR}/users_raw_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df_raw.to_csv(raw_file, index=False)\n",
    "logger.info(f\"üíæ Donn√©es brutes sauvegard√©es: {raw_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 √âtape 2 : Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"Transforme et nettoie les donn√©es\"\"\"\n",
    "    logger.info(\"üîÑ D√©but de la transformation\")\n",
    "    \n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    # 1. Normaliser les colonnes imbriqu√©es\n",
    "    if 'address' in df.columns:\n",
    "        address_df = pd.json_normalize(df['address'])\n",
    "        address_df.columns = ['address_' + col for col in address_df.columns]\n",
    "        df_transformed = pd.concat([df_transformed.drop('address', axis=1), address_df], axis=1)\n",
    "        logger.info(\"‚úÖ Colonnes adresse normalis√©es\")\n",
    "    \n",
    "    # 2. Nettoyer les noms de colonnes\n",
    "    df_transformed.columns = df_transformed.columns.str.lower().str.replace('.', '_')\n",
    "    logger.info(\"‚úÖ Noms de colonnes nettoy√©s\")\n",
    "    \n",
    "    # 3. G√©rer les valeurs manquantes\n",
    "    null_counts = df_transformed.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        logger.warning(f\"‚ö†Ô∏è {null_counts.sum()} valeurs manquantes d√©tect√©es\")\n",
    "        df_transformed = df_transformed.dropna()\n",
    "        logger.info(\"‚úÖ Valeurs manquantes supprim√©es\")\n",
    "    \n",
    "    # 4. Cr√©er des colonnes d√©riv√©es\n",
    "    if 'name' in df_transformed.columns:\n",
    "        df_transformed['name_length'] = df_transformed['name'].str.len()\n",
    "        logger.info(\"‚úÖ Colonne d√©riv√©e 'name_length' cr√©√©e\")\n",
    "    \n",
    "    # 5. Ajouter metadata\n",
    "    df_transformed['processed_at'] = datetime.now().isoformat()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Transformation termin√©e: {len(df_transformed)} lignes\")\n",
    "    return df_transformed\n",
    "\n",
    "# Test transformation\n",
    "df_transformed = transform_data(df_raw)\n",
    "print(\"\\nüìä Donn√©es transform√©es :\")\n",
    "print(df_transformed.head())\n",
    "print(f\"\\nColonnes: {df_transformed.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 √âtape 3 : Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df):\n",
    "    \"\"\"Valide la qualit√© des donn√©es\"\"\"\n",
    "    logger.info(\"üîç D√©but de la validation\")\n",
    "    \n",
    "    validator = DataValidator(df)\n",
    "    \n",
    "    # D√©finir les r√®gles de validation\n",
    "    required_columns = ['id', 'name', 'email']\n",
    "    validator.check_columns(required_columns)\n",
    "    validator.check_nulls(max_null_pct=config.MAX_NULL_PCT)\n",
    "    validator.check_duplicates(subset=['id'])\n",
    "    \n",
    "    # G√©n√©rer le rapport\n",
    "    report = validator.report()\n",
    "    \n",
    "    if report['is_valid']:\n",
    "        logger.info(\"‚úÖ Validation r√©ussie\")\n",
    "    else:\n",
    "        logger.error(f\"‚ùå Validation √©chou√©e: {report['total_errors']} erreurs\")\n",
    "        for error in report['errors']:\n",
    "            logger.error(f\"  - {error}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test validation\n",
    "validation_report = validate_data(df_transformed)\n",
    "print(\"\\nüìã Rapport de validation :\")\n",
    "print(json.dumps(validation_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 √âtape 4 : Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, base_filename):\n",
    "    \"\"\"Exporte les donn√©es dans plusieurs formats\"\"\"\n",
    "    logger.info(\"üíæ D√©but de l'export\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    files_created = []\n",
    "    \n",
    "    for format_type in config.EXPORT_FORMATS:\n",
    "        filename = f\"{config.OUTPUT_DIR}/{base_filename}_{timestamp}.{format_type}\"\n",
    "        \n",
    "        try:\n",
    "            if format_type == 'csv':\n",
    "                df.to_csv(filename, index=False)\n",
    "            elif format_type == 'parquet':\n",
    "                df.to_parquet(filename, index=False)\n",
    "            elif format_type == 'json':\n",
    "                df.to_json(filename, orient='records', indent=2)\n",
    "            \n",
    "            file_size = Path(filename).stat().st_size / 1024  # KB\n",
    "            logger.info(f\"‚úÖ Export {format_type.upper()}: {filename} ({file_size:.2f} KB)\")\n",
    "            files_created.append(filename)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erreur export {format_type}: {e}\")\n",
    "    \n",
    "    return files_created\n",
    "\n",
    "# Test export\n",
    "exported_files = load_data(df_transformed, 'users_processed')\n",
    "print(\"\\nüì¶ Fichiers export√©s :\")\n",
    "for file in exported_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Pipeline complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Ex√©cute le pipeline complet\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"üöÄ D√âMARRAGE DU PIPELINE\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT\n",
    "        logger.info(\"\\nüì• PHASE 1: EXTRACTION\")\n",
    "        data = extract_from_api(f\"{config.API_URL}/users\")\n",
    "        df = pd.DataFrame(data)\n",
    "        logger.info(f\"Lignes extraites: {len(df)}\")\n",
    "        \n",
    "        # TRANSFORM\n",
    "        logger.info(\"\\nüîÑ PHASE 2: TRANSFORMATION\")\n",
    "        df_clean = transform_data(df)\n",
    "        logger.info(f\"Lignes apr√®s transformation: {len(df_clean)}\")\n",
    "        \n",
    "        # VALIDATE\n",
    "        logger.info(\"\\nüîç PHASE 3: VALIDATION\")\n",
    "        validation = validate_data(df_clean)\n",
    "        \n",
    "        if not validation['is_valid']:\n",
    "            logger.error(\"‚ùå Validation √©chou√©e, arr√™t du pipeline\")\n",
    "            return False\n",
    "        \n",
    "        # LOAD\n",
    "        logger.info(\"\\nüíæ PHASE 4: EXPORT\")\n",
    "        files = load_data(df_clean, 'users_final')\n",
    "        \n",
    "        # STATISTIQUES\n",
    "        duration = time.time() - start_time\n",
    "        logger.info(\"\\n\" + \"=\"*50)\n",
    "        logger.info(\"üìä STATISTIQUES DU PIPELINE\")\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(f\"Dur√©e totale: {duration:.2f}s\")\n",
    "        logger.info(f\"Lignes trait√©es: {len(df_clean)}\")\n",
    "        logger.info(f\"Fichiers cr√©√©s: {len(files)}\")\n",
    "        logger.info(f\"Taux de r√©ussite: 100%\")\n",
    "        logger.info(\"=\"*50)\n",
    "        logger.info(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
    "        logger.info(\"=\"*50)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ERREUR FATALE: {e}\")\n",
    "        logger.exception(\"Stack trace:\")\n",
    "        return False\n",
    "\n",
    "# Ex√©cuter le pipeline\n",
    "success = run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice Final : Pipeline Complet\n",
    "\n",
    "**Objectif** : Cr√©er votre propre pipeline ETL\n",
    "\n",
    "1. Extraire des donn√©es de posts depuis JSONPlaceholder\n",
    "2. Enrichir avec les donn√©es utilisateurs\n",
    "3. Calculer des statistiques (posts par utilisateur, mots par post, etc.)\n",
    "4. Valider la qualit√©\n",
    "5. Exporter dans tous les formats\n",
    "6. Ajouter un logging complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √Ä VOUS DE JOUER ! üéÆ\n",
    "# Cr√©ez votre pipeline complet ici\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n<summary>üí° Cliquer pour voir la solution</summary>\n\n```python\nimport requests, pandas as pd, logging, time\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO, format='%(message)s')\nlog = logging.getLogger()\n\ndef extract():\n    log.info(\"üì• Extract...\")\n    posts = pd.DataFrame(requests.get(\"https://jsonplaceholder.typicode.com/posts\").json())\n    users = pd.DataFrame(requests.get(\"https://jsonplaceholder.typicode.com/users\").json())\n    return posts, users\n\ndef transform(posts, users):\n    log.info(\"üîÑ Transform...\")\n    users = users[['id','name','email']].rename(columns={'id':'userId','name':'author'})\n    df = posts.merge(users, on='userId')\n    df['words'] = df['body'].str.split().str.len()\n    return df\n\ndef validate(df):\n    log.info(\"üîç Validate...\")\n    return not df['id'].isna().any() and not df['id'].duplicated().any()\n\ndef load(df):\n    log.info(\"üíæ Load...\")\n    Path('output').mkdir(exist_ok=True)\n    df.to_csv('output/posts.csv', index=False)\n    df.to_json('output/posts.json', orient='records')\n\ndef run():\n    start = time.time()\n    log.info(\"üöÄ START\")\n    posts, users = extract()\n    df = transform(posts, users)\n    if not validate(df): return log.error(\"‚ùå FAILED\")\n    load(df)\n    log.info(f\"‚úÖ DONE in {time.time()-start:.1f}s - {len(df)} rows\")\n\nrun()\n```\n\n</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BONUS : Configuration et Tests\n",
    "\n",
    "Pour aller plus loin dans la professionnalisation de votre code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Gestion des configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer python-dotenv\n",
    "!pip install python-dotenv\n",
    "\n",
    "# Cr√©er un fichier .env (√† ne JAMAIS commiter)\n",
    "env_content = \"\"\"\n",
    "API_KEY=votre_cle_api_secrete\n",
    "DATABASE_URL=postgresql://user:password@localhost:5432/db\n",
    "ENVIRONMENT=development\n",
    "\"\"\"\n",
    "\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"‚úÖ Fichier .env cr√©√©\")\n",
    "print(\"‚ö†Ô∏è N'oubliez pas d'ajouter .env √† votre .gitignore !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Charger les variables d'environnement\n",
    "load_dotenv()\n",
    "\n",
    "# Acc√©der aux variables\n",
    "api_key = os.getenv('API_KEY')\n",
    "db_url = os.getenv('DATABASE_URL')\n",
    "env = os.getenv('ENVIRONMENT')\n",
    "\n",
    "print(f\"üîë API Key: {api_key[:10]}...\")\n",
    "print(f\"üóÑÔ∏è Database URL: {db_url[:30]}...\")\n",
    "print(f\"üåç Environment: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Tests unitaires basiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de fonction √† tester\n",
    "def calculer_age_moyen(df, colonne='age'):\n",
    "    \"\"\"Calcule l'√¢ge moyen d'un DataFrame\"\"\"\n",
    "    if colonne not in df.columns:\n",
    "        raise ValueError(f\"Colonne '{colonne}' introuvable\")\n",
    "    return df[colonne].mean()\n",
    "\n",
    "# Tests\n",
    "def test_calculer_age_moyen():\n",
    "    # Test avec donn√©es valides\n",
    "    df_test = pd.DataFrame({'age': [20, 30, 40]})\n",
    "    assert calculer_age_moyen(df_test) == 30, \"Test 1 √©chou√©\"\n",
    "    print(\"‚úÖ Test 1: donn√©es valides\")\n",
    "    \n",
    "    # Test avec colonne manquante\n",
    "    try:\n",
    "        calculer_age_moyen(pd.DataFrame({'nom': ['Alice']}), 'age')\n",
    "        print(\"‚ùå Test 2 √©chou√©: devrait lever une exception\")\n",
    "    except ValueError:\n",
    "        print(\"‚úÖ Test 2: exception lev√©e correctement\")\n",
    "    \n",
    "    # Test avec valeurs nulles\n",
    "    df_null = pd.DataFrame({'age': [20, None, 40]})\n",
    "    result = calculer_age_moyen(df_null)\n",
    "    assert result == 30, \"Test 3 √©chou√©\"\n",
    "    print(\"‚úÖ Test 3: gestion des nulls\")\n",
    "    \n",
    "    print(\"\\nüéâ Tous les tests passent !\")\n",
    "\n",
    "# Ex√©cuter les tests\n",
    "test_calculer_age_moyen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# R√©sum√© et Prochaines √âtapes\n",
    "\n",
    "## Ce que tu as appris \n",
    "\n",
    "| Section | Comp√©tences acquises |\n",
    "|---------|---------------------|\n",
    "| **Pandas** | Manipulation de donn√©es, nettoyage, agr√©gations, merges |\n",
    "| **Matplotlib** | Graphiques de base, personnalisation, export |\n",
    "| **Seaborn** | Visualisations statistiques, heatmaps, pair plots |\n",
    "| **Texte & Regex** | Nettoyage, parsing de logs, expressions r√©guli√®res |\n",
    "| **APIs** | Appels REST, pagination, retry logic |\n",
    "| **Validation** | Sch√©mas, checks de qualit√© |\n",
    "| **Pipeline ETL** | Architecture compl√®te Extract-Transform-Load |\n",
    "| **Bonnes pratiques** | Logging, configuration, tests |\n",
    "\n",
    "## Ressources pour aller plus loin\n",
    "\n",
    "### Documentation officielle\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)\n",
    "- [Seaborn Documentation](https://seaborn.pydata.org/)\n",
    "- [Requests Documentation](https://requests.readthedocs.io/)\n",
    "\n",
    "### Tutoriels et cours\n",
    "- [Real Python - Pandas](https://realpython.com/pandas-python-explore-dataset/)\n",
    "- [Kaggle Learn](https://www.kaggle.com/learn)\n",
    "- [DataCamp](https://www.datacamp.com/)\n",
    "\n",
    "### Outils avanc√©s √† explorer\n",
    "- **Polars** ‚Äî Alternative plus rapide √† Pandas\n",
    "- **Great Expectations** ‚Äî Validation de donn√©es avanc√©e\n",
    "- **Pandera** ‚Äî Sch√©mas de validation pour DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Prochaine √©tape\n",
    "\n",
    "Maintenant que tu ma√Ætrises le traitement de donn√©es, passons aux **bases de donn√©es** !\n",
    "\n",
    "üëâ **Module suivant : `06_intro_databases`** ‚Äî Introduction aux bases de donn√©es\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **F√©licitations !** Tu as termin√© le module Python Data Processing pour Data Engineers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}