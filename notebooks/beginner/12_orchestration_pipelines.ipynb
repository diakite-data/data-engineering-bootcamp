{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â° Orchestration de Pipelines Data\n",
    "\n",
    "Ce module prÃ©sente les outils d'**orchestration** pour automatiser l'exÃ©cution de vos pipelines de donnÃ©es.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ PrÃ©requis\n",
    "\n",
    "| Niveau | CompÃ©tence |\n",
    "|--------|------------|\n",
    "| âœ… Requis | Avoir suivi le module `11_pyspark_for_data_engineering` |\n",
    "| âœ… Requis | Comprendre les pipelines ETL |\n",
    "| âœ… Requis | MaÃ®triser Python (modules 04-05) |\n",
    "| âœ… Requis | ConnaÃ®tre les bases de Linux (ligne de commande) |\n",
    "\n",
    "## ğŸ¯ Objectifs du module\n",
    "\n",
    "Ã€ la fin de ce notebook, tu seras capable de :\n",
    "\n",
    "- âœ… Comprendre ce qu'est l'orchestration de pipelines\n",
    "- âœ… Utiliser le Planificateur Windows (niveau dÃ©butant)\n",
    "- âœ… Configurer des tÃ¢ches avec Crontab (niveau intermÃ©diaire)\n",
    "- âœ… Comprendre l'**architecture** d'Apache Airflow\n",
    "- âœ… CrÃ©er des **DAGs** avec Apache Airflow\n",
    "- âœ… Utiliser les diffÃ©rents types d'**Operators**\n",
    "- âœ… GÃ©rer les **dÃ©pendances** et le **passage de donnÃ©es** (XCom)\n",
    "- âœ… Configurer les **alertes** et le **monitoring**\n",
    "- âœ… Choisir le bon outil selon ton besoin\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ L'orchestration dans l'Ã©cosystÃ¨me Data Engineering\n",
    "\n",
    "Tu as appris Ã  crÃ©er des pipelines ETL avec PySpark. Mais comment les **automatiser** pour qu'ils s'exÃ©cutent rÃ©guliÃ¨rement sans intervention manuelle ?\n",
    "\n",
    "### Le problÃ¨me\n",
    "\n",
    "```\n",
    "Sans orchestration :\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                             â”‚\n",
    "â”‚   ğŸ˜° \"Il faut que je lance mon script tous les jours...\"   â”‚\n",
    "â”‚   ğŸ˜° \"J'ai oubliÃ© de lancer le pipeline hier !\"            â”‚\n",
    "â”‚   ğŸ˜° \"Le script B a plantÃ© car A n'avait pas fini...\"      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Avec orchestration :\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                             â”‚\n",
    "â”‚   âœ… Scripts exÃ©cutÃ©s automatiquement                       â”‚\n",
    "â”‚   âœ… Alertes en cas d'Ã©chec                                 â”‚\n",
    "â”‚   âœ… DÃ©pendances respectÃ©es (A â†’ B â†’ C)                     â”‚\n",
    "â”‚   âœ… Logs et monitoring centralisÃ©s                         â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Position dans le pipeline Data\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     PIPELINE DATA                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Sources        ETL              Destination                   â”‚\n",
    "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   APIs     â”€â”                 â”Œâ”€â–º  Data Warehouse               â”‚\n",
    "â”‚   Fichiers â”€â”¼â”€â”€â–º  PySpark  â”€â”€â”€â”¼â”€â–º  Data Lake                    â”‚\n",
    "â”‚   BDD      â”€â”˜                 â””â”€â–º  Dashboard                    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚            â–²                                                    â”‚\n",
    "â”‚            â”‚                                                    â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\n",
    "â”‚   â”‚  ORCHESTRATION  â”‚  â—„â”€â”€ Quand ? Dans quel ordre ?           â”‚\n",
    "â”‚   â”‚  (Airflow/Cron) â”‚                                           â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Comparaison rapide des outils\n",
    "\n",
    "| CritÃ¨re | Windows Task | Crontab | Airflow |\n",
    "|---------|--------------|---------|----------|\n",
    "| **FacilitÃ©** | â­â­â­ | â­â­ | â­ |\n",
    "| **Interface** | GUI | CLI | Web |\n",
    "| **DÃ©pendances** | âŒ | âŒ | âœ… |\n",
    "| **Monitoring** | Basique | Non | Complet |\n",
    "| **Retry auto** | âŒ | âŒ | âœ… |\n",
    "| **IdÃ©al pour** | 1-5 scripts | 5-15 scripts | 10+ pipelines |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/diakite-data/data-engineering-bootcamp/blob/main/notebooks/beginner/12_orchestration_pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
    "",
    "> ğŸ’¡ **Conseil** : Cliquez sur le badge ci-dessus pour exÃ©cuter ce notebook directement dans Google Colab (aucune installation requise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸªŸ Niveau 1 : Planificateur Windows\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Un outil **intÃ©grÃ© Ã  Windows** pour exÃ©cuter des programmes automatiquement.\n",
    "\n",
    "## Comment l'utiliser ?\n",
    "\n",
    "### Ouvrir le planificateur :\n",
    "```\n",
    "Windows + R â†’ Taper 'taskschd.msc' â†’ EntrÃ©e\n",
    "```\n",
    "\n",
    "### CrÃ©er une tÃ¢che :\n",
    "1. **Action** â†’ CrÃ©er une tÃ¢che de base\n",
    "2. **Nom** : \"Mon script quotidien\"\n",
    "3. **DÃ©clencheur** : Quotidien Ã  2h du matin\n",
    "4. **Action** : DÃ©marrer `python.exe` avec `C:\\scripts\\mon_script.py`\n",
    "5. **Terminer**\n",
    "\n",
    "âœ… **VoilÃ  !** Votre script s'exÃ©cutera automatiquement tous les jours Ã  2h.\n",
    "\n",
    "## âœ… Forces\n",
    "\n",
    "âœ… **TrÃ¨s facile** - Interface graphique intuitive  \n",
    "âœ… **DÃ©jÃ  installÃ©** - Pas de setup  \n",
    "âœ… **Parfait pour dÃ©buter** - Pas de code complexe  \n",
    "\n",
    "## âŒ Faiblesses\n",
    "\n",
    "âŒ **Pas de dÃ©pendances** - Si tÃ¢che A doit finir avant tÃ¢che B â†’ compliquÃ©  \n",
    "âŒ **Monitoring limitÃ©** - Difficile de voir l'Ã©tat global  \n",
    "âŒ **Windows uniquement** - Ne fonctionne pas sur Linux  \n",
    "\n",
    "## Quand l'utiliser ?\n",
    "\n",
    "âœ… **OUI** : Vous avez 1-5 scripts simples sur Windows  \n",
    "âŒ **NON** : Vous avez besoin que script B attende script A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ§ Niveau 2 : Crontab (Linux/Mac)\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Le **planificateur standard** sur Linux/Mac.\n",
    "\n",
    "## Syntaxe de base\n",
    "\n",
    "```bash\n",
    "minute heure jour mois jour_semaine commande\n",
    "```\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\n",
    "â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\n",
    "â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\n",
    "â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\n",
    "â”‚ â”‚ â”‚ â”‚ â”‚\n",
    "* * * * * commande\n",
    "```\n",
    "\n",
    "### Exemples simples :\n",
    "\n",
    "```bash\n",
    "# Tous les jours Ã  2h du matin\n",
    "0 2 * * * python3 /home/user/script.py\n",
    "\n",
    "# Toutes les heures\n",
    "0 * * * * python3 /home/user/hourly.py\n",
    "\n",
    "# Lundi Ã  vendredi Ã  9h\n",
    "0 9 * * 1-5 python3 /home/user/weekday.py\n",
    "\n",
    "# Toutes les 15 minutes\n",
    "*/15 * * * * python3 /home/user/check.py\n",
    "\n",
    "# Le 1er de chaque mois Ã  minuit\n",
    "0 0 1 * * python3 /home/user/monthly.py\n",
    "```\n",
    "\n",
    "## Comment l'utiliser ?\n",
    "\n",
    "```bash\n",
    "# Ã‰diter votre crontab\n",
    "crontab -e\n",
    "\n",
    "# Voir les tÃ¢ches planifiÃ©es\n",
    "crontab -l\n",
    "\n",
    "# Ajouter vos lignes\n",
    "0 2 * * * python3 /home/user/backup.py >> /var/log/backup.log 2>&1\n",
    "\n",
    "# Sauvegarder et quitter\n",
    "# âœ… C'est fait !\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Astuce** : Utilise [crontab.guru](https://crontab.guru/) pour tester tes expressions !\n",
    "\n",
    "## âœ… Forces\n",
    "\n",
    "âœ… **Universel** - Sur TOUS les serveurs Linux  \n",
    "âœ… **TrÃ¨s lÃ©ger** - Presque pas de ressources  \n",
    "âœ… **Simple** - Une ligne = une tÃ¢che  \n",
    "âœ… **Gratuit** - DÃ©jÃ  installÃ©  \n",
    "\n",
    "## âŒ Faiblesses\n",
    "\n",
    "âŒ **Pas de dÃ©pendances** - MÃªme problÃ¨me que Windows  \n",
    "âŒ **Pas de monitoring** - Aucune interface  \n",
    "âŒ **Pas de retry** - Si Ã§a Ã©choue, il faut attendre le prochain run  \n",
    "âŒ **Logs manuels** - Il faut les gÃ©rer soi-mÃªme  \n",
    "\n",
    "## Quand l'utiliser ?\n",
    "\n",
    "âœ… **OUI** : Serveur Linux, 5-15 scripts indÃ©pendants  \n",
    "âŒ **NON** : Scripts avec dÃ©pendances complexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸŒ¬ï¸ Niveau 3 : Apache Airflow\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Un **orchestrateur professionnel** pour gÃ©rer des workflows complexes, crÃ©Ã© par **Airbnb** en 2014 et devenu projet **Apache** en 2016.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    APACHE AIRFLOW                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   \"Airflow is a platform to programmatically author,           â”‚\n",
    "â”‚    schedule, and monitor workflows.\"                            â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   â€¢ CrÃ©Ã© par Airbnb (2014)                                      â”‚\n",
    "â”‚   â€¢ Apache Top-Level Project (2019)                             â”‚\n",
    "â”‚   â€¢ 30,000+ GitHub stars                                        â”‚\n",
    "â”‚   â€¢ UtilisÃ© par : Airbnb, Netflix, Spotify, Twitter, Adobe...   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### La grande diffÃ©rence avec Cron :\n",
    "\n",
    "```python\n",
    "# âŒ Avec Cron (problÃ¨me) :\n",
    "0 2 * * * python extract.py\n",
    "30 2 * * * python transform.py  # Et si extract prend plus de 30min ?\n",
    "0 3 * * * python load.py        # Et si transform a plantÃ© ?\n",
    "\n",
    "# âœ… Avec Airflow (solution) :\n",
    "extract >> transform >> load  # transform ATTEND que extract soit terminÃ© !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ—ï¸ Architecture d'Airflow\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        ARCHITECTURE AIRFLOW                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\n",
    "â”‚   â”‚   WEB SERVER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    SCHEDULER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  EXECUTOR   â”‚  â”‚\n",
    "â”‚   â”‚   (Flask UI)    â”‚         â”‚  (Orchestrate)  â”‚         â”‚  (Workers)  â”‚  â”‚\n",
    "â”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\n",
    "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚            â”‚                           â”‚                         â”‚         â”‚\n",
    "â”‚            â”‚                           â–¼                         â”‚         â”‚\n",
    "â”‚            â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚         â”‚\n",
    "â”‚            â”‚                  â”‚                 â”‚                â”‚         â”‚\n",
    "â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    METADATA     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
    "â”‚                               â”‚    DATABASE     â”‚                          â”‚\n",
    "â”‚                               â”‚  (PostgreSQL)   â”‚                          â”‚\n",
    "â”‚                               â”‚                 â”‚                          â”‚\n",
    "â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚                                        â–²                                   â”‚\n",
    "â”‚                                        â”‚                                   â”‚\n",
    "â”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\n",
    "â”‚                               â”‚                 â”‚                          â”‚\n",
    "â”‚                               â”‚    DAG FILES    â”‚                          â”‚\n",
    "â”‚                               â”‚   (Python .py)  â”‚                          â”‚\n",
    "â”‚                               â”‚                 â”‚                          â”‚\n",
    "â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Composants clÃ©s\n",
    "\n",
    "| Composant | RÃ´le | Description |\n",
    "|-----------|------|-------------|\n",
    "| **Web Server** | Interface UI | Dashboard Flask pour visualiser les DAGs |\n",
    "| **Scheduler** | Planification | DÃ©cide quand exÃ©cuter les tÃ¢ches |\n",
    "| **Executor** | ExÃ©cution | Lance les tÃ¢ches (Local, Celery, K8s...) |\n",
    "| **Metadata DB** | Stockage | Ã‰tat des DAGs, logs, historique |\n",
    "| **DAG Files** | DÃ©finition | Fichiers Python dÃ©finissant les workflows |\n",
    "\n",
    "### Types d'Executors\n",
    "\n",
    "| Executor | Usage | ScalabilitÃ© |\n",
    "|----------|-------|-------------|\n",
    "| **SequentialExecutor** | Dev/Test | 1 tÃ¢che Ã  la fois |\n",
    "| **LocalExecutor** | Petite prod | ParallÃ¨le sur 1 machine |\n",
    "| **CeleryExecutor** | Production | Workers distribuÃ©s |\n",
    "| **KubernetesExecutor** | Cloud | Pod par tÃ¢che |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Concepts clÃ©s d'Airflow\n",
    "\n",
    "### 1ï¸âƒ£ DAG (Directed Acyclic Graph)\n",
    "\n",
    "Un **DAG** est un graphe de tÃ¢ches **sans cycle** : les donnÃ©es vont toujours dans une direction.\n",
    "\n",
    "```\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚ Extract â”‚\n",
    "     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "          â–¼\n",
    "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚Transformâ”‚\n",
    "     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
    "          â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
    "    â–¼           â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Load DWâ”‚  â”‚Load S3 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 2ï¸âƒ£ Task\n",
    "\n",
    "Une **Task** est une unitÃ© de travail dans un DAG (une Ã©tape).\n",
    "\n",
    "### 3ï¸âƒ£ Operator\n",
    "\n",
    "Un **Operator** dÃ©finit **ce que fait** une tÃ¢che.\n",
    "\n",
    "| Operator | Usage |\n",
    "|----------|-------|\n",
    "| `PythonOperator` | ExÃ©cuter une fonction Python |\n",
    "| `BashOperator` | ExÃ©cuter une commande bash |\n",
    "| `EmailOperator` | Envoyer un email |\n",
    "| `PostgresOperator` | ExÃ©cuter du SQL |\n",
    "| `S3ToRedshiftOperator` | Copier S3 â†’ Redshift |\n",
    "\n",
    "### 4ï¸âƒ£ Task Instance\n",
    "\n",
    "Une **Task Instance** = une exÃ©cution spÃ©cifique d'une Task Ã  une date donnÃ©e.\n",
    "\n",
    "### 5ï¸âƒ£ DAG Run\n",
    "\n",
    "Un **DAG Run** = une exÃ©cution complÃ¨te du DAG.\n",
    "\n",
    "```\n",
    "DAG: etl_pipeline\n",
    "â”œâ”€â”€ DAG Run 2024-01-15 âœ…\n",
    "â”‚   â”œâ”€â”€ extract (Task Instance) âœ…\n",
    "â”‚   â”œâ”€â”€ transform (Task Instance) âœ…\n",
    "â”‚   â””â”€â”€ load (Task Instance) âœ…\n",
    "â”‚\n",
    "â”œâ”€â”€ DAG Run 2024-01-16 â³\n",
    "â”‚   â”œâ”€â”€ extract (Task Instance) âœ…\n",
    "â”‚   â”œâ”€â”€ transform (Task Instance) â³ running\n",
    "â”‚   â””â”€â”€ load (Task Instance) â¸ï¸ waiting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Installation locale d'Airflow\n",
    "\n",
    "```bash\n",
    "# CrÃ©er un environnement virtuel\n",
    "python -m venv airflow_venv\n",
    "source airflow_venv/bin/activate  # Linux/Mac\n",
    "# ou : airflow_venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# DÃ©finir le home Airflow\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# Installer Airflow (version contrainte pour compatibilitÃ©)\n",
    "AIRFLOW_VERSION=2.8.1\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "\n",
    "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "\n",
    "# Initialiser la base de donnÃ©es\n",
    "airflow db init\n",
    "\n",
    "# CrÃ©er un utilisateur admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com \\\n",
    "    --password admin\n",
    "\n",
    "# Lancer le webserver (Terminal 1)\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Lancer le scheduler (Terminal 2)\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "ğŸ‘‰ AccÃ©der : **http://localhost:8080** (login: admin / admin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Ton premier DAG\n",
    "\n",
    "CrÃ©er le fichier `~/airflow/dags/mon_premier_dag.py` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/airflow/dags/mon_premier_dag.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Arguments par dÃ©faut pour toutes les tÃ¢ches\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['data-team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# DÃ©finir le DAG\n",
    "dag = DAG(\n",
    "    dag_id='mon_premier_dag',\n",
    "    default_args=default_args,\n",
    "    description='Mon premier pipeline ETL',\n",
    "    schedule_interval='@daily',  # ExÃ©cution quotidienne\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,  # Ne pas exÃ©cuter les runs passÃ©s\n",
    "    tags=['etl', 'tutorial'],\n",
    ")\n",
    "\n",
    "# Fonctions Python\n",
    "def extract():\n",
    "    \"\"\"Extraire les donnÃ©es\"\"\"\n",
    "    print(\"ğŸ“¥ Extraction des donnÃ©es depuis l'API...\")\n",
    "    # Simuler extraction\n",
    "    data = {'records': 1000, 'source': 'api'}\n",
    "    return data  # RetournÃ© via XCom\n",
    "\n",
    "def transform(**context):\n",
    "    \"\"\"Transformer les donnÃ©es\"\"\"\n",
    "    # RÃ©cupÃ©rer les donnÃ©es de extract via XCom\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract')\n",
    "    print(f\"ğŸ”„ Transformation de {data['records']} enregistrements\")\n",
    "    return {'records': data['records'], 'cleaned': True}\n",
    "\n",
    "def load(**context):\n",
    "    \"\"\"Charger les donnÃ©es\"\"\"\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='transform')\n",
    "    print(f\"ğŸ’¾ Chargement de {data['records']} enregistrements nettoyÃ©s\")\n",
    "\n",
    "# CrÃ©er les tÃ¢ches\n",
    "task_start = BashOperator(\n",
    "    task_id='start',\n",
    "    bash_command='echo \"ğŸš€ DÃ©marrage du pipeline Ã  $(date)\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_extract = PythonOperator(\n",
    "    task_id='extract',\n",
    "    python_callable=extract,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_transform = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=transform,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_load = PythonOperator(\n",
    "    task_id='load',\n",
    "    python_callable=load,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_end = BashOperator(\n",
    "    task_id='end',\n",
    "    bash_command='echo \"âœ… Pipeline terminÃ© avec succÃ¨s !\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# DÃ©finir les dÃ©pendances\n",
    "task_start >> task_extract >> task_transform >> task_load >> task_end\n",
    "\n",
    "# Ã‰quivalent Ã  :\n",
    "# task_start.set_downstream(task_extract)\n",
    "# task_extract.set_downstream(task_transform)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš™ï¸ ParamÃ¨tres importants du DAG\n",
    "\n",
    "### Schedule Interval (frÃ©quence d'exÃ©cution)\n",
    "\n",
    "| Preset | Ã‰quivalent Cron | Description |\n",
    "|--------|-----------------|-------------|\n",
    "| `@once` | - | Une seule fois |\n",
    "| `@hourly` | `0 * * * *` | Chaque heure |\n",
    "| `@daily` | `0 0 * * *` | Chaque jour Ã  minuit |\n",
    "| `@weekly` | `0 0 * * 0` | Chaque dimanche |\n",
    "| `@monthly` | `0 0 1 * *` | Le 1er du mois |\n",
    "| `@yearly` | `0 0 1 1 *` | Le 1er janvier |\n",
    "| `None` | - | DÃ©clenchÃ© manuellement |\n",
    "| `'0 6 * * 1-5'` | Cron | Lun-Ven Ã  6h |\n",
    "\n",
    "### Catchup\n",
    "\n",
    "```python\n",
    "# catchup=True (dÃ©faut) :\n",
    "# Si start_date=2024-01-01 et on est le 2024-01-10,\n",
    "# Airflow va exÃ©cuter les 10 DAG Runs manquÃ©s !\n",
    "\n",
    "# catchup=False :\n",
    "# ExÃ©cute seulement Ã  partir de maintenant\n",
    "```\n",
    "\n",
    "### Default Args importants\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'data_team',           # PropriÃ©taire\n",
    "    'depends_on_past': False,       # DÃ©pend du run prÃ©cÃ©dent ?\n",
    "    'email_on_failure': True,       # Email si Ã©chec\n",
    "    'retries': 3,                   # Nombre de retry\n",
    "    'retry_delay': timedelta(minutes=5),  # DÃ©lai entre retries\n",
    "    'execution_timeout': timedelta(hours=1),  # Timeout\n",
    "    'sla': timedelta(hours=2),      # SLA (alerte si dÃ©passÃ©)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”§ Les Operators les plus utilisÃ©s\n",
    "\n",
    "### PythonOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def my_function(name, **context):\n",
    "    print(f\"Hello {name}!\")\n",
    "    print(f\"Execution date: {context['ds']}\")\n",
    "    return \"success\"\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='python_task',\n",
    "    python_callable=my_function,\n",
    "    op_kwargs={'name': 'World'},  # Arguments de la fonction\n",
    ")\n",
    "```\n",
    "\n",
    "### BashOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "task = BashOperator(\n",
    "    task_id='bash_task',\n",
    "    bash_command='echo \"Date: {{ ds }}\" && python /scripts/etl.py',\n",
    ")\n",
    "```\n",
    "\n",
    "### PostgresOperator\n",
    "\n",
    "```python\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "task = PostgresOperator(\n",
    "    task_id='create_table',\n",
    "    postgres_conn_id='my_postgres',  # Connexion dÃ©finie dans UI\n",
    "    sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS users (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR(100)\n",
    "        );\n",
    "    \"\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "### EmailOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "task = EmailOperator(\n",
    "    task_id='send_report',\n",
    "    to='team@company.com',\n",
    "    subject='Pipeline Report - {{ ds }}',\n",
    "    html_content='<h1>Pipeline completed!</h1>',\n",
    ")\n",
    "```\n",
    "\n",
    "### EmptyOperator (anciennement DummyOperator)\n",
    "\n",
    "```python\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Utile pour crÃ©er des points de jonction\n",
    "start = EmptyOperator(task_id='start')\n",
    "end = EmptyOperator(task_id='end')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“¬ XCom â€” Passer des donnÃ©es entre tÃ¢ches\n",
    "\n",
    "**XCom** (Cross-Communication) permet de partager des donnÃ©es entre tÃ¢ches.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         XCom          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Task A  â”‚ â”€â”€â”€â”€â”€â”€â”€ data â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Task B  â”‚\n",
    "â”‚  return  â”‚                       â”‚ xcom_pull â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### MÃ©thode 1 : Return (automatique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XCom avec return (automatique)\n",
    "\n",
    "def extract():\n",
    "    data = {'records': 1000, 'status': 'ok'}\n",
    "    return data  # Automatiquement stockÃ© dans XCom\n",
    "\n",
    "def transform(**context):\n",
    "    # RÃ©cupÃ©rer via ti (task instance)\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract')\n",
    "    print(f\"ReÃ§u: {data}\")\n",
    "    return {'records': data['records'], 'transformed': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MÃ©thode 2 : Push/Pull explicite\n",
    "\n",
    "```python\n",
    "def task_a(**context):\n",
    "    # Push explicite avec une clÃ©\n",
    "    context['ti'].xcom_push(key='my_data', value={'count': 42})\n",
    "    context['ti'].xcom_push(key='status', value='success')\n",
    "\n",
    "def task_b(**context):\n",
    "    # Pull avec la clÃ©\n",
    "    data = context['ti'].xcom_pull(task_ids='task_a', key='my_data')\n",
    "    status = context['ti'].xcom_pull(task_ids='task_a', key='status')\n",
    "```\n",
    "\n",
    "### âš ï¸ Limites de XCom\n",
    "\n",
    "| Limite | Description |\n",
    "|--------|-------------|\n",
    "| **Taille** | Max ~48KB par dÃ©faut (stockÃ© en DB) |\n",
    "| **SÃ©rialisation** | Doit Ãªtre JSON-sÃ©rialisable |\n",
    "| **Pas pour big data** | Utiliser S3/GCS pour gros fichiers |\n",
    "\n",
    "```python\n",
    "# âŒ Mauvaise pratique\n",
    "def extract():\n",
    "    df = pd.read_csv('big_file.csv')  # 1GB\n",
    "    return df.to_dict()  # âŒ Trop gros pour XCom !\n",
    "\n",
    "# âœ… Bonne pratique\n",
    "def extract():\n",
    "    df = pd.read_csv('big_file.csv')\n",
    "    path = '/data/output/extract_2024-01-15.parquet'\n",
    "    df.to_parquet(path)\n",
    "    return path  # âœ… Passer le chemin, pas les donnÃ©es\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ TaskFlow API (Airflow 2.0+)\n",
    "\n",
    "Syntaxe moderne et plus **Pythonic** avec des dÃ©corateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaskFlow API - Syntaxe moderne (Airflow 2.0+)\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='etl_taskflow',\n",
    "    schedule='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['etl', 'taskflow'],\n",
    ")\n",
    "def etl_pipeline():\n",
    "    \n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\"Extraire les donnÃ©es\"\"\"\n",
    "        return {'records': 1000, 'source': 'api'}\n",
    "    \n",
    "    @task()\n",
    "    def transform(data: dict):\n",
    "        \"\"\"Transformer les donnÃ©es\"\"\"\n",
    "        return {\n",
    "            'records': data['records'],\n",
    "            'cleaned': True\n",
    "        }\n",
    "    \n",
    "    @task()\n",
    "    def load(data: dict):\n",
    "        \"\"\"Charger les donnÃ©es\"\"\"\n",
    "        print(f\"Loading {data['records']} records\")\n",
    "    \n",
    "    # DÃ©finir le flow - XCom automatique !\n",
    "    raw_data = extract()\n",
    "    clean_data = transform(raw_data)\n",
    "    load(clean_data)\n",
    "\n",
    "# Instancier le DAG\n",
    "etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages TaskFlow API\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **XCom automatique** | Les retours sont passÃ©s automatiquement |\n",
    "| **Code plus lisible** | Ressemble Ã  du Python normal |\n",
    "| **Type hints** | Support des annotations de type |\n",
    "| **Moins de boilerplate** | Pas besoin de crÃ©er des Operators |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‘ï¸ Sensors â€” Attendre une condition\n",
    "\n",
    "Les **Sensors** attendent qu'une condition soit remplie avant de continuer.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                              â”‚\n",
    "â”‚   FileSensor          S3KeySensor         HttpSensor         â”‚\n",
    "â”‚   \"Fichier existe?\"   \"Fichier sur S3?\"   \"API disponible?\"  â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚        â³                   â³                   â³            â”‚\n",
    "â”‚        â”‚                    â”‚                    â”‚           â”‚\n",
    "â”‚        â–¼                    â–¼                    â–¼           â”‚\n",
    "â”‚       âœ…                   âœ…                   âœ…           â”‚\n",
    "â”‚   Continuer            Continuer            Continuer        â”‚\n",
    "â”‚                                                              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### FileSensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileSensor - Attendre qu'un fichier existe\n",
    "\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "wait_for_file = FileSensor(\n",
    "    task_id='wait_for_file',\n",
    "    filepath='/data/input/daily_export.csv',\n",
    "    poke_interval=60,      # VÃ©rifier toutes les 60 secondes\n",
    "    timeout=3600,          # Timeout aprÃ¨s 1 heure\n",
    "    mode='poke',           # poke ou reschedule\n",
    ")\n",
    "\n",
    "# Le pipeline attend le fichier avant de continuer\n",
    "wait_for_file >> process_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres Sensors utiles\n",
    "\n",
    "```python\n",
    "# S3KeySensor - Attendre un fichier sur S3\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "\n",
    "wait_s3 = S3KeySensor(\n",
    "    task_id='wait_for_s3',\n",
    "    bucket_name='my-bucket',\n",
    "    bucket_key='data/{{ ds }}/export.csv',\n",
    "    aws_conn_id='aws_default',\n",
    ")\n",
    "\n",
    "# HttpSensor - Attendre qu'une API rÃ©ponde\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "\n",
    "wait_api = HttpSensor(\n",
    "    task_id='wait_for_api',\n",
    "    http_conn_id='api_connection',\n",
    "    endpoint='health',\n",
    "    response_check=lambda response: response.status_code == 200,\n",
    ")\n",
    "\n",
    "# ExternalTaskSensor - Attendre un autre DAG\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "\n",
    "wait_other_dag = ExternalTaskSensor(\n",
    "    task_id='wait_upstream',\n",
    "    external_dag_id='upstream_dag',\n",
    "    external_task_id='final_task',\n",
    ")\n",
    "```\n",
    "\n",
    "### Mode poke vs reschedule\n",
    "\n",
    "| Mode | Description | Ressources |\n",
    "|------|-------------|------------|\n",
    "| `poke` | Garde un worker slot | Consomme plus |\n",
    "| `reschedule` | LibÃ¨re le slot entre checks | RecommandÃ© |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”€ Branching â€” Logique conditionnelle\n",
    "\n",
    "ExÃ©cuter diffÃ©rentes tÃ¢ches selon une condition.\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Check   â”‚\n",
    "                    â”‚ Conditionâ”‚\n",
    "                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                         â”‚\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚          â”‚          â”‚\n",
    "              â–¼          â–¼          â–¼\n",
    "         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "         â”‚ Path A â”‚ â”‚ Path B â”‚ â”‚ Path C â”‚\n",
    "         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching - ExÃ©cution conditionnelle\n",
    "\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "def choose_branch(**context):\n",
    "    \"\"\"DÃ©cider quelle branche exÃ©cuter\"\"\"\n",
    "    # Exemple : vÃ©rifier le jour de la semaine\n",
    "    day = context['ds_nodash']  # Format YYYYMMDD\n",
    "    \n",
    "    # Logique mÃ©tier\n",
    "    if int(day) % 2 == 0:\n",
    "        return 'process_even'  # Retourner le task_id Ã  exÃ©cuter\n",
    "    else:\n",
    "        return 'process_odd'\n",
    "\n",
    "branch = BranchPythonOperator(\n",
    "    task_id='branch',\n",
    "    python_callable=choose_branch,\n",
    ")\n",
    "\n",
    "process_even = EmptyOperator(task_id='process_even')\n",
    "process_odd = EmptyOperator(task_id='process_odd')\n",
    "end = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')\n",
    "\n",
    "# DÃ©finir le flow\n",
    "branch >> [process_even, process_odd] >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸšï¸ Trigger Rules\n",
    "\n",
    "ContrÃ´ler **quand** une tÃ¢che s'exÃ©cute en fonction du statut des tÃ¢ches parentes.\n",
    "\n",
    "| Trigger Rule | ExÃ©cute si... |\n",
    "|--------------|---------------|\n",
    "| `all_success` | Tous les parents ont rÃ©ussi (dÃ©faut) |\n",
    "| `all_failed` | Tous les parents ont Ã©chouÃ© |\n",
    "| `all_done` | Tous les parents sont terminÃ©s (peu importe le statut) |\n",
    "| `one_success` | Au moins un parent a rÃ©ussi |\n",
    "| `one_failed` | Au moins un parent a Ã©chouÃ© |\n",
    "| `none_failed` | Aucun parent n'a Ã©chouÃ© (succÃ¨s ou skipped) |\n",
    "| `none_skipped` | Aucun parent n'a Ã©tÃ© skipped |\n",
    "\n",
    "```python\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "# TÃ¢che de notification en cas d'Ã©chec\n",
    "notify_failure = EmailOperator(\n",
    "    task_id='notify_failure',\n",
    "    to='team@company.com',\n",
    "    subject='Pipeline Failed!',\n",
    "    html_content='...',\n",
    "    trigger_rule=TriggerRule.ONE_FAILED,  # ExÃ©cute si un parent Ã©choue\n",
    ")\n",
    "\n",
    "# TÃ¢che finale qui s'exÃ©cute toujours\n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup',\n",
    "    bash_command='rm -rf /tmp/data/*',\n",
    "    trigger_rule=TriggerRule.ALL_DONE,  # Toujours exÃ©cuter\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ” Connections et Variables\n",
    "\n",
    "### Connections\n",
    "\n",
    "Stocker les informations de connexion aux systÃ¨mes externes.\n",
    "\n",
    "**Dans l'UI** : Admin â†’ Connections â†’ +\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Add Connection                                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Connection Id:   â”‚ my_postgres                 â”‚           â”‚\n",
    "â”‚  Connection Type: â”‚ Postgres           â–¼        â”‚           â”‚\n",
    "â”‚  Host:            â”‚ localhost                   â”‚           â”‚\n",
    "â”‚  Schema:          â”‚ mydb                        â”‚           â”‚\n",
    "â”‚  Login:           â”‚ admin                       â”‚           â”‚\n",
    "â”‚  Password:        â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                    â”‚           â”‚\n",
    "â”‚  Port:            â”‚ 5432                        â”‚           â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚                              [ Save ]                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Utilisation dans le code** :\n",
    "\n",
    "```python\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "def query_postgres():\n",
    "    hook = PostgresHook(postgres_conn_id='my_postgres')\n",
    "    df = hook.get_pandas_df('SELECT * FROM users')\n",
    "    return df\n",
    "```\n",
    "\n",
    "### Variables\n",
    "\n",
    "Stocker des configurations rÃ©utilisables.\n",
    "\n",
    "**Dans l'UI** : Admin â†’ Variables â†’ +\n",
    "\n",
    "```python\n",
    "from airflow.models import Variable\n",
    "\n",
    "# RÃ©cupÃ©rer une variable\n",
    "api_key = Variable.get('API_KEY')\n",
    "\n",
    "# Variable JSON\n",
    "config = Variable.get('pipeline_config', deserialize_json=True)\n",
    "# config = {'batch_size': 1000, 'env': 'prod'}\n",
    "\n",
    "# Dans un template Jinja\n",
    "# {{ var.value.API_KEY }}\n",
    "# {{ var.json.pipeline_config.batch_size }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Monitoring et Alertes\n",
    "\n",
    "### Interface Web\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Airflow - DAGs                                                             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  DAG                    Schedule    Owner    Runs   Recent Tasks            â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€    â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\n",
    "â”‚  â–¶ etl_pipeline         @daily      team     125    âœ…âœ…âœ…âœ…âœ…               â”‚\n",
    "â”‚  â–¶ data_quality_check   @hourly     team     560    âœ…âœ…âœ…âŒâœ…               â”‚\n",
    "â”‚  â–¶ weekly_report        @weekly     team     52     âœ…âœ…âœ…âœ…âœ…               â”‚\n",
    "â”‚  â¸ maintenance          None        admin    3      âœ…âœ…âœ…                   â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â”‚  âœ… Success  âŒ Failed  â³ Running  â¸ Paused                                â”‚\n",
    "â”‚                                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Vues disponibles\n",
    "\n",
    "| Vue | Description |\n",
    "|-----|-------------|\n",
    "| **Grid** | Vue matricielle des runs |\n",
    "| **Graph** | Graphe du DAG |\n",
    "| **Calendar** | Historique par date |\n",
    "| **Gantt** | Timeline d'exÃ©cution |\n",
    "| **Code** | Code source du DAG |\n",
    "\n",
    "### Configurer les alertes email\n",
    "\n",
    "```python\n",
    "# airflow.cfg\n",
    "[smtp]\n",
    "smtp_host = smtp.gmail.com\n",
    "smtp_port = 587\n",
    "smtp_user = airflow@company.com\n",
    "smtp_password = your_password\n",
    "smtp_mail_from = airflow@company.com\n",
    "\n",
    "# Dans le DAG\n",
    "default_args = {\n",
    "    'email': ['team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "```\n",
    "\n",
    "### Alertes Slack\n",
    "\n",
    "```python\n",
    "from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\n",
    "\n",
    "def alert_slack_on_failure(context):\n",
    "    \"\"\"Callback en cas d'Ã©chec\"\"\"\n",
    "    slack_msg = f\"\"\"\n",
    "        :red_circle: Task Failed!\n",
    "        *DAG*: {context['dag'].dag_id}\n",
    "        *Task*: {context['task'].task_id}\n",
    "        *Execution Time*: {context['execution_date']}\n",
    "    \"\"\"\n",
    "    return SlackWebhookOperator(\n",
    "        task_id='slack_alert',\n",
    "        slack_webhook_conn_id='slack_webhook',\n",
    "        message=slack_msg,\n",
    "    ).execute(context)\n",
    "\n",
    "# Utiliser le callback\n",
    "default_args = {\n",
    "    'on_failure_callback': alert_slack_on_failure,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Bonnes pratiques Airflow\n",
    "\n",
    "### 1. Structure des DAGs\n",
    "\n",
    "```\n",
    "airflow/\n",
    "â”œâ”€â”€ dags/\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ etl/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â”œâ”€â”€ daily_etl.py\n",
    "â”‚   â”‚   â””â”€â”€ weekly_report.py\n",
    "â”‚   â”œâ”€â”€ utils/\n",
    "â”‚   â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”‚   â””â”€â”€ helpers.py\n",
    "â”‚   â””â”€â”€ config/\n",
    "â”‚       â””â”€â”€ settings.py\n",
    "â”œâ”€â”€ plugins/\n",
    "â”‚   â””â”€â”€ custom_operators/\n",
    "â””â”€â”€ tests/\n",
    "    â””â”€â”€ test_dags.py\n",
    "```\n",
    "\n",
    "### 2. Idempotence\n",
    "\n",
    "```python\n",
    "# âŒ Non idempotent - accumule des donnÃ©es\n",
    "def bad_load():\n",
    "    db.execute(\"INSERT INTO table VALUES (...)\")\n",
    "\n",
    "# âœ… Idempotent - mÃªme rÃ©sultat si relancÃ©\n",
    "def good_load():\n",
    "    db.execute(\"DELETE FROM table WHERE date = '{{ ds }}'\")\n",
    "    db.execute(\"INSERT INTO table SELECT ... WHERE date = '{{ ds }}'\")\n",
    "```\n",
    "\n",
    "### 3. AtomicitÃ© des tÃ¢ches\n",
    "\n",
    "```python\n",
    "# âŒ TÃ¢che trop grosse\n",
    "def do_everything():\n",
    "    extract()\n",
    "    transform()\n",
    "    load()\n",
    "\n",
    "# âœ… TÃ¢ches atomiques\n",
    "extract >> transform >> load\n",
    "```\n",
    "\n",
    "### 4. Ne pas mettre de logique dans le DAG\n",
    "\n",
    "```python\n",
    "# âŒ Code exÃ©cutÃ© Ã  chaque parsing\n",
    "data = fetch_from_api()  # AppelÃ© toutes les 30s !\n",
    "\n",
    "# âœ… Logique dans les tasks\n",
    "@task\n",
    "def fetch_data():\n",
    "    return fetch_from_api()\n",
    "```\n",
    "\n",
    "### 5. Tester les DAGs\n",
    "\n",
    "```python\n",
    "# tests/test_dags.py\n",
    "import pytest\n",
    "from airflow.models import DagBag\n",
    "\n",
    "def test_dag_loaded():\n",
    "    dag_bag = DagBag()\n",
    "    assert len(dag_bag.import_errors) == 0\n",
    "\n",
    "def test_dag_structure():\n",
    "    dag_bag = DagBag()\n",
    "    dag = dag_bag.get_dag('etl_pipeline')\n",
    "    assert dag is not None\n",
    "    assert len(dag.tasks) == 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Forces d'Airflow\n",
    "\n",
    "âœ… **Gestion des dÃ©pendances** - `A >> B` = B attend A  \n",
    "âœ… **Retry automatique** - RÃ©essaie en cas d'Ã©chec  \n",
    "âœ… **Interface web** - Visualisation complÃ¨te  \n",
    "âœ… **Monitoring** - Logs centralisÃ©s  \n",
    "âœ… **Alertes** - Email/Slack en cas d'Ã©chec  \n",
    "âœ… **Scalable** - GÃ¨re 100+ pipelines  \n",
    "âœ… **Extensible** - Custom operators, hooks  \n",
    "âœ… **CommunautÃ©** - TrÃ¨s active, beaucoup de providers  \n",
    "\n",
    "## âŒ Faiblesses d'Airflow\n",
    "\n",
    "âŒ **Complexe** - Courbe d'apprentissage  \n",
    "âŒ **Ressources** - Besoin de 4-8 GB RAM  \n",
    "âŒ **Overkill** - Pour 1-3 scripts simples  \n",
    "âŒ **Setup** - Installation et configuration  \n",
    "âŒ **Pas pour le streaming** - Batch only (utiliser Kafka)  \n",
    "\n",
    "## Quand utiliser Airflow ?\n",
    "\n",
    "âœ… **OUI** : 10+ pipelines, dÃ©pendances complexes, production  \n",
    "âŒ **NON** : 1-5 scripts simples sans dÃ©pendances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“Š Tableau de dÃ©cision\n",
    "\n",
    "## Quel outil choisir ?\n",
    "\n",
    "| Situation | Outil recommandÃ© |\n",
    "|-----------|------------------|\n",
    "| J'ai 1-3 scripts sur Windows | ğŸªŸ **Task Scheduler** |\n",
    "| J'ai 1-3 scripts sur Linux | ğŸ§ **Crontab** |\n",
    "| J'ai 5-10 scripts indÃ©pendants | ğŸ§ **Crontab** |\n",
    "| J'ai 10+ scripts avec dÃ©pendances | ğŸŒ¬ï¸ **Airflow** |\n",
    "| Script B doit attendre script A | ğŸŒ¬ï¸ **Airflow** |\n",
    "| Je veux un dashboard | ğŸŒ¬ï¸ **Airflow** |\n",
    "| Je dÃ©bute en automatisation | ğŸªŸ **Task Scheduler** |\n",
    "| Production critique | ğŸŒ¬ï¸ **Airflow** |\n",
    "\n",
    "## Progression naturelle\n",
    "\n",
    "```\n",
    "1. DÃ©butez avec Task Scheduler ou Cron\n",
    "2. Quand vous avez 5+ scripts â†’ Pensez Ã  migrer\n",
    "3. Quand vous avez des dÃ©pendances â†’ Migrez vers Airflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ RÃ©sumÃ©\n",
    "\n",
    "## Points clÃ©s\n",
    "\n",
    "### ğŸªŸ Windows Task Scheduler\n",
    "- **Pour qui** : DÃ©butants sur Windows\n",
    "- **Force** : TrÃ¨s facile (GUI)\n",
    "- **Faiblesse** : Pas de dÃ©pendances\n",
    "- **Limite** : 5 scripts max\n",
    "\n",
    "### ğŸ§ Crontab\n",
    "- **Pour qui** : Utilisateurs Linux\n",
    "- **Force** : Universel, lÃ©ger\n",
    "- **Faiblesse** : Pas de monitoring\n",
    "- **Limite** : 15 scripts max\n",
    "\n",
    "### ğŸŒ¬ï¸ Airflow\n",
    "- **Pour qui** : Production, Ã©quipes data\n",
    "- **Force** : DÃ©pendances, monitoring, scalable\n",
    "- **Faiblesse** : Complexe, ressources\n",
    "- **Limite** : Aucune (scalable)\n",
    "\n",
    "### Concepts Airflow Ã  retenir\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **DAG** | Graphe de tÃ¢ches (workflow) |\n",
    "| **Task** | UnitÃ© de travail |\n",
    "| **Operator** | Type de tÃ¢che (Python, Bash, SQL...) |\n",
    "| **XCom** | Passage de donnÃ©es entre tÃ¢ches |\n",
    "| **Sensor** | Attendre une condition |\n",
    "| **Connection** | Credentials stockÃ©s |\n",
    "| **Variable** | Configuration stockÃ©e |\n",
    "\n",
    "## Ressources\n",
    "\n",
    "- **Crontab** : [crontab.guru](https://crontab.guru/) (tester expressions)\n",
    "- **Airflow** : [airflow.apache.org](https://airflow.apache.org/)\n",
    "- **Airflow Tutorial** : [Documentation officielle](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ§ª Quiz Final\n",
    "\n",
    "### â“ Q1. Vous avez 2 scripts Python Ã  exÃ©cuter tous les jours sur Windows. Quel outil ?\n",
    "a) Airflow  \n",
    "b) Crontab  \n",
    "c) Windows Task Scheduler  \n",
    "d) Kubernetes\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : c** â€“ Pour 2 scripts simples sur Windows, utilisez le **Planificateur de tÃ¢ches** : facile, natif, gratuit.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q2. Expression crontab pour \"tous les lundis Ã  9h\" ?\n",
    "a) `0 9 * * 1`  \n",
    "b) `9 0 * * 1`  \n",
    "c) `* 9 * * 1`  \n",
    "d) `0 9 1 * *`\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : a** â€“ Format : `minute heure jour mois jour_semaine`. `0 9 * * 1` = 0 minutes, 9h, lundi (1).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q3. Que signifie DAG dans Airflow ?\n",
    "a) Data Analysis Graph  \n",
    "b) Directed Acyclic Graph  \n",
    "c) Dynamic Airflow Generator  \n",
    "d) Database Access Gateway\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ **Directed Acyclic Graph** = graphe orientÃ© sans cycle. Les tÃ¢ches vont toujours dans une direction.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q4. Dans Airflow, que signifie `task_a >> task_b` ?\n",
    "a) task_a et task_b en parallÃ¨le  \n",
    "b) task_b attend que task_a finisse  \n",
    "c) task_a attend task_b  \n",
    "d) Erreur de syntaxe\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ `A >> B` signifie \"B **attend** que A soit terminÃ©\". C'est la gestion des dÃ©pendances !\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q5. Quel composant Airflow dÃ©cide quand exÃ©cuter les tÃ¢ches ?\n",
    "a) Web Server  \n",
    "b) Executor  \n",
    "c) Scheduler  \n",
    "d) Metadata DB\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : c** â€“ Le **Scheduler** analyse les DAGs et dÃ©cide quand lancer les tÃ¢ches.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q6. Comment passer des donnÃ©es entre tÃ¢ches Airflow ?\n",
    "a) Variables globales  \n",
    "b) XCom  \n",
    "c) Fichiers partagÃ©s uniquement  \n",
    "d) Pas possible\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ **XCom** (Cross-Communication) permet de passer des donnÃ©es entre tÃ¢ches via `xcom_push` et `xcom_pull`.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q7. Qu'est-ce qu'un Sensor dans Airflow ?\n",
    "a) Un outil de monitoring  \n",
    "b) Un Operator qui attend une condition  \n",
    "c) Un type de DAG  \n",
    "d) Un systÃ¨me d'alerte\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ Un **Sensor** est un Operator spÃ©cial qui attend qu'une condition soit remplie (fichier existe, API disponible...).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q8. Quand NE PAS utiliser Airflow ?\n",
    "a) 10+ pipelines complexes  \n",
    "b) 1-3 scripts simples  \n",
    "c) Production critique  \n",
    "d) Besoin de monitoring\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ Pour **1-3 scripts simples**, Airflow est **overkill**. Utilisez Cron ou Task Scheduler Ã  la place.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q9. Quel est l'avantage de la TaskFlow API ?\n",
    "a) Plus performant  \n",
    "b) XCom automatique et code plus lisible  \n",
    "c) Compatible Python 2  \n",
    "d) Pas besoin de Scheduler\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : b** â€“ TaskFlow API (dÃ©corateurs `@dag`, `@task`) rend le code plus Pythonic avec XCom automatique.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### â“ Q10. Vous avez 15 scripts avec des dÃ©pendances. Quel outil ?\n",
    "a) Windows Task Scheduler  \n",
    "b) Crontab  \n",
    "c) Apache Airflow  \n",
    "d) Aucun, faire manuellement\n",
    "\n",
    "<details>\n",
    "<summary>ğŸ’¡ Voir la rÃ©ponse</summary>\n",
    "âœ… **RÃ©ponse : c** â€“ Avec **15 scripts** et des **dÃ©pendances**, il est temps de passer Ã  **Airflow** !\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Votre score\n",
    "\n",
    "- **10/10** : ğŸ† Expert orchestration !\n",
    "- **8-9/10** : ğŸŒŸ TrÃ¨s bien !\n",
    "- **6-7/10** : ğŸ’ª Bon dÃ©but !\n",
    "- **< 6/10** : ğŸ“š Relisez le notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Ressources\n",
    "\n",
    "### Outils\n",
    "- [Crontab Guru](https://crontab.guru/) â€” Tester et gÃ©nÃ©rer des expressions cron\n",
    "- [Apache Airflow](https://airflow.apache.org/) â€” Documentation officielle\n",
    "- [Airflow Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html)\n",
    "- [Astronomer](https://www.astronomer.io/) â€” Guides et bonnes pratiques\n",
    "\n",
    "### Alternatives Ã  Airflow\n",
    "\n",
    "| Outil | Description | Cas d'usage |\n",
    "|-------|-------------|-------------|\n",
    "| **Prefect** | Orchestration moderne, Pythonic | Alternative plus simple Ã  Airflow |\n",
    "| **Dagster** | Data orchestration avec types | Pipelines ML |\n",
    "| **Luigi** | Par Spotify, simple | Pipelines batch |\n",
    "| **Mage** | Low-code, moderne | Prototypage rapide |\n",
    "| **Kestra** | Event-driven, YAML | Workflows dÃ©claratifs |\n",
    "\n",
    "### Cloud managed\n",
    "\n",
    "| Cloud | Service |\n",
    "|-------|--------|\n",
    "| **AWS** | MWAA (Managed Airflow), Step Functions |\n",
    "| **GCP** | Cloud Composer (Managed Airflow) |\n",
    "| **Azure** | Data Factory, Synapse Pipelines |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Fin du niveau dÃ©butant !\n",
    "\n",
    "Tu as terminÃ© le parcours **Data Engineering - From Zero to Hero** niveau dÃ©butant ! ğŸ‰\n",
    "\n",
    "### ğŸ“‹ RÃ©capitulatif des modules\n",
    "\n",
    "| # | Module | CompÃ©tence acquise |\n",
    "|---|--------|--------------------|\n",
    "| 01 | Introduction | Vision du mÃ©tier |\n",
    "| 02 | Bash | Ligne de commande |\n",
    "| 03 | Git | Versioning |\n",
    "| 04 | Python Basics | Programmation |\n",
    "| 05 | Python Data Processing | Pandas, visualisation |\n",
    "| 06 | Intro Bases Relationnelles | Concepts relationnels |\n",
    "| 07 | SQL | RequÃªtes SQL |\n",
    "| 08 | Big Data & NoSQL | SystÃ¨mes distribuÃ©s |\n",
    "| 09 | MongoDB | Base NoSQL document |\n",
    "| 10 | Elasticsearch | Recherche et indexation |\n",
    "| 11 | PySpark | Traitement distribuÃ© |\n",
    "| **12** | **Orchestration** | **Airflow, pipelines** |\n",
    "\n",
    "### ğŸ Module BONUS disponible\n",
    "\n",
    "| # | Module | Description |\n",
    "|---|--------|-------------|\n",
    "| 13 | **BONUS FastAPI** | CrÃ©er des APIs REST pour exposer tes donnÃ©es |\n",
    "\n",
    "ğŸ‘‰ Parfait pour exposer les rÃ©sultats de tes pipelines via API !\n",
    "\n",
    "---\n",
    "\n",
    "## â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire\n",
    "\n",
    "Tu es maintenant prÃªt pour le **niveau intermÃ©diaire** qui couvrira :\n",
    "\n",
    "| Module | Description |\n",
    "|--------|-------------|\n",
    "| **Docker** | Conteneurisation des pipelines |\n",
    "| **Data Lakes** | Parquet, Delta Lake, Iceberg |\n",
    "| **Kafka** | Streaming en temps rÃ©el |\n",
    "| **dbt** | Transformation dans le warehouse |\n",
    "| **Data Quality** | Great Expectations, tests |\n",
    "| **Cloud** | AWS / GCP / Azure |\n",
    "| **CI/CD** | GitHub Actions, tests automatisÃ©s |\n",
    "| **Kibana** | Dashboards et monitoring |\n",
    "| **Projet intÃ©grateur** | Pipeline complet end-to-end |\n",
    "\n",
    "---\n",
    "\n",
    "ğŸš€ **Bravo ! Tu as maintenant toutes les bases pour construire des pipelines de donnÃ©es !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}