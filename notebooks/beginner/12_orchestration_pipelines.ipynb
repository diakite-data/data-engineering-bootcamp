{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚è∞ Orchestration de Pipelines Data\n",
    "\n",
    "Ce module pr√©sente les outils d'**orchestration** pour automatiser l'ex√©cution de vos pipelines de donn√©es.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "| Niveau | Comp√©tence |\n",
    "|--------|------------|\n",
    "| ‚úÖ Requis | Avoir suivi le module `11_pyspark_for_data_engineering` |\n",
    "| ‚úÖ Requis | Comprendre les pipelines ETL |\n",
    "| ‚úÖ Requis | Ma√Ætriser Python (modules 04-05) |\n",
    "| ‚úÖ Requis | Conna√Ætre les bases de Linux (ligne de commande) |\n",
    "\n",
    "## üéØ Objectifs du module\n",
    "\n",
    "√Ä la fin de ce notebook, tu seras capable de :\n",
    "\n",
    "- ‚úÖ Comprendre ce qu'est l'orchestration de pipelines\n",
    "- ‚úÖ Utiliser le Planificateur Windows (niveau d√©butant)\n",
    "- ‚úÖ Configurer des t√¢ches avec Crontab (niveau interm√©diaire)\n",
    "- ‚úÖ Comprendre l'**architecture** d'Apache Airflow\n",
    "- ‚úÖ Cr√©er des **DAGs** avec Apache Airflow\n",
    "- ‚úÖ Utiliser les diff√©rents types d'**Operators**\n",
    "- ‚úÖ G√©rer les **d√©pendances** et le **passage de donn√©es** (XCom)\n",
    "- ‚úÖ Configurer les **alertes** et le **monitoring**\n",
    "- ‚úÖ Choisir le bon outil selon ton besoin\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ L'orchestration dans l'√©cosyst√®me Data Engineering\n",
    "\n",
    "Tu as appris √† cr√©er des pipelines ETL avec PySpark. Mais comment les **automatiser** pour qu'ils s'ex√©cutent r√©guli√®rement sans intervention manuelle ?\n",
    "\n",
    "### Le probl√®me\n",
    "\n",
    "```\n",
    "Sans orchestration :\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ   üò∞ \"Il faut que je lance mon script tous les jours...\"   ‚îÇ\n",
    "‚îÇ   üò∞ \"J'ai oubli√© de lancer le pipeline hier !\"            ‚îÇ\n",
    "‚îÇ   üò∞ \"Le script B a plant√© car A n'avait pas fini...\"      ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "Avec orchestration :\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ   ‚úÖ Scripts ex√©cut√©s automatiquement                       ‚îÇ\n",
    "‚îÇ   ‚úÖ Alertes en cas d'√©chec                                 ‚îÇ\n",
    "‚îÇ   ‚úÖ D√©pendances respect√©es (A ‚Üí B ‚Üí C)                     ‚îÇ\n",
    "‚îÇ   ‚úÖ Logs et monitoring centralis√©s                         ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Position dans le pipeline Data\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     PIPELINE DATA                               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   Sources        ETL              Destination                   ‚îÇ\n",
    "‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ       ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                   ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   APIs     ‚îÄ‚îê                 ‚îå‚îÄ‚ñ∫  Data Warehouse               ‚îÇ\n",
    "‚îÇ   Fichiers ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫  PySpark  ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚ñ∫  Data Lake                    ‚îÇ\n",
    "‚îÇ   BDD      ‚îÄ‚îò                 ‚îî‚îÄ‚ñ∫  Dashboard                    ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ            ‚ñ≤                                                    ‚îÇ\n",
    "‚îÇ            ‚îÇ                                                    ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                           ‚îÇ\n",
    "‚îÇ   ‚îÇ  ORCHESTRATION  ‚îÇ  ‚óÑ‚îÄ‚îÄ Quand ? Dans quel ordre ?           ‚îÇ\n",
    "‚îÇ   ‚îÇ  (Airflow/Cron) ‚îÇ                                           ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                           ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Comparaison rapide des outils\n",
    "\n",
    "| Crit√®re | Windows Task | Crontab | Airflow |\n",
    "|---------|--------------|---------|----------|\n",
    "| **Facilit√©** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê |\n",
    "| **Interface** | GUI | CLI | Web |\n",
    "| **D√©pendances** | ‚ùå | ‚ùå | ‚úÖ |\n",
    "| **Monitoring** | Basique | Non | Complet |\n",
    "| **Retry auto** | ‚ùå | ‚ùå | ‚úÖ |\n",
    "| **Id√©al pour** | 1-5 scripts | 5-15 scripts | 10+ pipelines |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ü™ü Niveau 1 : Planificateur Windows\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Un outil **int√©gr√© √† Windows** pour ex√©cuter des programmes automatiquement.\n",
    "\n",
    "## Comment l'utiliser ?\n",
    "\n",
    "### Ouvrir le planificateur :\n",
    "```\n",
    "Windows + R ‚Üí Taper 'taskschd.msc' ‚Üí Entr√©e\n",
    "```\n",
    "\n",
    "### Cr√©er une t√¢che :\n",
    "1. **Action** ‚Üí Cr√©er une t√¢che de base\n",
    "2. **Nom** : \"Mon script quotidien\"\n",
    "3. **D√©clencheur** : Quotidien √† 2h du matin\n",
    "4. **Action** : D√©marrer `python.exe` avec `C:\\scripts\\mon_script.py`\n",
    "5. **Terminer**\n",
    "\n",
    "‚úÖ **Voil√† !** Votre script s'ex√©cutera automatiquement tous les jours √† 2h.\n",
    "\n",
    "## ‚úÖ Forces\n",
    "\n",
    "‚úÖ **Tr√®s facile** - Interface graphique intuitive  \n",
    "‚úÖ **D√©j√† install√©** - Pas de setup  \n",
    "‚úÖ **Parfait pour d√©buter** - Pas de code complexe  \n",
    "\n",
    "## ‚ùå Faiblesses\n",
    "\n",
    "‚ùå **Pas de d√©pendances** - Si t√¢che A doit finir avant t√¢che B ‚Üí compliqu√©  \n",
    "‚ùå **Monitoring limit√©** - Difficile de voir l'√©tat global  \n",
    "‚ùå **Windows uniquement** - Ne fonctionne pas sur Linux  \n",
    "\n",
    "## Quand l'utiliser ?\n",
    "\n",
    "‚úÖ **OUI** : Vous avez 1-5 scripts simples sur Windows  \n",
    "‚ùå **NON** : Vous avez besoin que script B attende script A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üêß Niveau 2 : Crontab (Linux/Mac)\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Le **planificateur standard** sur Linux/Mac.\n",
    "\n",
    "## Syntaxe de base\n",
    "\n",
    "```bash\n",
    "minute heure jour mois jour_semaine commande\n",
    "```\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)\n",
    "‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ heure (0 - 23)\n",
    "‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ jour du mois (1 - 31)\n",
    "‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ mois (1 - 12)\n",
    "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ jour de la semaine (0 - 6) (dimanche = 0)\n",
    "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ\n",
    "* * * * * commande\n",
    "```\n",
    "\n",
    "### Exemples simples :\n",
    "\n",
    "```bash\n",
    "# Tous les jours √† 2h du matin\n",
    "0 2 * * * python3 /home/user/script.py\n",
    "\n",
    "# Toutes les heures\n",
    "0 * * * * python3 /home/user/hourly.py\n",
    "\n",
    "# Lundi √† vendredi √† 9h\n",
    "0 9 * * 1-5 python3 /home/user/weekday.py\n",
    "\n",
    "# Toutes les 15 minutes\n",
    "*/15 * * * * python3 /home/user/check.py\n",
    "\n",
    "# Le 1er de chaque mois √† minuit\n",
    "0 0 1 * * python3 /home/user/monthly.py\n",
    "```\n",
    "\n",
    "## Comment l'utiliser ?\n",
    "\n",
    "```bash\n",
    "# √âditer votre crontab\n",
    "crontab -e\n",
    "\n",
    "# Voir les t√¢ches planifi√©es\n",
    "crontab -l\n",
    "\n",
    "# Ajouter vos lignes\n",
    "0 2 * * * python3 /home/user/backup.py >> /var/log/backup.log 2>&1\n",
    "\n",
    "# Sauvegarder et quitter\n",
    "# ‚úÖ C'est fait !\n",
    "```\n",
    "\n",
    "> üí° **Astuce** : Utilise [crontab.guru](https://crontab.guru/) pour tester tes expressions !\n",
    "\n",
    "## ‚úÖ Forces\n",
    "\n",
    "‚úÖ **Universel** - Sur TOUS les serveurs Linux  \n",
    "‚úÖ **Tr√®s l√©ger** - Presque pas de ressources  \n",
    "‚úÖ **Simple** - Une ligne = une t√¢che  \n",
    "‚úÖ **Gratuit** - D√©j√† install√©  \n",
    "\n",
    "## ‚ùå Faiblesses\n",
    "\n",
    "‚ùå **Pas de d√©pendances** - M√™me probl√®me que Windows  \n",
    "‚ùå **Pas de monitoring** - Aucune interface  \n",
    "‚ùå **Pas de retry** - Si √ßa √©choue, il faut attendre le prochain run  \n",
    "‚ùå **Logs manuels** - Il faut les g√©rer soi-m√™me  \n",
    "\n",
    "## Quand l'utiliser ?\n",
    "\n",
    "‚úÖ **OUI** : Serveur Linux, 5-15 scripts ind√©pendants  \n",
    "‚ùå **NON** : Scripts avec d√©pendances complexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üå¨Ô∏è Niveau 3 : Apache Airflow\n",
    "\n",
    "## C'est quoi ?\n",
    "\n",
    "Un **orchestrateur professionnel** pour g√©rer des workflows complexes, cr√©√© par **Airbnb** en 2014 et devenu projet **Apache** en 2016.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    APACHE AIRFLOW                               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   \"Airflow is a platform to programmatically author,           ‚îÇ\n",
    "‚îÇ    schedule, and monitor workflows.\"                            ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Cr√©√© par Airbnb (2014)                                      ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Apache Top-Level Project (2019)                             ‚îÇ\n",
    "‚îÇ   ‚Ä¢ 30,000+ GitHub stars                                        ‚îÇ\n",
    "‚îÇ   ‚Ä¢ Utilis√© par : Airbnb, Netflix, Spotify, Twitter, Adobe...   ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### La grande diff√©rence avec Cron :\n",
    "\n",
    "```python\n",
    "# ‚ùå Avec Cron (probl√®me) :\n",
    "0 2 * * * python extract.py\n",
    "30 2 * * * python transform.py  # Et si extract prend plus de 30min ?\n",
    "0 3 * * * python load.py        # Et si transform a plant√© ?\n",
    "\n",
    "# ‚úÖ Avec Airflow (solution) :\n",
    "extract >> transform >> load  # transform ATTEND que extract soit termin√© !\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture d'Airflow\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                        ARCHITECTURE AIRFLOW                                 ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇ   ‚îÇ                 ‚îÇ         ‚îÇ                 ‚îÇ         ‚îÇ             ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   WEB SERVER    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    SCHEDULER    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  EXECUTOR   ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ   (Flask UI)    ‚îÇ         ‚îÇ  (Orchestrate)  ‚îÇ         ‚îÇ  (Workers)  ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îÇ                 ‚îÇ         ‚îÇ                 ‚îÇ         ‚îÇ             ‚îÇ  ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "‚îÇ            ‚îÇ                           ‚îÇ                         ‚îÇ         ‚îÇ\n",
    "‚îÇ            ‚îÇ                           ‚ñº                         ‚îÇ         ‚îÇ\n",
    "‚îÇ            ‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ         ‚îÇ\n",
    "‚îÇ            ‚îÇ                  ‚îÇ                 ‚îÇ                ‚îÇ         ‚îÇ\n",
    "‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ    METADATA     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ\n",
    "‚îÇ                               ‚îÇ    DATABASE     ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ  (PostgreSQL)   ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ                 ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ\n",
    "‚îÇ                                        ‚ñ≤                                   ‚îÇ\n",
    "‚îÇ                                        ‚îÇ                                   ‚îÇ\n",
    "‚îÇ                               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ                 ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ    DAG FILES    ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ   (Python .py)  ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îÇ                 ‚îÇ                          ‚îÇ\n",
    "‚îÇ                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Composants cl√©s\n",
    "\n",
    "| Composant | R√¥le | Description |\n",
    "|-----------|------|-------------|\n",
    "| **Web Server** | Interface UI | Dashboard Flask pour visualiser les DAGs |\n",
    "| **Scheduler** | Planification | D√©cide quand ex√©cuter les t√¢ches |\n",
    "| **Executor** | Ex√©cution | Lance les t√¢ches (Local, Celery, K8s...) |\n",
    "| **Metadata DB** | Stockage | √âtat des DAGs, logs, historique |\n",
    "| **DAG Files** | D√©finition | Fichiers Python d√©finissant les workflows |\n",
    "\n",
    "### Types d'Executors\n",
    "\n",
    "| Executor | Usage | Scalabilit√© |\n",
    "|----------|-------|-------------|\n",
    "| **SequentialExecutor** | Dev/Test | 1 t√¢che √† la fois |\n",
    "| **LocalExecutor** | Petite prod | Parall√®le sur 1 machine |\n",
    "| **CeleryExecutor** | Production | Workers distribu√©s |\n",
    "| **KubernetesExecutor** | Cloud | Pod par t√¢che |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Concepts cl√©s d'Airflow\n",
    "\n",
    "### 1Ô∏è‚É£ DAG (Directed Acyclic Graph)\n",
    "\n",
    "Un **DAG** est un graphe de t√¢ches **sans cycle** : les donn√©es vont toujours dans une direction.\n",
    "\n",
    "```\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇ Extract ‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ\n",
    "          ‚ñº\n",
    "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "     ‚îÇTransform‚îÇ\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "          ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚ñº           ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇLoad DW‚îÇ  ‚îÇLoad S3 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ Task\n",
    "\n",
    "Une **Task** est une unit√© de travail dans un DAG (une √©tape).\n",
    "\n",
    "### 3Ô∏è‚É£ Operator\n",
    "\n",
    "Un **Operator** d√©finit **ce que fait** une t√¢che.\n",
    "\n",
    "| Operator | Usage |\n",
    "|----------|-------|\n",
    "| `PythonOperator` | Ex√©cuter une fonction Python |\n",
    "| `BashOperator` | Ex√©cuter une commande bash |\n",
    "| `EmailOperator` | Envoyer un email |\n",
    "| `PostgresOperator` | Ex√©cuter du SQL |\n",
    "| `S3ToRedshiftOperator` | Copier S3 ‚Üí Redshift |\n",
    "\n",
    "### 4Ô∏è‚É£ Task Instance\n",
    "\n",
    "Une **Task Instance** = une ex√©cution sp√©cifique d'une Task √† une date donn√©e.\n",
    "\n",
    "### 5Ô∏è‚É£ DAG Run\n",
    "\n",
    "Un **DAG Run** = une ex√©cution compl√®te du DAG.\n",
    "\n",
    "```\n",
    "DAG: etl_pipeline\n",
    "‚îú‚îÄ‚îÄ DAG Run 2024-01-15 ‚úÖ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ extract (Task Instance) ‚úÖ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transform (Task Instance) ‚úÖ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ load (Task Instance) ‚úÖ\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ DAG Run 2024-01-16 ‚è≥\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ extract (Task Instance) ‚úÖ\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transform (Task Instance) ‚è≥ running\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ load (Task Instance) ‚è∏Ô∏è waiting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Installation locale d'Airflow\n",
    "\n",
    "```bash\n",
    "# Cr√©er un environnement virtuel\n",
    "python -m venv airflow_venv\n",
    "source airflow_venv/bin/activate  # Linux/Mac\n",
    "# ou : airflow_venv\\Scripts\\activate  # Windows\n",
    "\n",
    "# D√©finir le home Airflow\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# Installer Airflow (version contrainte pour compatibilit√©)\n",
    "AIRFLOW_VERSION=2.8.1\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "\n",
    "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "\n",
    "# Initialiser la base de donn√©es\n",
    "airflow db init\n",
    "\n",
    "# Cr√©er un utilisateur admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com \\\n",
    "    --password admin\n",
    "\n",
    "# Lancer le webserver (Terminal 1)\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Lancer le scheduler (Terminal 2)\n",
    "airflow scheduler\n",
    "```\n",
    "\n",
    "üëâ Acc√©der : **http://localhost:8080** (login: admin / admin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Ton premier DAG\n",
    "\n",
    "Cr√©er le fichier `~/airflow/dags/mon_premier_dag.py` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/airflow/dags/mon_premier_dag.py\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Arguments par d√©faut pour toutes les t√¢ches\n",
    "default_args = {\n",
    "    'owner': 'data_engineer',\n",
    "    'depends_on_past': False,\n",
    "    'email': ['data-team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# D√©finir le DAG\n",
    "dag = DAG(\n",
    "    dag_id='mon_premier_dag',\n",
    "    default_args=default_args,\n",
    "    description='Mon premier pipeline ETL',\n",
    "    schedule_interval='@daily',  # Ex√©cution quotidienne\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,  # Ne pas ex√©cuter les runs pass√©s\n",
    "    tags=['etl', 'tutorial'],\n",
    ")\n",
    "\n",
    "# Fonctions Python\n",
    "def extract():\n",
    "    \"\"\"Extraire les donn√©es\"\"\"\n",
    "    print(\"üì• Extraction des donn√©es depuis l'API...\")\n",
    "    # Simuler extraction\n",
    "    data = {'records': 1000, 'source': 'api'}\n",
    "    return data  # Retourn√© via XCom\n",
    "\n",
    "def transform(**context):\n",
    "    \"\"\"Transformer les donn√©es\"\"\"\n",
    "    # R√©cup√©rer les donn√©es de extract via XCom\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract')\n",
    "    print(f\"üîÑ Transformation de {data['records']} enregistrements\")\n",
    "    return {'records': data['records'], 'cleaned': True}\n",
    "\n",
    "def load(**context):\n",
    "    \"\"\"Charger les donn√©es\"\"\"\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='transform')\n",
    "    print(f\"üíæ Chargement de {data['records']} enregistrements nettoy√©s\")\n",
    "\n",
    "# Cr√©er les t√¢ches\n",
    "task_start = BashOperator(\n",
    "    task_id='start',\n",
    "    bash_command='echo \"üöÄ D√©marrage du pipeline √† $(date)\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_extract = PythonOperator(\n",
    "    task_id='extract',\n",
    "    python_callable=extract,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_transform = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=transform,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_load = PythonOperator(\n",
    "    task_id='load',\n",
    "    python_callable=load,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_end = BashOperator(\n",
    "    task_id='end',\n",
    "    bash_command='echo \"‚úÖ Pipeline termin√© avec succ√®s !\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# D√©finir les d√©pendances\n",
    "task_start >> task_extract >> task_transform >> task_load >> task_end\n",
    "\n",
    "# √âquivalent √† :\n",
    "# task_start.set_downstream(task_extract)\n",
    "# task_extract.set_downstream(task_transform)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Param√®tres importants du DAG\n",
    "\n",
    "### Schedule Interval (fr√©quence d'ex√©cution)\n",
    "\n",
    "| Preset | √âquivalent Cron | Description |\n",
    "|--------|-----------------|-------------|\n",
    "| `@once` | - | Une seule fois |\n",
    "| `@hourly` | `0 * * * *` | Chaque heure |\n",
    "| `@daily` | `0 0 * * *` | Chaque jour √† minuit |\n",
    "| `@weekly` | `0 0 * * 0` | Chaque dimanche |\n",
    "| `@monthly` | `0 0 1 * *` | Le 1er du mois |\n",
    "| `@yearly` | `0 0 1 1 *` | Le 1er janvier |\n",
    "| `None` | - | D√©clench√© manuellement |\n",
    "| `'0 6 * * 1-5'` | Cron | Lun-Ven √† 6h |\n",
    "\n",
    "### Catchup\n",
    "\n",
    "```python\n",
    "# catchup=True (d√©faut) :\n",
    "# Si start_date=2024-01-01 et on est le 2024-01-10,\n",
    "# Airflow va ex√©cuter les 10 DAG Runs manqu√©s !\n",
    "\n",
    "# catchup=False :\n",
    "# Ex√©cute seulement √† partir de maintenant\n",
    "```\n",
    "\n",
    "### Default Args importants\n",
    "\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'data_team',           # Propri√©taire\n",
    "    'depends_on_past': False,       # D√©pend du run pr√©c√©dent ?\n",
    "    'email_on_failure': True,       # Email si √©chec\n",
    "    'retries': 3,                   # Nombre de retry\n",
    "    'retry_delay': timedelta(minutes=5),  # D√©lai entre retries\n",
    "    'execution_timeout': timedelta(hours=1),  # Timeout\n",
    "    'sla': timedelta(hours=2),      # SLA (alerte si d√©pass√©)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Les Operators les plus utilis√©s\n",
    "\n",
    "### PythonOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def my_function(name, **context):\n",
    "    print(f\"Hello {name}!\")\n",
    "    print(f\"Execution date: {context['ds']}\")\n",
    "    return \"success\"\n",
    "\n",
    "task = PythonOperator(\n",
    "    task_id='python_task',\n",
    "    python_callable=my_function,\n",
    "    op_kwargs={'name': 'World'},  # Arguments de la fonction\n",
    ")\n",
    "```\n",
    "\n",
    "### BashOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "task = BashOperator(\n",
    "    task_id='bash_task',\n",
    "    bash_command='echo \"Date: {{ ds }}\" && python /scripts/etl.py',\n",
    ")\n",
    "```\n",
    "\n",
    "### PostgresOperator\n",
    "\n",
    "```python\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "\n",
    "task = PostgresOperator(\n",
    "    task_id='create_table',\n",
    "    postgres_conn_id='my_postgres',  # Connexion d√©finie dans UI\n",
    "    sql=\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS users (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            name VARCHAR(100)\n",
    "        );\n",
    "    \"\"\",\n",
    ")\n",
    "```\n",
    "\n",
    "### EmailOperator\n",
    "\n",
    "```python\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "task = EmailOperator(\n",
    "    task_id='send_report',\n",
    "    to='team@company.com',\n",
    "    subject='Pipeline Report - {{ ds }}',\n",
    "    html_content='<h1>Pipeline completed!</h1>',\n",
    ")\n",
    "```\n",
    "\n",
    "### EmptyOperator (anciennement DummyOperator)\n",
    "\n",
    "```python\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "# Utile pour cr√©er des points de jonction\n",
    "start = EmptyOperator(task_id='start')\n",
    "end = EmptyOperator(task_id='end')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¨ XCom ‚Äî Passer des donn√©es entre t√¢ches\n",
    "\n",
    "**XCom** (Cross-Communication) permet de partager des donn√©es entre t√¢ches.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         XCom          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Task A  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ data ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ‚îÇ   Task B  ‚îÇ\n",
    "‚îÇ  return  ‚îÇ                       ‚îÇ xcom_pull ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### M√©thode 1 : Return (automatique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XCom avec return (automatique)\n",
    "\n",
    "def extract():\n",
    "    data = {'records': 1000, 'status': 'ok'}\n",
    "    return data  # Automatiquement stock√© dans XCom\n",
    "\n",
    "def transform(**context):\n",
    "    # R√©cup√©rer via ti (task instance)\n",
    "    ti = context['ti']\n",
    "    data = ti.xcom_pull(task_ids='extract')\n",
    "    print(f\"Re√ßu: {data}\")\n",
    "    return {'records': data['records'], 'transformed': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√©thode 2 : Push/Pull explicite\n",
    "\n",
    "```python\n",
    "def task_a(**context):\n",
    "    # Push explicite avec une cl√©\n",
    "    context['ti'].xcom_push(key='my_data', value={'count': 42})\n",
    "    context['ti'].xcom_push(key='status', value='success')\n",
    "\n",
    "def task_b(**context):\n",
    "    # Pull avec la cl√©\n",
    "    data = context['ti'].xcom_pull(task_ids='task_a', key='my_data')\n",
    "    status = context['ti'].xcom_pull(task_ids='task_a', key='status')\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Limites de XCom\n",
    "\n",
    "| Limite | Description |\n",
    "|--------|-------------|\n",
    "| **Taille** | Max ~48KB par d√©faut (stock√© en DB) |\n",
    "| **S√©rialisation** | Doit √™tre JSON-s√©rialisable |\n",
    "| **Pas pour big data** | Utiliser S3/GCS pour gros fichiers |\n",
    "\n",
    "```python\n",
    "# ‚ùå Mauvaise pratique\n",
    "def extract():\n",
    "    df = pd.read_csv('big_file.csv')  # 1GB\n",
    "    return df.to_dict()  # ‚ùå Trop gros pour XCom !\n",
    "\n",
    "# ‚úÖ Bonne pratique\n",
    "def extract():\n",
    "    df = pd.read_csv('big_file.csv')\n",
    "    path = '/data/output/extract_2024-01-15.parquet'\n",
    "    df.to_parquet(path)\n",
    "    return path  # ‚úÖ Passer le chemin, pas les donn√©es\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ TaskFlow API (Airflow 2.0+)\n",
    "\n",
    "Syntaxe moderne et plus **Pythonic** avec des d√©corateurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TaskFlow API - Syntaxe moderne (Airflow 2.0+)\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime\n",
    "\n",
    "@dag(\n",
    "    dag_id='etl_taskflow',\n",
    "    schedule='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['etl', 'taskflow'],\n",
    ")\n",
    "def etl_pipeline():\n",
    "    \n",
    "    @task()\n",
    "    def extract():\n",
    "        \"\"\"Extraire les donn√©es\"\"\"\n",
    "        return {'records': 1000, 'source': 'api'}\n",
    "    \n",
    "    @task()\n",
    "    def transform(data: dict):\n",
    "        \"\"\"Transformer les donn√©es\"\"\"\n",
    "        return {\n",
    "            'records': data['records'],\n",
    "            'cleaned': True\n",
    "        }\n",
    "    \n",
    "    @task()\n",
    "    def load(data: dict):\n",
    "        \"\"\"Charger les donn√©es\"\"\"\n",
    "        print(f\"Loading {data['records']} records\")\n",
    "    \n",
    "    # D√©finir le flow - XCom automatique !\n",
    "    raw_data = extract()\n",
    "    clean_data = transform(raw_data)\n",
    "    load(clean_data)\n",
    "\n",
    "# Instancier le DAG\n",
    "etl_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avantages TaskFlow API\n",
    "\n",
    "| Avantage | Description |\n",
    "|----------|-------------|\n",
    "| **XCom automatique** | Les retours sont pass√©s automatiquement |\n",
    "| **Code plus lisible** | Ressemble √† du Python normal |\n",
    "| **Type hints** | Support des annotations de type |\n",
    "| **Moins de boilerplate** | Pas besoin de cr√©er des Operators |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üëÅÔ∏è Sensors ‚Äî Attendre une condition\n",
    "\n",
    "Les **Sensors** attendent qu'une condition soit remplie avant de continuer.\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ   FileSensor          S3KeySensor         HttpSensor         ‚îÇ\n",
    "‚îÇ   \"Fichier existe?\"   \"Fichier sur S3?\"   \"API disponible?\"  ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ        ‚è≥                   ‚è≥                   ‚è≥            ‚îÇ\n",
    "‚îÇ        ‚îÇ                    ‚îÇ                    ‚îÇ           ‚îÇ\n",
    "‚îÇ        ‚ñº                    ‚ñº                    ‚ñº           ‚îÇ\n",
    "‚îÇ       ‚úÖ                   ‚úÖ                   ‚úÖ           ‚îÇ\n",
    "‚îÇ   Continuer            Continuer            Continuer        ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### FileSensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileSensor - Attendre qu'un fichier existe\n",
    "\n",
    "from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "wait_for_file = FileSensor(\n",
    "    task_id='wait_for_file',\n",
    "    filepath='/data/input/daily_export.csv',\n",
    "    poke_interval=60,      # V√©rifier toutes les 60 secondes\n",
    "    timeout=3600,          # Timeout apr√®s 1 heure\n",
    "    mode='poke',           # poke ou reschedule\n",
    ")\n",
    "\n",
    "# Le pipeline attend le fichier avant de continuer\n",
    "wait_for_file >> process_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autres Sensors utiles\n",
    "\n",
    "```python\n",
    "# S3KeySensor - Attendre un fichier sur S3\n",
    "from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n",
    "\n",
    "wait_s3 = S3KeySensor(\n",
    "    task_id='wait_for_s3',\n",
    "    bucket_name='my-bucket',\n",
    "    bucket_key='data/{{ ds }}/export.csv',\n",
    "    aws_conn_id='aws_default',\n",
    ")\n",
    "\n",
    "# HttpSensor - Attendre qu'une API r√©ponde\n",
    "from airflow.providers.http.sensors.http import HttpSensor\n",
    "\n",
    "wait_api = HttpSensor(\n",
    "    task_id='wait_for_api',\n",
    "    http_conn_id='api_connection',\n",
    "    endpoint='health',\n",
    "    response_check=lambda response: response.status_code == 200,\n",
    ")\n",
    "\n",
    "# ExternalTaskSensor - Attendre un autre DAG\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "\n",
    "wait_other_dag = ExternalTaskSensor(\n",
    "    task_id='wait_upstream',\n",
    "    external_dag_id='upstream_dag',\n",
    "    external_task_id='final_task',\n",
    ")\n",
    "```\n",
    "\n",
    "### Mode poke vs reschedule\n",
    "\n",
    "| Mode | Description | Ressources |\n",
    "|------|-------------|------------|\n",
    "| `poke` | Garde un worker slot | Consomme plus |\n",
    "| `reschedule` | Lib√®re le slot entre checks | Recommand√© |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÄ Branching ‚Äî Logique conditionnelle\n",
    "\n",
    "Ex√©cuter diff√©rentes t√¢ches selon une condition.\n",
    "\n",
    "```\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ  Check   ‚îÇ\n",
    "                    ‚îÇ Condition‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚îÇ\n",
    "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "              ‚îÇ          ‚îÇ          ‚îÇ\n",
    "              ‚ñº          ‚ñº          ‚ñº\n",
    "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "         ‚îÇ Path A ‚îÇ ‚îÇ Path B ‚îÇ ‚îÇ Path C ‚îÇ\n",
    "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Branching - Ex√©cution conditionnelle\n",
    "\n",
    "from airflow.operators.python import BranchPythonOperator\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "\n",
    "def choose_branch(**context):\n",
    "    \"\"\"D√©cider quelle branche ex√©cuter\"\"\"\n",
    "    # Exemple : v√©rifier le jour de la semaine\n",
    "    day = context['ds_nodash']  # Format YYYYMMDD\n",
    "    \n",
    "    # Logique m√©tier\n",
    "    if int(day) % 2 == 0:\n",
    "        return 'process_even'  # Retourner le task_id √† ex√©cuter\n",
    "    else:\n",
    "        return 'process_odd'\n",
    "\n",
    "branch = BranchPythonOperator(\n",
    "    task_id='branch',\n",
    "    python_callable=choose_branch,\n",
    ")\n",
    "\n",
    "process_even = EmptyOperator(task_id='process_even')\n",
    "process_odd = EmptyOperator(task_id='process_odd')\n",
    "end = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')\n",
    "\n",
    "# D√©finir le flow\n",
    "branch >> [process_even, process_odd] >> end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéöÔ∏è Trigger Rules\n",
    "\n",
    "Contr√¥ler **quand** une t√¢che s'ex√©cute en fonction du statut des t√¢ches parentes.\n",
    "\n",
    "| Trigger Rule | Ex√©cute si... |\n",
    "|--------------|---------------|\n",
    "| `all_success` | Tous les parents ont r√©ussi (d√©faut) |\n",
    "| `all_failed` | Tous les parents ont √©chou√© |\n",
    "| `all_done` | Tous les parents sont termin√©s (peu importe le statut) |\n",
    "| `one_success` | Au moins un parent a r√©ussi |\n",
    "| `one_failed` | Au moins un parent a √©chou√© |\n",
    "| `none_failed` | Aucun parent n'a √©chou√© (succ√®s ou skipped) |\n",
    "| `none_skipped` | Aucun parent n'a √©t√© skipped |\n",
    "\n",
    "```python\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "\n",
    "# T√¢che de notification en cas d'√©chec\n",
    "notify_failure = EmailOperator(\n",
    "    task_id='notify_failure',\n",
    "    to='team@company.com',\n",
    "    subject='Pipeline Failed!',\n",
    "    html_content='...',\n",
    "    trigger_rule=TriggerRule.ONE_FAILED,  # Ex√©cute si un parent √©choue\n",
    ")\n",
    "\n",
    "# T√¢che finale qui s'ex√©cute toujours\n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup',\n",
    "    bash_command='rm -rf /tmp/data/*',\n",
    "    trigger_rule=TriggerRule.ALL_DONE,  # Toujours ex√©cuter\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîê Connections et Variables\n",
    "\n",
    "### Connections\n",
    "\n",
    "Stocker les informations de connexion aux syst√®mes externes.\n",
    "\n",
    "**Dans l'UI** : Admin ‚Üí Connections ‚Üí +\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Add Connection                                             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ  Connection Id:   ‚îÇ my_postgres                 ‚îÇ           ‚îÇ\n",
    "‚îÇ  Connection Type: ‚îÇ Postgres           ‚ñº        ‚îÇ           ‚îÇ\n",
    "‚îÇ  Host:            ‚îÇ localhost                   ‚îÇ           ‚îÇ\n",
    "‚îÇ  Schema:          ‚îÇ mydb                        ‚îÇ           ‚îÇ\n",
    "‚îÇ  Login:           ‚îÇ admin                       ‚îÇ           ‚îÇ\n",
    "‚îÇ  Password:        ‚îÇ ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢                    ‚îÇ           ‚îÇ\n",
    "‚îÇ  Port:            ‚îÇ 5432                        ‚îÇ           ‚îÇ\n",
    "‚îÇ                                                             ‚îÇ\n",
    "‚îÇ                              [ Save ]                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**Utilisation dans le code** :\n",
    "\n",
    "```python\n",
    "from airflow.hooks.postgres_hook import PostgresHook\n",
    "\n",
    "def query_postgres():\n",
    "    hook = PostgresHook(postgres_conn_id='my_postgres')\n",
    "    df = hook.get_pandas_df('SELECT * FROM users')\n",
    "    return df\n",
    "```\n",
    "\n",
    "### Variables\n",
    "\n",
    "Stocker des configurations r√©utilisables.\n",
    "\n",
    "**Dans l'UI** : Admin ‚Üí Variables ‚Üí +\n",
    "\n",
    "```python\n",
    "from airflow.models import Variable\n",
    "\n",
    "# R√©cup√©rer une variable\n",
    "api_key = Variable.get('API_KEY')\n",
    "\n",
    "# Variable JSON\n",
    "config = Variable.get('pipeline_config', deserialize_json=True)\n",
    "# config = {'batch_size': 1000, 'env': 'prod'}\n",
    "\n",
    "# Dans un template Jinja\n",
    "# {{ var.value.API_KEY }}\n",
    "# {{ var.json.pipeline_config.batch_size }}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Monitoring et Alertes\n",
    "\n",
    "### Interface Web\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Airflow - DAGs                                                             ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  DAG                    Schedule    Owner    Runs   Recent Tasks            ‚îÇ\n",
    "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÇ\n",
    "‚îÇ  ‚ñ∂ etl_pipeline         @daily      team     125    ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ               ‚îÇ\n",
    "‚îÇ  ‚ñ∂ data_quality_check   @hourly     team     560    ‚úÖ‚úÖ‚úÖ‚ùå‚úÖ               ‚îÇ\n",
    "‚îÇ  ‚ñ∂ weekly_report        @weekly     team     52     ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ               ‚îÇ\n",
    "‚îÇ  ‚è∏ maintenance          None        admin    3      ‚úÖ‚úÖ‚úÖ                   ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îÇ  ‚úÖ Success  ‚ùå Failed  ‚è≥ Running  ‚è∏ Paused                                ‚îÇ\n",
    "‚îÇ                                                                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Vues disponibles\n",
    "\n",
    "| Vue | Description |\n",
    "|-----|-------------|\n",
    "| **Grid** | Vue matricielle des runs |\n",
    "| **Graph** | Graphe du DAG |\n",
    "| **Calendar** | Historique par date |\n",
    "| **Gantt** | Timeline d'ex√©cution |\n",
    "| **Code** | Code source du DAG |\n",
    "\n",
    "### Configurer les alertes email\n",
    "\n",
    "```python\n",
    "# airflow.cfg\n",
    "[smtp]\n",
    "smtp_host = smtp.gmail.com\n",
    "smtp_port = 587\n",
    "smtp_user = airflow@company.com\n",
    "smtp_password = your_password\n",
    "smtp_mail_from = airflow@company.com\n",
    "\n",
    "# Dans le DAG\n",
    "default_args = {\n",
    "    'email': ['team@company.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "}\n",
    "```\n",
    "\n",
    "### Alertes Slack\n",
    "\n",
    "```python\n",
    "from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\n",
    "\n",
    "def alert_slack_on_failure(context):\n",
    "    \"\"\"Callback en cas d'√©chec\"\"\"\n",
    "    slack_msg = f\"\"\"\n",
    "        :red_circle: Task Failed!\n",
    "        *DAG*: {context['dag'].dag_id}\n",
    "        *Task*: {context['task'].task_id}\n",
    "        *Execution Time*: {context['execution_date']}\n",
    "    \"\"\"\n",
    "    return SlackWebhookOperator(\n",
    "        task_id='slack_alert',\n",
    "        slack_webhook_conn_id='slack_webhook',\n",
    "        message=slack_msg,\n",
    "    ).execute(context)\n",
    "\n",
    "# Utiliser le callback\n",
    "default_args = {\n",
    "    'on_failure_callback': alert_slack_on_failure,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Bonnes pratiques Airflow\n",
    "\n",
    "### 1. Structure des DAGs\n",
    "\n",
    "```\n",
    "airflow/\n",
    "‚îú‚îÄ‚îÄ dags/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ etl/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ daily_etl.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ weekly_report.py\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ utils/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ helpers.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ config/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ settings.py\n",
    "‚îú‚îÄ‚îÄ plugins/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ custom_operators/\n",
    "‚îî‚îÄ‚îÄ tests/\n",
    "    ‚îî‚îÄ‚îÄ test_dags.py\n",
    "```\n",
    "\n",
    "### 2. Idempotence\n",
    "\n",
    "```python\n",
    "# ‚ùå Non idempotent - accumule des donn√©es\n",
    "def bad_load():\n",
    "    db.execute(\"INSERT INTO table VALUES (...)\")\n",
    "\n",
    "# ‚úÖ Idempotent - m√™me r√©sultat si relanc√©\n",
    "def good_load():\n",
    "    db.execute(\"DELETE FROM table WHERE date = '{{ ds }}'\")\n",
    "    db.execute(\"INSERT INTO table SELECT ... WHERE date = '{{ ds }}'\")\n",
    "```\n",
    "\n",
    "### 3. Atomicit√© des t√¢ches\n",
    "\n",
    "```python\n",
    "# ‚ùå T√¢che trop grosse\n",
    "def do_everything():\n",
    "    extract()\n",
    "    transform()\n",
    "    load()\n",
    "\n",
    "# ‚úÖ T√¢ches atomiques\n",
    "extract >> transform >> load\n",
    "```\n",
    "\n",
    "### 4. Ne pas mettre de logique dans le DAG\n",
    "\n",
    "```python\n",
    "# ‚ùå Code ex√©cut√© √† chaque parsing\n",
    "data = fetch_from_api()  # Appel√© toutes les 30s !\n",
    "\n",
    "# ‚úÖ Logique dans les tasks\n",
    "@task\n",
    "def fetch_data():\n",
    "    return fetch_from_api()\n",
    "```\n",
    "\n",
    "### 5. Tester les DAGs\n",
    "\n",
    "```python\n",
    "# tests/test_dags.py\n",
    "import pytest\n",
    "from airflow.models import DagBag\n",
    "\n",
    "def test_dag_loaded():\n",
    "    dag_bag = DagBag()\n",
    "    assert len(dag_bag.import_errors) == 0\n",
    "\n",
    "def test_dag_structure():\n",
    "    dag_bag = DagBag()\n",
    "    dag = dag_bag.get_dag('etl_pipeline')\n",
    "    assert dag is not None\n",
    "    assert len(dag.tasks) == 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Forces d'Airflow\n",
    "\n",
    "‚úÖ **Gestion des d√©pendances** - `A >> B` = B attend A  \n",
    "‚úÖ **Retry automatique** - R√©essaie en cas d'√©chec  \n",
    "‚úÖ **Interface web** - Visualisation compl√®te  \n",
    "‚úÖ **Monitoring** - Logs centralis√©s  \n",
    "‚úÖ **Alertes** - Email/Slack en cas d'√©chec  \n",
    "‚úÖ **Scalable** - G√®re 100+ pipelines  \n",
    "‚úÖ **Extensible** - Custom operators, hooks  \n",
    "‚úÖ **Communaut√©** - Tr√®s active, beaucoup de providers  \n",
    "\n",
    "## ‚ùå Faiblesses d'Airflow\n",
    "\n",
    "‚ùå **Complexe** - Courbe d'apprentissage  \n",
    "‚ùå **Ressources** - Besoin de 4-8 GB RAM  \n",
    "‚ùå **Overkill** - Pour 1-3 scripts simples  \n",
    "‚ùå **Setup** - Installation et configuration  \n",
    "‚ùå **Pas pour le streaming** - Batch only (utiliser Kafka)  \n",
    "\n",
    "## Quand utiliser Airflow ?\n",
    "\n",
    "‚úÖ **OUI** : 10+ pipelines, d√©pendances complexes, production  \n",
    "‚ùå **NON** : 1-5 scripts simples sans d√©pendances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìä Tableau de d√©cision\n",
    "\n",
    "## Quel outil choisir ?\n",
    "\n",
    "| Situation | Outil recommand√© |\n",
    "|-----------|------------------|\n",
    "| J'ai 1-3 scripts sur Windows | ü™ü **Task Scheduler** |\n",
    "| J'ai 1-3 scripts sur Linux | üêß **Crontab** |\n",
    "| J'ai 5-10 scripts ind√©pendants | üêß **Crontab** |\n",
    "| J'ai 10+ scripts avec d√©pendances | üå¨Ô∏è **Airflow** |\n",
    "| Script B doit attendre script A | üå¨Ô∏è **Airflow** |\n",
    "| Je veux un dashboard | üå¨Ô∏è **Airflow** |\n",
    "| Je d√©bute en automatisation | ü™ü **Task Scheduler** |\n",
    "| Production critique | üå¨Ô∏è **Airflow** |\n",
    "\n",
    "## Progression naturelle\n",
    "\n",
    "```\n",
    "1. D√©butez avec Task Scheduler ou Cron\n",
    "2. Quand vous avez 5+ scripts ‚Üí Pensez √† migrer\n",
    "3. Quand vous avez des d√©pendances ‚Üí Migrez vers Airflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì R√©sum√©\n",
    "\n",
    "## Points cl√©s\n",
    "\n",
    "### ü™ü Windows Task Scheduler\n",
    "- **Pour qui** : D√©butants sur Windows\n",
    "- **Force** : Tr√®s facile (GUI)\n",
    "- **Faiblesse** : Pas de d√©pendances\n",
    "- **Limite** : 5 scripts max\n",
    "\n",
    "### üêß Crontab\n",
    "- **Pour qui** : Utilisateurs Linux\n",
    "- **Force** : Universel, l√©ger\n",
    "- **Faiblesse** : Pas de monitoring\n",
    "- **Limite** : 15 scripts max\n",
    "\n",
    "### üå¨Ô∏è Airflow\n",
    "- **Pour qui** : Production, √©quipes data\n",
    "- **Force** : D√©pendances, monitoring, scalable\n",
    "- **Faiblesse** : Complexe, ressources\n",
    "- **Limite** : Aucune (scalable)\n",
    "\n",
    "### Concepts Airflow √† retenir\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **DAG** | Graphe de t√¢ches (workflow) |\n",
    "| **Task** | Unit√© de travail |\n",
    "| **Operator** | Type de t√¢che (Python, Bash, SQL...) |\n",
    "| **XCom** | Passage de donn√©es entre t√¢ches |\n",
    "| **Sensor** | Attendre une condition |\n",
    "| **Connection** | Credentials stock√©s |\n",
    "| **Variable** | Configuration stock√©e |\n",
    "\n",
    "## Ressources\n",
    "\n",
    "- **Crontab** : [crontab.guru](https://crontab.guru/) (tester expressions)\n",
    "- **Airflow** : [airflow.apache.org](https://airflow.apache.org/)\n",
    "- **Airflow Tutorial** : [Documentation officielle](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Quiz Final\n",
    "\n",
    "### ‚ùì Q1. Vous avez 2 scripts Python √† ex√©cuter tous les jours sur Windows. Quel outil ?\n",
    "a) Airflow  \n",
    "b) Crontab  \n",
    "c) Windows Task Scheduler  \n",
    "d) Kubernetes\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : c** ‚Äì Pour 2 scripts simples sur Windows, utilisez le **Planificateur de t√¢ches** : facile, natif, gratuit.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q2. Expression crontab pour \"tous les lundis √† 9h\" ?\n",
    "a) `0 9 * * 1`  \n",
    "b) `9 0 * * 1`  \n",
    "c) `* 9 * * 1`  \n",
    "d) `0 9 1 * *`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : a** ‚Äì Format : `minute heure jour mois jour_semaine`. `0 9 * * 1` = 0 minutes, 9h, lundi (1).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q3. Que signifie DAG dans Airflow ?\n",
    "a) Data Analysis Graph  \n",
    "b) Directed Acyclic Graph  \n",
    "c) Dynamic Airflow Generator  \n",
    "d) Database Access Gateway\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì **Directed Acyclic Graph** = graphe orient√© sans cycle. Les t√¢ches vont toujours dans une direction.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q4. Dans Airflow, que signifie `task_a >> task_b` ?\n",
    "a) task_a et task_b en parall√®le  \n",
    "b) task_b attend que task_a finisse  \n",
    "c) task_a attend task_b  \n",
    "d) Erreur de syntaxe\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì `A >> B` signifie \"B **attend** que A soit termin√©\". C'est la gestion des d√©pendances !\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q5. Quel composant Airflow d√©cide quand ex√©cuter les t√¢ches ?\n",
    "a) Web Server  \n",
    "b) Executor  \n",
    "c) Scheduler  \n",
    "d) Metadata DB\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : c** ‚Äì Le **Scheduler** analyse les DAGs et d√©cide quand lancer les t√¢ches.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q6. Comment passer des donn√©es entre t√¢ches Airflow ?\n",
    "a) Variables globales  \n",
    "b) XCom  \n",
    "c) Fichiers partag√©s uniquement  \n",
    "d) Pas possible\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì **XCom** (Cross-Communication) permet de passer des donn√©es entre t√¢ches via `xcom_push` et `xcom_pull`.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q7. Qu'est-ce qu'un Sensor dans Airflow ?\n",
    "a) Un outil de monitoring  \n",
    "b) Un Operator qui attend une condition  \n",
    "c) Un type de DAG  \n",
    "d) Un syst√®me d'alerte\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Un **Sensor** est un Operator sp√©cial qui attend qu'une condition soit remplie (fichier existe, API disponible...).\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q8. Quand NE PAS utiliser Airflow ?\n",
    "a) 10+ pipelines complexes  \n",
    "b) 1-3 scripts simples  \n",
    "c) Production critique  \n",
    "d) Besoin de monitoring\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Pour **1-3 scripts simples**, Airflow est **overkill**. Utilisez Cron ou Task Scheduler √† la place.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q9. Quel est l'avantage de la TaskFlow API ?\n",
    "a) Plus performant  \n",
    "b) XCom automatique et code plus lisible  \n",
    "c) Compatible Python 2  \n",
    "d) Pas besoin de Scheduler\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì TaskFlow API (d√©corateurs `@dag`, `@task`) rend le code plus Pythonic avec XCom automatique.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q10. Vous avez 15 scripts avec des d√©pendances. Quel outil ?\n",
    "a) Windows Task Scheduler  \n",
    "b) Crontab  \n",
    "c) Apache Airflow  \n",
    "d) Aucun, faire manuellement\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : c** ‚Äì Avec **15 scripts** et des **d√©pendances**, il est temps de passer √† **Airflow** !\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Votre score\n",
    "\n",
    "- **10/10** : üèÜ Expert orchestration !\n",
    "- **8-9/10** : üåü Tr√®s bien !\n",
    "- **6-7/10** : üí™ Bon d√©but !\n",
    "- **< 6/10** : üìö Relisez le notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Ressources\n",
    "\n",
    "### Outils\n",
    "- [Crontab Guru](https://crontab.guru/) ‚Äî Tester et g√©n√©rer des expressions cron\n",
    "- [Apache Airflow](https://airflow.apache.org/) ‚Äî Documentation officielle\n",
    "- [Airflow Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/index.html)\n",
    "- [Astronomer](https://www.astronomer.io/) ‚Äî Guides et bonnes pratiques\n",
    "\n",
    "### Alternatives √† Airflow\n",
    "\n",
    "| Outil | Description | Cas d'usage |\n",
    "|-------|-------------|-------------|\n",
    "| **Prefect** | Orchestration moderne, Pythonic | Alternative plus simple √† Airflow |\n",
    "| **Dagster** | Data orchestration avec types | Pipelines ML |\n",
    "| **Luigi** | Par Spotify, simple | Pipelines batch |\n",
    "| **Mage** | Low-code, moderne | Prototypage rapide |\n",
    "| **Kestra** | Event-driven, YAML | Workflows d√©claratifs |\n",
    "\n",
    "### Cloud managed\n",
    "\n",
    "| Cloud | Service |\n",
    "|-------|--------|\n",
    "| **AWS** | MWAA (Managed Airflow), Step Functions |\n",
    "| **GCP** | Cloud Composer (Managed Airflow) |\n",
    "| **Azure** | Data Factory, Synapse Pipelines |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Fin du niveau d√©butant !\n",
    "\n",
    "Tu as termin√© le parcours **Data Engineering - From Zero to Hero** niveau d√©butant ! üéâ\n",
    "\n",
    "### üìã R√©capitulatif des modules\n",
    "\n",
    "| # | Module | Comp√©tence acquise |\n",
    "|---|--------|--------------------|\n",
    "| 01 | Introduction | Vision du m√©tier |\n",
    "| 02 | Bash | Ligne de commande |\n",
    "| 03 | Git | Versioning |\n",
    "| 04 | Python Basics | Programmation |\n",
    "| 05 | Python Data Processing | Pandas, visualisation |\n",
    "| 06 | Intro Bases Relationnelles | Concepts relationnels |\n",
    "| 07 | SQL | Requ√™tes SQL |\n",
    "| 08 | Big Data & NoSQL | Syst√®mes distribu√©s |\n",
    "| 09 | MongoDB | Base NoSQL document |\n",
    "| 10 | Elasticsearch | Recherche et indexation |\n",
    "| 11 | PySpark | Traitement distribu√© |\n",
    "| **12** | **Orchestration** | **Airflow, pipelines** |\n",
    "\n",
    "### üéÅ Module BONUS disponible\n",
    "\n",
    "| # | Module | Description |\n",
    "|---|--------|-------------|\n",
    "| 13 | **BONUS FastAPI** | Cr√©er des APIs REST pour exposer tes donn√©es |\n",
    "\n",
    "üëâ Parfait pour exposer les r√©sultats de tes pipelines via API !\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Prochaine √©tape : Niveau Interm√©diaire\n",
    "\n",
    "Tu es maintenant pr√™t pour le **niveau interm√©diaire** qui couvrira :\n",
    "\n",
    "| Module | Description |\n",
    "|--------|-------------|\n",
    "| **Docker** | Conteneurisation des pipelines |\n",
    "| **Data Lakes** | Parquet, Delta Lake, Iceberg |\n",
    "| **Kafka** | Streaming en temps r√©el |\n",
    "| **dbt** | Transformation dans le warehouse |\n",
    "| **Data Quality** | Great Expectations, tests |\n",
    "| **Cloud** | AWS / GCP / Azure |\n",
    "| **CI/CD** | GitHub Actions, tests automatis√©s |\n",
    "| **Kibana** | Dashboards et monitoring |\n",
    "| **Projet int√©grateur** | Pipeline complet end-to-end |\n",
    "\n",
    "---\n",
    "\n",
    "üöÄ **Bravo ! Tu as maintenant toutes les bases pour construire des pipelines de donn√©es !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
