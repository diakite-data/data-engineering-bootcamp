{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° PySpark for Data Engineering\n",
    "\n",
    "Ce module pr√©sente **PySpark**, l'API Python pour Apache Spark ‚Äî le moteur de traitement distribu√© le plus utilis√© en Big Data.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Pr√©requis\n",
    "\n",
    "| Niveau | Comp√©tence |\n",
    "|--------|------------|\n",
    "| ‚úÖ Requis | Avoir suivi le module `08_intro_big_data_distributed` |\n",
    "| ‚úÖ Requis | Comprendre les 5V du Big Data |\n",
    "| ‚úÖ Requis | Comprendre MapReduce et ses limites |\n",
    "| ‚úÖ Requis | Ma√Ætriser Python (modules 04-05) |\n",
    "| ‚úÖ Requis | Ma√Ætriser SQL (module 07) |\n",
    "\n",
    "## üéØ Objectifs du module\n",
    "\n",
    "√Ä la fin de ce notebook, tu seras capable de :\n",
    "- ‚úÖ Comprendre l'architecture Spark (Driver, Executors, Cluster Manager)\n",
    "- ‚úÖ Cr√©er et manipuler des DataFrames distribu√©s\n",
    "- ‚úÖ √âcrire des transformations et actions\n",
    "- ‚úÖ Utiliser Spark SQL\n",
    "- ‚úÖ Optimiser les performances (partitioning, caching, broadcast)\n",
    "- ‚úÖ Lire/√©crire des fichiers (CSV, JSON, Parquet)\n",
    "- ‚úÖ D√©couvrir le streaming temps r√©el\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ PySpark dans l'√©cosyst√®me Big Data\n",
    "\n",
    "Tu as vu dans le module 08 que **Spark a remplac√© MapReduce** comme moteur de traitement Big Data. Voici pourquoi :\n",
    "\n",
    "### Rappel : MapReduce vs Spark\n",
    "\n",
    "```\n",
    "MapReduce :  DISQUE ‚Üí Map ‚Üí DISQUE ‚Üí Shuffle ‚Üí DISQUE ‚Üí Reduce ‚Üí DISQUE\n",
    "                  ‚Üë           ‚Üë              ‚Üë              ‚Üë\n",
    "                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                               LENT ! (I/O disque)\n",
    "\n",
    "Spark :      DISQUE ‚Üí Transformations ‚Üí M√âMOIRE ‚Üí ... ‚Üí M√âMOIRE ‚Üí Action\n",
    "                                          ‚Üë                ‚Üë\n",
    "                                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                           RAPIDE ! (in-memory)\n",
    "```\n",
    "\n",
    "### Rappel : Les 5V et Spark\n",
    "\n",
    "| V | Comment Spark r√©pond |\n",
    "|---|----------------------|\n",
    "| **Volume** | Traitement distribu√© sur cluster (To ‚Üí Po) |\n",
    "| **Velocity** | Spark Streaming pour le temps r√©el |\n",
    "| **Variety** | Lit CSV, JSON, Parquet, JDBC, Avro... |\n",
    "| **Veracity** | Transformations pour nettoyer les donn√©es |\n",
    "| **Value** | Spark SQL, MLlib pour extraire de la valeur |\n",
    "\n",
    "### Position dans l'√©cosyst√®me\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                     √âCOSYST√àME BIG DATA                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   Sources           Traitement              Stockage            ‚îÇ\n",
    "‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ          ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îÇ   Kafka    ‚îÄ‚îê                         ‚îå‚îÄ‚ñ∫  Data Lake (S3)      ‚îÇ\n",
    "‚îÇ   Fichiers ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫  ‚ö° SPARK ‚ö°  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚ñ∫  Data Warehouse       ‚îÇ\n",
    "‚îÇ   JDBC     ‚îÄ‚î§     (PySpark)          ‚îú‚îÄ‚ñ∫  NoSQL (MongoDB)     ‚îÇ\n",
    "‚îÇ   APIs     ‚îÄ‚îò                         ‚îî‚îÄ‚ñ∫  Elasticsearch       ‚îÇ\n",
    "‚îÇ                                                                 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> üí° **Ce notebook est interactif** : tu peux ex√©cuter toutes les cellules de code !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation et Setup\n",
    "\n",
    "PySpark n√©cessite Java. V√©rifions d'abord l'installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation de PySpark\n",
    "!pip install pyspark pandas numpy pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier Java\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports de base\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Introduction √† Spark\n",
    "\n",
    "## Qu'est-ce que Spark ?\n",
    "\n",
    "Apache Spark est un moteur de traitement distribu√© ultra-rapide pour le Big Data.\n",
    "\n",
    "### üîë Concepts cl√©s\n",
    "\n",
    "- **SparkSession** : Point d'entr√©e de toute application Spark\n",
    "- **DataFrame** : Collection distribu√©e de donn√©es organis√©es en colonnes\n",
    "- **RDD** : Resilient Distributed Dataset (bas niveau)\n",
    "- **Transformations** : Op√©rations lazy (map, filter, select, etc.)\n",
    "- **Actions** : D√©clenchent l'ex√©cution (count, collect, show, etc.)\n",
    "\n",
    "### üöÄ Avantages de Spark\n",
    "\n",
    "- **Vitesse** : 100x plus rapide que MapReduce\n",
    "- **Scalabilit√©** : De quelques MB √† plusieurs PB\n",
    "- **Simplicit√©** : API unifi√©e (Python, Scala, Java, R)\n",
    "- **Versatilit√©** : Batch, Streaming, ML, Graph processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Cr√©er une SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Data Engineering Tutorial\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ SparkSession cr√©√©e\")\n",
    "print(f\"Version Spark : {spark.version}\")\n",
    "print(f\"Application : {spark.sparkContext.appName}\")\n",
    "print(f\"Master : {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du logging\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"‚úÖ Logging configur√© sur ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Premiers DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 1 : Depuis une liste Python\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"Paris\", 45000),\n",
    "    (2, \"Bob\", 30, \"Lyon\", 55000),\n",
    "    (3, \"Charlie\", 35, \"Paris\", 60000),\n",
    "    (4, \"David\", 28, \"Marseille\", 50000),\n",
    "    (5, \"Eve\", 32, \"Lyon\", 58000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"üìä Premier DataFrame cr√©√© :\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 2 : Depuis un Pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'produit': ['A', 'B', 'C', 'D'],\n",
    "    'prix': [10.5, 20.0, 15.75, 30.0],\n",
    "    'quantite': [100, 50, 75, 25]\n",
    "})\n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"üìä DataFrame depuis Pandas :\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 3 : Avec un sch√©ma explicite\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nom\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"ville\", StringType(), True),\n",
    "    StructField(\"salaire\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_with_schema = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"üìä DataFrame avec sch√©ma explicite :\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Explorer un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le sch√©ma\n",
    "print(\"üìã Sch√©ma du DataFrame :\")\n",
    "df.printSchema()\n",
    "\n",
    "# Afficher les premi√®res lignes\n",
    "print(\"\\nüîù Premi√®res lignes :\")\n",
    "df.show(3)\n",
    "\n",
    "# Compter les lignes\n",
    "print(f\"\\nüìè Nombre de lignes : {df.count()}\")\n",
    "\n",
    "# Colonnes\n",
    "print(f\"\\nüìã Colonnes : {df.columns}\")\n",
    "\n",
    "# Types de donn√©es\n",
    "print(\"\\nüî§ Types de donn√©es :\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives\n",
    "print(\"üìä Statistiques descriptives :\")\n",
    "df.describe().show()\n",
    "\n",
    "# Statistiques sur colonnes sp√©cifiques\n",
    "print(\"\\nüìä Statistiques sur 'age' et 'salaire' :\")\n",
    "df.select('age', 'salaire').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Transformations de base\n",
    "\n",
    "Les transformations sont **lazy** : elles ne s'ex√©cutent que lorsqu'une action est appel√©e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 S√©lection de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©lectionner des colonnes\n",
    "print(\"üìå S√©lection de colonnes :\")\n",
    "df.select(\"nom\", \"ville\").show()\n",
    "\n",
    "# Avec alias\n",
    "print(\"\\nüìå Avec alias :\")\n",
    "df.select(\n",
    "    F.col(\"nom\").alias(\"employee_name\"),\n",
    "    F.col(\"salaire\").alias(\"salary\")\n",
    ").show()\n",
    "\n",
    "# S√©lectionner avec expressions\n",
    "print(\"\\nüìå Avec expressions :\")\n",
    "df.select(\n",
    "    \"nom\",\n",
    "    (F.col(\"salaire\") * 12).alias(\"salaire_annuel\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les lignes\n",
    "print(\"üîç Employ√©s de Paris :\")\n",
    "df.filter(F.col(\"ville\") == \"Paris\").show()\n",
    "\n",
    "# Filtres multiples avec AND\n",
    "print(\"\\nüîç Employ√©s de Paris avec salaire > 50000 :\")\n",
    "df.filter(\n",
    "    (F.col(\"ville\") == \"Paris\") & \n",
    "    (F.col(\"salaire\") > 50000)\n",
    ").show()\n",
    "\n",
    "# Filtres avec OR\n",
    "print(\"\\nüîç Employ√©s de Paris OU Lyon :\")\n",
    "df.filter(\n",
    "    (F.col(\"ville\") == \"Paris\") | \n",
    "    (F.col(\"ville\") == \"Lyon\")\n",
    ").show()\n",
    "\n",
    "# Filtrer avec IN\n",
    "print(\"\\nüîç Villes avec IN :\")\n",
    "df.filter(F.col(\"ville\").isin([\"Paris\", \"Lyon\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtres avanc√©s\n",
    "print(\"üîç Noms commen√ßant par 'A' :\")\n",
    "df.filter(F.col(\"nom\").startswith(\"A\")).show()\n",
    "\n",
    "print(\"\\nüîç Noms contenant 'li' :\")\n",
    "df.filter(F.col(\"nom\").contains(\"li\")).show()\n",
    "\n",
    "print(\"\\nüîç Age entre 25 et 30 :\")\n",
    "df.filter(F.col(\"age\").between(25, 30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Ajouter et modifier des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une nouvelle colonne\n",
    "df_with_bonus = df.withColumn(\n",
    "    \"bonus\",\n",
    "    F.col(\"salaire\") * 0.1\n",
    ")\n",
    "\n",
    "print(\"‚ûï Ajout de la colonne 'bonus' :\")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# Modifier une colonne existante\n",
    "df_modified = df.withColumn(\n",
    "    \"salaire\",\n",
    "    F.col(\"salaire\") * 1.05  # Augmentation de 5%\n",
    ")\n",
    "\n",
    "print(\"\\n‚úèÔ∏è Salaire augment√© de 5% :\")\n",
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter plusieurs colonnes\n",
    "df_enriched = df \\\n",
    "    .withColumn(\"salaire_mensuel\", F.col(\"salaire\")) \\\n",
    "    .withColumn(\"salaire_annuel\", F.col(\"salaire\") * 12) \\\n",
    "    .withColumn(\"bonus\", F.col(\"salaire\") * 0.1) \\\n",
    "    .withColumn(\"total_annuel\", F.col(\"salaire_annuel\") + F.col(\"bonus\"))\n",
    "\n",
    "print(\"üìä DataFrame enrichi :\")\n",
    "df_enriched.select(\"nom\", \"salaire_mensuel\", \"salaire_annuel\", \"bonus\", \"total_annuel\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Renommer et supprimer des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer une colonne\n",
    "df_renamed = df.withColumnRenamed(\"nom\", \"employee_name\")\n",
    "print(\"‚úèÔ∏è Colonne renomm√©e :\")\n",
    "df_renamed.show(3)\n",
    "\n",
    "# Supprimer des colonnes\n",
    "df_dropped = df.drop(\"age\", \"ville\")\n",
    "print(\"\\nüóëÔ∏è Colonnes supprim√©es :\")\n",
    "df_dropped.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier par salaire (ascendant)\n",
    "print(\"üìä Tri par salaire (croissant) :\")\n",
    "df.orderBy(\"salaire\").show()\n",
    "\n",
    "# Trier par salaire (descendant)\n",
    "print(\"\\nüìä Tri par salaire (d√©croissant) :\")\n",
    "df.orderBy(F.col(\"salaire\").desc()).show()\n",
    "\n",
    "# Tri multiple\n",
    "print(\"\\nüìä Tri par ville puis salaire :\")\n",
    "df.orderBy(\"ville\", F.col(\"salaire\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Agr√©gations et GroupBy\n",
    "\n",
    "Les agr√©gations permettent de calculer des statistiques sur les donn√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Agr√©gations simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques de base\n",
    "print(\"üìä Statistiques simples :\")\n",
    "df.select(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n",
    "    F.min(\"salaire\").alias(\"salaire_min\"),\n",
    "    F.max(\"salaire\").alias(\"salaire_max\"),\n",
    "    F.sum(\"salaire\").alias(\"salaire_total\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agr√©gations multiples\n",
    "from pyspark.sql.functions import stddev, variance\n",
    "\n",
    "print(\"üìä Statistiques avanc√©es :\")\n",
    "df.agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.avg(\"age\").alias(\"age_moyen\"),\n",
    "    F.stddev(\"salaire\").alias(\"salaire_stddev\"),\n",
    "    F.variance(\"salaire\").alias(\"salaire_variance\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouper par ville\n",
    "print(\"üìä Statistiques par ville :\")\n",
    "df.groupBy(\"ville\").agg(\n",
    "    F.count(\"*\").alias(\"nb_employes\"),\n",
    "    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n",
    "    F.min(\"salaire\").alias(\"salaire_min\"),\n",
    "    F.max(\"salaire\").alias(\"salaire_max\")\n",
    ").orderBy(\"ville\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame plus complexe pour les exemples\n",
    "data_ventes = [\n",
    "    (\"2024-01\", \"Paris\", \"Produit A\", 100, 1500),\n",
    "    (\"2024-01\", \"Paris\", \"Produit B\", 50, 2000),\n",
    "    (\"2024-01\", \"Lyon\", \"Produit A\", 75, 1200),\n",
    "    (\"2024-02\", \"Paris\", \"Produit A\", 120, 1800),\n",
    "    (\"2024-02\", \"Lyon\", \"Produit B\", 60, 2400),\n",
    "    (\"2024-02\", \"Marseille\", \"Produit A\", 90, 1350),\n",
    "]\n",
    "\n",
    "columns_ventes = [\"mois\", \"ville\", \"produit\", \"quantite\", \"montant\"]\n",
    "df_ventes = spark.createDataFrame(data_ventes, columns_ventes)\n",
    "\n",
    "print(\"üìä Donn√©es de ventes :\")\n",
    "df_ventes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GroupBy multiple\n",
    "print(\"üìä Ventes par mois et ville :\")\n",
    "df_ventes.groupBy(\"mois\", \"ville\").agg(\n",
    "    F.sum(\"quantite\").alias(\"total_quantite\"),\n",
    "    F.sum(\"montant\").alias(\"total_montant\"),\n",
    "    F.count(\"*\").alias(\"nb_transactions\")\n",
    ").orderBy(\"mois\", \"ville\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agr√©gations conditionnelles\n",
    "print(\"üìä Agr√©gations conditionnelles :\")\n",
    "df_ventes.groupBy(\"ville\").agg(\n",
    "    F.sum(\"montant\").alias(\"total\"),\n",
    "    F.sum(F.when(F.col(\"produit\") == \"Produit A\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_a\"),\n",
    "    F.sum(F.when(F.col(\"produit\") == \"Produit B\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_b\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking dans chaque ville\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"ville\").orderBy(F.col(\"salaire\").desc())\n",
    "\n",
    "df_ranked = df.withColumn(\n",
    "    \"rank\",\n",
    "    F.row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "print(\"üèÜ Ranking des salaires par ville :\")\n",
    "df_ranked.orderBy(\"ville\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculs cumulatifs\n",
    "window_cumul = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_cumul = df.withColumn(\n",
    "    \"salaire_cumul\",\n",
    "    F.sum(\"salaire\").over(window_cumul)\n",
    ")\n",
    "\n",
    "print(\"üìà Salaire cumul√© par ville :\")\n",
    "df_cumul.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_cumul\").orderBy(\"ville\", \"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de moyennes mobiles\n",
    "window_rolling = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(-1, 1)\n",
    "\n",
    "df_rolling = df.withColumn(\n",
    "    \"salaire_avg_3\",\n",
    "    F.avg(\"salaire\").over(window_rolling)\n",
    ")\n",
    "\n",
    "print(\"üìä Moyenne mobile sur 3 lignes :\")\n",
    "df_rolling.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_avg_3\").orderBy(\"ville\", \"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Jointures\n",
    "\n",
    "Les jointures permettent de combiner plusieurs DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Cr√©er des DataFrames pour les exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame employ√©s\n",
    "employes = spark.createDataFrame([\n",
    "    (1, \"Alice\", \"IT\"),\n",
    "    (2, \"Bob\", \"Finance\"),\n",
    "    (3, \"Charlie\", \"IT\"),\n",
    "    (4, \"David\", \"HR\")\n",
    "], [\"emp_id\", \"nom\", \"dept_id\"])\n",
    "\n",
    "# DataFrame d√©partements\n",
    "departements = spark.createDataFrame([\n",
    "    (\"IT\", \"Information Technology\", \"Paris\"),\n",
    "    (\"Finance\", \"Finance Department\", \"Lyon\"),\n",
    "    (\"HR\", \"Human Resources\", \"Marseille\"),\n",
    "    (\"Marketing\", \"Marketing Department\", \"Paris\")\n",
    "], [\"dept_id\", \"dept_name\", \"location\"])\n",
    "\n",
    "print(\"üë• Employ√©s :\")\n",
    "employes.show()\n",
    "\n",
    "print(\"\\nüè¢ D√©partements :\")\n",
    "departements.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Types de jointures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN (par d√©faut)\n",
    "print(\"üîó INNER JOIN :\")\n",
    "employes.join(departements, \"dept_id\", \"inner\").show()\n",
    "\n",
    "# LEFT JOIN\n",
    "print(\"\\nüîó LEFT JOIN :\")\n",
    "employes.join(departements, \"dept_id\", \"left\").show()\n",
    "\n",
    "# RIGHT JOIN\n",
    "print(\"\\nüîó RIGHT JOIN :\")\n",
    "employes.join(departements, \"dept_id\", \"right\").show()\n",
    "\n",
    "# FULL OUTER JOIN\n",
    "print(\"\\nüîó FULL OUTER JOIN :\")\n",
    "employes.join(departements, \"dept_id\", \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure avec colonnes diff√©rentes\n",
    "employes_alt = employes.withColumnRenamed(\"dept_id\", \"department\")\n",
    "\n",
    "print(\"üîó Jointure avec colonnes diff√©rentes :\")\n",
    "employes_alt.join(\n",
    "    departements,\n",
    "    employes_alt.department == departements.dept_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    employes_alt[\"*\"],\n",
    "    departements.dept_name,\n",
    "    departements.location\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointures multiples\n",
    "salaires = spark.createDataFrame([\n",
    "    (1, 45000),\n",
    "    (2, 55000),\n",
    "    (3, 50000),\n",
    "    (4, 48000)\n",
    "], [\"emp_id\", \"salaire\"])\n",
    "\n",
    "print(\"üîó Jointures multiples :\")\n",
    "result = employes \\\n",
    "    .join(departements, \"dept_id\", \"inner\") \\\n",
    "    .join(salaires, \"emp_id\", \"inner\")\n",
    "\n",
    "result.select(\"nom\", \"dept_name\", \"location\", \"salaire\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Lecture et √©criture de fichiers\n",
    "\n",
    "Spark supporte de nombreux formats de fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er des donn√©es de test\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# √âcrire en CSV\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"data/employes.csv\")\n",
    "\n",
    "print(\"‚úÖ CSV √©crit\")\n",
    "\n",
    "# Lire le CSV\n",
    "df_from_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/employes.csv\")\n",
    "\n",
    "print(\"\\nüìÇ CSV lu :\")\n",
    "df_from_csv.show(3)\n",
    "df_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âcrire en JSON\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"data/employes.json\")\n",
    "\n",
    "print(\"‚úÖ JSON √©crit\")\n",
    "\n",
    "# Lire le JSON\n",
    "df_from_json = spark.read.json(\"data/employes.json\")\n",
    "\n",
    "print(\"\\nüìÇ JSON lu :\")\n",
    "df_from_json.show(3)\n",
    "df_from_json.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Parquet (Format recommand√©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âcrire en Parquet\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/employes.parquet\")\n",
    "\n",
    "print(\"‚úÖ Parquet √©crit\")\n",
    "\n",
    "# Lire le Parquet\n",
    "df_from_parquet = spark.read.parquet(\"data/employes.parquet\")\n",
    "\n",
    "print(\"\\nüìÇ Parquet lu :\")\n",
    "df_from_parquet.show(3)\n",
    "df_from_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet avec partitionnement\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ville\") \\\n",
    "    .parquet(\"data/employes_partitioned.parquet\")\n",
    "\n",
    "print(\"‚úÖ Parquet partitionn√© √©crit\")\n",
    "\n",
    "# Lire avec filtre de partition (tr√®s performant)\n",
    "df_paris = spark.read \\\n",
    "    .parquet(\"data/employes_partitioned.parquet\") \\\n",
    "    .filter(F.col(\"ville\") == \"Paris\")\n",
    "\n",
    "print(\"\\nüìÇ Parquet partitionn√© lu (ville=Paris) :\")\n",
    "df_paris.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Options d'√©criture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode d'√©criture\n",
    "# - \"overwrite\" : √âcrase les donn√©es existantes\n",
    "# - \"append\" : Ajoute aux donn√©es existantes\n",
    "# - \"ignore\" : Ne fait rien si le fichier existe\n",
    "# - \"error\" (default) : Erreur si le fichier existe\n",
    "\n",
    "# Compression\n",
    "df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .parquet(\"data/employes_compressed.parquet\")\n",
    "\n",
    "print(\"‚úÖ Parquet compress√© √©crit\")\n",
    "\n",
    "# Contr√¥ler le nombre de fichiers\n",
    "df.coalesce(1).write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"data/employes_single_file.csv\")\n",
    "\n",
    "print(\"‚úÖ CSV en un seul fichier √©crit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Spark SQL\n",
    "\n",
    "Spark permet d'ex√©cuter des requ√™tes SQL sur les DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Cr√©er des vues temporaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er une vue temporaire\n",
    "df.createOrReplaceTempView(\"employes\")\n",
    "df_ventes.createOrReplaceTempView(\"ventes\")\n",
    "\n",
    "print(\"‚úÖ Vues temporaires cr√©√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Requ√™tes SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requ√™te SQL simple\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT nom, ville, salaire\n",
    "    FROM employes\n",
    "    WHERE salaire > 50000\n",
    "    ORDER BY salaire DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Employ√©s avec salaire > 50000 :\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agr√©gation avec SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ville,\n",
    "        COUNT(*) as nb_employes,\n",
    "        AVG(salaire) as salaire_moyen,\n",
    "        MIN(salaire) as salaire_min,\n",
    "        MAX(salaire) as salaire_max\n",
    "    FROM employes\n",
    "    GROUP BY ville\n",
    "    ORDER BY salaire_moyen DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Statistiques par ville :\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions en SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        nom,\n",
    "        ville,\n",
    "        salaire,\n",
    "        ROW_NUMBER() OVER (PARTITION BY ville ORDER BY salaire DESC) as rank_ville,\n",
    "        DENSE_RANK() OVER (ORDER BY salaire DESC) as rank_global\n",
    "    FROM employes\n",
    "    ORDER BY ville, rank_ville\n",
    "\"\"\")\n",
    "\n",
    "print(\"üèÜ Ranking avec SQL :\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTE (Common Table Expression)\n",
    "result = spark.sql(\"\"\"\n",
    "    WITH stats_ville AS (\n",
    "        SELECT \n",
    "            ville,\n",
    "            AVG(salaire) as salaire_moyen\n",
    "        FROM employes\n",
    "        GROUP BY ville\n",
    "    )\n",
    "    SELECT \n",
    "        e.nom,\n",
    "        e.ville,\n",
    "        e.salaire,\n",
    "        s.salaire_moyen,\n",
    "        ROUND(e.salaire - s.salaire_moyen, 2) as diff_moyenne\n",
    "    FROM employes e\n",
    "    JOIN stats_ville s ON e.ville = s.ville\n",
    "    ORDER BY e.ville, e.salaire DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Comparaison √† la moyenne :\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Gestion des donn√©es manquantes\n",
    "\n",
    "PySpark offre plusieurs m√©thodes pour g√©rer les nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un DataFrame avec des nulls\n",
    "data_with_nulls = [\n",
    "    (1, \"Alice\", 25, \"Paris\", 45000),\n",
    "    (2, \"Bob\", None, \"Lyon\", 55000),\n",
    "    (3, \"Charlie\", 35, None, 60000),\n",
    "    (4, \"David\", 28, \"Marseille\", None),\n",
    "    (5, None, 32, \"Lyon\", 58000)\n",
    "]\n",
    "\n",
    "df_nulls = spark.createDataFrame(data_with_nulls, [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"])\n",
    "\n",
    "print(\"üìä DataFrame avec nulls :\")\n",
    "df_nulls.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes avec des nulls\n",
    "print(\"üóëÔ∏è Suppression de toutes les lignes avec nulls :\")\n",
    "df_nulls.dropna().show()\n",
    "\n",
    "# Supprimer seulement si toutes les colonnes sont nulles\n",
    "print(\"\\nüóëÔ∏è Suppression si toutes les colonnes sont nulles :\")\n",
    "df_nulls.dropna(how='all').show()\n",
    "\n",
    "# Supprimer les nulls sur des colonnes sp√©cifiques\n",
    "print(\"\\nüóëÔ∏è Suppression des nulls sur 'nom' et 'ville' :\")\n",
    "df_nulls.dropna(subset=[\"nom\", \"ville\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplir les nulls avec des valeurs par d√©faut\n",
    "print(\"‚ú® Remplir tous les nulls avec 0 :\")\n",
    "df_nulls.fillna(0).show()\n",
    "\n",
    "# Remplir avec des valeurs diff√©rentes par colonne\n",
    "print(\"\\n‚ú® Remplir avec des valeurs sp√©cifiques :\")\n",
    "df_nulls.fillna({\n",
    "    \"nom\": \"Inconnu\",\n",
    "    \"age\": 30,\n",
    "    \"ville\": \"Non sp√©cifi√©\",\n",
    "    \"salaire\": 50000\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les nulls par la moyenne/m√©diane\n",
    "from pyspark.sql.functions import mean, median\n",
    "\n",
    "# Calculer la moyenne\n",
    "age_moyen = df_nulls.select(mean(\"age\")).first()[0]\n",
    "salaire_moyen = df_nulls.select(mean(\"salaire\")).first()[0]\n",
    "\n",
    "print(f\"Age moyen : {age_moyen}\")\n",
    "print(f\"Salaire moyen : {salaire_moyen}\")\n",
    "\n",
    "# Remplir avec les moyennes\n",
    "df_filled = df_nulls.fillna({\n",
    "    \"age\": age_moyen,\n",
    "    \"salaire\": salaire_moyen\n",
    "})\n",
    "\n",
    "print(\"\\n‚ú® Nulls remplac√©s par les moyennes :\")\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ UDFs (User Defined Functions)\n",
    "\n",
    "Les UDFs permettent d'appliquer des fonctions Python personnalis√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# D√©finir une fonction Python\n",
    "def categoriser_age(age):\n",
    "    if age is None:\n",
    "        return \"Inconnu\"\n",
    "    elif age < 30:\n",
    "        return \"Junior\"\n",
    "    elif age < 40:\n",
    "        return \"Senior\"\n",
    "    else:\n",
    "        return \"Expert\"\n",
    "\n",
    "# Enregistrer comme UDF\n",
    "categoriser_age_udf = udf(categoriser_age, StringType())\n",
    "\n",
    "# Utiliser l'UDF\n",
    "df_with_category = df.withColumn(\n",
    "    \"categorie\",\n",
    "    categoriser_age_udf(F.col(\"age\"))\n",
    ")\n",
    "\n",
    "print(\"üîß DataFrame avec cat√©gorie :\")\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF avec plusieurs param√®tres\n",
    "def calculer_bonus(salaire, performance):\n",
    "    if salaire is None or performance is None:\n",
    "        return 0\n",
    "    return salaire * performance * 0.1\n",
    "\n",
    "calculer_bonus_udf = udf(calculer_bonus, IntegerType())\n",
    "\n",
    "# Ajouter une colonne de performance\n",
    "df_perf = df.withColumn(\"performance\", F.lit(1.2))\n",
    "\n",
    "# Appliquer l'UDF\n",
    "df_with_bonus = df_perf.withColumn(\n",
    "    \"bonus\",\n",
    "    calculer_bonus_udf(F.col(\"salaire\"), F.col(\"performance\"))\n",
    ")\n",
    "\n",
    "print(\"üí∞ DataFrame avec bonus :\")\n",
    "df_with_bonus.select(\"nom\", \"salaire\", \"performance\", \"bonus\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Attention : Les UDFs sont moins performantes que les fonctions natives\n",
    "# Pr√©f√©rer les fonctions natives quand c'est possible\n",
    "\n",
    "# Avec UDF (plus lent)\n",
    "double_udf = udf(lambda x: x * 2, IntegerType())\n",
    "df.withColumn(\"salaire_double_udf\", double_udf(F.col(\"salaire\")))\n",
    "\n",
    "# Avec fonction native (plus rapide)\n",
    "df.withColumn(\"salaire_double\", F.col(\"salaire\") * 2)\n",
    "\n",
    "print(\"‚úÖ Pr√©f√©rez toujours les fonctions natives !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Optimisation et Performance\n",
    "\n",
    "Quelques techniques pour optimiser vos jobs Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Partitionnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le nombre de partitions\n",
    "print(f\"Nombre de partitions : {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Repartitionner (shuffle)\n",
    "df_repartitioned = df.repartition(4)\n",
    "print(f\"Apr√®s repartition : {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce (pas de shuffle, moins co√ªteux)\n",
    "df_coalesced = df.coalesce(2)\n",
    "print(f\"Apr√®s coalesce : {df_coalesced.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartitionner par colonne (utile avant les groupBy)\n",
    "df_repartitioned_by_ville = df.repartition(\"ville\")\n",
    "\n",
    "# Maintenant les groupBy sur 'ville' seront plus efficaces\n",
    "df_repartitioned_by_ville.groupBy(\"ville\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache un DataFrame en m√©moire\n",
    "df_cached = df.cache()\n",
    "\n",
    "# Premi√®re action : calcul complet\n",
    "print(\"Premi√®re action (calcul complet) :\")\n",
    "df_cached.count()\n",
    "\n",
    "# Deuxi√®me action : utilise le cache (beaucoup plus rapide)\n",
    "print(\"\\nDeuxi√®me action (utilise le cache) :\")\n",
    "df_cached.show()\n",
    "\n",
    "# Lib√©rer le cache\n",
    "df_cached.unpersist()\n",
    "print(\"\\n‚úÖ Cache lib√©r√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist avec diff√©rents niveaux de stockage\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# MEMORY_ONLY : En m√©moire uniquement\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# MEMORY_AND_DISK : M√©moire + disque si n√©cessaire\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# DISK_ONLY : Disque uniquement\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "print(\"‚úÖ Diff√©rents niveaux de persistance disponibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Broadcast Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les petits DataFrames (< 10MB), utilisez broadcast\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# departements est petit, on le broadcast\n",
    "result = employes.join(\n",
    "    broadcast(departements),\n",
    "    \"dept_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"üöÄ Broadcast join :\")\n",
    "result.show()\n",
    "\n",
    "# √âvite le shuffle, beaucoup plus rapide !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.4 √âviter les UDFs quand possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Avec UDF (lent)\n",
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "add_ten_udf = udf(add_ten, IntegerType())\n",
    "df.withColumn(\"salaire_plus_10_udf\", add_ten_udf(F.col(\"salaire\")))\n",
    "\n",
    "# ‚úÖ Avec fonction native (rapide)\n",
    "df.withColumn(\"salaire_plus_10\", F.col(\"salaire\") + 10)\n",
    "\n",
    "print(\"‚úÖ Les fonctions natives sont toujours plus rapides !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.5 Explain Plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voir le plan d'ex√©cution\n",
    "print(\"üìä Plan d'ex√©cution :\")\n",
    "df.filter(F.col(\"salaire\") > 50000).explain()\n",
    "\n",
    "# Plan d√©taill√©\n",
    "print(\"\\nüìä Plan d'ex√©cution d√©taill√© :\")\n",
    "df.filter(F.col(\"salaire\") > 50000).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üîü Structured Streaming (Bonus)\n",
    "\n",
    "Traitement de donn√©es en temps r√©el avec Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un streaming DataFrame depuis un dossier\n",
    "# Les fichiers ajout√©s au dossier seront trait√©s automatiquement\n",
    "\n",
    "schema_stream = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Lire un stream depuis un dossier\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(schema_stream) \\\n",
    "    .json(\"data/stream_input/\")\n",
    "\n",
    "print(\"‚úÖ Stream cr√©√©\")\n",
    "print(f\"Is streaming: {stream_df.isStreaming}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations sur le stream (comme un DataFrame normal)\n",
    "stream_processed = stream_df \\\n",
    "    .filter(F.col(\"value\") > 100) \\\n",
    "    .groupBy(\"user_id\", \"action\") \\\n",
    "    .count()\n",
    "\n",
    "print(\"‚úÖ Transformations appliqu√©es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âcrire le stream vers un sink\n",
    "# Note: Ce code est un exemple, il n√©cessite un stream actif pour s'ex√©cuter\n",
    "\n",
    "# query = stream_processed.writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "\n",
    "# # Attendre la fin du stream\n",
    "# query.awaitTermination()\n",
    "\n",
    "print(\"üí° Exemple de streaming (n√©cessite des donn√©es en entr√©e pour s'ex√©cuter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Pipeline ETL Complet avec PySpark\n",
    "\n",
    "Construisons un pipeline ETL complet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Cr√©er la structure de dossiers\n",
    "os.makedirs('spark_pipeline/raw', exist_ok=True)\n",
    "os.makedirs('spark_pipeline/processed', exist_ok=True)\n",
    "os.makedirs('spark_pipeline/output', exist_ok=True)\n",
    "os.makedirs('spark_pipeline/logs', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Structure cr√©√©e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(spark, path):\n",
    "    \"\"\"Extrait des donn√©es depuis plusieurs sources\"\"\"\n",
    "    print(f\"üì• Extraction depuis {path}\")\n",
    "    \n",
    "    # Cr√©er des donn√©es de test\n",
    "    data = [\n",
    "        (1, \"2024-01-15\", \"Paris\", \"Produit A\", 100, 1500, \"online\"),\n",
    "        (2, \"2024-01-15\", \"Lyon\", \"Produit B\", 50, 2000, \"store\"),\n",
    "        (3, \"2024-01-16\", \"Paris\", \"Produit A\", 75, 1200, \"online\"),\n",
    "        (4, \"2024-01-16\", \"Marseille\", \"Produit C\", 120, 1800, \"online\"),\n",
    "        (5, \"2024-01-17\", \"Lyon\", \"Produit B\", 60, 2400, \"store\"),\n",
    "        (6, \"2024-01-17\", None, \"Produit A\", 90, None, \"online\"),  # Donn√©es sales\n",
    "    ]\n",
    "    \n",
    "    columns = [\"id\", \"date\", \"ville\", \"produit\", \"quantite\", \"montant\", \"canal\"]\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    # Sauvegarder les donn√©es brutes\n",
    "    df.write.mode(\"overwrite\").parquet(f\"{path}/ventes_raw.parquet\")\n",
    "    \n",
    "    print(f\"‚úÖ {df.count()} lignes extraites\")\n",
    "    return df\n",
    "\n",
    "# Test\n",
    "df_raw = extract_data(spark, \"spark_pipeline/raw\")\n",
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    \"\"\"Transforme et nettoie les donn√©es\"\"\"\n",
    "    print(\"üîÑ Transformation des donn√©es\")\n",
    "    \n",
    "    # 1. Convertir la date\n",
    "    df = df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n",
    "    \n",
    "    # 2. G√©rer les valeurs manquantes\n",
    "    df = df.fillna({\n",
    "        \"ville\": \"Inconnu\",\n",
    "        \"montant\": 0\n",
    "    })\n",
    "    \n",
    "    # 3. Filtrer les donn√©es invalides\n",
    "    df = df.filter(\n",
    "        (F.col(\"quantite\") > 0) & \n",
    "        (F.col(\"montant\") >= 0)\n",
    "    )\n",
    "    \n",
    "    # 4. Cr√©er des colonnes d√©riv√©es\n",
    "    df = df.withColumn(\n",
    "        \"prix_unitaire\",\n",
    "        F.when(F.col(\"quantite\") > 0, F.col(\"montant\") / F.col(\"quantite\")).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"annee\",\n",
    "        F.year(F.col(\"date\"))\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"mois\",\n",
    "        F.month(F.col(\"date\"))\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"jour_semaine\",\n",
    "        F.dayofweek(F.col(\"date\"))\n",
    "    )\n",
    "    \n",
    "    # 5. Cat√©goriser\n",
    "    df = df.withColumn(\n",
    "        \"categorie_montant\",\n",
    "        F.when(F.col(\"montant\") < 1500, \"Faible\")\n",
    "         .when(F.col(\"montant\") < 2000, \"Moyen\")\n",
    "         .otherwise(\"√âlev√©\")\n",
    "    )\n",
    "    \n",
    "    # 6. Ajouter metadata\n",
    "    df = df.withColumn(\"processed_at\", F.current_timestamp())\n",
    "    \n",
    "    print(f\"‚úÖ {df.count()} lignes transform√©es\")\n",
    "    return df\n",
    "\n",
    "# Test\n",
    "df_transformed = transform_data(df_raw)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(df):\n",
    "    \"\"\"Cr√©e des agr√©gations m√©tier\"\"\"\n",
    "    print(\"üìä Agr√©gation des donn√©es\")\n",
    "    \n",
    "    # Agr√©gation par ville et produit\n",
    "    agg_ville_produit = df.groupBy(\"ville\", \"produit\").agg(\n",
    "        F.sum(\"quantite\").alias(\"total_quantite\"),\n",
    "        F.sum(\"montant\").alias(\"total_montant\"),\n",
    "        F.avg(\"prix_unitaire\").alias(\"prix_moyen\"),\n",
    "        F.count(\"*\").alias(\"nb_transactions\")\n",
    "    ).orderBy(\"ville\", \"produit\")\n",
    "    \n",
    "    print(\"\\nüìä Agr√©gation par ville et produit :\")\n",
    "    agg_ville_produit.show()\n",
    "    \n",
    "    # Agr√©gation par canal\n",
    "    agg_canal = df.groupBy(\"canal\").agg(\n",
    "        F.sum(\"montant\").alias(\"total_montant\"),\n",
    "        F.count(\"*\").alias(\"nb_transactions\"),\n",
    "        F.avg(\"montant\").alias(\"montant_moyen\")\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä Agr√©gation par canal :\")\n",
    "    agg_canal.show()\n",
    "    \n",
    "    # Agr√©gation temporelle\n",
    "    agg_temporelle = df.groupBy(\"annee\", \"mois\").agg(\n",
    "        F.sum(\"montant\").alias(\"total_montant\"),\n",
    "        F.count(\"*\").alias(\"nb_transactions\")\n",
    "    ).orderBy(\"annee\", \"mois\")\n",
    "    \n",
    "    print(\"\\nüìä Agr√©gation temporelle :\")\n",
    "    agg_temporelle.show()\n",
    "    \n",
    "    return {\n",
    "        \"ville_produit\": agg_ville_produit,\n",
    "        \"canal\": agg_canal,\n",
    "        \"temporelle\": agg_temporelle\n",
    "    }\n",
    "\n",
    "# Test\n",
    "aggregations = aggregate_data(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, aggregations, output_path):\n",
    "    \"\"\"Charge les donn√©es dans le datalake\"\"\"\n",
    "    print(\"üíæ Chargement des donn√©es\")\n",
    "    \n",
    "    # 1. Donn√©es transform√©es (partitionn√©es par date)\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"annee\", \"mois\") \\\n",
    "        .parquet(f\"{output_path}/ventes_transformed\")\n",
    "    print(\"‚úÖ Donn√©es transform√©es sauvegard√©es\")\n",
    "    \n",
    "    # 2. Agr√©gations\n",
    "    for name, agg_df in aggregations.items():\n",
    "        agg_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .parquet(f\"{output_path}/agg_{name}\")\n",
    "        print(f\"‚úÖ Agr√©gation '{name}' sauvegard√©e\")\n",
    "    \n",
    "    # 3. Export CSV pour l'analyse\n",
    "    df.coalesce(1).write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(f\"{output_path}/ventes_export.csv\")\n",
    "    print(\"‚úÖ Export CSV cr√©√©\")\n",
    "\n",
    "# Test\n",
    "load_data(df_transformed, aggregations, \"spark_pipeline/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Pipeline complet orchestr√©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(spark):\n",
    "    \"\"\"Ex√©cute le pipeline ETL complet\"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(\"=\"*60)\n",
    "    print(\"üöÄ D√âMARRAGE DU PIPELINE PYSPARK\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # EXTRACT\n",
    "        print(\"\\nüì• PHASE 1: EXTRACTION\")\n",
    "        df_raw = extract_data(spark, \"spark_pipeline/raw\")\n",
    "        \n",
    "        # TRANSFORM\n",
    "        print(\"\\nüîÑ PHASE 2: TRANSFORMATION\")\n",
    "        df_transformed = transform_data(df_raw)\n",
    "        \n",
    "        # Cache pour les performances\n",
    "        df_transformed.cache()\n",
    "        \n",
    "        # AGGREGATE\n",
    "        print(\"\\nüìä PHASE 3: AGR√âGATION\")\n",
    "        aggregations = aggregate_data(df_transformed)\n",
    "        \n",
    "        # LOAD\n",
    "        print(\"\\nüíæ PHASE 4: CHARGEMENT\")\n",
    "        load_data(df_transformed, aggregations, \"spark_pipeline/output\")\n",
    "        \n",
    "        # STATISTICS\n",
    "        duration = time.time() - start_time\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìä STATISTIQUES DU PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Dur√©e totale: {duration:.2f}s\")\n",
    "        print(f\"Lignes trait√©es: {df_transformed.count()}\")\n",
    "        print(f\"Partitions: {df_transformed.rdd.getNumPartitions()}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"‚úÖ PIPELINE TERMIN√â AVEC SUCC√àS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Lib√©rer le cache\n",
    "        df_transformed.unpersist()\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERREUR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Ex√©cuter le pipeline\n",
    "success = run_pipeline(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì R√©sum√© et Prochaines √âtapes\n",
    "\n",
    "## Ce que vous avez appris ‚úÖ\n",
    "\n",
    "1. **Fondamentaux Spark** : Architecture, concepts, SparkSession\n",
    "2. **DataFrames** : Cr√©ation, transformations, actions\n",
    "3. **Transformations** : Select, filter, withColumn, orderBy\n",
    "4. **Agr√©gations** : GroupBy, agr√©gations complexes, window functions\n",
    "5. **Jointures** : Inner, left, right, outer, broadcast\n",
    "6. **I/O** : CSV, JSON, Parquet avec partitionnement\n",
    "7. **Spark SQL** : Requ√™tes SQL, CTEs, window functions\n",
    "8. **Optimisation** : Partitionnement, caching, broadcast joins\n",
    "9. **UDFs** : Fonctions personnalis√©es\n",
    "10. **Streaming** : Traitement temps r√©el (introduction)\n",
    "11. **Pipeline ETL** : Architecture compl√®te production-ready\n",
    "\n",
    "## Diff√©rences cl√©s Pandas vs PySpark üîÑ\n",
    "\n",
    "| Aspect | Pandas | PySpark |\n",
    "|--------|--------|----------|\n",
    "| **Ex√©cution** | Eager (imm√©diate) | Lazy (diff√©r√©e) |\n",
    "| **Donn√©es** | En m√©moire (single machine) | Distribu√©es (cluster) |\n",
    "| **Scalabilit√©** | Limit√© √† la RAM | Quasi illimit√© |\n",
    "| **API** | `df[df['col'] > 5]` | `df.filter(F.col('col') > 5)` |\n",
    "| **Performances** | Rapide pour petites donn√©es | Rapide pour Big Data |\n",
    "\n",
    "## Quand utiliser PySpark ? ü§î\n",
    "\n",
    "‚úÖ **Utilisez PySpark si :**\n",
    "- Donn√©es > 10 GB\n",
    "- Besoin de parall√©lisation\n",
    "- Traitement distribu√© n√©cessaire\n",
    "- Streaming en temps r√©el\n",
    "\n",
    "‚ùå **Utilisez Pandas si :**\n",
    "- Donn√©es < 10 GB\n",
    "- Prototypage rapide\n",
    "- Analyses exploratoires\n",
    "- Machine learning local\n",
    "\n",
    "## Prochaines √©tapes üöÄ\n",
    "\n",
    "1. **Pratiquer** : Cr√©er des pipelines avec vos propres donn√©es\n",
    "2. **Approfondir** :\n",
    "   - MLlib (Machine Learning)\n",
    "   - GraphX (Graph processing)\n",
    "   - Delta Lake (ACID transactions)\n",
    "3. **Production** :\n",
    "   - Databricks\n",
    "   - AWS EMR\n",
    "   - Azure Synapse\n",
    "   - Google Dataproc\n",
    "4. **Orchestration** :\n",
    "   - Apache Airflow\n",
    "   - Prefect\n",
    "   - Dagster\n",
    "\n",
    "## Ressources üìö\n",
    "\n",
    "- [Documentation PySpark](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark by Examples](https://sparkbyexamples.com/)\n",
    "- [Learning Spark (O'Reilly)](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training)\n",
    "\n",
    "---\n",
    "\n",
    "**F√©licitations ! üéâ Vous ma√Ætrisez maintenant les fondamentaux de PySpark !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üß™ Quiz Final\n",
    "\n",
    "Testez vos connaissances PySpark !\n",
    "\n",
    "### ‚ùì Q1. Quelle est la diff√©rence principale entre une **transformation** et une **action** dans Spark ?\n",
    "a) Les transformations sont plus rapides que les actions  \n",
    "b) Les transformations sont lazy (√©valu√©es plus tard), les actions d√©clenchent l'ex√©cution  \n",
    "c) Les transformations modifient les donn√©es, les actions les affichent seulement  \n",
    "d) Il n'y a pas de diff√©rence\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Les **transformations** sont **lazy** (comme `filter`, `select`, `groupBy`) : elles construisent un plan d'ex√©cution mais ne l'ex√©cutent pas imm√©diatement. Les **actions** (comme `count`, `show`, `collect`) d√©clenchent l'ex√©cution r√©elle du job Spark.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q2. Quel format de fichier est **recommand√©** pour stocker des donn√©es dans un data lake avec Spark ?\n",
    "a) CSV  \n",
    "b) JSON  \n",
    "c) Parquet  \n",
    "d) Excel\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : c** ‚Äì **Parquet** est le format recommand√© car il est **columnaire** (lecture rapide de colonnes sp√©cifiques), **compress√©** (prend moins d'espace), et conserve les **types de donn√©es** et le **sch√©ma**.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q3. Comment cr√©er une nouvelle colonne avec PySpark ?\n",
    "a) `df['new_col'] = df['old_col'] * 2`  \n",
    "b) `df.withColumn('new_col', F.col('old_col') * 2)`  \n",
    "c) `df.add_column('new_col', 'old_col * 2')`  \n",
    "d) `df.new_column('new_col', df['old_col'] * 2)`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì On utilise `withColumn('nom', expression)` avec `F.col()` pour r√©f√©rencer les colonnes. Les DataFrames Spark sont **immutables**, donc `withColumn` retourne un **nouveau** DataFrame.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q4. Quelle commande permet de voir le **plan d'ex√©cution** d'une requ√™te ?\n",
    "a) `df.show()`  \n",
    "b) `df.explain()`  \n",
    "c) `df.describe()`  \n",
    "d) `df.plan()`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì La commande `df.explain()` affiche le **plan d'ex√©cution physique** de Spark. Utilisez `df.explain(True)` pour voir le plan d√©taill√© avec toutes les √©tapes d'optimisation.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q5. Qu'est-ce qu'un **broadcast join** ?\n",
    "a) Une jointure qui diffuse les r√©sultats √† tous les n≈ìuds  \n",
    "b) Une jointure optimis√©e o√π la petite table est copi√©e sur tous les n≈ìuds  \n",
    "c) Une jointure qui utilise le r√©seau pour communiquer  \n",
    "d) Une jointure qui trie les donn√©es avant de les joindre\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Un **broadcast join** copie la **petite table** (< 10 MB) sur tous les n≈ìuds workers pour √©viter le shuffle. C'est beaucoup plus rapide ! Utilisez `F.broadcast(small_df)` pour forcer un broadcast join.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q6. Pourquoi utilise-t-on `cache()` ou `persist()` ?\n",
    "a) Pour sauvegarder les donn√©es sur disque  \n",
    "b) Pour acc√©l√©rer les calculs en gardant le DataFrame en m√©moire  \n",
    "c) Pour compresser les donn√©es  \n",
    "d) Pour cr√©er un backup automatique\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì `cache()` et `persist()` stockent le DataFrame **en m√©moire** pour √©viter de **recalculer** les m√™mes transformations plusieurs fois. Utilisez-les quand vous r√©utilisez un DataFrame plusieurs fois dans votre code.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q7. Quelle est la diff√©rence entre `repartition()` et `coalesce()` ?\n",
    "a) Aucune diff√©rence  \n",
    "b) `repartition()` fait un full shuffle, `coalesce()` r√©duit sans shuffle  \n",
    "c) `coalesce()` est plus lent que `repartition()`  \n",
    "d) `repartition()` supprime les donn√©es, `coalesce()` les garde\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì `repartition(n)` fait un **full shuffle** (co√ªteux) pour redistribuer les donn√©es. `coalesce(n)` **r√©duit** le nombre de partitions **sans shuffle** en combinant les partitions existantes. Utilisez `coalesce()` pour r√©duire les partitions, `repartition()` pour augmenter.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q8. Comment ex√©cuter une requ√™te SQL dans PySpark ?\n",
    "a) `df.sql('SELECT * FROM table')`  \n",
    "b) `spark.sql('SELECT * FROM table')` apr√®s avoir cr√©√© une vue temporaire  \n",
    "c) `df.query('SELECT * FROM table')`  \n",
    "d) `spark.execute('SELECT * FROM table')`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì On doit d'abord cr√©er une **vue temporaire** avec `df.createOrReplaceTempView('table')`, puis ex√©cuter `spark.sql('SELECT * FROM table')`.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q9. Quelle est la meilleure strat√©gie pour √©crire des donn√©es Parquet partitionn√©es ?\n",
    "a) `df.write.parquet('path')`  \n",
    "b) `df.write.partitionBy('date').parquet('path')`  \n",
    "c) `df.write.csv('path')`  \n",
    "d) `df.write.json('path')`\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Utiliser `partitionBy('colonne')` cr√©e une **hi√©rarchie de dossiers** par valeur de colonne (ex: `date=2024-01-01/`, `date=2024-01-02/`). Cela permet de **lire uniquement les partitions n√©cessaires** (partition pruning) et am√©liore consid√©rablement les performances.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì Q10. Quand devriez-vous utiliser PySpark plut√¥t que Pandas ?\n",
    "a) Toujours, PySpark est toujours meilleur  \n",
    "b) Quand les donn√©es d√©passent la m√©moire d'une seule machine (> 100 GB)  \n",
    "c) Pour les petits datasets (< 1 GB)  \n",
    "d) Jamais, Pandas est suffisant\n",
    "\n",
    "<details>\n",
    "<summary>üí° Voir la r√©ponse</summary>\n",
    "‚úÖ **R√©ponse : b** ‚Äì Utilisez **PySpark** quand vos donn√©es sont **trop volumineuses** pour tenir en m√©moire sur une seule machine (g√©n√©ralement > 100 GB), ou quand vous avez besoin de **traitement distribu√©** et de **parall√©lisation**. Pour les petits datasets (< 10 GB), **Pandas** est plus simple et souvent plus rapide.\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Votre score\n",
    "\n",
    "- **10/10** : üèÜ Expert PySpark ! Pr√™t pour la production\n",
    "- **8-9/10** : üåü Excellent ! Pratiquez les concepts avanc√©s\n",
    "- **6-7/10** : üí™ Bon niveau ! Revoyez les optimisations\n",
    "- **< 6/10** : üìö Relisez le notebook et pratiquez les exemples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Ressources\n",
    "\n",
    "- [Documentation PySpark](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark by Examples](https://sparkbyexamples.com/)\n",
    "- [Learning Spark (O'Reilly)](https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/)\n",
    "- [Databricks Academy](https://www.databricks.com/learn/training) ‚Äî Cours gratuits\n",
    "\n",
    "### üè≠ Plateformes Cloud\n",
    "\n",
    "| Plateforme | Service Spark |\n",
    "|------------|---------------|\n",
    "| **Databricks** | Databricks Lakehouse |\n",
    "| **AWS** | EMR (Elastic MapReduce) |\n",
    "| **Azure** | Synapse Analytics, HDInsight |\n",
    "| **GCP** | Dataproc |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Prochaine √©tape\n",
    "\n",
    "Tu ma√Ætrises maintenant le traitement Big Data avec PySpark ! \n",
    "\n",
    "Pour continuer ton parcours Data Engineering, \n",
    "\n",
    "üëâ **Module suivant : `12_orchestration_pipelines.ipynb`** ‚Äî Orchestration de pipelines\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **F√©licitations !** Tu as termin√© le module PySpark et le parcours sur les bases de donn√©es et le Big Data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fermer la SparkSession\n",
    "spark.stop()\n",
    "print(\"‚úÖ SparkSession ferm√©e\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
