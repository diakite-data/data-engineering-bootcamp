[
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre les commandes Bash essentielles pour manipuler des fichiers, automatiser des tÃ¢ches, et interagir avec ton environnement systÃ¨me â€” des compÃ©tences indispensables pour un Data Engineer !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 01_intro_data_engineering\n\n\nâœ… Requis\nAvoir accÃ¨s Ã  un terminal (Linux, Mac, ou Windows avec WSL/Git Bash)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nNaviguer dans lâ€™arborescence de fichiers\nManipuler des fichiers et dossiers\nFiltrer et rechercher dans des donnÃ©es\nÃ‰crire des scripts Bash pour automatiser des tÃ¢ches\nPlanifier des jobs avec cron",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ§  Câ€™est quoi le langage Bash ?",
    "text": "ğŸ§  Câ€™est quoi le langage Bash ?\nBash (abrÃ©viation de Bourne Again SHell) est un langage de commande et de script utilisÃ© dans la majoritÃ© des systÃ¨mes Unix/Linux (et mÃªme sous Windows via WSL ou Git Bash).\nIl te permet de :\n\nğŸ“‚ Naviguer dans les dossiers\n\nğŸ“„ Manipuler des fichiers et des donnÃ©es\n\nâš™ï¸ Automatiser des tÃ¢ches rÃ©pÃ©titives\n\nğŸ”„ Ã‰crire des scripts shell pour lancer des traitements de donnÃ©es\n\n\n\nğŸ§° Pourquoi câ€™est utile pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nLancer des pipelines ETL\npython etl_pipeline.py && echo \"Success\" \\|\\| echo \"Failed\"\n\n\nÃ‰crire des jobs cron\nExtraction automatique de donnÃ©es chaque nuit Ã  2h\n\n\nManipuler des fichiers\nFusionner 100 fichiers CSV en un seul\n\n\nOrchestrer des outils\nLancer Docker, Spark, ou Airflow depuis un script\n\n\nAnalyser des logs\nTrouver toutes les erreurs dans les logs du jour\n\n\n\n\nğŸ’¡ En bref : le Bash est ton couteau suisse pour parler avec ton ordinateur et piloter lâ€™Ã©cosystÃ¨me data.\n\n\nâ„¹ï¸ Le savais-tu ?\nLe mot Bash signifie â€œBourne Again SHellâ€, un jeu de mots sur :\n\nLe shell Unix original : le Bourne Shell (sh), dÃ©veloppÃ© dans les annÃ©es 1970 par Stephen Bourne\nLâ€™expression anglaise â€œborn againâ€ = renaÃ®tre\n\nBash est donc une nouvelle version amÃ©liorÃ©e du shell Bourne, libre, puissante, et utilisÃ©e par dÃ©faut dans la plupart des systÃ¨mes Unix/Linux modernes.\nğŸ“– Biographie de Stephen R. Bourne sur Wikipedia",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ’» Comment accÃ©der Ã  Bash ?",
    "text": "ğŸ’» Comment accÃ©der Ã  Bash ?\n\n\n\n\n\n\n\nSystÃ¨me\nComment y accÃ©der\n\n\n\n\nğŸ§ Linux\nBash est installÃ© par dÃ©faut. Ouvre un Terminal\n\n\nğŸ macOS\nOuvre Terminal (Applications â†’ Utilitaires â†’ Terminal)\n\n\nğŸªŸ Windows\nInstalle WSL (Windows Subsystem for Linux) ou Git Bash\n\n\n\n\nInstallation de WSL sur Windows\n# Dans PowerShell en administrateur\nwsl --install\nAprÃ¨s redÃ©marrage, tu auras accÃ¨s Ã  un terminal Linux complet !\n\n\nVÃ©rifier ta version de Bash\nbash --version",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“ 1. Navigation & exploration",
    "text": "ğŸ“ 1. Navigation & exploration\nLes commandes de base pour se dÃ©placer dans lâ€™arborescence :\n\n\nCode\n%%bash\n# Affiche le chemin du dossier courant (Print Working Directory)\npwd\n\n# Liste les fichiers du dossier courant\nls\n\n# Liste avec dÃ©tails (permissions, taille, date)\nls -lh\n\n# Liste incluant les fichiers cachÃ©s\nls -la\n\n# Affiche l'arborescence (si installÃ©)\n# tree\n\n# Change de dossier\ncd /tmp\npwd\n\n# Revenir au dossier prÃ©cÃ©dent\ncd -\n\n# Aller au dossier home\ncd ~\n\n\n\nğŸ”‘ Raccourcis de navigation essentiels\n\n\n\nSymbole\nSignification\nExemple\n\n\n\n\n.\nDossier courant\n./script.sh\n\n\n..\nDossier parent\ncd ..\n\n\n~\nDossier home\ncd ~\n\n\n/\nRacine du systÃ¨me\ncd /\n\n\n-\nDossier prÃ©cÃ©dent\ncd -",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“‚ 2. CrÃ©ation et manipulation de fichiers",
    "text": "ğŸ“‚ 2. CrÃ©ation et manipulation de fichiers\nCrÃ©er, copier, dÃ©placer, supprimer :\n\n\nCode\n%%bash\n# CrÃ©er un dossier\nmkdir data\n\n# CrÃ©er un dossier avec ses parents (pas d'erreur si existe)\nmkdir -p data/raw/2024\n\n# CrÃ©er un fichier vide\ntouch data/fichier.csv\n\n# CrÃ©er plusieurs fichiers\ntouch data/file1.csv data/file2.csv data/file3.csv\n\n# Copier un fichier\ncp data/fichier.csv data/fichier_backup.csv\n\n# Copier un dossier entier (rÃ©cursif)\ncp -r data/ data_backup/\n\n# DÃ©placer / Renommer un fichier\nmv data/fichier.csv data/nouveau_nom.csv\n\n# Supprimer un fichier\nrm data/file1.csv\n\n# Supprimer un dossier vide\nrmdir data/raw/2024\n\n# Supprimer un dossier et son contenu (âš ï¸ DANGEREUX)\nrm -r data_backup/",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“„ 3. Lecture de contenu",
    "text": "ğŸ“„ 3. Lecture de contenu\nLire, afficher, compter les lignes :\n\n\nCode\n%%bash\n# CrÃ©ons d'abord un fichier exemple\ncat &lt;&lt; 'EOF' &gt; ventes.csv\ndate,produit,quantite,prix\n2024-01-01,Laptop,5,999.99\n2024-01-02,Souris,20,29.99\n2024-01-03,Clavier,15,79.99\n2024-01-04,Ã‰cran,8,299.99\n2024-01-05,Laptop,3,999.99\n2024-01-06,Souris,25,29.99\n2024-01-07,Casque,12,149.99\nEOF\n\necho \"âœ… Fichier ventes.csv crÃ©Ã©\"\n\n\n\n\nCode\n%%bash\n# Affiche le contenu entier\necho \"=== cat ===\"\ncat ventes.csv\n\necho \"\"\necho \"=== head (3 premiÃ¨res lignes) ===\"\nhead -n 3 ventes.csv\n\necho \"\"\necho \"=== tail (2 derniÃ¨res lignes) ===\"\ntail -n 2 ventes.csv\n\necho \"\"\necho \"=== wc (comptage) ===\"\nwc -l ventes.csv    # Nombre de lignes\nwc -w ventes.csv    # Nombre de mots\nwc -c ventes.csv    # Nombre de caractÃ¨res\n\n\n\nğŸ“– Lire des gros fichiers avec less\nPour les fichiers volumineux, utilise less qui permet de naviguer :\nless gros_fichier.csv\n\n\n\nTouche\nAction\n\n\n\n\nâ†“ ou j\nLigne suivante\n\n\nâ†‘ ou k\nLigne prÃ©cÃ©dente\n\n\nSpace\nPage suivante\n\n\nb\nPage prÃ©cÃ©dente\n\n\n/mot\nRechercher â€œmotâ€\n\n\nn\nOccurrence suivante\n\n\nq\nQuitter",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ” 4. Recherche & filtrage",
    "text": "ğŸ” 4. Recherche & filtrage\nExtraire des informations prÃ©cises â€” essentiel pour un Data Engineer !\n\n\nCode\n%%bash\necho \"=== grep : recherche de motifs ===\"\n\n# Rechercher les lignes contenant \"Laptop\"\necho \"Lignes avec 'Laptop':\"\ngrep \"Laptop\" ventes.csv\n\necho \"\"\n# Recherche insensible Ã  la casse\necho \"Recherche insensible Ã  la casse (-i):\"\ngrep -i \"laptop\" ventes.csv\n\necho \"\"\n# Compter le nombre de correspondances\necho \"Nombre de lignes avec 'Souris':\"\ngrep -c \"Souris\" ventes.csv\n\necho \"\"\n# Afficher les numÃ©ros de ligne\necho \"Avec numÃ©ros de ligne (-n):\"\ngrep -n \"99.99\" ventes.csv\n\necho \"\"\n# Inverser la recherche (lignes qui NE contiennent PAS)\necho \"Lignes SANS 'Laptop' (-v):\"\ngrep -v \"Laptop\" ventes.csv\n\n\n\n\nCode\n%%bash\necho \"=== find : trouver des fichiers ===\"\n\n# CrÃ©er quelques fichiers pour l'exemple\nmkdir -p projet/data projet/scripts\ntouch projet/data/users.csv projet/data/sales.csv projet/data/old.json\ntouch projet/scripts/etl.py projet/scripts/utils.py\n\n# Trouver tous les fichiers .csv\necho \"Fichiers .csv:\"\nfind projet/ -name \"*.csv\"\n\necho \"\"\n# Trouver tous les fichiers .py\necho \"Fichiers .py:\"\nfind projet/ -name \"*.py\"\n\necho \"\"\n# Trouver les fichiers modifiÃ©s dans les derniÃ¨res 24h\necho \"Fichiers modifiÃ©s rÃ©cemment:\"\nfind projet/ -mtime -1 -type f\n\n# Nettoyage\nrm -r projet/\n\n\n\n\nCode\n%%bash\necho \"=== cut : extraire des colonnes ===\"\n\n# Extraire la 2Ã¨me colonne (produit)\necho \"Colonne 'produit':\"\ncut -d',' -f2 ventes.csv\n\necho \"\"\necho \"=== sort : trier ===\"\n# Trier par produit (2Ã¨me colonne)\necho \"TriÃ© par produit:\"\ntail -n +2 ventes.csv | sort -t',' -k2\n\necho \"\"\necho \"=== uniq : valeurs uniques ===\"\n# Liste des produits uniques\necho \"Produits uniques:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq\n\necho \"\"\n# Compter les occurrences\necho \"Comptage par produit:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ”— 5. Pipes & redirections",
    "text": "ğŸ”— 5. Pipes & redirections\nLe pipe (|) est lâ€™outil le plus puissant de Bash : il permet de chaÃ®ner des commandes en envoyant la sortie dâ€™une commande vers lâ€™entrÃ©e de la suivante.\n\n\nCode\n%%bash\necho \"=== Exemples de pipes ===\"\n\n# Trouver les ventes de Laptop et compter\necho \"Nombre de ventes Laptop:\"\ncat ventes.csv | grep \"Laptop\" | wc -l\n\necho \"\"\n# Top 3 des produits les plus vendus\necho \"Top 3 produits (par nombre de lignes):\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c | sort -rn | head -3\n\necho \"\"\n# Pipeline complexe : produits avec prix &gt; 100\necho \"Produits avec prix &gt; 100:\"\ntail -n +2 ventes.csv | awk -F',' '$4 &gt; 100 {print $2, $4}' | sort -u\n\n\n\n\nCode\n%%bash\necho \"=== Redirections ===\"\n\n# Rediriger vers un fichier (Ã©crase)\ngrep \"Laptop\" ventes.csv &gt; laptops.txt\necho \"Contenu de laptops.txt:\"\ncat laptops.txt\n\necho \"\"\n# Ajouter Ã  un fichier (append)\ngrep \"Ã‰cran\" ventes.csv &gt;&gt; laptops.txt\necho \"AprÃ¨s ajout:\"\ncat laptops.txt\n\necho \"\"\n# Rediriger les erreurs\nls fichier_inexistant 2&gt; erreurs.log\necho \"Erreur capturÃ©e:\"\ncat erreurs.log\n\n# Nettoyage\nrm -f laptops.txt erreurs.log\n\n\n\nğŸ“‹ RÃ©capitulatif des redirections\n\n\n\n\n\n\n\n\nSymbole\nDescription\nExemple\n\n\n\n\n&gt;\nRedirige stdout vers fichier (Ã©crase)\necho \"hello\" &gt; file.txt\n\n\n&gt;&gt;\nRedirige stdout vers fichier (ajoute)\necho \"world\" &gt;&gt; file.txt\n\n\n2&gt;\nRedirige stderr vers fichier\ncmd 2&gt; errors.log\n\n\n&&gt;\nRedirige stdout ET stderr\ncmd &&gt; all.log\n\n\n&lt;\nUtilise fichier comme entrÃ©e\nwc -l &lt; file.txt\n\n\n\\|\nPipe : stdout â†’ stdin suivant\ncat file \\| grep mot",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ” 6. Variables et boucles",
    "text": "ğŸ” 6. Variables et boucles\nAutomatiser avec des scripts bash :\n\n\nCode\n%%bash\necho \"=== Variables ===\"\n\n# DÃ©clarer une variable (PAS d'espace autour du =)\nnom=\"Data Engineer\"\nannee=2024\ndossier_data=\"/home/user/data\"\n\n# Utiliser une variable avec $\necho \"Bienvenue $nom !\"\necho \"Nous sommes en $annee\"\n\n# Utiliser ${} pour Ã©viter l'ambiguÃ¯tÃ©\necho \"Fichier: ${dossier_data}/ventes.csv\"\n\necho \"\"\necho \"=== Variables d'environnement ===\"\necho \"Home: $HOME\"\necho \"User: $USER\"\necho \"Shell: $SHELL\"\necho \"Path: $PATH\" | cut -c1-50  # TronquÃ© pour l'affichage\n\n\n\n\nCode\n%%bash\necho \"=== Boucle for ===\"\n\n# CrÃ©er des fichiers de test\nmkdir -p data_test\ntouch data_test/jan.csv data_test/feb.csv data_test/mar.csv\n\n# Boucle sur les fichiers CSV\nfor fichier in data_test/*.csv; do\n    echo \"ğŸ“„ Traitement de: $fichier\"\n    echo \"   Nom: $(basename \"$fichier\")\"\ndone\n\necho \"\"\necho \"=== Boucle avec sÃ©quence ===\"\nfor i in {1..5}; do\n    echo \"ItÃ©ration $i\"\ndone\n\necho \"\"\necho \"=== Boucle while ===\"\ncompteur=1\nwhile [ $compteur -le 3 ]; do\n    echo \"Compteur: $compteur\"\n    ((compteur++))\ndone\n\n# Nettoyage\nrm -r data_test/",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ”€ 7. Conditions (if/else)",
    "text": "ğŸ”€ 7. Conditions (if/else)\nPrendre des dÃ©cisions dans tes scripts :\n\n\nCode\n%%bash\necho \"=== Conditions de base ===\"\n\n# VÃ©rifier si un fichier existe\nif [ -f \"ventes.csv\" ]; then\n    echo \"âœ… Le fichier ventes.csv existe\"\nelse\n    echo \"âŒ Le fichier n'existe pas\"\nfi\n\necho \"\"\n# VÃ©rifier si un dossier existe\nif [ -d \"/tmp\" ]; then\n    echo \"âœ… Le dossier /tmp existe\"\nfi\n\necho \"\"\n# Comparer des nombres\nnb_lignes=$(wc -l &lt; ventes.csv)\necho \"Nombre de lignes: $nb_lignes\"\n\nif [ $nb_lignes -gt 5 ]; then\n    echo \"ğŸ“Š Fichier volumineux (&gt; 5 lignes)\"\nelse\n    echo \"ğŸ“„ Petit fichier\"\nfi\n\n\n\nğŸ“‹ OpÃ©rateurs de test\n\n\n\nTest fichiers\nDescription\n\n\n\n\n-f fichier\nFichier existe\n\n\n-d dossier\nDossier existe\n\n\n-r fichier\nFichier lisible\n\n\n-w fichier\nFichier modifiable\n\n\n-s fichier\nFichier non vide\n\n\n\n\n\n\nTest nombres\nDescription\n\n\n\n\n-eq\nÃ‰gal\n\n\n-ne\nDiffÃ©rent\n\n\n-gt\nPlus grand que\n\n\n-lt\nPlus petit que\n\n\n-ge\nPlus grand ou Ã©gal\n\n\n-le\nPlus petit ou Ã©gal\n\n\n\n\n\n\nTest chaÃ®nes\nDescription\n\n\n\n\n=\nÃ‰gal\n\n\n!=\nDiffÃ©rent\n\n\n-z\nChaÃ®ne vide\n\n\n-n\nChaÃ®ne non vide",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âš™ï¸ 8. CrÃ©er et exÃ©cuter un script Bash",
    "text": "âš™ï¸ 8. CrÃ©er et exÃ©cuter un script Bash\nUn script Bash est simplement un fichier texte contenant des commandes :\n\n\nCode\n%%bash\n# CrÃ©er un script complet\ncat &lt;&lt; 'EOF' &gt; mon_script.sh\n#!/bin/bash\n# Script de traitement de donnÃ©es\n# Auteur: Data Engineer\n# Date: 2024\n\necho \"ğŸš€ DÃ©marrage du script\"\necho \"ğŸ“… Date: $(date)\"\necho \"ğŸ‘¤ Utilisateur: $USER\"\necho \"ğŸ“‚ Dossier: $(pwd)\"\n\n# VÃ©rifier si un argument est passÃ©\nif [ -z \"$1\" ]; then\n    echo \"âš ï¸ Usage: ./mon_script.sh &lt;nom_fichier&gt;\"\n    exit 1\nfi\n\necho \"ğŸ“„ Fichier Ã  traiter: $1\"\necho \"âœ… Script terminÃ©\"\nEOF\n\n# Rendre exÃ©cutable\nchmod +x mon_script.sh\n\n# ExÃ©cuter le script\necho \"=== ExÃ©cution sans argument ===\"\n./mon_script.sh\n\necho \"\"\necho \"=== ExÃ©cution avec argument ===\"\n./mon_script.sh ventes.csv\n\n# Nettoyage\nrm mon_script.sh",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "â° 9. Automatisation avec Cron",
    "text": "â° 9. Automatisation avec Cron\nCron permet de planifier lâ€™exÃ©cution automatique de scripts â€” indispensable pour les pipelines ETL !\n\nğŸ“… Format dâ€™une ligne crontab\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande Ã  exÃ©cuter\n\n\nğŸ”§ Exemples courants pour Data Engineers\n\n\n\nExpression\nDescription\nCas dâ€™usage\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h\nETL nocturne\n\n\n*/15 * * * *\nToutes les 15 minutes\nMonitoring\n\n\n0 0 * * 0\nChaque dimanche Ã  minuit\nRapport hebdomadaire\n\n\n0 9 1 * *\nLe 1er de chaque mois Ã  9h\nRapport mensuel\n\n\n0 */4 * * *\nToutes les 4 heures\nSynchronisation donnÃ©es\n\n\n\n\n\nğŸ’» Commandes cron\n# Ã‰diter la crontab\ncrontab -e\n\n# Lister les jobs planifiÃ©s\ncrontab -l\n\n# Supprimer tous les jobs\ncrontab -r\n\n\nğŸ“ Exemple de crontab pour Data Engineer\n# ETL quotidien Ã  2h du matin\n0 2 * * * /home/user/scripts/etl_pipeline.sh &gt;&gt; /var/log/etl.log 2&gt;&1\n\n# Backup des donnÃ©es chaque dimanche Ã  3h\n0 3 * * 0 /home/user/scripts/backup.sh\n\n# Nettoyage des fichiers temporaires chaque jour Ã  4h\n0 4 * * * find /tmp -mtime +7 -delete\n\nğŸ’¡ Astuce : Utilise crontab.guru pour gÃ©nÃ©rer facilement des expressions cron !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âœ… Exercice pratique",
    "text": "âœ… Exercice pratique\n\nğŸ§  Instructions\n\nCrÃ©e un dossier de travail nommÃ© mon_premier_script\nEntre dans ce dossier\nCrÃ©e un fichier script appelÃ© bonjour.sh\nÃ‰dite ce fichier et Ã©cris un script qui :\n\nAffiche â€œBonjour Data Engineer ğŸ‘‹â€\nAffiche la date du jour\nTe souhaite une bonne session\n\nRends le script exÃ©cutable\nCrÃ©e un sous-dossier nommÃ© data/ et place-y quelques fichiers .csv (mÃªme vides)\nAjoute une Ã©tape dans le script pour :\n\nAfficher tous les fichiers .csv prÃ©sents dans le dossier data/\nPour chaque fichier .csv, afficher son nom avec un message comme :\nğŸ‘‰ â€œFichier trouvÃ© : nom_du_fichier.csv âœ…â€\n\n\nğŸ“Œ Quelle structure utiliser ? (indice : boucle for)\n\n\nâœ… Correction\n\n\nğŸ“¥ Afficher la correction complÃ¨te\n\n#!/bin/bash\n\n# 1. ğŸ“¢ Afficher un message de bienvenue\necho \"Bonjour Data Engineer ğŸ‘‹\"\n\n# 2. ğŸ—“ï¸ Afficher la date du jour\necho \"Date: $(date)\"\n\n# 3. ğŸ’¬ Souhaiter une bonne session\necho \"Bonne session de travail ğŸ’ª\"\n\n# 4. ğŸ“ CrÃ©er le dossier 'data/' s'il n'existe pas\nmkdir -p data\n\n# 5. ğŸ—‚ï¸ CrÃ©er quelques fichiers de test\ntouch data/fichier1.csv data/fichier2.csv data/fichier3.csv\n\n# 6. ğŸ” Lister les fichiers CSV\necho \"\"\necho \"ğŸ” Recherche de fichiers CSV dans ./data...\"\n\nfor fichier in data/*.csv; do\n    if [ -f \"$fichier\" ]; then\n        echo \"Fichier trouvÃ© : $(basename \"$fichier\") âœ…\"\n    fi\ndone\n\n# 7. âœ… Fin du script\necho \"\"\necho \"Traitement terminÃ© âœ…\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“„ Cheatsheet Bash â€“ Commandes essentielles",
    "text": "ğŸ“„ Cheatsheet Bash â€“ Commandes essentielles\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nğŸ“ Navigation\npwd\nAffiche le chemin actuel\n\n\n\ncd dossier/\nSe dÃ©placer dans un dossier\n\n\n\nls -lh\nListe les fichiers avec dÃ©tails\n\n\nğŸ“„ Fichiers\ntouch nom.txt\nCrÃ©er un fichier vide\n\n\n\ncp fichier.txt dossier/\nCopier un fichier\n\n\n\nmv fichier.txt nouveau.txt\nRenommer ou dÃ©placer\n\n\n\nrm fichier.txt\nSupprimer un fichier\n\n\nğŸ“š Dossiers\nmkdir dossier/\nCrÃ©er un dossier\n\n\n\nmkdir -p a/b/c\nCrÃ©er avec parents\n\n\n\nrm -r dossier/\nSupprimer dossier + contenu\n\n\nğŸ“– Lecture\ncat fichier.txt\nAfficher tout le contenu\n\n\n\nhead -n 10 fichier.txt\n10 premiÃ¨res lignes\n\n\n\ntail -n 10 fichier.txt\n10 derniÃ¨res lignes\n\n\n\nwc -l fichier.txt\nCompter les lignes\n\n\nğŸ” Recherche\ngrep \"mot\" fichier.txt\nRechercher un mot\n\n\n\nfind . -name \"*.csv\"\nTrouver des fichiers\n\n\n\ncut -d',' -f1 fichier.csv\nExtraire une colonne\n\n\nğŸ”— Pipes\ncmd1 \\| cmd2\nChaÃ®ner des commandes\n\n\n\ncmd &gt; fichier.txt\nRediriger vers fichier\n\n\n\ncmd &gt;&gt; fichier.txt\nAjouter Ã  un fichier\n\n\nğŸ§  Scripts\nchmod +x script.sh\nRendre exÃ©cutable\n\n\n\n./script.sh\nLancer un script\n\n\nğŸ”„ Boucles\nfor f in *.csv; do ...; done\nBoucle sur fichiers\n\n\nâ° Cron\ncrontab -e\nÃ‰diter les tÃ¢ches planifiÃ©es\n\n\n\ncrontab -l\nLister les tÃ¢ches\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Bash Cheatsheet PDF (fr)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "âš ï¸ Erreurs classiques Ã  Ã©viter",
    "text": "âš ï¸ Erreurs classiques Ã  Ã©viter\n\n\n\n\n\n\n\n\nâŒ Erreur\nğŸ’¥ ConsÃ©quence\nâœ… Bonne pratique\n\n\n\n\nrm -rf /\nSupprime TOUT le systÃ¨me !\nToujours vÃ©rifier le chemin avant rm -rf\n\n\n$fichier sans guillemets\nBug si espaces dans le nom\nUtiliser \"$fichier\"\n\n\nsudo sans rÃ©flÃ©chir\nÃ‰crase des fichiers systÃ¨me\nComprendre la commande avant dâ€™utiliser sudo\n\n\nScript non testÃ© en prod\nPerte de donnÃ©es\nToujours tester en sandbox dâ€™abord\n\n\nVAR = valeur (avec espaces)\nErreur de syntaxe\nVAR=valeur (sans espaces)\n\n\nOublier #!/bin/bash\nScript peut mal sâ€™exÃ©cuter\nToujours commencer par le shebang\n\n\n\n\nğŸ§  Conseil : Avant dâ€™exÃ©cuter une commande destructive (rm, mv), utilise echo pour voir ce qui serait affectÃ© :\n# Au lieu de :\nrm -rf data/*.csv\n\n# D'abord tester avec :\necho data/*.csv",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle commande affiche le chemin du dossier courant ?\n\ncd\n\npwd\n\nls\n\npath\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” pwd = Print Working Directory\n\n\n\n\nâ“ Q2. Que fait la commande rm -rf mon_dossier/ ?\n\nRedÃ©marre lâ€™ordinateur\n\nRÃ©organise un fichier\n\nSupprime un dossier et son contenu\n\nReformate le disque\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” -r = rÃ©cursif, -f = force (sans confirmation)\n\n\n\n\nâ“ Q3. Quelle commande affiche les 10 premiÃ¨res lignes dâ€™un fichier ?\n\nhead -n 10\n\ncat -10\n\nstart 10\n\ntop 10\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” head -n 10 fichier.txt\n\n\n\n\nâ“ Q4. Pourquoi Ã©crire \"$fichier\" au lieu de $fichier ?\n\nPour que Bash reconnaisse les fichiers CSV\n\nPour faire du style\n\nPour Ã©viter les bugs avec les noms contenant des espaces\n\nÃ‡a nâ€™a pas dâ€™importance\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les guillemets protÃ¨gent les valeurs contenant des espaces\n\n\n\n\nâ“ Q5. Que signifie le | (pipe) en Bash ?\n\nInterrompre une commande\n\nExÃ©cuter un script\n\nEnvoyer la sortie dâ€™une commande vers lâ€™entrÃ©e dâ€™une autre\n\nCrÃ©er un fichier temporaire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le pipe chaÃ®ne les commandes : cmd1 | cmd2\n\n\n\n\nâ“ Q6. Quelle expression cron exÃ©cute un script tous les jours Ã  2h du matin ?\n\n2 0 * * *\n\n0 2 * * *\n\n* 2 * * *\n\n0 0 2 * *\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Format : minute heure jour mois jour_semaine",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸš€ Mini-projet : Archiver intelligemment des fichiers CSV",
    "text": "ğŸš€ Mini-projet : Archiver intelligemment des fichiers CSV\n\nğŸ¯ Objectif\nCrÃ©er un script Bash rÃ©aliste qui automatise lâ€™archivage de fichiers .csv selon leur anciennetÃ©.\n\n\nğŸ”§ Contexte\nTu travailles dans une Ã©quipe data. Chaque jour, des fichiers .csv sont dÃ©posÃ©s dans un dossier data/.\nTu dois crÃ©er un script qui :\n\nğŸ“¦ RepÃ¨re tous les fichiers .csv modifiÃ©s il y a plus de 7 jours\nğŸ—‚ï¸ Les archive dans un fichier .tar.gz nommÃ© archive_YYYYMMDD.tar.gz\nğŸ§¹ DÃ©place ces fichiers dans un dossier archive/\n\n\n\nğŸ§  Contraintes\n\nLe script doit fonctionner mÃªme si aucun fichier nâ€™est Ã©ligible\nLâ€™archive doit Ãªtre horodatÃ©e automatiquement\nLe dossier archive/ doit Ãªtre crÃ©Ã© sâ€™il nâ€™existe pas\nAjouter du logging pour tracer les actions\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n#!/bin/bash\n#\n# Script: archive_csv.sh\n# Description: Archive les fichiers CSV de plus de 7 jours\n# Auteur: Data Engineer\n#\n\n# Configuration\nDATA_DIR=\"./data\"\nARCHIVE_DIR=\"./archive\"\nDAYS_OLD=7\nDATE_TAG=$(date +%Y%m%d)\nARCHIVE_NAME=\"archive_${DATE_TAG}.tar.gz\"\nLOG_FILE=\"archive.log\"\n\n# Fonction de logging\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog \"ğŸš€ DÃ©marrage du script d'archivage\"\n\n# VÃ©rifier que le dossier source existe\nif [ ! -d \"$DATA_DIR\" ]; then\n    log \"âŒ Erreur: Le dossier $DATA_DIR n'existe pas\"\n    exit 1\nfi\n\n# CrÃ©er le dossier d'archive si nÃ©cessaire\nmkdir -p \"$ARCHIVE_DIR\"\nlog \"ğŸ“ Dossier d'archive: $ARCHIVE_DIR\"\n\n# Trouver les fichiers CSV de plus de 7 jours\nOLD_FILES=$(find \"$DATA_DIR\" -name \"*.csv\" -mtime +$DAYS_OLD -type f)\n\n# VÃ©rifier s'il y a des fichiers Ã  archiver\nif [ -z \"$OLD_FILES\" ]; then\n    log \"â„¹ï¸ Aucun fichier CSV de plus de $DAYS_OLD jours trouvÃ©\"\n    exit 0\nfi\n\n# Compter les fichiers\nNB_FILES=$(echo \"$OLD_FILES\" | wc -l)\nlog \"ğŸ“Š $NB_FILES fichier(s) Ã  archiver\"\n\n# CrÃ©er l'archive\nlog \"ğŸ“¦ CrÃ©ation de l'archive $ARCHIVE_NAME...\"\necho \"$OLD_FILES\" | tar -czvf \"$ARCHIVE_DIR/$ARCHIVE_NAME\" -T -\n\nif [ $? -eq 0 ]; then\n    log \"âœ… Archive crÃ©Ã©e avec succÃ¨s\"\n    \n    # DÃ©placer les fichiers archivÃ©s\n    for file in $OLD_FILES; do\n        mv \"$file\" \"$ARCHIVE_DIR/\"\n        log \"   â†³ DÃ©placÃ©: $(basename \"$file\")\"\n    done\n    \n    log \"ğŸ‰ Archivage terminÃ© avec succÃ¨s\"\nelse\n    log \"âŒ Erreur lors de la crÃ©ation de l'archive\"\n    exit 1\nfi\nPour lâ€™utiliser :\nchmod +x archive_csv.sh\n./archive_csv.sh\nPour lâ€™automatiser avec cron (tous les jours Ã  3h) :\n0 3 * * * /home/user/scripts/archive_csv.sh",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Sites & outils\n\nExplainShell â€” Explique nâ€™importe quelle commande Bash\nShellCheck â€” VÃ©rifie la syntaxe de tes scripts\nCrontab Guru â€” GÃ©nÃ©rateur dâ€™expressions cron\nLinux Command â€” Tutoriel complet\n\n\n\nğŸ“– Documentation\n\nGNU Bash Manual\nAdvanced Bash-Scripting Guide\n\n\n\nğŸ® Pratique\n\nOverTheWire - Bandit â€” Jeu pour apprendre Bash\nCmdchallenge â€” DÃ©fis en ligne de commande",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ§° Bash pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Bash, passons Ã  un autre outil essentiel : Git !\nğŸ‘‰ Module suivant : 03_git_for_data_engineers.ipynb â€” Versionner ton code et collaborer\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Bash pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§° Bash pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/tmp.html",
    "href": "notebooks/beginner/tmp.html",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "",
    "text": "Code\n---\n\n## ğŸªœ ProblÃ©matique 7 : Filtrer aprÃ¨s agrÃ©gation â€” `bucket_selector` (Ã©quivalent `HAVING`)\n\n```bash\ncurl -X GET \"http://localhost:9200/clients/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": {\n        \"field\": \"pays.keyword\"\n      },\n      \"aggs\": {\n        \"nb_docs\": {\n          \"value_count\": { \"field\": \"pays.keyword\" }\n        },\n        \"filtre_nb\": {\n          \"bucket_selector\": {\n            \"buckets_path\": { \"total\": \"nb_docs\" },\n            \"script\": \"params.total &gt; 1\"\n          }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT pays, COUNT(*) as nb_clients\nFROM clients\nGROUP BY pays\nHAVING COUNT(*) &gt; 1;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 8 : Champs calculÃ©s â€” `script_fields` (Ã©quivalent `CASE`)\n\n```bash\ncurl -X GET \"http://localhost:9200/clients/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"script_fields\": {\n    \"continent\": {\n      \"script\": {\n        \"lang\": \"painless\",\n        \"source\": \"if (doc['pays.keyword'].value == 'France') { return 'Europe'; } else { return 'Autre'; }\"\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT nom, pays,\nCASE \n  WHEN pays = 'France' THEN 'Europe'\n  ELSE 'Autre'\nEND AS continent\nFROM clients;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 9 : AgrÃ©gations imbriquÃ©es (Ã©quivalent sous-requÃªtes)\n\n```bash\ncurl -X GET \"http://localhost:9200/ventes/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays.keyword\" },\n      \"aggs\": {\n        \"par_produit\": {\n          \"terms\": { \"field\": \"produit.keyword\" }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT pays, produit, COUNT(*)\nFROM ventes\nGROUP BY pays, produit;\n```\n\n---\n\n## ğŸªœ ProblÃ©matique 10 : Fonctions analytiques â€” `top_hits`, `bucket_sort` (Ã©quivalent `RANK()`)\n\n```bash\ncurl -X GET \"http://localhost:9200/ventes/_search\" -H 'Content-Type: application/json' -d '\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays.keyword\" },\n      \"aggs\": {\n        \"top_vente\": {\n          \"top_hits\": {\n            \"size\": 1,\n            \"sort\": [ { \"montant\": { \"order\": \"desc\" } } ]\n          }\n        }\n      }\n    }\n  }\n}'\n```\n\nâœ… **InterprÃ©tation** :\nEquivalent de :\n```sql\nSELECT *\nFROM (\n  SELECT *, RANK() OVER (PARTITION BY pays ORDER BY montant DESC) as rang\n  FROM ventes\n) t\nWHERE rang = 1;\n```\n\n\n\n\n\nCode\noui mais pour le contenu ; il faut faire dÃ©finition ; rappeler les notions shards et replicas , installations\n\n\n\n\nCode\nbeeruschatgpt_db_user   ------  cH0cBlDbokqCohcB"
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente Elasticsearch, le moteur de recherche et dâ€™analytics distribuÃ©.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre lâ€™architecture Elasticsearch (index, shards, replicas)\nâœ… Utiliser Elasticvue pour interagir avec le cluster\nâœ… CrÃ©er des index avec mapping appropriÃ©\nâœ… CrÃ©er des Index Templates pour automatiser les mappings\nâœ… Indexer, rechercher, modifier et supprimer des documents\nâœ… Ã‰crire des requÃªtes de recherche (match, bool, fuzzy, range)\nâœ… RÃ©aliser des agrÃ©gations analytiques",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âš ï¸ Version Elasticsearch",
    "text": "âš ï¸ Version Elasticsearch\nCe cours utilise Elasticsearch 8.x (version recommandÃ©e : 8.12+).\n\n\n\n\n\n\n\nVersion\nNotes\n\n\n\n\n8.x\nâœ… RecommandÃ©e â€” SÃ©curitÃ© par dÃ©faut, nouvelles fonctionnalitÃ©s\n\n\n7.x\nâš ï¸ Encore supportÃ©e mais migration conseillÃ©e\n\n\n6.x et avant\nâŒ ObsolÃ¨te\n\n\n\n\nğŸ’¡ Les exemples de ce cours fonctionnent avec ES 7.x et 8.x. Pour ES 8.x, la sÃ©curitÃ© est activÃ©e par dÃ©faut â€” nous la dÃ©sactivons pour les tests locaux.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 les diffÃ©rents types de bases NoSQL. Elasticsearch est un moteur de recherche (Search Engine), parfois classÃ© Ã  part des bases NoSQL traditionnelles.\n\nPosition dans les types NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Document  â”‚ ClÃ©-Valeurâ”‚  Colonnes â”‚  Graphe   â”‚ Search Engine  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚                â”‚\nâ”‚  MongoDB  â”‚   Redis   â”‚ Cassandra â”‚   Neo4j   â”‚ ELASTICSEARCH  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚     â—„â”€â”€â”€       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment Elasticsearch rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs nÅ“uds)\n\n\nVelocity\nIndexation temps rÃ©el, near real-time search\n\n\nVariety\nDocuments JSON flexibles, analyse full-text\n\n\nVeracity\nScoring de pertinence, recherche approximative\n\n\nValue\nRecherche instantanÃ©e, dashboards Kibana\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nElasticsearch\n\n\n\n\nCAP\nAP (Availability + Partition tolerance) par dÃ©faut\n\n\nBASE\nEventually consistent (cohÃ©rence Ã©ventuelle)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Pourquoi Elasticsearch en Data Engineering ?",
    "text": "ğŸ¯ Pourquoi Elasticsearch en Data Engineering ?\n\n\n\n\n\n\n\nCas dâ€™usage\nDescription\n\n\n\n\nğŸ“Š Logs & Monitoring\nStack ELK (Elasticsearch, Logstash, Kibana)\n\n\nğŸ” Recherche full-text\nMoteur de recherche pour sites web, e-commerce\n\n\nğŸ“ˆ Analytics temps rÃ©el\nDashboards et mÃ©triques en temps rÃ©el\n\n\nğŸ”” Alerting\nDÃ©tection dâ€™anomalies, alertes sur seuils\n\n\n\n\nğŸ’¡ ELK Stack = Elasticsearch + Logstash + Kibana â€” trÃ¨s utilisÃ© en entreprise.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“š 1. Concepts fondamentaux",
    "text": "ğŸ“š 1. Concepts fondamentaux\n\nğŸ“– Vocabulaire\n\n\n\nSQL\nElasticsearch\nDescription\n\n\n\n\nDatabase\nCluster\nEnsemble de nÅ“uds\n\n\nTable\nIndex\nCollection de documents\n\n\nRow\nDocument\nUne entrÃ©e (JSON)\n\n\nColumn\nField\nUn champ du document\n\n\nSchema\nMapping\nStructure des champs\n\n\n\n\n\nğŸ§± Shards & Replicas\nIndex \"clients\"\nâ”œâ”€â”€ Primary Shard 0  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 0\nâ”œâ”€â”€ Primary Shard 1  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 1\nâ””â”€â”€ Primary Shard 2  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 2\n\n\n\nConcept\nRÃ´le\n\n\n\n\nShard\nPartition des donnÃ©es (scalabilitÃ©)\n\n\nReplica\nCopie dâ€™un shard (haute disponibilitÃ©)\n\n\n\n\n\nğŸ“Š Ã‰tat du cluster\n\n\n\nÃ‰tat\nSignification\n\n\n\n\nğŸŸ¢ Green\nTous les shards OK\n\n\nğŸŸ¡ Yellow\nPrimaires OK, replicas non allouÃ©s (1 seul nÅ“ud)\n\n\nğŸ”´ Red\nDonnÃ©es inaccessibles",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âš™ï¸ 2. Installation",
    "text": "âš™ï¸ 2. Installation\n\nğŸ“¦ Ã‰tape 1 : TÃ©lÃ©charger Elasticsearch\nğŸ‘‰ https://www.elastic.co/downloads/elasticsearch\n\nTÃ©lÃ©charger le ZIP pour ton OS\nDÃ©zipper dans un dossier (ex: C:\\elasticsearch)\nModifier config/elasticsearch.yml :\n\n# DÃ©sactiver la sÃ©curitÃ© pour les tests\nxpack.security.enabled: false\n\nLancer Elasticsearch :\n\n# Windows\n.\\bin\\elasticsearch.bat\n\n# macOS / Linux\n./bin/elasticsearch\n\nVÃ©rifier : ouvrir http://localhost:9200 dans le navigateur\n\n\n\n\nğŸ§­ Ã‰tape 2 : Installer Elasticvue\nElasticvue est une interface graphique pour Elasticsearch â€” beaucoup plus simple que les commandes curl !\nOptions dâ€™installation :\n\n\n\nOption\nLien\n\n\n\n\nğŸŒ App Web (recommandÃ©)\nhttps://app.elasticvue.com\n\n\nğŸ§© Extension Chrome\nChrome Web Store\n\n\nğŸ¦Š Extension Firefox\nFirefox Add-ons\n\n\nğŸ’» App Desktop\nelasticvue.com",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ§­ 3. Guide Elasticvue â€” Prise en main",
    "text": "ğŸ§­ 3. Guide Elasticvue â€” Prise en main\n\nğŸ”Œ 3.1 Connexion au cluster\n\nOuvrir Elasticvue\nCliquer sur â€œAdd Clusterâ€\nRemplir :\n\nName : Local (ou ce que tu veux)\nURI : http://localhost:9200\n\nCliquer â€œConnectâ€\n\nâœ… Tu devrais voir le statut du cluster (ğŸŸ¢ Green ou ğŸŸ¡ Yellow)\n\n\n\nğŸ—‚ï¸ 3.2 Interface principale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Elasticvue                                    [Cluster: Local] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚             â”‚                                               â”‚\nâ”‚  ğŸ“Š Home    â”‚   Cluster Health: ğŸŸ¢ Green                    â”‚\nâ”‚             â”‚   Nodes: 1                                    â”‚\nâ”‚  ğŸ“ Indices â”‚   Indices: 3                                  â”‚\nâ”‚             â”‚   Documents: 1,234                            â”‚\nâ”‚  ğŸ” Search  â”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚  ğŸ“ REST    â”‚   â—„â”€â”€ C'est ici qu'on Ã©crit les requÃªtes !   â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚  âš™ï¸ Settingsâ”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“ Onglets importants :\n\n\n\nOnglet\nUsage\n\n\n\n\nIndices\nVoir/crÃ©er/supprimer des index\n\n\nSearch\nRechercher visuellement dans un index\n\n\nREST\nÃ‰crire des requÃªtes (comme Kibana Dev Tools)\n\n\n\n\n\n\nğŸ“ 3.3 Utiliser la console REST\nLâ€™onglet REST permet dâ€™exÃ©cuter des requÃªtes Elasticsearch avec une syntaxe simple.\n\nğŸ“Œ Format des requÃªtes\nMÃ‰THODE /chemin\n{\n  \"corps\": \"de la requÃªte en JSON\"\n}\n\n\nğŸ¯ Exemple pas Ã  pas\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  REST Query                                        [â–¶ Run]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                              â”‚\nâ”‚  GET /clients/_search                                        â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"query\": {                                                â”‚\nâ”‚      \"match\": { \"pays\": \"France\" }                          â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ”‚                                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Response (200 OK)                                           â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"hits\": {                                                 â”‚\nâ”‚      \"total\": { \"value\": 2 },                                â”‚\nâ”‚      \"hits\": [...]                                           â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâŒ¨ï¸ Raccourcis utiles\n\n\n\nRaccourci\nAction\n\n\n\n\nCtrl + Enter\nExÃ©cuter la requÃªte\n\n\nCtrl + /\nCommenter une ligne\n\n\nCtrl + Space\nAutocomplÃ©tion",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ› ï¸ 4. CrÃ©er un index et insÃ©rer des donnÃ©es",
    "text": "ğŸ› ï¸ 4. CrÃ©er un index et insÃ©rer des donnÃ©es\n\nğŸ“Œ 4.1 CrÃ©er lâ€™index clients\nDans lâ€™onglet REST, copier-coller :\nPUT /clients\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"nom\": { \"type\": \"text\" },\n      \"email\": { \"type\": \"keyword\" },\n      \"pays\": { \"type\": \"keyword\" },\n      \"age\": { \"type\": \"integer\" },\n      \"salaire\": { \"type\": \"float\" }\n    }\n  }\n}\nâœ… RÃ©ponse attendue : { \"acknowledged\": true }\n\n\n\nğŸ“ 4.2 Comprendre le mapping\n\n\n\nType\nUsage\nRecherche full-text\nAgrÃ©gation\n\n\n\n\ntext\nTexte analysÃ©\nâœ… Oui\nâŒ Non\n\n\nkeyword\nValeur exacte\nâŒ Non\nâœ… Oui\n\n\ninteger, float\nNombres\nRange âœ…\nâœ… Oui\n\n\ndate\nDates\nRange âœ…\nâœ… Oui\n\n\nboolean\ntrue/false\nâœ…\nâœ… Oui\n\n\n\n\nğŸ’¡ RÃ¨gle : Utilise keyword pour les champs sur lesquels tu veux faire des agrÃ©gations (GROUP BY).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ Index Templates â€” Automatiser les mappings",
    "text": "ğŸ“‹ Index Templates â€” Automatiser les mappings\n\nğŸ¤” Pourquoi utiliser des templates ?\nQuand tu gÃ¨res des logs ou des donnÃ©es temporelles, tu crÃ©es souvent des index par jour ou par mois :\nlogs-2024-01-01\nlogs-2024-01-02\nlogs-2024-01-03\n...\nProblÃ¨me : Sans template, chaque index utilise le mapping dynamique (Elasticsearch devine les types). Câ€™est risquÃ© !\nSolution : CrÃ©er un Index Template qui sâ€™applique automatiquement Ã  tous les index correspondant Ã  un pattern.\n\n\n\nğŸ“Œ CrÃ©er un Index Template\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"priority\": 1,\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": { \"type\": \"date\" },\n        \"level\": { \"type\": \"keyword\" },\n        \"message\": { \"type\": \"text\" },\n        \"service\": { \"type\": \"keyword\" },\n        \"host\": { \"type\": \"keyword\" },\n        \"response_time_ms\": { \"type\": \"integer\" }\n      }\n    }\n  }\n}\nâœ… Maintenant, tout index crÃ©Ã© avec le pattern logs-* aura automatiquement ce mapping !\n\n\n\nğŸ§ª Tester le template\n# CrÃ©er un index qui match le pattern\nPOST /logs-2024-01-15/_doc\n{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"level\": \"ERROR\",\n  \"message\": \"Connection timeout to database\",\n  \"service\": \"api-gateway\",\n  \"host\": \"server-01\",\n  \"response_time_ms\": 5000\n}\n# VÃ©rifier que le mapping a Ã©tÃ© appliquÃ©\nGET /logs-2024-01-15/_mapping\n\n\n\nğŸ“‹ Lister les templates existants\nGET /_index_template\n\n\nğŸ—‘ï¸ Supprimer un template\nDELETE /_index_template/logs_template\n\n\n\nğŸ’¡ Bonnes pratiques Index Templates\n\n\n\n\n\n\n\nPratique\nExplication\n\n\n\n\nToujours dÃ©finir un mapping\nNe pas laisser ES deviner les types\n\n\nUtiliser keyword pour les agrÃ©gations\nlevel, service, host â†’ keyword\n\n\nUtiliser text pour la recherche\nmessage â†’ text\n\n\nDÃ©finir date explicitement\nÃ‰vite les erreurs de parsing\n\n\nNommer clairement\nlogs_template, metrics_template\n\n\nUtiliser priority\nQuand plusieurs templates matchent\n\n\n\n\n\n\nğŸ“ 4.3 InsÃ©rer un document\nPOST /clients/_doc\n{\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@email.com\",\n  \"pays\": \"France\",\n  \"age\": 30,\n  \"salaire\": 55000\n}\n\n\n\nğŸ“ 4.4 InsÃ©rer plusieurs documents (Bulk)\nPOST /_bulk\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Bob Martin\", \"email\": \"bob@email.com\", \"pays\": \"France\", \"age\": 25, \"salaire\": 48000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Charlie Konan\", \"email\": \"charlie@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 35, \"salaire\": 62000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Diana Schmidt\", \"email\": \"diana@email.com\", \"pays\": \"Allemagne\", \"age\": 28, \"salaire\": 51000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Eve Kouassi\", \"email\": \"eve@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 32, \"salaire\": 58000 }\n\nâš ï¸ Attention : En bulk, chaque ligne doit Ãªtre sur une seule ligne (pas de formatage JSON multi-lignes).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ” 5. RequÃªtes de recherche",
    "text": "ğŸ” 5. RequÃªtes de recherche\n\nğŸ“Œ 5.1 Afficher tous les documents\nGET /clients/_search\nâœ… SQL Ã©quivalent : SELECT * FROM clients\n\n\n\nğŸ“Œ 5.2 Filtrer avec match (full-text)\nGET /clients/_search\n{\n  \"query\": {\n    \"match\": { \"nom\": \"Alice\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE nom LIKE '%Alice%'\n\n\n\nğŸ“Œ 5.3 Filtrer avec term (exact match)\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France'\n\nğŸ’¡ Utilise term pour les champs keyword, match pour les champs text.\n\n\n\n\nğŸ“Œ 5.4 Filtrer par plage de valeurs (range)\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"age\": {\n        \"gte\": 25,\n        \"lte\": 35\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE age BETWEEN 25 AND 35\n\n\n\nOpÃ©rateur\nSignification\n\n\n\n\ngt\n&gt; (greater than)\n\n\ngte\n&gt;= (greater than or equal)\n\n\nlt\n&lt; (less than)\n\n\nlte\n&lt;= (less than or equal)\n\n\n\n\n\n\nğŸ“Œ 5.5 Conditions multiples (bool)\nLa requÃªte bool combine plusieurs conditions :\n\n\n\nClause\nComportement\nSQL Ã©quivalent\n\n\n\n\nmust\nToutes les conditions requises\nAND\n\n\nshould\nAu moins une condition\nOR\n\n\nmust_not\nExclure\nNOT / !=\n\n\nfilter\nComme must mais sans score\nWHERE (optimisÃ©)\n\n\n\n\nExemple : Clients franÃ§ais de plus de 25 ans\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"range\": { \"age\": { \"gt\": 25 } } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France' AND age &gt; 25\n\n\n\nExemple : Clients franÃ§ais OU ivoiriens, mais PAS Bob\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"term\": { \"pays\": \"CÃ´te d'Ivoire\" } }\n      ],\n      \"minimum_should_match\": 1,\n      \"must_not\": [\n        { \"match\": { \"nom\": \"Bob\" } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE (pays = 'France' OR pays = 'CÃ´te d''Ivoire') AND nom != 'Bob'\n\n\n\n\nğŸ“Œ 5.6 Recherche floue (fuzzy)\nTrouve des rÃ©sultats mÃªme avec des fautes de frappe :\nGET /clients/_search\n{\n  \"query\": {\n    \"fuzzy\": {\n      \"nom\": {\n        \"value\": \"Alise\",\n        \"fuzziness\": \"AUTO\"\n      }\n    }\n  }\n}\nâœ… Trouve â€œAliceâ€ mÃªme si on tape â€œAliseâ€ !\n\nğŸ’¡ Pas dâ€™Ã©quivalent simple en SQL â€” câ€™est la force dâ€™Elasticsearch.\n\n\n\n\nğŸ“Œ 5.7 Trier et limiter les rÃ©sultats\nGET /clients/_search\n{\n  \"query\": { \"match_all\": {} },\n  \"sort\": [\n    { \"salaire\": \"desc\" }\n  ],\n  \"size\": 3\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients ORDER BY salaire DESC LIMIT 3",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“Š 6. AgrÃ©gations (GROUP BY)",
    "text": "ğŸ“Š 6. AgrÃ©gations (GROUP BY)\n\nğŸ“Œ 6.1 Compter par catÃ©gorie (terms)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT pays, COUNT(*) FROM clients GROUP BY pays\n\nğŸ’¡ size: 0 = ne pas retourner les documents, seulement lâ€™agrÃ©gation.\n\n\n\n\nğŸ“Œ 6.2 MÃ©triques (sum, avg, min, max)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n    \"salaire_max\": { \"max\": { \"field\": \"salaire\" } },\n    \"salaire_min\": { \"min\": { \"field\": \"salaire\" } },\n    \"salaire_total\": { \"sum\": { \"field\": \"salaire\" } }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT AVG(salaire), MAX(salaire), MIN(salaire), SUM(salaire) FROM clients\n\n\n\nğŸ“Œ 6.3 Stats complÃ¨tes en une requÃªte\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"stats_salaire\": {\n      \"stats\": { \"field\": \"salaire\" }\n    }\n  }\n}\nRÃ©sultat : count, min, max, avg, sum en une seule requÃªte !\n\n\n\nğŸ“Œ 6.4 AgrÃ©gations imbriquÃ©es (GROUP BY + mÃ©triques)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n        \"age_moyen\": { \"avg\": { \"field\": \"age\" } }\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, AVG(salaire) AS salaire_moyen, AVG(age) AS age_moyen\nFROM clients\nGROUP BY pays;\n\n\n\nğŸ“Œ 6.5 Filtrer avant dâ€™agrÃ©ger\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"salaire\": { \"gte\": 50000 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, COUNT(*)\nFROM clients\nWHERE salaire &gt;= 50000\nGROUP BY pays;",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "âœï¸ 7. Modifier et supprimer",
    "text": "âœï¸ 7. Modifier et supprimer\n\nğŸ“Œ 7.1 Mettre Ã  jour un document (par ID)\nPOST /clients/_update/1\n{\n  \"doc\": {\n    \"salaire\": 60000\n  }\n}\n\n\n\nğŸ“Œ 7.2 Mettre Ã  jour par requÃªte\nPOST /clients/_update_by_query\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.salaire += 1000\"\n  }\n}\nâœ… SQL Ã©quivalent : UPDATE clients SET salaire = salaire + 1000 WHERE pays = 'France'\n\n\n\nğŸ“Œ 7.3 Supprimer un document (par ID)\nDELETE /clients/_doc/1\n\n\n\nğŸ“Œ 7.4 Supprimer par requÃªte\nPOST /clients/_delete_by_query\n{\n  \"query\": {\n    \"range\": { \"age\": { \"lt\": 18 } }\n  }\n}\nâœ… SQL Ã©quivalent : DELETE FROM clients WHERE age &lt; 18\n\n\n\nğŸ“Œ 7.5 Supprimer un index entier\nDELETE /clients\n\nâš ï¸ Attention : IrrÃ©versible !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet Elasticsearch",
    "text": "ğŸ“‹ Cheatsheet Elasticsearch\n\nğŸ”§ Gestion des index\n\n\n\nAction\nRequÃªte\n\n\n\n\nCrÃ©er un index\nPUT /mon_index\n\n\nSupprimer\nDELETE /mon_index\n\n\nLister\nGET /_cat/indices?v\n\n\nVoir mapping\nGET /mon_index/_mapping\n\n\nSantÃ© cluster\nGET /_cluster/health\n\n\n\n\n\nğŸ“ CRUD Documents\n\n\n\nAction\nRequÃªte\n\n\n\n\nInsÃ©rer\nPOST /index/_doc\n\n\nLire (ID)\nGET /index/_doc/1\n\n\nMettre Ã  jour\nPOST /index/_update/1\n\n\nSupprimer\nDELETE /index/_doc/1\n\n\nBulk\nPOST /_bulk\n\n\n\n\n\nğŸ” Types de requÃªtes\n\n\n\nType\nUsage\n\n\n\n\nmatch\nFull-text (champs text)\n\n\nterm\nExact (champs keyword)\n\n\nrange\nPlage (nombres, dates)\n\n\nbool\nCombiner (must, should, must_not)\n\n\nfuzzy\nTolÃ©rant aux fautes\n\n\n\n\n\nğŸ“Š AgrÃ©gations\n\n\n\nType\nSQL Ã©quivalent\n\n\n\n\nterms\nGROUP BY\n\n\nsum, avg, min, max\nFonctions dâ€™agrÃ©gation\n\n\nstats\nToutes les stats\n\n\ncardinality\nCOUNT(DISTINCT)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Exercices pratiques",
    "text": "ğŸ¯ Exercices pratiques\nUtilise lâ€™onglet REST dâ€™Elasticvue pour rÃ©soudre ces exercices.\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nAfficher tous les clients.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nTrouver les clients de CÃ´te dâ€™Ivoire.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"CÃ´te d'Ivoire\" }\n  }\n}\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nTrouver les clients avec un salaire entre 50000 et 60000, triÃ©s par Ã¢ge dÃ©croissant.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"salaire\": { \"gte\": 50000, \"lte\": 60000 }\n    }\n  },\n  \"sort\": [{ \"age\": \"desc\" }]\n}\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nCalculer le nombre de clients par pays.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nCalculer le salaire moyen par pays, seulement pour les clients de plus de 25 ans.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"age\": { \"gt\": 25 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } }\n      }\n    }\n  }\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quelle requÃªte crÃ©e un index ?\n\nPOST /clients\n\nPUT /clients\n\nGET /clients\n\nCREATE /clients\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” PUT /index crÃ©e un nouvel index.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre text et keyword ?\n\nAucune diffÃ©rence\n\ntext est analysÃ© (tokenisÃ©), keyword est stockÃ© tel quel\n\nkeyword est plus rapide\n\ntext est pour les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” text est dÃ©coupÃ© en tokens pour la recherche full-text, keyword reste intact pour les matchs exacts et agrÃ©gations.\n\n\n\n\nâ“ Q3. Quelle requÃªte utiliser pour un GROUP BY ?\n\nmatch\n\nbool\n\nterms (dans aggs)\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Lâ€™agrÃ©gation terms groupe les documents par valeur de champ.\n\n\n\n\nâ“ Q4. Que fait size: 0 dans une requÃªte dâ€™agrÃ©gation ?\n\nSupprime les donnÃ©es\n\nRetourne uniquement lâ€™agrÃ©gation, pas les documents\n\nLimite Ã  0 rÃ©sultat dâ€™agrÃ©gation\n\nErreur\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” size: 0 Ã©vite de retourner les documents, seulement les rÃ©sultats dâ€™agrÃ©gation.\n\n\n\n\nâ“ Q5. Quelle requÃªte permet de chercher â€œAliceâ€ mÃªme si on tape â€œAliseâ€ ?\n\nmatch\n\nterm\n\nfuzzy\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” fuzzy tolÃ¨re les fautes de frappe.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nElasticsearch Documentation\nElasticvue â€” Interface graphique utilisÃ©e dans ce cours\nKibana â€” Visualisation et dashboards\nElastic Cloud â€” Version cloud managÃ©e",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ” Elasticsearch for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant les bases NoSQL (MongoDB et Elasticsearch) ! Passons au traitement distribuÃ© avec PySpark.\nğŸ‘‰ Module suivant : 11_pyspark_for_data_engineering.ipynb â€” PySpark pour le traitement Big Data\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Elasticsearch pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ” Elasticsearch for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html",
    "href": "notebooks/beginner/12_orchestration_pipelines.html",
    "title": "â° Orchestration de Pipelines Data",
    "section": "",
    "text": "Ce module prÃ©sente les outils dâ€™orchestration pour automatiser lâ€™exÃ©cution de vos pipelines de donnÃ©es.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 11_pyspark_for_data_engineering\n\n\nâœ… Requis\nComprendre les pipelines ETL\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nConnaÃ®tre les bases de Linux (ligne de commande)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre ce quâ€™est lâ€™orchestration de pipelines\nâœ… Utiliser le Planificateur Windows (niveau dÃ©butant)\nâœ… Configurer des tÃ¢ches avec Crontab (niveau intermÃ©diaire)\nâœ… Comprendre lâ€™architecture dâ€™Apache Airflow\nâœ… CrÃ©er des DAGs avec Apache Airflow\nâœ… Utiliser les diffÃ©rents types dâ€™Operators\nâœ… GÃ©rer les dÃ©pendances et le passage de donnÃ©es (XCom)\nâœ… Configurer les alertes et le monitoring\nâœ… Choisir le bon outil selon ton besoin",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering",
    "text": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering\nTu as appris Ã  crÃ©er des pipelines ETL avec PySpark. Mais comment les automatiser pour quâ€™ils sâ€™exÃ©cutent rÃ©guliÃ¨rement sans intervention manuelle ?\n\nLe problÃ¨me\nSans orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   ğŸ˜° \"Il faut que je lance mon script tous les jours...\"   â”‚\nâ”‚   ğŸ˜° \"J'ai oubliÃ© de lancer le pipeline hier !\"            â”‚\nâ”‚   ğŸ˜° \"Le script B a plantÃ© car A n'avait pas fini...\"      â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nAvec orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   âœ… Scripts exÃ©cutÃ©s automatiquement                       â”‚\nâ”‚   âœ… Alertes en cas d'Ã©chec                                 â”‚\nâ”‚   âœ… DÃ©pendances respectÃ©es (A â†’ B â†’ C)                     â”‚\nâ”‚   âœ… Logs et monitoring centralisÃ©s                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPosition dans le pipeline Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     PIPELINE DATA                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources        ETL              Destination                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚\nâ”‚                                                                 â”‚\nâ”‚   APIs     â”€â”                 â”Œâ”€â–º  Data Warehouse               â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  PySpark  â”€â”€â”€â”¼â”€â–º  Data Lake                    â”‚\nâ”‚   BDD      â”€â”˜                 â””â”€â–º  Dashboard                    â”‚\nâ”‚                                                                 â”‚\nâ”‚            â–²                                                    â”‚\nâ”‚            â”‚                                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\nâ”‚   â”‚  ORCHESTRATION  â”‚  â—„â”€â”€ Quand ? Dans quel ordre ?           â”‚\nâ”‚   â”‚  (Airflow/Cron) â”‚                                           â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Comparaison rapide des outils",
    "text": "ğŸ“Š Comparaison rapide des outils\n\n\n\nCritÃ¨re\nWindows Task\nCrontab\nAirflow\n\n\n\n\nFacilitÃ©\nâ­â­â­\nâ­â­\nâ­\n\n\nInterface\nGUI\nCLI\nWeb\n\n\nDÃ©pendances\nâŒ\nâŒ\nâœ…\n\n\nMonitoring\nBasique\nNon\nComplet\n\n\nRetry auto\nâŒ\nâŒ\nâœ…\n\n\nIdÃ©al pour\n1-5 scripts\n5-15 scripts\n10+ pipelines",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn outil intÃ©grÃ© Ã  Windows pour exÃ©cuter des programmes automatiquement.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n\nOuvrir le planificateur :\nWindows + R â†’ Taper 'taskschd.msc' â†’ EntrÃ©e\n\n\nCrÃ©er une tÃ¢che :\n\nAction â†’ CrÃ©er une tÃ¢che de base\nNom : â€œMon script quotidienâ€\nDÃ©clencheur : Quotidien Ã  2h du matin\nAction : DÃ©marrer python.exe avec C:\\scripts\\mon_script.py\nTerminer\n\nâœ… VoilÃ  ! Votre script sâ€™exÃ©cutera automatiquement tous les jours Ã  2h.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces",
    "text": "âœ… Forces\nâœ… TrÃ¨s facile - Interface graphique intuitive\nâœ… DÃ©jÃ  installÃ© - Pas de setup\nâœ… Parfait pour dÃ©buter - Pas de code complexe",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses",
    "text": "âŒ Faiblesses\nâŒ Pas de dÃ©pendances - Si tÃ¢che A doit finir avant tÃ¢che B â†’ compliquÃ©\nâŒ Monitoring limitÃ© - Difficile de voir lâ€™Ã©tat global\nâŒ Windows uniquement - Ne fonctionne pas sur Linux",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nâœ… OUI : Vous avez 1-5 scripts simples sur Windows\nâŒ NON : Vous avez besoin que script B attende script A",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nLe planificateur standard sur Linux/Mac.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Syntaxe de base",
    "text": "Syntaxe de base\nminute heure jour mois jour_semaine commande\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande\n\nExemples simples :\n# Tous les jours Ã  2h du matin\n0 2 * * * python3 /home/user/script.py\n\n# Toutes les heures\n0 * * * * python3 /home/user/hourly.py\n\n# Lundi Ã  vendredi Ã  9h\n0 9 * * 1-5 python3 /home/user/weekday.py\n\n# Toutes les 15 minutes\n*/15 * * * * python3 /home/user/check.py\n\n# Le 1er de chaque mois Ã  minuit\n0 0 1 * * python3 /home/user/monthly.py",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n# Ã‰diter votre crontab\ncrontab -e\n\n# Voir les tÃ¢ches planifiÃ©es\ncrontab -l\n\n# Ajouter vos lignes\n0 2 * * * python3 /home/user/backup.py &gt;&gt; /var/log/backup.log 2&gt;&1\n\n# Sauvegarder et quitter\n# âœ… C'est fait !\n\nğŸ’¡ Astuce : Utilise crontab.guru pour tester tes expressions !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces",
    "text": "âœ… Forces\nâœ… Universel - Sur TOUS les serveurs Linux\nâœ… TrÃ¨s lÃ©ger - Presque pas de ressources\nâœ… Simple - Une ligne = une tÃ¢che\nâœ… Gratuit - DÃ©jÃ  installÃ©",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses",
    "text": "âŒ Faiblesses\nâŒ Pas de dÃ©pendances - MÃªme problÃ¨me que Windows\nâŒ Pas de monitoring - Aucune interface\nâŒ Pas de retry - Si Ã§a Ã©choue, il faut attendre le prochain run\nâŒ Logs manuels - Il faut les gÃ©rer soi-mÃªme",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nâœ… OUI : Serveur Linux, 5-15 scripts indÃ©pendants\nâŒ NON : Scripts avec dÃ©pendances complexes",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn orchestrateur professionnel pour gÃ©rer des workflows complexes, crÃ©Ã© par Airbnb en 2014 et devenu projet Apache en 2016.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    APACHE AIRFLOW                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   \"Airflow is a platform to programmatically author,           â”‚\nâ”‚    schedule, and monitor workflows.\"                            â”‚\nâ”‚                                                                 â”‚\nâ”‚   â€¢ CrÃ©Ã© par Airbnb (2014)                                      â”‚\nâ”‚   â€¢ Apache Top-Level Project (2019)                             â”‚\nâ”‚   â€¢ 30,000+ GitHub stars                                        â”‚\nâ”‚   â€¢ UtilisÃ© par : Airbnb, Netflix, Spotify, Twitter, Adobe...   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nLa grande diffÃ©rence avec Cron :\n# âŒ Avec Cron (problÃ¨me) :\n0 2 * * * python extract.py\n30 2 * * * python transform.py  # Et si extract prend plus de 30min ?\n0 3 * * * python load.py        # Et si transform a plantÃ© ?\n\n# âœ… Avec Airflow (solution) :\nextract &gt;&gt; transform &gt;&gt; load  # transform ATTEND que extract soit terminÃ© !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ—ï¸ Architecture dâ€™Airflow",
    "text": "ğŸ—ï¸ Architecture dâ€™Airflow\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        ARCHITECTURE AIRFLOW                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â”‚   WEB SERVER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    SCHEDULER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  EXECUTOR   â”‚  â”‚\nâ”‚   â”‚   (Flask UI)    â”‚         â”‚  (Orchestrate)  â”‚         â”‚  (Workers)  â”‚  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚            â”‚                           â”‚                         â”‚         â”‚\nâ”‚            â”‚                           â–¼                         â”‚         â”‚\nâ”‚            â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚         â”‚\nâ”‚            â”‚                  â”‚                 â”‚                â”‚         â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    METADATA     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                               â”‚    DATABASE     â”‚                          â”‚\nâ”‚                               â”‚  (PostgreSQL)   â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                        â–²                                   â”‚\nâ”‚                                        â”‚                                   â”‚\nâ”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â”‚    DAG FILES    â”‚                          â”‚\nâ”‚                               â”‚   (Python .py)  â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nComposants clÃ©s\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nDescription\n\n\n\n\nWeb Server\nInterface UI\nDashboard Flask pour visualiser les DAGs\n\n\nScheduler\nPlanification\nDÃ©cide quand exÃ©cuter les tÃ¢ches\n\n\nExecutor\nExÃ©cution\nLance les tÃ¢ches (Local, Celery, K8sâ€¦)\n\n\nMetadata DB\nStockage\nÃ‰tat des DAGs, logs, historique\n\n\nDAG Files\nDÃ©finition\nFichiers Python dÃ©finissant les workflows\n\n\n\n\n\nTypes dâ€™Executors\n\n\n\nExecutor\nUsage\nScalabilitÃ©\n\n\n\n\nSequentialExecutor\nDev/Test\n1 tÃ¢che Ã  la fois\n\n\nLocalExecutor\nPetite prod\nParallÃ¨le sur 1 machine\n\n\nCeleryExecutor\nProduction\nWorkers distribuÃ©s\n\n\nKubernetesExecutor\nCloud\nPod par tÃ¢che",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“š Concepts clÃ©s dâ€™Airflow",
    "text": "ğŸ“š Concepts clÃ©s dâ€™Airflow\n\n1ï¸âƒ£ DAG (Directed Acyclic Graph)\nUn DAG est un graphe de tÃ¢ches sans cycle : les donnÃ©es vont toujours dans une direction.\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚ Extract â”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n          â–¼\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚Transformâ”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n    â–¼           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Load DWâ”‚  â”‚Load S3 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2ï¸âƒ£ Task\nUne Task est une unitÃ© de travail dans un DAG (une Ã©tape).\n\n\n3ï¸âƒ£ Operator\nUn Operator dÃ©finit ce que fait une tÃ¢che.\n\n\n\nOperator\nUsage\n\n\n\n\nPythonOperator\nExÃ©cuter une fonction Python\n\n\nBashOperator\nExÃ©cuter une commande bash\n\n\nEmailOperator\nEnvoyer un email\n\n\nPostgresOperator\nExÃ©cuter du SQL\n\n\nS3ToRedshiftOperator\nCopier S3 â†’ Redshift\n\n\n\n\n\n4ï¸âƒ£ Task Instance\nUne Task Instance = une exÃ©cution spÃ©cifique dâ€™une Task Ã  une date donnÃ©e.\n\n\n5ï¸âƒ£ DAG Run\nUn DAG Run = une exÃ©cution complÃ¨te du DAG.\nDAG: etl_pipeline\nâ”œâ”€â”€ DAG Run 2024-01-15 âœ…\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) âœ…\nâ”‚   â””â”€â”€ load (Task Instance) âœ…\nâ”‚\nâ”œâ”€â”€ DAG Run 2024-01-16 â³\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) â³ running\nâ”‚   â””â”€â”€ load (Task Instance) â¸ï¸ waiting",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”§ Installation locale dâ€™Airflow",
    "text": "ğŸ”§ Installation locale dâ€™Airflow\n# CrÃ©er un environnement virtuel\npython -m venv airflow_venv\nsource airflow_venv/bin/activate  # Linux/Mac\n# ou : airflow_venv\\Scripts\\activate  # Windows\n\n# DÃ©finir le home Airflow\nexport AIRFLOW_HOME=~/airflow\n\n# Installer Airflow (version contrainte pour compatibilitÃ©)\nAIRFLOW_VERSION=2.8.1\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# Initialiser la base de donnÃ©es\nairflow db init\n\n# CrÃ©er un utilisateur admin\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin\n\n# Lancer le webserver (Terminal 1)\nairflow webserver --port 8080\n\n# Lancer le scheduler (Terminal 2)\nairflow scheduler\nğŸ‘‰ AccÃ©der : http://localhost:8080 (login: admin / admin)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“ Ton premier DAG",
    "text": "ğŸ“ Ton premier DAG\nCrÃ©er le fichier ~/airflow/dags/mon_premier_dag.py :\n\n\nCode\n# ~/airflow/dags/mon_premier_dag.py\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\n# Arguments par dÃ©faut pour toutes les tÃ¢ches\ndefault_args = {\n    'owner': 'data_engineer',\n    'depends_on_past': False,\n    'email': ['data-team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DÃ©finir le DAG\ndag = DAG(\n    dag_id='mon_premier_dag',\n    default_args=default_args,\n    description='Mon premier pipeline ETL',\n    schedule_interval='@daily',  # ExÃ©cution quotidienne\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Ne pas exÃ©cuter les runs passÃ©s\n    tags=['etl', 'tutorial'],\n)\n\n# Fonctions Python\ndef extract():\n    \"\"\"Extraire les donnÃ©es\"\"\"\n    print(\"ğŸ“¥ Extraction des donnÃ©es depuis l'API...\")\n    # Simuler extraction\n    data = {'records': 1000, 'source': 'api'}\n    return data  # RetournÃ© via XCom\n\ndef transform(**context):\n    \"\"\"Transformer les donnÃ©es\"\"\"\n    # RÃ©cupÃ©rer les donnÃ©es de extract via XCom\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ğŸ”„ Transformation de {data['records']} enregistrements\")\n    return {'records': data['records'], 'cleaned': True}\n\ndef load(**context):\n    \"\"\"Charger les donnÃ©es\"\"\"\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='transform')\n    print(f\"ğŸ’¾ Chargement de {data['records']} enregistrements nettoyÃ©s\")\n\n# CrÃ©er les tÃ¢ches\ntask_start = BashOperator(\n    task_id='start',\n    bash_command='echo \"ğŸš€ DÃ©marrage du pipeline Ã  $(date)\"',\n    dag=dag,\n)\n\ntask_extract = PythonOperator(\n    task_id='extract',\n    python_callable=extract,\n    dag=dag,\n)\n\ntask_transform = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\ntask_load = PythonOperator(\n    task_id='load',\n    python_callable=load,\n    dag=dag,\n)\n\ntask_end = BashOperator(\n    task_id='end',\n    bash_command='echo \"âœ… Pipeline terminÃ© avec succÃ¨s !\"',\n    dag=dag,\n)\n\n# DÃ©finir les dÃ©pendances\ntask_start &gt;&gt; task_extract &gt;&gt; task_transform &gt;&gt; task_load &gt;&gt; task_end\n\n# Ã‰quivalent Ã  :\n# task_start.set_downstream(task_extract)\n# task_extract.set_downstream(task_transform)\n# etc.",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âš™ï¸ ParamÃ¨tres importants du DAG",
    "text": "âš™ï¸ ParamÃ¨tres importants du DAG\n\nSchedule Interval (frÃ©quence dâ€™exÃ©cution)\n\n\n\nPreset\nÃ‰quivalent Cron\nDescription\n\n\n\n\n@once\n-\nUne seule fois\n\n\n@hourly\n0 * * * *\nChaque heure\n\n\n@daily\n0 0 * * *\nChaque jour Ã  minuit\n\n\n@weekly\n0 0 * * 0\nChaque dimanche\n\n\n@monthly\n0 0 1 * *\nLe 1er du mois\n\n\n@yearly\n0 0 1 1 *\nLe 1er janvier\n\n\nNone\n-\nDÃ©clenchÃ© manuellement\n\n\n'0 6 * * 1-5'\nCron\nLun-Ven Ã  6h\n\n\n\n\n\nCatchup\n# catchup=True (dÃ©faut) :\n# Si start_date=2024-01-01 et on est le 2024-01-10,\n# Airflow va exÃ©cuter les 10 DAG Runs manquÃ©s !\n\n# catchup=False :\n# ExÃ©cute seulement Ã  partir de maintenant\n\n\nDefault Args importants\ndefault_args = {\n    'owner': 'data_team',           # PropriÃ©taire\n    'depends_on_past': False,       # DÃ©pend du run prÃ©cÃ©dent ?\n    'email_on_failure': True,       # Email si Ã©chec\n    'retries': 3,                   # Nombre de retry\n    'retry_delay': timedelta(minutes=5),  # DÃ©lai entre retries\n    'execution_timeout': timedelta(hours=1),  # Timeout\n    'sla': timedelta(hours=2),      # SLA (alerte si dÃ©passÃ©)\n}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”§ Les Operators les plus utilisÃ©s",
    "text": "ğŸ”§ Les Operators les plus utilisÃ©s\n\nPythonOperator\nfrom airflow.operators.python import PythonOperator\n\ndef my_function(name, **context):\n    print(f\"Hello {name}!\")\n    print(f\"Execution date: {context['ds']}\")\n    return \"success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    op_kwargs={'name': 'World'},  # Arguments de la fonction\n)\n\n\nBashOperator\nfrom airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Date: {{ ds }}\" && python /scripts/etl.py',\n)\n\n\nPostgresOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',  # Connexion dÃ©finie dans UI\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100)\n        );\n    \"\"\",\n)\n\n\nEmailOperator\nfrom airflow.operators.email import EmailOperator\n\ntask = EmailOperator(\n    task_id='send_report',\n    to='team@company.com',\n    subject='Pipeline Report - {{ ds }}',\n    html_content='&lt;h1&gt;Pipeline completed!&lt;/h1&gt;',\n)\n\n\nEmptyOperator (anciennement DummyOperator)\nfrom airflow.operators.empty import EmptyOperator\n\n# Utile pour crÃ©er des points de jonction\nstart = EmptyOperator(task_id='start')\nend = EmptyOperator(task_id='end')",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“¬ XCom â€” Passer des donnÃ©es entre tÃ¢ches",
    "text": "ğŸ“¬ XCom â€” Passer des donnÃ©es entre tÃ¢ches\nXCom (Cross-Communication) permet de partager des donnÃ©es entre tÃ¢ches.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         XCom          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Task A  â”‚ â”€â”€â”€â”€â”€â”€â”€ data â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Task B  â”‚\nâ”‚  return  â”‚                       â”‚ xcom_pull â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nMÃ©thode 1 : Return (automatique)\n\n\nCode\n# XCom avec return (automatique)\n\ndef extract():\n    data = {'records': 1000, 'status': 'ok'}\n    return data  # Automatiquement stockÃ© dans XCom\n\ndef transform(**context):\n    # RÃ©cupÃ©rer via ti (task instance)\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ReÃ§u: {data}\")\n    return {'records': data['records'], 'transformed': True}\n\n\n\n\nMÃ©thode 2 : Push/Pull explicite\ndef task_a(**context):\n    # Push explicite avec une clÃ©\n    context['ti'].xcom_push(key='my_data', value={'count': 42})\n    context['ti'].xcom_push(key='status', value='success')\n\ndef task_b(**context):\n    # Pull avec la clÃ©\n    data = context['ti'].xcom_pull(task_ids='task_a', key='my_data')\n    status = context['ti'].xcom_pull(task_ids='task_a', key='status')\n\n\nâš ï¸ Limites de XCom\n\n\n\nLimite\nDescription\n\n\n\n\nTaille\nMax ~48KB par dÃ©faut (stockÃ© en DB)\n\n\nSÃ©rialisation\nDoit Ãªtre JSON-sÃ©rialisable\n\n\nPas pour big data\nUtiliser S3/GCS pour gros fichiers\n\n\n\n# âŒ Mauvaise pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')  # 1GB\n    return df.to_dict()  # âŒ Trop gros pour XCom !\n\n# âœ… Bonne pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')\n    path = '/data/output/extract_2024-01-15.parquet'\n    df.to_parquet(path)\n    return path  # âœ… Passer le chemin, pas les donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ¯ TaskFlow API (Airflow 2.0+)",
    "text": "ğŸ¯ TaskFlow API (Airflow 2.0+)\nSyntaxe moderne et plus Pythonic avec des dÃ©corateurs :\n\n\nCode\n# TaskFlow API - Syntaxe moderne (Airflow 2.0+)\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='etl_taskflow',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'taskflow'],\n)\ndef etl_pipeline():\n    \n    @task()\n    def extract():\n        \"\"\"Extraire les donnÃ©es\"\"\"\n        return {'records': 1000, 'source': 'api'}\n    \n    @task()\n    def transform(data: dict):\n        \"\"\"Transformer les donnÃ©es\"\"\"\n        return {\n            'records': data['records'],\n            'cleaned': True\n        }\n    \n    @task()\n    def load(data: dict):\n        \"\"\"Charger les donnÃ©es\"\"\"\n        print(f\"Loading {data['records']} records\")\n    \n    # DÃ©finir le flow - XCom automatique !\n    raw_data = extract()\n    clean_data = transform(raw_data)\n    load(clean_data)\n\n# Instancier le DAG\netl_pipeline()\n\n\n\nAvantages TaskFlow API\n\n\n\nAvantage\nDescription\n\n\n\n\nXCom automatique\nLes retours sont passÃ©s automatiquement\n\n\nCode plus lisible\nRessemble Ã  du Python normal\n\n\nType hints\nSupport des annotations de type\n\n\nMoins de boilerplate\nPas besoin de crÃ©er des Operators",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ‘ï¸ Sensors â€” Attendre une condition",
    "text": "ğŸ‘ï¸ Sensors â€” Attendre une condition\nLes Sensors attendent quâ€™une condition soit remplie avant de continuer.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                              â”‚\nâ”‚   FileSensor          S3KeySensor         HttpSensor         â”‚\nâ”‚   \"Fichier existe?\"   \"Fichier sur S3?\"   \"API disponible?\"  â”‚\nâ”‚                                                              â”‚\nâ”‚        â³                   â³                   â³            â”‚\nâ”‚        â”‚                    â”‚                    â”‚           â”‚\nâ”‚        â–¼                    â–¼                    â–¼           â”‚\nâ”‚       âœ…                   âœ…                   âœ…           â”‚\nâ”‚   Continuer            Continuer            Continuer        â”‚\nâ”‚                                                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nFileSensor\n\n\nCode\n# FileSensor - Attendre qu'un fichier existe\n\nfrom airflow.sensors.filesystem import FileSensor\n\nwait_for_file = FileSensor(\n    task_id='wait_for_file',\n    filepath='/data/input/daily_export.csv',\n    poke_interval=60,      # VÃ©rifier toutes les 60 secondes\n    timeout=3600,          # Timeout aprÃ¨s 1 heure\n    mode='poke',           # poke ou reschedule\n)\n\n# Le pipeline attend le fichier avant de continuer\nwait_for_file &gt;&gt; process_file\n\n\n\n\nAutres Sensors utiles\n# S3KeySensor - Attendre un fichier sur S3\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n\nwait_s3 = S3KeySensor(\n    task_id='wait_for_s3',\n    bucket_name='my-bucket',\n    bucket_key='data/{{ ds }}/export.csv',\n    aws_conn_id='aws_default',\n)\n\n# HttpSensor - Attendre qu'une API rÃ©ponde\nfrom airflow.providers.http.sensors.http import HttpSensor\n\nwait_api = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='api_connection',\n    endpoint='health',\n    response_check=lambda response: response.status_code == 200,\n)\n\n# ExternalTaskSensor - Attendre un autre DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\nwait_other_dag = ExternalTaskSensor(\n    task_id='wait_upstream',\n    external_dag_id='upstream_dag',\n    external_task_id='final_task',\n)\n\n\nMode poke vs reschedule\n\n\n\nMode\nDescription\nRessources\n\n\n\n\npoke\nGarde un worker slot\nConsomme plus\n\n\nreschedule\nLibÃ¨re le slot entre checks\nRecommandÃ©",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ”€ Branching â€” Logique conditionnelle",
    "text": "ğŸ”€ Branching â€” Logique conditionnelle\nExÃ©cuter diffÃ©rentes tÃ¢ches selon une condition.\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Check   â”‚\n                    â”‚ Conditionâ”‚\n                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n                         â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚          â”‚          â”‚\n              â–¼          â–¼          â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Path A â”‚ â”‚ Path B â”‚ â”‚ Path C â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n# Branching - ExÃ©cution conditionnelle\n\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndef choose_branch(**context):\n    \"\"\"DÃ©cider quelle branche exÃ©cuter\"\"\"\n    # Exemple : vÃ©rifier le jour de la semaine\n    day = context['ds_nodash']  # Format YYYYMMDD\n    \n    # Logique mÃ©tier\n    if int(day) % 2 == 0:\n        return 'process_even'  # Retourner le task_id Ã  exÃ©cuter\n    else:\n        return 'process_odd'\n\nbranch = BranchPythonOperator(\n    task_id='branch',\n    python_callable=choose_branch,\n)\n\nprocess_even = EmptyOperator(task_id='process_even')\nprocess_odd = EmptyOperator(task_id='process_odd')\nend = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')\n\n# DÃ©finir le flow\nbranch &gt;&gt; [process_even, process_odd] &gt;&gt; end",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸšï¸ Trigger Rules",
    "text": "ğŸšï¸ Trigger Rules\nContrÃ´ler quand une tÃ¢che sâ€™exÃ©cute en fonction du statut des tÃ¢ches parentes.\n\n\n\nTrigger Rule\nExÃ©cute siâ€¦\n\n\n\n\nall_success\nTous les parents ont rÃ©ussi (dÃ©faut)\n\n\nall_failed\nTous les parents ont Ã©chouÃ©\n\n\nall_done\nTous les parents sont terminÃ©s (peu importe le statut)\n\n\none_success\nAu moins un parent a rÃ©ussi\n\n\none_failed\nAu moins un parent a Ã©chouÃ©\n\n\nnone_failed\nAucun parent nâ€™a Ã©chouÃ© (succÃ¨s ou skipped)\n\n\nnone_skipped\nAucun parent nâ€™a Ã©tÃ© skipped\n\n\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# TÃ¢che de notification en cas d'Ã©chec\nnotify_failure = EmailOperator(\n    task_id='notify_failure',\n    to='team@company.com',\n    subject='Pipeline Failed!',\n    html_content='...',\n    trigger_rule=TriggerRule.ONE_FAILED,  # ExÃ©cute si un parent Ã©choue\n)\n\n# TÃ¢che finale qui s'exÃ©cute toujours\ncleanup = BashOperator(\n    task_id='cleanup',\n    bash_command='rm -rf /tmp/data/*',\n    trigger_rule=TriggerRule.ALL_DONE,  # Toujours exÃ©cuter\n)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ” Connections et Variables",
    "text": "ğŸ” Connections et Variables\n\nConnections\nStocker les informations de connexion aux systÃ¨mes externes.\nDans lâ€™UI : Admin â†’ Connections â†’ +\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Add Connection                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  Connection Id:   â”‚ my_postgres                 â”‚           â”‚\nâ”‚  Connection Type: â”‚ Postgres           â–¼        â”‚           â”‚\nâ”‚  Host:            â”‚ localhost                   â”‚           â”‚\nâ”‚  Schema:          â”‚ mydb                        â”‚           â”‚\nâ”‚  Login:           â”‚ admin                       â”‚           â”‚\nâ”‚  Password:        â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                    â”‚           â”‚\nâ”‚  Port:            â”‚ 5432                        â”‚           â”‚\nâ”‚                                                             â”‚\nâ”‚                              [ Save ]                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nUtilisation dans le code :\nfrom airflow.hooks.postgres_hook import PostgresHook\n\ndef query_postgres():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df('SELECT * FROM users')\n    return df\n\n\nVariables\nStocker des configurations rÃ©utilisables.\nDans lâ€™UI : Admin â†’ Variables â†’ +\nfrom airflow.models import Variable\n\n# RÃ©cupÃ©rer une variable\napi_key = Variable.get('API_KEY')\n\n# Variable JSON\nconfig = Variable.get('pipeline_config', deserialize_json=True)\n# config = {'batch_size': 1000, 'env': 'prod'}\n\n# Dans un template Jinja\n# {{ var.value.API_KEY }}\n# {{ var.json.pipeline_config.batch_size }}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Monitoring et Alertes",
    "text": "ğŸ“Š Monitoring et Alertes\n\nInterface Web\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Airflow - DAGs                                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  DAG                    Schedule    Owner    Runs   Recent Tasks            â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€    â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚  â–¶ etl_pipeline         @daily      team     125    âœ…âœ…âœ…âœ…âœ…               â”‚\nâ”‚  â–¶ data_quality_check   @hourly     team     560    âœ…âœ…âœ…âŒâœ…               â”‚\nâ”‚  â–¶ weekly_report        @weekly     team     52     âœ…âœ…âœ…âœ…âœ…               â”‚\nâ”‚  â¸ maintenance          None        admin    3      âœ…âœ…âœ…                   â”‚\nâ”‚                                                                             â”‚\nâ”‚  âœ… Success  âŒ Failed  â³ Running  â¸ Paused                                â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVues disponibles\n\n\n\nVue\nDescription\n\n\n\n\nGrid\nVue matricielle des runs\n\n\nGraph\nGraphe du DAG\n\n\nCalendar\nHistorique par date\n\n\nGantt\nTimeline dâ€™exÃ©cution\n\n\nCode\nCode source du DAG\n\n\n\n\n\nConfigurer les alertes email\n# airflow.cfg\n[smtp]\nsmtp_host = smtp.gmail.com\nsmtp_port = 587\nsmtp_user = airflow@company.com\nsmtp_password = your_password\nsmtp_mail_from = airflow@company.com\n\n# Dans le DAG\ndefault_args = {\n    'email': ['team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n}\n\n\nAlertes Slack\nfrom airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\n\ndef alert_slack_on_failure(context):\n    \"\"\"Callback en cas d'Ã©chec\"\"\"\n    slack_msg = f\"\"\"\n        :red_circle: Task Failed!\n        *DAG*: {context['dag'].dag_id}\n        *Task*: {context['task'].task_id}\n        *Execution Time*: {context['execution_date']}\n    \"\"\"\n    return SlackWebhookOperator(\n        task_id='slack_alert',\n        slack_webhook_conn_id='slack_webhook',\n        message=slack_msg,\n    ).execute(context)\n\n# Utiliser le callback\ndefault_args = {\n    'on_failure_callback': alert_slack_on_failure,\n}",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Bonnes pratiques Airflow",
    "text": "âœ… Bonnes pratiques Airflow\n\n1. Structure des DAGs\nairflow/\nâ”œâ”€â”€ dags/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ etl/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ daily_etl.py\nâ”‚   â”‚   â””â”€â”€ weekly_report.py\nâ”‚   â”œâ”€â”€ utils/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ helpers.py\nâ”‚   â””â”€â”€ config/\nâ”‚       â””â”€â”€ settings.py\nâ”œâ”€â”€ plugins/\nâ”‚   â””â”€â”€ custom_operators/\nâ””â”€â”€ tests/\n    â””â”€â”€ test_dags.py\n\n\n2. Idempotence\n# âŒ Non idempotent - accumule des donnÃ©es\ndef bad_load():\n    db.execute(\"INSERT INTO table VALUES (...)\")\n\n# âœ… Idempotent - mÃªme rÃ©sultat si relancÃ©\ndef good_load():\n    db.execute(\"DELETE FROM table WHERE date = '{{ ds }}'\")\n    db.execute(\"INSERT INTO table SELECT ... WHERE date = '{{ ds }}'\")\n\n\n3. AtomicitÃ© des tÃ¢ches\n# âŒ TÃ¢che trop grosse\ndef do_everything():\n    extract()\n    transform()\n    load()\n\n# âœ… TÃ¢ches atomiques\nextract &gt;&gt; transform &gt;&gt; load\n\n\n4. Ne pas mettre de logique dans le DAG\n# âŒ Code exÃ©cutÃ© Ã  chaque parsing\ndata = fetch_from_api()  # AppelÃ© toutes les 30s !\n\n# âœ… Logique dans les tasks\n@task\ndef fetch_data():\n    return fetch_from_api()\n\n\n5. Tester les DAGs\n# tests/test_dags.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dag_bag = DagBag()\n    assert len(dag_bag.import_errors) == 0\n\ndef test_dag_structure():\n    dag_bag = DagBag()\n    dag = dag_bag.get_dag('etl_pipeline')\n    assert dag is not None\n    assert len(dag.tasks) == 5",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âœ… Forces dâ€™Airflow",
    "text": "âœ… Forces dâ€™Airflow\nâœ… Gestion des dÃ©pendances - A &gt;&gt; B = B attend A\nâœ… Retry automatique - RÃ©essaie en cas dâ€™Ã©chec\nâœ… Interface web - Visualisation complÃ¨te\nâœ… Monitoring - Logs centralisÃ©s\nâœ… Alertes - Email/Slack en cas dâ€™Ã©chec\nâœ… Scalable - GÃ¨re 100+ pipelines\nâœ… Extensible - Custom operators, hooks\nâœ… CommunautÃ© - TrÃ¨s active, beaucoup de providers",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses dâ€™Airflow",
    "text": "âŒ Faiblesses dâ€™Airflow\nâŒ Complexe - Courbe dâ€™apprentissage\nâŒ Ressources - Besoin de 4-8 GB RAM\nâŒ Overkill - Pour 1-3 scripts simples\nâŒ Setup - Installation et configuration\nâŒ Pas pour le streaming - Batch only (utiliser Kafka)",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quand utiliser Airflow ?",
    "text": "Quand utiliser Airflow ?\nâœ… OUI : 10+ pipelines, dÃ©pendances complexes, production\nâŒ NON : 1-5 scripts simples sans dÃ©pendances",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Quel outil choisir ?",
    "text": "Quel outil choisir ?\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nJâ€™ai 1-3 scripts sur Windows\nğŸªŸ Task Scheduler\n\n\nJâ€™ai 1-3 scripts sur Linux\nğŸ§ Crontab\n\n\nJâ€™ai 5-10 scripts indÃ©pendants\nğŸ§ Crontab\n\n\nJâ€™ai 10+ scripts avec dÃ©pendances\nğŸŒ¬ï¸ Airflow\n\n\nScript B doit attendre script A\nğŸŒ¬ï¸ Airflow\n\n\nJe veux un dashboard\nğŸŒ¬ï¸ Airflow\n\n\nJe dÃ©bute en automatisation\nğŸªŸ Task Scheduler\n\n\nProduction critique\nğŸŒ¬ï¸ Airflow",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Progression naturelle",
    "text": "Progression naturelle\n1. DÃ©butez avec Task Scheduler ou Cron\n2. Quand vous avez 5+ scripts â†’ Pensez Ã  migrer\n3. Quand vous avez des dÃ©pendances â†’ Migrez vers Airflow",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Points clÃ©s",
    "text": "Points clÃ©s\n\nğŸªŸ Windows Task Scheduler\n\nPour qui : DÃ©butants sur Windows\nForce : TrÃ¨s facile (GUI)\nFaiblesse : Pas de dÃ©pendances\nLimite : 5 scripts max\n\n\n\nğŸ§ Crontab\n\nPour qui : Utilisateurs Linux\nForce : Universel, lÃ©ger\nFaiblesse : Pas de monitoring\nLimite : 15 scripts max\n\n\n\nğŸŒ¬ï¸ Airflow\n\nPour qui : Production, Ã©quipes data\nForce : DÃ©pendances, monitoring, scalable\nFaiblesse : Complexe, ressources\nLimite : Aucune (scalable)\n\n\n\nConcepts Airflow Ã  retenir\n\n\n\nConcept\nDescription\n\n\n\n\nDAG\nGraphe de tÃ¢ches (workflow)\n\n\nTask\nUnitÃ© de travail\n\n\nOperator\nType de tÃ¢che (Python, Bash, SQLâ€¦)\n\n\nXCom\nPassage de donnÃ©es entre tÃ¢ches\n\n\nSensor\nAttendre une condition\n\n\nConnection\nCredentials stockÃ©s\n\n\nVariable\nConfiguration stockÃ©e",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "title": "â° Orchestration de Pipelines Data",
    "section": "Ressources",
    "text": "Ressources\n\nCrontab : crontab.guru (tester expressions)\nAirflow : airflow.apache.org\nAirflow Tutorial : Documentation officielle",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert orchestration !\n8-9/10 : ğŸŒŸ TrÃ¨s bien !\n6-7/10 : ğŸ’ª Bon dÃ©but !\n&lt; 6/10 : ğŸ“š Relisez le notebook",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nOutils\n\nCrontab Guru â€” Tester et gÃ©nÃ©rer des expressions cron\nApache Airflow â€” Documentation officielle\nAirflow Tutorial\nAstronomer â€” Guides et bonnes pratiques\n\n\n\nAlternatives Ã  Airflow\n\n\n\n\n\n\n\n\nOutil\nDescription\nCas dâ€™usage\n\n\n\n\nPrefect\nOrchestration moderne, Pythonic\nAlternative plus simple Ã  Airflow\n\n\nDagster\nData orchestration avec types\nPipelines ML\n\n\nLuigi\nPar Spotify, simple\nPipelines batch\n\n\nMage\nLow-code, moderne\nPrototypage rapide\n\n\nKestra\nEvent-driven, YAML\nWorkflows dÃ©claratifs\n\n\n\n\n\nCloud managed\n\n\n\nCloud\nService\n\n\n\n\nAWS\nMWAA (Managed Airflow), Step Functions\n\n\nGCP\nCloud Composer (Managed Airflow)\n\n\nAzure\nData Factory, Synapse Pipelines",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "title": "â° Orchestration de Pipelines Data",
    "section": "ğŸ‰ Fin du niveau dÃ©butant !",
    "text": "ğŸ‰ Fin du niveau dÃ©butant !\nTu as terminÃ© le parcours Data Engineering - From Zero to Hero niveau dÃ©butant ! ğŸ‰\n\nğŸ“‹ RÃ©capitulatif des modules\n\n\n\n#\nModule\nCompÃ©tence acquise\n\n\n\n\n01\nIntroduction\nVision du mÃ©tier\n\n\n02\nBash\nLigne de commande\n\n\n03\nGit\nVersioning\n\n\n04\nPython Basics\nProgrammation\n\n\n05\nPython Data Processing\nPandas, visualisation\n\n\n06\nIntro Bases Relationnelles\nConcepts relationnels\n\n\n07\nSQL\nRequÃªtes SQL\n\n\n08\nBig Data & NoSQL\nSystÃ¨mes distribuÃ©s\n\n\n09\nMongoDB\nBase NoSQL document\n\n\n10\nElasticsearch\nRecherche et indexation\n\n\n11\nPySpark\nTraitement distribuÃ©\n\n\n12\nOrchestration\nAirflow, pipelines\n\n\n\n\n\nğŸ Module BONUS disponible\n\n\n\n\n\n\n\n\n#\nModule\nDescription\n\n\n\n\n13\nBONUS FastAPI\nCrÃ©er des APIs REST pour exposer tes donnÃ©es\n\n\n\nğŸ‘‰ Parfait pour exposer les rÃ©sultats de tes pipelines via API !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "title": "â° Orchestration de Pipelines Data",
    "section": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire",
    "text": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire\nTu es maintenant prÃªt pour le niveau intermÃ©diaire qui couvrira :\n\n\n\nModule\nDescription\n\n\n\n\nDocker\nConteneurisation des pipelines\n\n\nData Lakes\nParquet, Delta Lake, Iceberg\n\n\nKafka\nStreaming en temps rÃ©el\n\n\ndbt\nTransformation dans le warehouse\n\n\nData Quality\nGreat Expectations, tests\n\n\nCloud\nAWS / GCP / Azure\n\n\nCI/CD\nGitHub Actions, tests automatisÃ©s\n\n\nKibana\nDashboards et monitoring\n\n\nProjet intÃ©grateur\nPipeline complet end-to-end\n\n\n\n\nğŸš€ Bravo ! Tu as maintenant toutes les bases pour construire des pipelines de donnÃ©es !",
    "crumbs": [
      "DÃ©butant",
      "â° Orchestration de Pipelines Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html",
    "href": "notebooks/beginner/01_intro_data_engineering.html",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "",
    "text": "Bienvenue dans cette premiÃ¨re leÃ§on du parcours Data Engineering â€” From Zero to Hero.\nDans ce notebook, nous allons dÃ©couvrir :",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\nAvant de commencer ce parcours, il est recommandÃ© dâ€™avoir :\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Basique\nSavoir utiliser un ordinateur et naviguer dans des fichiers\n\n\nâœ… Basique\nConnaÃ®tre les concepts de base dâ€™internet\n\n\n\n\nğŸ’¡ Pas de panique ! Ce parcours est conÃ§u pour les dÃ©butants. Nous couvrirons tout ce dont tu as besoin.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "href": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“Œ 1. Câ€™est quoi le Data Engineering ?",
    "text": "ğŸ“Œ 1. Câ€™est quoi le Data Engineering ?\nLe Data Engineering (ou ingÃ©nierie des donnÃ©es) est une discipline qui consiste Ã  concevoir, construire, maintenir et optimiser les systÃ¨mes de traitement de donnÃ©es.\nLe Data Engineer, spÃ©cialiste en gestion des donnÃ©es, conÃ§oit et maintient lâ€™infrastructure data (bases de donnÃ©es, entrepÃ´ts de donnÃ©es, lacs de donnÃ©es) et dÃ©veloppe des pipelines automatisÃ©s qui extraient, transforment et chargent les donnÃ©es dans des systÃ¨mes adaptÃ©s.\nCâ€™est le socle technique qui garantit la qualitÃ©, la disponibilitÃ© et la sÃ©curitÃ© des donnÃ©es utilisÃ©es par les Data Analysts et Data Scientists pour gÃ©nÃ©rer des insights et orienter les stratÃ©gies dâ€™entreprise.\n\nğŸ¢ Exemples concrets en entreprise\n\n\n\n\n\n\n\nEntreprise\nCas dâ€™usage Data Engineering\n\n\n\n\nNetflix\nPipeline qui collecte les donnÃ©es de visionnage de millions dâ€™utilisateurs pour alimenter le systÃ¨me de recommandation\n\n\nUber\nTraitement en temps rÃ©el des donnÃ©es GPS de milliers de chauffeurs pour optimiser les trajets et les prix\n\n\nSpotify\nAgrÃ©gation des donnÃ©es dâ€™Ã©coute pour gÃ©nÃ©rer les playlists â€œDiscover Weeklyâ€ personnalisÃ©es\n\n\nAirbnb\nPipeline de donnÃ©es pour analyser les prix du marchÃ© et suggÃ©rer des tarifs aux hÃ´tes\n\n\nE-commerce\nSynchronisation des stocks entre le site web, lâ€™ERP et les entrepÃ´ts en temps rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "href": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ‘¨ğŸ½â€ğŸ’» 2. DiffÃ©rences entre mÃ©tiers",
    "text": "ğŸ‘¨ğŸ½â€ğŸ’» 2. DiffÃ©rences entre mÃ©tiers\n\n\n\n\n\n\n\n\n\nMÃ©tier\nRÃ´le Principal\nFocus\nOutils ClÃ©s\n\n\n\n\nğŸ”§ Data Engineer\nConstruire et maintenir lâ€™infrastructure de donnÃ©es\nInfrastructure & Pipelines\nApache Airflow, Apache Spark, Kafka, Snowflake, dbt, Python, SQL, Prefect, Docker, Kubernetes, etcâ€¦\n\n\nğŸ”¬ Data Scientist\nExtraire des insights et crÃ©er des modÃ¨les prÃ©dictifs\nModÃ©lisation & ML\nPython, R, scikit-learn, TensorFlow, PyTorch, Jupyter, MLflow, Pandas, XGBoost, etcâ€¦\n\n\nğŸ“ˆ Data Analyst\nTransformer les donnÃ©es en insights actionnables\nBusiness Intelligence\nSQL, Excel, Power BI, Tableau, Looker, Google Analytics, Python (basique)\n\n\n\n\nğŸ”„ Comment ces rÃ´les collaborent ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Data Engineer  â”‚ â”€â”€â–¶ â”‚  Data Scientist â”‚ â”€â”€â–¶ â”‚  Data Analyst   â”‚\nâ”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚\nâ”‚ â€¢ Collecte      â”‚     â”‚ â€¢ ModÃ©lisation  â”‚     â”‚ â€¢ Visualisation â”‚\nâ”‚ â€¢ Pipelines     â”‚     â”‚ â€¢ PrÃ©dictions   â”‚     â”‚ â€¢ Reporting     â”‚\nâ”‚ â€¢ Infrastructureâ”‚     â”‚ â€¢ ML/AI         â”‚     â”‚ â€¢ Insights      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                                               â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback & Besoins â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "href": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ—ï¸ 3. Architecture typique dâ€™un pipeline de donnÃ©es moderne",
    "text": "ğŸ—ï¸ 3. Architecture typique dâ€™un pipeline de donnÃ©es moderne\nUn pipeline de donnÃ©es moderne suit gÃ©nÃ©ralement ce flux :\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚                ğŸ›¡ï¸  GOUVERNANCE DATA                    â”‚\n                 â”‚ Catalogue, QualitÃ©, Lineage, SÃ©curitÃ©, RBAC, Privacy   â”‚\n                 â”‚ Compliance, MDM                                        â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   ğŸ“¥ SOURCES      â”‚   â†’     â”‚               ğŸ’¾ STOCKAGE                 â”‚\nâ”‚ API, DB, Logs,    â”‚         â”‚         (Data Lakehouse : S3, ADLSâ€¦)     â”‚\nâ”‚ CSV, IoT          â”‚         â”‚                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   BRONZE : DonnÃ©es brutes (Raw)           â”‚\n                             â”‚   - IngÃ©rÃ©es telles quelles                â”‚\n                             â”‚   - Formats variÃ©s (JSON, CSV, Parquet)    â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   - Pas de qualitÃ© garantie                â”‚\n   â”‚   ğŸ”„ INGESTION        â”‚  â”‚                                           â”‚\n   â”‚ ETL/ELT, Streams     â”‚  â”‚   SILVER : DonnÃ©es nettoyÃ©es (Clean)       â”‚\n   â”‚ Airbyte, Fivetran    â”‚  â”‚   - NormalisÃ©es, typÃ©es, dÃ©dupliquÃ©es      â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   - QualitÃ© contrÃ´lÃ©e                      â”‚\n                             â”‚   - Jointures simples / enrichissement     â”‚\n                             â”‚                                           â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   GOLD : DonnÃ©es business (Curated)        â”‚\n   â”‚ âš™ï¸ TRANSFORMATION      â”‚  â”‚   - ModÃ¨les analytiques (Star Schema)      â”‚\n   â”‚ dbt, SQL, Spark,      â”‚  â”‚   - KPIs, mÃ©triques, tables prÃªtes BI     â”‚\n   â”‚ Pandas, MLflow        â”‚  â”‚   - Haute qualitÃ© et gouvernance forte     â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                              â”‚        ğŸ“Š EXPOSITION         â”‚\n                              â”‚ Dashboards, ML, APIs, Apps   â”‚\n                              â”‚ Power BI, Tableau, Looker    â”‚\n                              â”‚ (âš ï¸ Toujours Ã  partir du GOLD)â”‚\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ” ETL vs ELT â€” Quelle est la diffÃ©rence ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nETL (Extract â†’ Transform â†’ Load)\nELT (Extract â†’ Load â†’ Transform)\n\n\n\n\nğŸ”„ Ordre\nExtraction â†’ Transformation â†’ Chargement\nExtraction â†’ Chargement â†’ Transformation\n\n\nğŸ“ Lieu de la transformation\nEn dehors du stockage (dans un script ou un outil)\nDirectement dans le data warehouse\n\n\nâœ… Avantages\nPlus de contrÃ´le sur la transformation\nPlus rapide sur des gros volumes\n\n\nâš ï¸ InconvÃ©nients\nPeut surcharger les outils intermÃ©diaires\nBesoin dâ€™un entrepÃ´t puissant (coÃ»ts)\n\n\nğŸ› ï¸ Outils typiques\nInformatica, Talend, scripts Python, â€¦\ndbt, Snowflake, BigQuery, â€¦\n\n\nğŸ“… Cas dâ€™usage\nDonnÃ©es sensibles nÃ©cessitant un prÃ©-traitement\nAnalytics modernes sur le cloud",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "href": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ•˜ 4. Pipelines batch vs streaming",
    "text": "ğŸ•˜ 4. Pipelines batch vs streaming\n\n\n\n\n\n\n\n\n\nMode\nDÃ©finition\nLatence\nExemples\n\n\n\n\nBatch\nTraitement pÃ©riodique (toutes les heures, tous les joursâ€¦)\nMinutes Ã  heures\nRapport quotidien, import CSV, agrÃ©gations nocturnes\n\n\nStreaming\nTraitement en temps rÃ©el, Ã©vÃ©nement par Ã©vÃ©nement\nMillisecondes Ã  secondes\nLogs serveurs, capteurs IoT, transactions bancaires, dÃ©tection de fraude\n\n\n\n\nğŸ¤” Comment choisir ?\n\n\n\n\n\n\n\nQuestion\nSi oui â†’\n\n\n\n\nLes donnÃ©es doivent-elles Ãªtre traitÃ©es immÃ©diatement ?\nStreaming\n\n\nLe volume est-il trÃ¨s Ã©levÃ© mais la latence peu critique ?\nBatch\n\n\nAvez-vous besoin de dÃ©tecter des anomalies en temps rÃ©el ?\nStreaming\n\n\nSâ€™agit-il de rapports quotidiens/hebdomadaires ?\nBatch\n\n\n\n\n\nğŸ§± Fondations des pipelines de donnÃ©es\nTout pipeline de donnÃ©es repose sur plusieurs piliers fondamentaux :\n\n\n\n\n\n\n\n\nğŸ”¢ Pilier\nğŸ’¡ Description\nğŸ“š Module associÃ©\n\n\n\n\n1. Data Collecting\nComment collecter les donnÃ©es brutes (fichiers, API, capteursâ€¦)\nPython, APIs\n\n\n2. Data Ingestion\nComment les intÃ©grer dans un systÃ¨me (DB, data lakeâ€¦)\nETL, Airbyte\n\n\n3. Data Storage\nComment et oÃ¹ les stocker (SQL, NoSQL, S3â€¦)\nDatabases, Cloud\n\n\n4. Data Processing\nComment les transformer, nettoyer, agrÃ©ger\nPython, Spark, dbt\n\n\n5. Data Modeling\nComment organiser les donnÃ©es pour lâ€™analyse\nSQL, dbt\n\n\n6. Data Quality & Governance\nComment garantir la fiabilitÃ© et la traÃ§abilitÃ©\nGreat Expectations\n\n\n7. Data Orchestration\nComment automatiser les tÃ¢ches et gÃ©rer les dÃ©pendances\nAirflow, Prefect\n\n\n8. ScalabilitÃ© & Performance\nComment faire face Ã  de gros volumes ou Ã  la charge\nSpark, Kubernetes\n\n\n9. SÃ©curitÃ© des donnÃ©es\nChiffrement, contrÃ´le dâ€™accÃ¨s, audit\nIAM, Vault\n\n\n10. DevOps pour la Data\nConteneurisation, CI/CD, monitoring\nDocker, GitHub Actions\n\n\n\n\nğŸ“˜ Ces concepts seront abordÃ©s progressivement dans le parcours.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§° 5. Panorama des outils du Data Engineer",
    "text": "ğŸ§° 5. Panorama des outils du Data Engineer\n\n\n\n\n\n\n\n\nDomaine\nOutils populaires\nNiveau\n\n\n\n\nğŸ“¥ Collecte\nPython, API REST, Scrapy, Kafka\nDÃ©butant â†’ IntermÃ©diaire\n\n\nğŸ”„ Ingestion\nAirbyte, Fivetran, Python scripts\nDÃ©butant\n\n\nğŸ’¾ Stockage\nPostgreSQL, Snowflake, S3, Delta Lake\nDÃ©butant â†’ AvancÃ©\n\n\nâš™ï¸ Traitement (Batch)\nPandas, Spark, dbt, SQL\nDÃ©butant â†’ AvancÃ©\n\n\nâš¡ Traitement (Streaming)\nKafka, Spark Streaming, Flink\nIntermÃ©diaire â†’ AvancÃ©\n\n\nğŸ¼ Orchestration\nApache Airflow, Prefect, Dagster\nIntermÃ©diaire\n\n\nğŸ³ DevOps & CI/CD\nDocker, GitHub Actions, Terraform\nIntermÃ©diaire\n\n\nğŸ“Š Monitoring\nGrafana, Prometheus, ELK Stack\nIntermÃ©diaire\n\n\n\n\nğŸ—ºï¸ Stack moderne typique (2024)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    STACK DATA MODERNE                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Orchestration    â”‚  Airflow / Prefect / Dagster            â”‚\nâ”‚  Transformation   â”‚  dbt / Spark / Python                   â”‚\nâ”‚  Warehouse        â”‚  Snowflake / BigQuery / Redshift        â”‚\nâ”‚  Ingestion        â”‚  Airbyte / Fivetran / Stitch            â”‚\nâ”‚  Sources          â”‚  APIs / Databases / SaaS / Files        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§  6. Soft Skills & Mindset du Data Engineer",
    "text": "ğŸ§  6. Soft Skills & Mindset du Data Engineer\nLe mÃ©tier de Data Engineer nâ€™est pas uniquement technique. Pour rÃ©ussir dans ce domaine, il faut aussi dÃ©velopper des compÃ©tences humaines et professionnelles essentielles :\n\n\n\n\n\n\n\n\nCompÃ©tence\nDescription\nPourquoi câ€™est important\n\n\n\n\nğŸ“ Documenter\nÃ‰crire une documentation claire pour son code et ses pipelines\nFacilite la maintenance et lâ€™onboarding des nouveaux membres\n\n\nğŸ¤ Collaborer\nTravailler avec les Ã©quipes Data Science, BI, Produit, DevOps\nLes donnÃ©es traversent toute lâ€™organisation\n\n\nğŸ¯ ÃŠtre rigoureux\nGarantir la qualitÃ©, la fiabilitÃ© et la traÃ§abilitÃ© des donnÃ©es\nUne erreur de donnÃ©es peut avoir des consÃ©quences business majeures\n\n\nğŸ•µğŸ½â€â™‚ï¸ Investiguer\nSavoir dÃ©bugger des anomalies, logs ou Ã©checs de pipeline\nLes pipelines cassent, il faut savoir pourquoi rapidement\n\n\nğŸ“š Apprendre en continu\nSe tenir Ã  jour sur les nouveaux outils et pratiques\nLe domaine Ã©volue trÃ¨s rapidement\n\n\nğŸ’¬ Communiquer\nExpliquer des concepts techniques Ã  des non-techniques\nAlignement avec les Ã©quipes mÃ©tier",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\nRÃ©ponds aux questions suivantes pour valider tes acquis ğŸ‘‡ğŸ½\n\n\nâ“ Q1. Quel est le rÃ´le principal dâ€™un Data Engineer ?\n\nCrÃ©er des modÃ¨les prÃ©dictifs\n\nVisualiser les donnÃ©es dans Power BI\n\nConcevoir et maintenir des pipelines de donnÃ©es\n\nFaire des analyses statistiques dans Excel\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le Data Engineer conÃ§oit et maintient des pipelines de donnÃ©es.\n\n\n\n\nâ“ Q2. Dans un pipeline ETL, que signifie le â€œTâ€ ?\n\nTransfer\n\nTrigger\n\nTransform\n\nTransport\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” â€œTâ€ signifie Transform, câ€™est lâ€™Ã©tape de transformation des donnÃ©es.\n\n\n\n\nâ“ Q3. Quelle est la principale diffÃ©rence entre ETL et ELT ?\n\nELT ne fait pas de transformation\n\nELT transforme les donnÃ©es aprÃ¨s les avoir chargÃ©es\n\nETL est utilisÃ© uniquement pour les fichiers CSV\n\nELT est un outil comme Apache Airflow\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ELT charge dâ€™abord les donnÃ©es, puis les transforme dans le data warehouse.\n\n\n\n\nâ“ Q4. Lequel de ces outils est utilisÃ© pour orchestrer des pipelines ?\n\nApache Kafka\n\nApache Airflow\n\nTableau\n\nPostgreSQL\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Apache Airflow est un outil dâ€™orchestration de pipelines.\n\n\n\n\nâ“ Q5. Le traitement batch consiste Ã  :\n\nTraiter les donnÃ©es en continu\n\nTraiter les donnÃ©es en petits lots Ã  la volÃ©e\n\nTraiter les donnÃ©es par groupe, Ã  intervalle rÃ©gulier\n\nTraiter uniquement les donnÃ©es texte\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le batch consiste Ã  traiter les donnÃ©es par lots Ã  intervalles dÃ©finis.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "href": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ§  Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL",
    "text": "ğŸ§  Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL\n\n\nâ“ Q6. Que signifie ETLt ?\n\nUne erreur dans la chaÃ®ne ETL\n\nUne combinaison hybride entre ETL et ELT\n\nUne technique de transfert via email\n\nUne transformation uniquement aprÃ¨s chargement\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ETLt correspond Ã  : Extract â†’ Transform (1) â†’ Load â†’ Transform (2).\nCâ€™est une approche hybride oÃ¹ une partie des transformations est faite avant le chargement, et une autre aprÃ¨s.\n\n\n\n\nâ“ Q7. Le Reverse ETL est utilisÃ© pour :\n\nRecharger les donnÃ©es sources depuis le warehouse\n\nSupprimer les donnÃ©es invalides dans un lac de donnÃ©es\n\nPousser les donnÃ©es du data warehouse vers les outils mÃ©tiers\n\nTransformer les donnÃ©es en reverse-engineering\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Reverse ETL consiste Ã  extraire les donnÃ©es dâ€™un data warehouse (ex. BigQuery) pour les charger dans des outils mÃ©tiers comme Salesforce, Notion, Slack, etc.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Lectures recommandÃ©es\n\nFundamentals of Data Engineering â€” Joe Reis & Matt Housley\nThe Data Warehouse Toolkit â€” Ralph Kimball\nDesigning Data-Intensive Applications â€” Martin Kleppmann\n\n\n\nğŸŒ Sites & Blogs\n\nData Engineering Weekly â€” Newsletter hebdomadaire\nSeattle Data Guy â€” ChaÃ®ne YouTube\nStart Data Engineering â€” Tutoriels pratiques\n\n\n\nğŸ“ Certifications\n\nGoogle Cloud Professional Data Engineer\nAWS Certified Data Engineer\nDatabricks Certified Data Engineer\n\n\n\nğŸ› ï¸ Outils Ã  explorer\n\ndbt â€” Transformation de donnÃ©es\nAirbyte â€” Ingestion de donnÃ©es (open source)\nApache Airflow â€” Orchestration",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "title": "ğŸ§  Introduction au Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu as une vue dâ€™ensemble du Data Engineering, passons Ã  la pratique !\nğŸ‘‰ Module suivant : 02_bash_for_data_engineers.ipynb â€” MaÃ®triser la ligne de commande\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le premier module du parcours Data Engineering.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ§  Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "",
    "text": "Ce module bonus couvre FastAPI, le framework Python moderne pour crÃ©er des APIs REST performantes. En tant que Data Engineer, tu auras souvent besoin dâ€™exposer des donnÃ©es via des APIs."
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#prÃ©requis",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 04_python_basics (fonctions, classes, type hints)\n\n\nâœ… Requis\nModule 05_python_data_processing (Pandas, JSON)\n\n\nâ­ Bonus\nModule 09_mongodb (pour les exemples avec DB)"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… CrÃ©er une API REST avec FastAPI\nâœ… DÃ©finir des modÃ¨les de donnÃ©es avec Pydantic\nâœ… CrÃ©er des endpoints CRUD (Create, Read, Update, Delete)\nâœ… Valider automatiquement les donnÃ©es entrantes\nâœ… Servir des donnÃ©es Pandas via API\nâœ… Connecter une API Ã  une base de donnÃ©es\nâœ… GÃ©nÃ©rer une documentation Swagger automatique\nâœ… DÃ©ployer une API avec Uvicorn\n\n\n\nğŸ’¡ Note : FastAPI sâ€™exÃ©cute comme un serveur. Certains exemples seront Ã  tester hors du notebook, dans des fichiers .py."
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ¤” Pourquoi FastAPI pour un Data Engineer ?",
    "text": "ğŸ¤” Pourquoi FastAPI pour un Data Engineer ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              CAS D'USAGE DATA ENGINEERING                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  ğŸ“Š Exposer des donnÃ©es traitÃ©es (datamart â†’ API)               â”‚\nâ”‚  ğŸ”„ CrÃ©er des webhooks pour dÃ©clencher des pipelines            â”‚\nâ”‚  ğŸ¤– Servir des prÃ©dictions ML en production                     â”‚\nâ”‚  ğŸ“ˆ Fournir des mÃ©triques pour les dashboards                   â”‚\nâ”‚  ğŸ”— CrÃ©er des microservices data                                â”‚\nâ”‚  âœ… Valider des donnÃ©es avant ingestion                         â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ†š FastAPI vs autres frameworks\n\n\n\nFramework\nPerformance\nTyping\nDoc auto\nAsync\nComplexitÃ©\n\n\n\n\nFastAPI\nâ­â­â­\nâœ… Natif\nâœ… Swagger\nâœ…\nSimple\n\n\nFlask\nâ­â­\nâŒ Manuel\nâŒ Extension\nâŒ\nSimple\n\n\nDjango REST\nâ­â­\nâŒ Manuel\nâœ…\nâš ï¸\nComplexe\n\n\nExpress (Node)\nâ­â­â­\nâŒ\nâŒ\nâœ…\nSimple\n\n\n\n\nğŸ’¡ FastAPI combine le meilleur : performance, simplicitÃ©, et documentation automatique."
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#installation",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#installation",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“¦ Installation",
    "text": "ğŸ“¦ Installation\n# Installation de base\npip install fastapi uvicorn\n\n# Avec toutes les dÃ©pendances optionnelles\npip install \"fastapi[all]\"\n\n\n\nPackage\nRÃ´le\n\n\n\n\nfastapi\nLe framework\n\n\nuvicorn\nServeur ASGI (pour lancer lâ€™API)\n\n\npydantic\nValidation de donnÃ©es (inclus avec FastAPI)\n\n\n\n\n\nCode\n# Installation\n!pip install fastapi uvicorn pydantic pandas --quiet\n\nprint(\"âœ… Packages installÃ©s !\")"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-minimale",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-minimale",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ Structure minimale",
    "text": "ğŸ“ Structure minimale\nCrÃ©er un fichier main.py :\n# main.py\nfrom fastapi import FastAPI\n\n# CrÃ©er l'application\napp = FastAPI()\n\n# Premier endpoint\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Hello Data Engineer!\"}\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\"}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#lancer-le-serveur",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#lancer-le-serveur",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸš€ Lancer le serveur",
    "text": "ğŸš€ Lancer le serveur\n# Dans le terminal\nuvicorn main:app --reload\n\n# main = fichier main.py\n# app = variable FastAPI()\n# --reload = redÃ©marre automatiquement si le code change\nRÃ©sultat :\nINFO:     Uvicorn running on http://127.0.0.1:8000\nINFO:     Started reloader process"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#tester-lapi",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#tester-lapi",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸŒ Tester lâ€™API",
    "text": "ğŸŒ Tester lâ€™API\n\n\n\nURL\nRÃ©sultat\n\n\n\n\nhttp://127.0.0.1:8000\n{\"message\": \"Hello Data Engineer!\"}\n\n\nhttp://127.0.0.1:8000/health\n{\"status\": \"healthy\"}\n\n\nhttp://127.0.0.1:8000/docs\nğŸ“„ Documentation Swagger\n\n\nhttp://127.0.0.1:8000/redoc\nğŸ“„ Documentation ReDoc"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ”„ Les mÃ©thodes HTTP",
    "text": "ğŸ”„ Les mÃ©thodes HTTP\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    MÃ‰THODES HTTP (CRUD)                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ MÃ©thode  â”‚ Action           â”‚ Exemple                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ GET      â”‚ Lire (Read)      â”‚ GET /users â†’ Liste des users      â”‚\nâ”‚ POST     â”‚ CrÃ©er (Create)   â”‚ POST /users â†’ CrÃ©er un user       â”‚\nâ”‚ PUT      â”‚ Remplacer        â”‚ PUT /users/1 â†’ Remplacer user 1   â”‚\nâ”‚ PATCH    â”‚ Modifier         â”‚ PATCH /users/1 â†’ Modifier user 1  â”‚\nâ”‚ DELETE   â”‚ Supprimer        â”‚ DELETE /users/1 â†’ Supprimer       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items\")\ndef get_items():\n    return {\"action\": \"Liste des items\"}\n\n@app.post(\"/items\")\ndef create_item():\n    return {\"action\": \"CrÃ©ation d'un item\"}\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int):\n    return {\"action\": f\"Mise Ã  jour de l'item {item_id}\"}\n\n@app.delete(\"/items/{item_id}\")\ndef delete_item(item_id: int):\n    return {\"action\": f\"Suppression de l'item {item_id}\"}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ ParamÃ¨tres de chemin (Path Parameters)",
    "text": "ğŸ“ ParamÃ¨tres de chemin (Path Parameters)\nLes paramÃ¨tres dans lâ€™URL permettent dâ€™identifier une ressource spÃ©cifique.\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# ParamÃ¨tre simple\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):  # Typage automatique !\n    return {\"user_id\": user_id}\n\n# Plusieurs paramÃ¨tres\n@app.get(\"/users/{user_id}/orders/{order_id}\")\ndef get_user_order(user_id: int, order_id: int):\n    return {\n        \"user_id\": user_id,\n        \"order_id\": order_id\n    }\n\nâœ… Validation automatique\nGET /users/42        â†’ {\"user_id\": 42}       âœ…\nGET /users/abc       â†’ Erreur 422            âŒ (pas un int)\nGET /users/-1        â†’ {\"user_id\": -1}       âœ… (mais logiquement faux)\n\n\nğŸ”’ Validation avancÃ©e avec Path\nfrom fastapi import FastAPI, Path\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(\n    user_id: int = Path(\n        ...,  # Requis\n        title=\"User ID\",\n        description=\"L'identifiant unique de l'utilisateur\",\n        ge=1,  # Greater or Equal (&gt;= 1)\n        le=10000  # Less or Equal (&lt;= 10000)\n    )\n):\n    return {\"user_id\": user_id}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ” ParamÃ¨tres de requÃªte (Query Parameters)",
    "text": "ğŸ” ParamÃ¨tres de requÃªte (Query Parameters)\nLes paramÃ¨tres aprÃ¨s le ? dans lâ€™URL pour filtrer, paginer, etc.\nGET /users?skip=0&limit=10&active=true\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              Query parameters\nfrom fastapi import FastAPI\nfrom typing import Optional\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(\n    skip: int = 0,           # DÃ©faut = 0\n    limit: int = 10,         # DÃ©faut = 10\n    active: bool = True,     # DÃ©faut = True\n    search: Optional[str] = None  # Optionnel\n):\n    return {\n        \"skip\": skip,\n        \"limit\": limit,\n        \"active\": active,\n        \"search\": search\n    }\n\nğŸ“Š Cas dâ€™usage Data Engineering : Pagination\n@app.get(\"/data\")\ndef get_data(\n    page: int = 1,\n    page_size: int = 100,\n    sort_by: str = \"created_at\",\n    order: str = \"desc\"\n):\n    # Calculer l'offset\n    offset = (page - 1) * page_size\n    \n    return {\n        \"page\": page,\n        \"page_size\": page_size,\n        \"offset\": offset,\n        \"sort_by\": sort_by,\n        \"order\": order\n    }"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ DÃ©finir un modÃ¨le",
    "text": "ğŸ“ DÃ©finir un modÃ¨le\nfrom pydantic import BaseModel, Field, EmailStr\nfrom typing import Optional\nfrom datetime import datetime\n\nclass User(BaseModel):\n    \"\"\"ModÃ¨le utilisateur pour l'API.\"\"\"\n    \n    id: Optional[int] = None\n    nom: str = Field(..., min_length=2, max_length=50)\n    email: EmailStr\n    age: int = Field(..., ge=0, le=150)\n    actif: bool = True\n    created_at: datetime = Field(default_factory=datetime.now)\n    \n    class Config:\n        # Exemple pour la documentation\n        json_schema_extra = {\n            \"example\": {\n                \"nom\": \"Alice Dupont\",\n                \"email\": \"alice@example.com\",\n                \"age\": 30,\n                \"actif\": True\n            }\n        }\n\nğŸ”’ Validateurs disponibles\n\n\n\nValidateur\nExemple\nDescription\n\n\n\n\nmin_length\nField(min_length=2)\nLongueur min string\n\n\nmax_length\nField(max_length=50)\nLongueur max string\n\n\nge\nField(ge=0)\nGreater or Equal\n\n\nle\nField(le=100)\nLess or Equal\n\n\ngt\nField(gt=0)\nGreater Than\n\n\nlt\nField(lt=100)\nLess Than\n\n\nregex\nField(regex='^[A-Z]')\nPattern regex\n\n\nEmailStr\nType spÃ©cial\nValide format email\n\n\n\n\n\nCode\n# Exemple Pydantic dans le notebook\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import Optional\nfrom datetime import datetime\n\nclass Transaction(BaseModel):\n    \"\"\"ModÃ¨le de transaction pour pipeline ETL.\"\"\"\n    \n    id: Optional[int] = None\n    montant: float = Field(..., gt=0, description=\"Montant positif\")\n    devise: str = Field(..., min_length=3, max_length=3)\n    description: str = Field(..., min_length=1, max_length=200)\n    timestamp: datetime = Field(default_factory=datetime.now)\n    \n    @field_validator('devise')\n    @classmethod\n    def devise_uppercase(cls, v):\n        return v.upper()\n\n# Test de validation\ntry:\n    # âœ… Valide\n    tx1 = Transaction(montant=100.50, devise=\"eur\", description=\"Achat\")\n    print(f\"âœ… Transaction valide : {tx1}\")\n    print(f\"   Devise convertie : {tx1.devise}\")\n    \n    # âŒ Invalide\n    tx2 = Transaction(montant=-50, devise=\"EUR\", description=\"Test\")\nexcept Exception as e:\n    print(f\"\\nâŒ Erreur de validation : {e}\")"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“¨ Corps de requÃªte (Request Body)",
    "text": "ğŸ“¨ Corps de requÃªte (Request Body)\nPour les requÃªtes POST/PUT, les donnÃ©es sont envoyÃ©es dans le body (corps) en JSON.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserCreate(BaseModel):\n    nom: str\n    email: str\n    age: int\n\nclass UserResponse(BaseModel):\n    id: int\n    nom: str\n    email: str\n    age: int\n\n# Base de donnÃ©es fictive\nusers_db = []\nuser_id_counter = 1\n\n@app.post(\"/users\", response_model=UserResponse)\ndef create_user(user: UserCreate):\n    global user_id_counter\n    \n    # CrÃ©er le user avec un ID\n    new_user = {\n        \"id\": user_id_counter,\n        \"nom\": user.nom,\n        \"email\": user.email,\n        \"age\": user.age\n    }\n    \n    users_db.append(new_user)\n    user_id_counter += 1\n    \n    return new_user\n\nğŸ“¤ RequÃªte avec curl\ncurl -X POST \"http://127.0.0.1:8000/users\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}'\n\n\nğŸ“¤ RequÃªte avec Python requests\nimport requests\n\nresponse = requests.post(\n    \"http://127.0.0.1:8000/users\",\n    json={\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}\n)\nprint(response.json())"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "âš ï¸ Gestion des erreurs",
    "text": "âš ï¸ Gestion des erreurs\nFastAPI utilise HTTPException pour retourner des erreurs HTTP propres.\nfrom fastapi import FastAPI, HTTPException, status\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    user = find_user_by_id(user_id)  # Fonction fictive\n    \n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User {user_id} non trouvÃ©\",\n            headers={\"X-Error\": \"User not found\"}\n        )\n    \n    return user\n\nğŸ“‹ Codes HTTP courants\n\n\n\nCode\nNom\nUsage\n\n\n\n\n200\nOK\nSuccÃ¨s (GET, PUT)\n\n\n201\nCreated\nCrÃ©ation rÃ©ussie (POST)\n\n\n204\nNo Content\nSuppression rÃ©ussie (DELETE)\n\n\n400\nBad Request\nRequÃªte invalide\n\n\n401\nUnauthorized\nNon authentifiÃ©\n\n\n403\nForbidden\nNon autorisÃ©\n\n\n404\nNot Found\nRessource introuvable\n\n\n422\nUnprocessable Entity\nErreur de validation\n\n\n500\nInternal Server Error\nErreur serveur"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ—„ï¸ Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)",
    "text": "ğŸ—„ï¸ Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)\n# database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# URL de connexion\nDATABASE_URL = \"postgresql://user:password@localhost:5432/mydb\"\n# Pour SQLite: \"sqlite:///./data.db\"\n\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Dependency pour injecter la session\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n# models.py\nfrom sqlalchemy import Column, Integer, String, Float, DateTime\nfrom database import Base\nfrom datetime import datetime\n\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    nom = Column(String(100), nullable=False)\n    email = Column(String(100), unique=True, index=True)\n    age = Column(Integer)\n    created_at = Column(DateTime, default=datetime.now)\n# main.py\nfrom fastapi import FastAPI, Depends\nfrom sqlalchemy.orm import Session\nfrom database import get_db\nimport models\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(db: Session = Depends(get_db), skip: int = 0, limit: int = 10):\n    users = db.query(models.User).offset(skip).limit(limit).all()\n    return users"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸƒ Avec MongoDB (PyMongo)",
    "text": "ğŸƒ Avec MongoDB (PyMongo)\n# mongodb_api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nfrom pymongo import MongoClient\nfrom bson import ObjectId\nfrom typing import Optional\n\napp = FastAPI(title=\"MongoDB API\")\n\n# Connexion MongoDB\nclient = MongoClient(\"mongodb://localhost:27017\")\ndb = client[\"data_engineering\"]\ncollection = db[\"pipelines\"]\n\n# Helper pour ObjectId\nclass PyObjectId(ObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n    \n    @classmethod\n    def validate(cls, v, field):\n        if not ObjectId.is_valid(v):\n            raise ValueError(\"Invalid ObjectId\")\n        return ObjectId(v)\n\n# ModÃ¨le\nclass Pipeline(BaseModel):\n    id: Optional[str] = Field(default=None, alias=\"_id\")\n    nom: str\n    description: str\n    schedule: str\n    \n    class Config:\n        populate_by_name = True\n\n# CRUD\n@app.get(\"/pipelines\")\ndef list_pipelines():\n    pipelines = []\n    for doc in collection.find():\n        doc[\"_id\"] = str(doc[\"_id\"])\n        pipelines.append(doc)\n    return pipelines\n\n@app.post(\"/pipelines\")\ndef create_pipeline(pipeline: Pipeline):\n    result = collection.insert_one(pipeline.model_dump(exclude={\"id\"}))\n    return {\"id\": str(result.inserted_id)}\n\n@app.get(\"/pipelines/{pipeline_id}\")\ndef get_pipeline(pipeline_id: str):\n    doc = collection.find_one({\"_id\": ObjectId(pipeline_id)})\n    if not doc:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    doc[\"_id\"] = str(doc[\"_id\"])\n    return doc\n\n@app.delete(\"/pipelines/{pipeline_id}\")\ndef delete_pipeline(pipeline_id: str):\n    result = collection.delete_one({\"_id\": ObjectId(pipeline_id)})\n    if result.deleted_count == 0:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    return {\"deleted\": True}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "â° Background Tasks (tÃ¢ches en arriÃ¨re-plan)",
    "text": "â° Background Tasks (tÃ¢ches en arriÃ¨re-plan)\nUtile pour les webhooks et le dÃ©clenchement de pipelines.\nfrom fastapi import FastAPI, BackgroundTasks\nimport time\n\napp = FastAPI()\n\ndef run_etl_pipeline(pipeline_id: int):\n    \"\"\"Simule l'exÃ©cution d'un pipeline ETL.\"\"\"\n    print(f\"ğŸš€ DÃ©marrage du pipeline {pipeline_id}\")\n    time.sleep(10)  # Simule un traitement long\n    print(f\"âœ… Pipeline {pipeline_id} terminÃ©\")\n\n@app.post(\"/pipelines/{pipeline_id}/run\")\ndef trigger_pipeline(pipeline_id: int, background_tasks: BackgroundTasks):\n    \"\"\"DÃ©clenche un pipeline en arriÃ¨re-plan.\"\"\"\n    \n    # Ajoute la tÃ¢che en arriÃ¨re-plan\n    background_tasks.add_task(run_etl_pipeline, pipeline_id)\n    \n    # RÃ©pond immÃ©diatement\n    return {\n        \"message\": f\"Pipeline {pipeline_id} dÃ©clenchÃ©\",\n        \"status\": \"running\"\n    }\n\nğŸ”— Webhook pour dÃ©clencher un pipeline\n@app.post(\"/webhook/data-arrival\")\ndef webhook_data_arrival(\n    payload: dict,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"Webhook appelÃ© quand de nouvelles donnÃ©es arrivent.\"\"\"\n    \n    source = payload.get(\"source\")\n    file_path = payload.get(\"file_path\")\n    \n    # DÃ©clencher le pipeline appropriÃ©\n    background_tasks.add_task(\n        process_new_data, \n        source=source, \n        file_path=file_path\n    )\n    \n    return {\"status\": \"accepted\", \"message\": \"Pipeline dÃ©clenchÃ©\"}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#upload-de-fichiers",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#upload-de-fichiers",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“ Upload de fichiers",
    "text": "ğŸ“ Upload de fichiers\nPermet dâ€™ingÃ©rer des fichiers (CSV, JSON, Parquet) via API.\nfrom fastapi import FastAPI, UploadFile, File\nimport pandas as pd\nimport io\n\napp = FastAPI()\n\n@app.post(\"/upload/csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    \"\"\"Upload et analyse un fichier CSV.\"\"\"\n    \n    # VÃ©rifier l'extension\n    if not file.filename.endswith('.csv'):\n        raise HTTPException(status_code=400, detail=\"Fichier CSV requis\")\n    \n    # Lire le contenu\n    contents = await file.read()\n    \n    # Charger dans Pandas\n    df = pd.read_csv(io.StringIO(contents.decode('utf-8')))\n    \n    return {\n        \"filename\": file.filename,\n        \"rows\": len(df),\n        \"columns\": list(df.columns),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n        \"preview\": df.head(5).to_dict(orient=\"records\")\n    }\n\n@app.post(\"/upload/batch\")\nasync def upload_multiple(files: list[UploadFile] = File(...)):\n    \"\"\"Upload plusieurs fichiers.\"\"\"\n    \n    results = []\n    for file in files:\n        results.append({\n            \"filename\": file.filename,\n            \"size\": len(await file.read())\n        })\n    \n    return {\"uploaded\": len(files), \"files\": results}"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#documentation-automatique",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#documentation-automatique",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“„ Documentation automatique",
    "text": "ğŸ“„ Documentation automatique\nFastAPI gÃ©nÃ¨re automatiquement une documentation interactive !\n\nğŸŒ URLs de documentation\n\n\n\nURL\nType\nDescription\n\n\n\n\n/docs\nSwagger UI\nInterface interactive pour tester\n\n\n/redoc\nReDoc\nDocumentation lisible\n\n\n/openapi.json\nOpenAPI\nSchÃ©ma JSON de lâ€™API\n\n\n\n\n\nğŸ¨ Personnaliser la documentation\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Data Pipeline API\",\n    description=\"\"\"\n    ## ğŸš€ API pour gÃ©rer les pipelines de donnÃ©es\n    \n    Cette API permet de :\n    - GÃ©rer les pipelines ETL\n    - Consulter les donnÃ©es\n    - DÃ©clencher des exÃ©cutions\n    \n    ### Authentification\n    Utilise un token Bearer dans le header `Authorization`.\n    \"\"\",\n    version=\"1.0.0\",\n    contact={\n        \"name\": \"Data Team\",\n        \"email\": \"data@company.com\"\n    },\n    license_info={\n        \"name\": \"MIT\"\n    }\n)\n\n# Documenter un endpoint\n@app.get(\n    \"/pipelines\",\n    summary=\"Liste les pipelines\",\n    description=\"Retourne tous les pipelines avec pagination\",\n    response_description=\"Liste des pipelines\",\n    tags=[\"Pipelines\"]\n)\ndef list_pipelines():\n    pass"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-docker",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#avec-docker",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ³ Avec Docker",
    "text": "ğŸ³ Avec Docker\n# Dockerfile\nFROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8000\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n# requirements.txt\nfastapi==0.109.0\nuvicorn==0.27.0\npydantic==2.5.0\npandas==2.1.0\n# Build et run\ndocker build -t data-api .\ndocker run -p 8000:8000 data-api"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#options-de-dÃ©ploiement-cloud",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#options-de-dÃ©ploiement-cloud",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "â˜ï¸ Options de dÃ©ploiement cloud",
    "text": "â˜ï¸ Options de dÃ©ploiement cloud\n\n\n\nService\nDifficultÃ©\nCoÃ»t\nNotes\n\n\n\n\nRailway\nâ­ Facile\nGratuit/Payant\nIdÃ©al pour dÃ©buter\n\n\nRender\nâ­ Facile\nGratuit/Payant\nAuto-deploy depuis Git\n\n\nFly.io\nâ­â­ Moyen\nGratuit/Payant\nDocker natif\n\n\nAWS Lambda\nâ­â­ Moyen\nPay-per-use\nAvec Mangum adapter\n\n\nGCP Cloud Run\nâ­â­ Moyen\nPay-per-use\nContainer serverless\n\n\nKubernetes\nâ­â­â­ AvancÃ©\nVariable\nProduction enterprise"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "âœ… Structure de projet recommandÃ©e",
    "text": "âœ… Structure de projet recommandÃ©e\nmy_api/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e FastAPI\nâ”‚   â”œâ”€â”€ config.py            # Configuration (env vars)\nâ”‚   â”œâ”€â”€ database.py          # Connexion DB\nâ”‚   â”œâ”€â”€ models/              # ModÃ¨les SQLAlchemy/Pydantic\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ schemas/             # SchÃ©mas Pydantic (request/response)\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ routers/             # Endpoints par domaine\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ pipelines.py\nâ”‚   â”‚   â””â”€â”€ data.py\nâ”‚   â””â”€â”€ services/            # Logique mÃ©tier\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ etl.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ test_pipelines.py\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ” SÃ©curitÃ©",
    "text": "ğŸ” SÃ©curitÃ©\n# Ne jamais hardcoder les secrets !\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nAPI_KEY = os.getenv(\"API_KEY\")"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#checklist-production",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#checklist-production",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ Checklist production",
    "text": "ğŸ“‹ Checklist production\n\n\n\nâœ…\nItem\n\n\n\n\nâ˜\nVariables dâ€™environnement pour les secrets\n\n\nâ˜\nValidation des entrÃ©es (Pydantic)\n\n\nâ˜\nGestion des erreurs (HTTPException)\n\n\nâ˜\nLogging structurÃ©\n\n\nâ˜\nRate limiting\n\n\nâ˜\nCORS configurÃ©\n\n\nâ˜\nTests automatisÃ©s\n\n\nâ˜\nDocumentation Ã  jour\n\n\nâ˜\nHealth check endpoint"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nCe que tu as appris\n\n\n\nConcept\nDescription\n\n\n\n\nFastAPI\nFramework moderne pour APIs REST\n\n\nPydantic\nValidation automatique des donnÃ©es\n\n\nEndpoints CRUD\nGET, POST, PUT, PATCH, DELETE\n\n\nPath/Query params\nParamÃ¨tres dâ€™URL et de requÃªte\n\n\nPandas + API\nServir des donnÃ©es via HTTP\n\n\nBackground tasks\nExÃ©cuter des tÃ¢ches asynchrones\n\n\nDocumentation\nSwagger automatique\n\n\n\n\n\nğŸ¯ Cas dâ€™usage Data Engineering\n\n\n\nCas\nExemple\n\n\n\n\nExposer des donnÃ©es\nAPI pour dashboard/BI\n\n\nWebhook\nDÃ©clencher un pipeline Ã  lâ€™arrivÃ©e de donnÃ©es\n\n\nValidation\nVÃ©rifier les donnÃ©es avant ingestion\n\n\nMonitoring\nAPI de statut des pipelines\n\n\nML Serving\nExposer des prÃ©dictions\n\n\n\n\n\nğŸ“¦ Commandes essentielles\n# Installation\npip install fastapi uvicorn\n\n# Lancer en dev\nuvicorn main:app --reload\n\n# Lancer en prod\nuvicorn main:app --host 0.0.0.0 --port 8000 --workers 4"
  },
  {
    "objectID": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/13_BONUS_fastapi_for_data_engineers.html#ressources",
    "title": "ğŸš€ BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation officielle\n\nFastAPI Documentation â€” Excellente et complÃ¨te\nPydantic Documentation\nUvicorn Documentation\n\n\n\nTutoriels\n\nFastAPI Tutorial â€” Tutoriel officiel\nReal Python - FastAPI\nTestDriven.io - FastAPI\n\n\n\nOutils complÃ©mentaires\n\nSQLModel â€” ORM par le crÃ©ateur de FastAPI\nStrawberry â€” GraphQL avec FastAPI\nCelery â€” TÃ¢ches asynchrones avancÃ©es\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant FastAPI pour le Data Engineering."
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente MongoDB, la base de donnÃ©es NoSQL documentaire la plus populaire.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Configurer MongoDB Atlas (gratuit, cloud)\nâœ… MaÃ®triser le CRUD (Create, Read, Update, Delete)\nâœ… Utiliser les opÃ©rateurs de filtrage et comparaison\nâœ… Ã‰crire des agrÃ©gations\nâœ… CrÃ©er des index pour optimiser les performances",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ MongoDB dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ MongoDB dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module prÃ©cÃ©dent que MongoDB est une base NoSQL documentaire. Voici comment elle rÃ©pond aux dÃ©fis du Big Data :\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment MongoDB rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs serveurs)\n\n\nVelocity\nÃ‰critures rapides, rÃ©plication temps rÃ©el\n\n\nVariety\nSchÃ©ma flexible (JSON), pas de structure rigide\n\n\nVeracity\nValidation de schÃ©ma optionnelle\n\n\nValue\nAgrÃ©gations puissantes, intÃ©gration BI\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nMongoDB\n\n\n\n\nCAP\nCP (Consistency + Partition tolerance) par dÃ©faut\n\n\nBASE\nCohÃ©rence configurable (forte ou Ã©ventuelle)\n\n\n\n\nğŸ’¡ Pas dâ€™installation nÃ©cessaire : Tout se fait dans le cloud avec MongoDB Atlas !\nğŸ“ Note : Les commandes sâ€™exÃ©cutent dans MongoDB Shell ou lâ€™interface web Atlas.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.1 Explication simple",
    "text": "1.1 Explication simple\nMongoDB est une base de donnÃ©es qui stocke les donnÃ©es au format JSON (comme des fichiers texte structurÃ©s).\n\nComparaison : SQL vs MongoDB\nBase de donnÃ©es SQL (MySQL, PostgreSQL) :\nTable : employes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom    â”‚ age â”‚ ville â”‚ salaire â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice    â”‚ 25  â”‚ Paris â”‚ 45000   â”‚\nâ”‚ 2  â”‚ Bob      â”‚ 30  â”‚ Lyon  â”‚ 50000   â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n-- RequÃªte SQL\nSELECT * FROM employes WHERE ville = 'Paris';\nMongoDB (NoSQL) :\nCollection : employes\n[\n  {\n    \"_id\": 1,\n    \"nom\": \"Alice\",\n    \"age\": 25,\n    \"ville\": \"Paris\",\n    \"salaire\": 45000,\n    \"competences\": [\"Python\", \"SQL\"]\n  },\n  {\n    \"_id\": 2,\n    \"nom\": \"Bob\",\n    \"age\": 30,\n    \"ville\": \"Lyon\",\n    \"salaire\": 50000,\n    \"competences\": [\"Java\", \"Docker\"]\n  }\n]\n\n// RequÃªte MongoDB\ndb.employes.find({ ville: \"Paris\" })\n\n\nğŸ”‘ DiffÃ©rences principales :\n\n\n\n\n\n\n\n\nAspect\nSQL\nMongoDB\n\n\n\n\nStructure\nTableaux avec colonnes fixes\nDocuments JSON flexibles\n\n\nSchÃ©ma\nRigide (dÃ©fini Ã  lâ€™avance)\nFlexible (peut changer)\n\n\nFormat\nLignes et colonnes\nDocuments JSON\n\n\nTableaux\nDifficile (tables sÃ©parÃ©es)\nFacile (intÃ©grÃ©)\n\n\nLangage\nSQL\nJavaScript/JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.2 Vocabulaire de base",
    "text": "1.2 Vocabulaire de base\n\n\n\n\n\n\n\n\n\nSQL\nMongoDB\nExplication\nExemple\n\n\n\n\nDatabase\nDatabase\nConteneur principal\nentreprise\n\n\nTable\nCollection\nGroupe de donnÃ©es similaires\nemployes, produits\n\n\nRow\nDocument\nUne entrÃ©e de donnÃ©es\n{nom: \"Alice\", age: 25}\n\n\nColumn\nField\nUn attribut\nnom, age, ville\n\n\n\n\nStructure visuelle :\nMongoDB Server\n  â””â”€â”€ Database: ma_boutique\n       â”œâ”€â”€ Collection: clients\n       â”‚    â”œâ”€â”€ Document 1: { nom: \"Alice\", email: \"alice@email.com\" }\n       â”‚    â””â”€â”€ Document 2: { nom: \"Bob\", email: \"bob@email.com\" }\n       â”‚\n       â””â”€â”€ Collection: produits\n            â”œâ”€â”€ Document 1: { nom: \"Laptop\", prix: 899 }\n            â””â”€â”€ Document 2: { nom: \"Souris\", prix: 29 }",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.3 Format des documents",
    "text": "1.3 Format des documents\nMongoDB stocke les donnÃ©es en BSON (Binary JSON).\n\nExemple de document complet :\n{\n  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  // ID unique automatique\n  \"nom\": \"Alice Dupont\",                        // String (texte)\n  \"age\": 28,                                     // Number (entier)\n  \"salaire\": 55000.50,                           // Number (dÃ©cimal)\n  \"actif\": true,                                 // Boolean (vrai/faux)\n  \"date_embauche\": ISODate(\"2022-01-15\"),       // Date\n  \"competences\": [\"Python\", \"SQL\", \"MongoDB\"],  // Array (tableau)\n  \"adresse\": {                                   // Object (objet imbriquÃ©)\n    \"rue\": \"123 Main St\",\n    \"ville\": \"Paris\",\n    \"code_postal\": \"75001\"\n  },\n  \"notes\": null                                  // Null (vide)\n}\n\n\nPoints importants :\n\nChaque document a un champ _id unique (crÃ©Ã© automatiquement si absent)\nOn peut imbriquer des objets et des tableaux\nPas besoin que tous les documents aient les mÃªmes champs\nLa syntaxe ressemble Ã  JavaScript/JSON",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?",
    "text": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?\n\nâœ… Avantages :\n1. FlexibilitÃ© du schÃ©ma\n\nPas besoin de dÃ©finir la structure Ã  lâ€™avance\nParfait pour des donnÃ©es qui changent souvent\nChaque document peut Ãªtre diffÃ©rent\n\n// Document 1 : simple\n{ nom: \"Alice\", age: 25 }\n\n// Document 2 : plus dÃ©taillÃ© (dans la mÃªme collection !)\n{ nom: \"Bob\", age: 30, ville: \"Lyon\", competences: [\"Python\"], manager: \"Alice\" }\n2. Format JSON natif\n\nLes APIs web utilisent JSON\nPas de conversion nÃ©cessaire\nFacile Ã  lire et manipuler\n\n3. Performance\n\nLectures et Ã©critures trÃ¨s rapides\nGÃ¨re facilement des millions de documents\nScalabilitÃ© horizontale (ajouter des serveurs)\n\n4. DonnÃ©es imbriquÃ©es\n\nStocke des structures complexes facilement\nPas besoin de multiples tables et jointures\n\n// Tout dans un seul document !\n{\n  \"commande_id\": \"CMD001\",\n  \"client\": { \"nom\": \"Alice\", \"email\": \"alice@example.com\" },\n  \"articles\": [\n    { \"produit\": \"Laptop\", \"quantite\": 1, \"prix\": 899 },\n    { \"produit\": \"Souris\", \"quantite\": 2, \"prix\": 29 }\n  ],\n  \"total\": 957\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "1.5 Quand utiliser MongoDB ?",
    "text": "1.5 Quand utiliser MongoDB ?\n\nâœ… Utilisez MongoDB pour :\n\n\n\nCas dâ€™usage\nPourquoi\n\n\n\n\nğŸ“± Applications web/mobile\nAPI JSON, scalabilitÃ©\n\n\nğŸ“Š Collecte de logs\nVolume Ã©levÃ©, structure flexible\n\n\nğŸ›’ E-commerce (catalogues)\nProduits avec attributs variables\n\n\nğŸ“¡ DonnÃ©es IoT\nMillions dâ€™Ã©critures/seconde\n\n\nğŸ“ CMS (contenu)\nStructures diverses\n\n\nğŸ”„ Prototypage rapide\nPas de schÃ©ma prÃ©dÃ©fini\n\n\nğŸŒ DonnÃ©es JSON dâ€™APIs\nFormat natif\n\n\n\n\n\nâŒ Nâ€™utilisez PAS MongoDB pour :\n\n\n\nCas dâ€™usage\nPrÃ©fÃ©rez SQL\n\n\n\n\nğŸ’° Transactions bancaires\nBesoin dâ€™ACID strict\n\n\nğŸ”— Relations multiples complexes\nNombreuses jointures\n\n\nğŸ“ˆ Reporting BI traditionnel\nRequÃªtes SQL ad-hoc\n\n\nğŸ“Š Data Warehouse\nSchÃ©ma en Ã©toile\n\n\n\n\n\nğŸ’¡ RÃ¨gle simple :\n\nDonnÃ©es flexibles, JSON, volume Ã©levÃ© â†’ MongoDB\n\nDonnÃ©es structurÃ©es, relations strictes, transactions complexes â†’ SQL (PostgreSQL, MySQL)",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.1 Pourquoi MongoDB Atlas ?",
    "text": "2.1 Pourquoi MongoDB Atlas ?\nMongoDB Atlas = MongoDB hÃ©bergÃ© dans le cloud (gratuit)\n\nAvantages :\nâœ… Gratuit : Tier M0 avec 512 MB de stockage\nâœ… Pas dâ€™installation : Tout dans le navigateur\nâœ… Interface graphique : Data Explorer facile\nâœ… SÃ©curisÃ© : Backup automatique\nâœ… MongoDB Shell intÃ©grÃ© : Testez vos commandes directement",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.2 CrÃ©er votre compte (5 minutes)",
    "text": "2.2 CrÃ©er votre compte (5 minutes)\n\nâ–¶ï¸ Ã‰tape 1 : Inscription\n\nAllez sur ğŸ‘‰ cloud.mongodb.com\nCliquez sur â€œTry Freeâ€\nInscrivez-vous avec email/mot de passe (ou Google)\n\n\n\nâ–¶ï¸ Ã‰tape 2 : CrÃ©er un cluster gratuit\n\nChoisissez M0 (FREE) ğŸ‰\nProvider : AWS (ou Google Cloud/Azure)\nRÃ©gion : Choisissez proche de vous\n\nEurope : Frankfurt, Paris, London\nAmÃ©rique : N. Virginia, Oregon\nAsie : Singapore, Mumbai\n\nCluster Name : Cluster0 (par dÃ©faut, vous pouvez changer)\nCliquez â€œCreateâ€\n\nâ³ Attendez 2-3 minutes que le cluster se crÃ©e\n\n\nâ–¶ï¸ Ã‰tape 3 : CrÃ©er un utilisateur\n\nDans la popup de sÃ©curitÃ© :\nAuthentication Method : Username and Password\nUsername : admin (ou votre choix)\nPassword : CrÃ©ez un mot de passe fort\n\nâš ï¸ NOTEZ-LE BIEN quelque part !\n\nCliquez â€œCreate Userâ€\n\n\n\nâ–¶ï¸ Ã‰tape 4 : Autoriser lâ€™accÃ¨s\n\nDans la popup : â€œWhere would you like to connect from?â€\nChoisissez â€œMy Local Environmentâ€\nOption 1 (recommandÃ©e pour apprendre) :\n\nCliquez â€œAdd My Current IP Addressâ€\n\nOption 2 (plus simple mais moins sÃ©curisÃ©e) :\n\nMettez 0.0.0.0/0 pour autoriser toutes les IPs\n\nCliquez â€œFinish and Closeâ€\n\nâœ… Votre cluster est prÃªt !",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "2.3 AccÃ©der Ã  MongoDB Shell",
    "text": "2.3 AccÃ©der Ã  MongoDB Shell\n\nMÃ©thode 1 : MongoDB Shell Web (le plus simple)\n\nDans MongoDB Atlas, allez sur â€œDatabaseâ€ (menu gauche)\nCliquez sur â€œBrowse Collectionsâ€ sur votre cluster\nEn bas de page, cliquez sur lâ€™onglet â€œMongoDB Shellâ€ ou â€œ&gt;_ mongoshâ€\nUne console sâ€™ouvre dans le navigateur âœ…\n\n\n\nMÃ©thode 2 : Data Explorer (interface graphique)\n\nCliquez sur â€œBrowse Collectionsâ€\nVous pouvez crÃ©er des bases de donnÃ©es et collections visuellement\nPratique pour visualiser, mais on va utiliser des commandes\n\n\n\nğŸ’¡ Dans ce tutoriel\nToutes les commandes ci-dessous sont Ã  taper dans : - MongoDB Shell Web (dans Atlas) - Ou mongosh (si vous lâ€™installez localement) - Ou Data Explorer â†’ Insert Document (pour ajouter des donnÃ©es)\nOn utilise la syntaxe JavaScript/MongoDB, pas Python !",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.1 CrÃ©er une base de donnÃ©es et une collection",
    "text": "3.1 CrÃ©er une base de donnÃ©es et une collection\n\nCrÃ©er/SÃ©lectionner une base de donnÃ©es\n// CrÃ©er ou utiliser la base de donnÃ©es \"ma_premiere_db\"\nuse ma_premiere_db\nNote : La base de donnÃ©es nâ€™est crÃ©Ã©e rÃ©ellement que quand vous insÃ©rez des donnÃ©es.\n\n\nVoir la base de donnÃ©es actuelle\ndb\n\n\nLister toutes les bases de donnÃ©es\nshow dbs\n\n\nLister les collections\nshow collections",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.2 CREATE - InsÃ©rer des documents",
    "text": "3.2 CREATE - InsÃ©rer des documents\n\nInsÃ©rer UN document\n// CrÃ©er la collection \"employes\" et insÃ©rer un document\ndb.employes.insertOne({\n  nom: \"Alice Dupont\",\n  age: 28,\n  poste: \"Data Engineer\",\n  salaire: 55000,\n  ville: \"Paris\"\n})\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedId: ObjectId(\"507f1f77bcf86cd799439011\")\n}\n\n\nInsÃ©rer PLUSIEURS documents\ndb.employes.insertMany([\n  {\n    nom: \"Bob Martin\",\n    age: 32,\n    poste: \"Data Analyst\",\n    salaire: 48000,\n    ville: \"Lyon\"\n  },\n  {\n    nom: \"Charlie Dubois\",\n    age: 35,\n    poste: \"Data Scientist\",\n    salaire: 65000,\n    ville: \"Paris\"\n  },\n  {\n    nom: \"David Laurent\",\n    age: 29,\n    poste: \"Data Engineer\",\n    salaire: 58000,\n    ville: \"Marseille\"\n  },\n  {\n    nom: \"Eve Bernard\",\n    age: 26,\n    poste: \"Data Analyst\",\n    salaire: 45000,\n    ville: \"Lyon\"\n  }\n])\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedIds: {\n    '0': ObjectId(\"...\"),\n    '1': ObjectId(\"...\"),\n    '2': ObjectId(\"...\"),\n    '3': ObjectId(\"...\")\n  }\n}",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.3 READ - Lire des documents",
    "text": "3.3 READ - Lire des documents\n\nLire TOUS les documents\ndb.employes.find()\n\n\nLire avec un affichage formatÃ© (pretty)\ndb.employes.find().pretty()\n\n\nLire UN seul document\ndb.employes.findOne()\n\n\nFiltrer : employÃ©s Ã  Paris\ndb.employes.find({ ville: \"Paris\" })\n\n\nFiltrer : salaire supÃ©rieur Ã  50000\ndb.employes.find({ salaire: { $gt: 50000 } })\nOpÃ©rateurs de comparaison : - $gt : greater than (&gt;) - $gte : greater than or equal (&gt;=) - $lt : less than (&lt;) - $lte : less than or equal (&lt;=) - $eq : equal (=) - $ne : not equal (!=)\n\n\nFiltrer avec plusieurs conditions (AND)\n// EmployÃ©s Ã  Paris avec salaire &gt; 50000\ndb.employes.find({\n  ville: \"Paris\",\n  salaire: { $gt: 50000 }\n})\n\n\nFiltrer avec OR\n// EmployÃ©s Ã  Paris OU Lyon\ndb.employes.find({\n  $or: [\n    { ville: \"Paris\" },\n    { ville: \"Lyon\" }\n  ]\n})\n\n\nSÃ©lectionner certains champs seulement (projection)\n// Afficher seulement nom et salaire (sans _id)\ndb.employes.find(\n  {},\n  { nom: 1, salaire: 1, _id: 0 }\n)\nNote : 1 = inclure, 0 = exclure",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.4 Trier, limiter, compter",
    "text": "3.4 Trier, limiter, compter\n\nTrier par salaire (croissant)\ndb.employes.find().sort({ salaire: 1 })\n\n\nTrier par salaire (dÃ©croissant)\ndb.employes.find().sort({ salaire: -1 })\nNote : 1 = croissant, -1 = dÃ©croissant\n\n\nLimiter les rÃ©sultats (top 3)\n// Top 3 salaires\ndb.employes.find().sort({ salaire: -1 }).limit(3)\n\n\nSauter des rÃ©sultats (pagination)\n// Sauter les 2 premiers, afficher les 3 suivants\ndb.employes.find().skip(2).limit(3)\n\n\nCompter les documents\n// Compter tous les employÃ©s\ndb.employes.countDocuments()\n\n// Compter les employÃ©s Ã  Paris\ndb.employes.countDocuments({ ville: \"Paris\" })\n\n// Compter les employÃ©s avec salaire &gt; 50000\ndb.employes.countDocuments({ salaire: { $gt: 50000 } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.5 UPDATE - Modifier des documents",
    "text": "3.5 UPDATE - Modifier des documents\n\nModifier UN document\n// Augmenter le salaire d'Alice\ndb.employes.updateOne(\n  { nom: \"Alice Dupont\" },           // Condition\n  { $set: { salaire: 60000 } }       // Modification\n)\nRÃ©sultat :\n{\n  acknowledged: true,\n  matchedCount: 1,\n  modifiedCount: 1\n}\n\n\nModifier PLUSIEURS documents\n// Augmenter tous les salaires de Lyon de 2000â‚¬\ndb.employes.updateMany(\n  { ville: \"Lyon\" },\n  { $inc: { salaire: 2000 } }       // $inc = incrÃ©menter\n)\n\n\nOpÃ©rateurs de modification\n// $set : dÃ©finir une valeur\n{ $set: { ville: \"Paris\" } }\n\n// $inc : incrÃ©menter\n{ $inc: { age: 1 } }\n\n// $mul : multiplier\n{ $mul: { salaire: 1.1 } }  // Augmentation de 10%\n\n// $unset : supprimer un champ\n{ $unset: { notes: \"\" } }\n\n// $rename : renommer un champ\n{ $rename: { \"nom\": \"nom_complet\" } }\n\n\nAjouter un champ Ã  tous les documents\ndb.employes.updateMany(\n  {},\n  { $set: { actif: true } }\n)",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "3.6 DELETE - Supprimer des documents",
    "text": "3.6 DELETE - Supprimer des documents\n\nSupprimer UN document\ndb.employes.deleteOne({ nom: \"Test User\" })\n\n\nSupprimer PLUSIEURS documents\n// Supprimer tous les employÃ©s de Test\ndb.employes.deleteMany({ ville: \"Test\" })\n\n\nâš ï¸ Supprimer TOUS les documents\n// ATTENTION : Supprime tout !\ndb.employes.deleteMany({})\n\n\nSupprimer une collection entiÃ¨re\ndb.employes.drop()\n\n\nSupprimer une base de donnÃ©es\ndb.dropDatabase()",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.1 Grouper et compter",
    "text": "4.1 Grouper et compter\n\nCompter les employÃ©s par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",              // Grouper par ville\n      nombre: { $sum: 1 }         // Compter\n    }\n  }\n])\nRÃ©sultat :\n[\n  { _id: \"Paris\", nombre: 2 },\n  { _id: \"Lyon\", nombre: 2 },\n  { _id: \"Marseille\", nombre: 1 }\n]\n\n\nSalaire moyen par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  },\n  {\n    $sort: { salaire_moyen: -1 }   // Trier par salaire moyen dÃ©croissant\n  }\n])\n\n\nGrouper par poste\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$poste\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.2 Filtrer avant dâ€™agrÃ©ger",
    "text": "4.2 Filtrer avant dâ€™agrÃ©ger\n// Statistiques pour les employÃ©s avec salaire &gt; 50000\ndb.employes.aggregate([\n  {\n    $match: { salaire: { $gt: 50000 } }   // Filtrer d'abord\n  },\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "4.3 Statistiques globales",
    "text": "4.3 Statistiques globales\n// Statistiques sur tous les salaires\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: null,                          // null = pas de groupement\n      total_employes: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_total: { $sum: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "Ce que vous avez appris âœ…",
    "text": "Ce que vous avez appris âœ…\n\nConcepts\n\nMongoDB = Base de donnÃ©es NoSQL (documents JSON)\nVocabulaire : Database â†’ Collection â†’ Document â†’ Field\nDiffÃ©rence SQL/NoSQL : Structure fixe vs flexible\nQuand utiliser MongoDB : APIs, logs, IoT, e-commerce\n\n\n\nCommandes essentielles\n// BASES\nuse ma_db                    // CrÃ©er/utiliser DB\nshow dbs                     // Lister les DBs\nshow collections             // Lister les collections\n\n// CREATE\ndb.collection.insertOne({...})\ndb.collection.insertMany([{...}, {...}])\n\n// READ\ndb.collection.find()                     // Tout\ndb.collection.find({ ville: \"Paris\" })   // FiltrÃ©\ndb.collection.findOne()\ndb.collection.find().sort({ age: -1 })   // TriÃ©\ndb.collection.find().limit(5)            // LimitÃ©\ndb.collection.countDocuments()\n\n// UPDATE\ndb.collection.updateOne({ _id: 1 }, { $set: { age: 30 } })\ndb.collection.updateMany({ ville: \"Lyon\" }, { $inc: { salaire: 1000 } })\n\n// DELETE\ndb.collection.deleteOne({ _id: 1 })\ndb.collection.deleteMany({ ville: \"Test\" })\n\n// AGGREGATE\ndb.collection.aggregate([\n  { $group: { _id: \"$ville\", count: { $sum: 1 } } }\n])",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "OpÃ©rateurs importants ğŸ“",
    "text": "OpÃ©rateurs importants ğŸ“\n\nComparaison\n\n$gt : &gt; (greater than)\n$gte : &gt;= (greater than or equal)\n$lt : &lt; (less than)\n$lte : &lt;= (less than or equal)\n$eq : = (equal)\n$ne : != (not equal)\n\n\n\nModification\n\n$set : DÃ©finir une valeur\n$inc : IncrÃ©menter\n$mul : Multiplier\n$unset : Supprimer un champ\n\n\n\nAgrÃ©gation\n\n$sum : Somme\n$avg : Moyenne\n$max : Maximum\n$min : Minimum\n$group : Grouper\n$match : Filtrer",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.1 Pourquoi les index ?",
    "text": "5.1 Pourquoi les index ?\nSans index, MongoDB doit scanner tous les documents pour trouver ceux qui correspondent au filtre. Avec un index, la recherche est beaucoup plus rapide.\n\n\n\nSans index\nAvec index\n\n\n\n\nScan complet (lent)\nRecherche directe (rapide)\n\n\nO(n)\nO(log n)\n\n\n1M docs = 1M comparaisons\n1M docs = ~20 comparaisons",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.2 CrÃ©er un index",
    "text": "5.2 CrÃ©er un index\n\nIndex simple (un champ)\n// Index sur le champ \"email\" (croissant)\ndb.employes.createIndex({ email: 1 })\n\n// Index sur le champ \"salaire\" (dÃ©croissant)\ndb.employes.createIndex({ salaire: -1 })\n\n\nIndex composÃ© (plusieurs champs)\n// Index sur ville + poste (pour requÃªtes frÃ©quentes)\ndb.employes.createIndex({ ville: 1, poste: 1 })\n\n\nIndex unique\n// EmpÃªche les doublons sur email\ndb.employes.createIndex({ email: 1 }, { unique: true })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.3 GÃ©rer les index",
    "text": "5.3 GÃ©rer les index\n// Lister tous les index\ndb.employes.getIndexes()\n\n// Supprimer un index\ndb.employes.dropIndex({ email: 1 })\n\n// Supprimer tous les index (sauf _id)\ndb.employes.dropIndexes()",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.4 Analyser les performances",
    "text": "5.4 Analyser les performances\n// Voir le plan d'exÃ©cution\ndb.employes.find({ ville: \"Paris\" }).explain(\"executionStats\")\nRegarder :\n\ntotalDocsExamined : Combien de documents scannÃ©s\nexecutionTimeMillis : Temps dâ€™exÃ©cution\nstage: \"IXSCAN\" = Index utilisÃ© âœ…\nstage: \"COLLSCAN\" = Scan complet âŒ",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "5.5 Bonnes pratiques",
    "text": "5.5 Bonnes pratiques\n\n\n\n\n\n\n\nRÃ¨gle\nExplication\n\n\n\n\nâœ… Indexer les champs de filtrage frÃ©quents\nville, email, date\n\n\nâœ… Indexer les champs de tri\nORDER BY = index\n\n\nâš ï¸ Pas trop dâ€™index\nChaque index ralentit les Ã©critures\n\n\nâš ï¸ Index composÃ© : ordre important\nLe champ le plus filtrant en premier",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.1 Recherche avec expressions rÃ©guliÃ¨res",
    "text": "6.1 Recherche avec expressions rÃ©guliÃ¨res\n// Noms commenÃ§ant par \"A\"\ndb.employes.find({ nom: { $regex: \"^A\" } })\n\n// Noms contenant \"dupont\" (insensible Ã  la casse)\ndb.employes.find({ nom: { $regex: \"dupont\", $options: \"i\" } })\n\n// Emails Gmail\ndb.employes.find({ email: { $regex: \"@gmail\\.com$\" } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.2 OpÃ©rateurs de tableau",
    "text": "6.2 OpÃ©rateurs de tableau\n// Document avec un tableau\ndb.employes.insertOne({\n  nom: \"Alice\",\n  competences: [\"Python\", \"SQL\", \"MongoDB\"]\n})\n\n// Trouver ceux qui ont \"Python\" dans leurs compÃ©tences\ndb.employes.find({ competences: \"Python\" })\n\n// Trouver ceux qui ont Python ET SQL\ndb.employes.find({ competences: { $all: [\"Python\", \"SQL\"] } })\n\n// Trouver ceux qui ont au moins 3 compÃ©tences\ndb.employes.find({ competences: { $size: 3 } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "6.3 Recherche dans les objets imbriquÃ©s",
    "text": "6.3 Recherche dans les objets imbriquÃ©s\n// Document avec objet imbriquÃ©\ndb.employes.insertOne({\n  nom: \"Bob\",\n  adresse: {\n    ville: \"Paris\",\n    code_postal: \"75001\"\n  }\n})\n\n// Rechercher par ville imbriquÃ©e (notation pointÃ©e)\ndb.employes.find({ \"adresse.ville\": \"Paris\" })\n\n// Rechercher par code postal\ndb.employes.find({ \"adresse.code_postal\": { $regex: \"^75\" } })",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet : SQL vs MongoDB",
    "text": "ğŸ“‹ Cheatsheet : SQL vs MongoDB\n\n\n\n\n\n\n\n\nOpÃ©ration\nSQL\nMongoDB\n\n\n\n\nTout sÃ©lectionner\nSELECT * FROM table\ndb.collection.find()\n\n\nFiltrer\nWHERE col = 'val'\nfind({ col: 'val' })\n\n\nSupÃ©rieur Ã \nWHERE col &gt; 10\nfind({ col: { $gt: 10 } })\n\n\nET\nWHERE a = 1 AND b = 2\nfind({ a: 1, b: 2 })\n\n\nOU\nWHERE a = 1 OR b = 2\nfind({ $or: [{a:1}, {b:2}] })\n\n\nIN\nWHERE col IN (1,2,3)\nfind({ col: { $in: [1,2,3] } })\n\n\nLIKE\nWHERE col LIKE '%val%'\nfind({ col: { $regex: 'val' } })\n\n\nProjection\nSELECT a, b FROM\nfind({}, { a:1, b:1 })\n\n\nTrier\nORDER BY col ASC\n.sort({ col: 1 })\n\n\nLimiter\nLIMIT 10\n.limit(10)\n\n\nCompter\nSELECT COUNT(*)\n.countDocuments()\n\n\nGrouper\nGROUP BY col\naggregate([{$group:{_id:'$col'}}])\n\n\nInsert\nINSERT INTO ... VALUES\ninsertOne({...})\n\n\nUpdate\nUPDATE ... SET ... WHERE\nupdateOne({filter}, {$set:{...}})\n\n\nDelete\nDELETE FROM ... WHERE\ndeleteOne({filter})",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ¯ Exercices pratiques â€” Ã€ toi de jouer !",
    "text": "ğŸ¯ Exercices pratiques â€” Ã€ toi de jouer !\n\nCollection disponible : employes\n{ nom, age, poste, salaire, ville, competences: [...] }\n\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nTrouver tous les employÃ©s qui habitent Ã  Lyon.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({ ville: \"Lyon\" })\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nCompter le nombre total dâ€™employÃ©s.\n\n\nğŸ’¡ Solution\n\ndb.employes.countDocuments()\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nTrouver les 3 employÃ©s les mieux payÃ©s (nom et salaire uniquement).\n\n\nğŸ’¡ Solution\n\ndb.employes.find({}, { nom: 1, salaire: 1, _id: 0 })\n  .sort({ salaire: -1 })\n  .limit(3)\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nAugmenter de 5% le salaire de tous les Data Engineers.\n\n\nğŸ’¡ Solution\n\ndb.employes.updateMany(\n  { poste: \"Data Engineer\" },\n  { $mul: { salaire: 1.05 } }\n)\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nCalculer le salaire moyen par poste, triÃ© du plus Ã©levÃ© au plus bas.\n\n\nğŸ’¡ Solution\n\ndb.employes.aggregate([\n  { $group: { _id: \"$poste\", salaire_moyen: { $avg: \"$salaire\" } } },\n  { $sort: { salaire_moyen: -1 } }\n])\n\n\n\n\nğŸ‹ï¸ Exercice 6 â€” AvancÃ©\nTrouver les employÃ©s qui ont â€œPythonâ€ dans leurs compÃ©tences et gagnent plus de 50000â‚¬.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({\n  competences: \"Python\",\n  salaire: { $gt: 50000 }\n})",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert MongoDB ! Vous Ãªtes prÃªt pour le niveau intermÃ©diaire\n7-9/10 : ğŸŒŸ TrÃ¨s bien ! Relisez les sections oÃ¹ vous avez hÃ©sitÃ©\n5-6/10 : ğŸ’ª Bon dÃ©but ! Pratiquez les commandes dans Atlas\n&lt; 5/10 : ğŸ“š Relisez le notebook et testez les exemples",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸƒ MongoDB for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant MongoDB ! Passons Ã  une autre base NoSQL trÃ¨s utilisÃ©e en Data Engineering.\nğŸ‘‰ Module suivant : 10_elasticsearch_for_data_engineers.ipynb â€” Elasticsearch pour la recherche et lâ€™analytics\n\n\nğŸ“š Ressources\n\nMongoDB Atlas â€” Votre compte\nMongoDB Documentation â€” Doc officielle\nMongoDB University â€” Cours gratuits\nMongoDB Cheat Sheet\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les bases de MongoDB.",
    "crumbs": [
      "DÃ©butant",
      "ğŸƒ MongoDB for Data Engineers"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "",
    "text": "Deviens un Data Engineer opÃ©rationnel, Ã©tape par Ã©tape.",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#pourquoi-ce-bootcamp",
    "href": "index.html#pourquoi-ce-bootcamp",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ¯ Pourquoi ce Bootcamp ?",
    "text": "ğŸ¯ Pourquoi ce Bootcamp ?\nBienvenue dans le Bootcamp Data Engineering â€“ From Zero to Hero, un programme structurÃ© pensÃ© pour :\n\napprendre les bases solides du Data Engineering,\n\nmaÃ®triser les outils modernes utilisÃ©s par les Data Engineers professionnels,\n\ncomprendre les architectures data dâ€™entreprise,\n\npratiquer avec des notebooks interactifs et des projets concrets,\n\nprogresser du niveau DÃ©butant â†’ IntermÃ©diaire â†’ AvancÃ©.",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#structure-du-programme",
    "href": "index.html#structure-du-programme",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ“˜ Structure du programme",
    "text": "ğŸ“˜ Structure du programme\n\nğŸŸ¦ DÃ©butant\n\nPython pour les Data Engineers\n\nBash, Git, Bases de donnÃ©es\n\nSQL, Pandas, PySpark\n\nArchitecture de bases & premiers pipelines\n\n\n\nğŸŸ© IntermÃ©diaire\n\nSpark avancÃ©, Scala\n\nKafka & streaming\n\nDocker, Kubernetes\n\nData Lakes, Lakehouse, Polars, DuckDB\n\nData Quality, Governance, CI/CD\n\n\n\nğŸŸ¥ AvancÃ©\n\nDistributed Systems\n\nAdvanced Streaming (Flink/Spark)\n\nLakehouse Internals\n\nMLOps, MLflow, Kubeflow\n\nSRE/Observability pour la Data\n\nFinOps et architectures cloud Ã  grande Ã©chelle",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "index.html#ready-to-start",
    "href": "index.html#ready-to-start",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸš€ Ready to Start?",
    "text": "ğŸš€ Ready to Start?\nğŸ‘‰ Navigue dans le menu Ã  gauche pour commencer ton parcours.\nğŸ‘‰ Tu peux contribuer ou proposer des amÃ©liorations via GitHub.\nBonne montÃ©e en compÃ©tence ! ğŸ’ª",
    "crumbs": [
      "Bootcamp Data Engineering â€“ From Zero to Hero"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nAvoir suivi le module 07_sql_for_data_engineers",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre ce quâ€™est le Big Data et ses caractÃ©ristiques (5V)\nâœ… Expliquer pourquoi le traitement distribuÃ© est nÃ©cessaire\nâœ… DÃ©crire lâ€™architecture Hadoop (HDFS, MapReduce, YARN)\nâœ… Comprendre le modÃ¨le MapReduce\nâœ… Expliquer pourquoi Spark a remplacÃ© MapReduce\nâœ… Comprendre le NoSQL et ses diffÃ©rents types\nâœ… Savoir quand utiliser SQL vs NoSQL\nâœ… ConnaÃ®tre le thÃ©orÃ¨me CAP\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique viendra avec MongoDB (module suivant) et PySpark !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸŒ 1. Câ€™est quoi le Big Data ?",
    "text": "ğŸŒ 1. Câ€™est quoi le Big Data ?\n\nğŸ“– DÃ©finition\nLe Big Data dÃ©signe des ensembles de donnÃ©es tellement volumineux et complexes quâ€™ils ne peuvent pas Ãªtre traitÃ©s par des outils traditionnels (Excel, bases SQL classiques, un seul serveur).\n\n\nğŸ“… Origine\nLe terme a Ã©mergÃ© dans les annÃ©es 2000 avec lâ€™explosion :\n\nğŸŒ Dâ€™Internet et des rÃ©seaux sociaux\nğŸ“± Des smartphones\nğŸ“¡ Des capteurs IoT\nğŸ’³ Des transactions en ligne\n\n\n\nğŸ“Š Ordres de grandeur\n1 Ko  (Kilooctet)   = 1 page de texte\n1 Mo  (MÃ©gaoctet)   = 1 photo HD\n1 Go  (Gigaoctet)   = 1 film HD\n1 To  (TÃ©raoctet)   = 1 000 films HD\n1 Po  (PÃ©taoctet)   = 1 000 To = 1 million de Go\n1 Eo  (Exaoctet)    = 1 000 Po\n1 Zo  (Zettaoctet)  = 1 000 Eo\n\n\nğŸ¢ Exemples concrets\n\n\n\nEntreprise\nVolume de donnÃ©es\n\n\n\n\nFacebook\n~4 Po gÃ©nÃ©rÃ©s par jour\n\n\nGoogle\n~20 Po traitÃ©s par jour\n\n\nNetflix\n~60 Po de vidÃ©os stockÃ©es\n\n\nCERN (LHC)\n~1 Po par seconde pendant les expÃ©riences",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“Š 2. Les 5V du Big Data",
    "text": "ğŸ“Š 2. Les 5V du Big Data\nLes caractÃ©ristiques du Big Data sont souvent rÃ©sumÃ©es par les 5V :\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   VOLUME    â”‚\n                    â”‚  (quantitÃ©) â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚                  â”‚                  â”‚\n        â–¼                  â–¼                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   VELOCITY    â”‚  â”‚   VARIETY     â”‚  â”‚   VERACITY    â”‚\nâ”‚   (vitesse)   â”‚  â”‚  (diversitÃ©)  â”‚  â”‚  (fiabilitÃ©)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                           â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚    VALUE    â”‚\n                    â”‚  (valeur)   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“¦ Volume â€” La quantitÃ© massive de donnÃ©es\n\n\n\n\n\n\n\nDÃ©fi\nSolution\n\n\n\n\nImpossible de stocker sur un seul disque\nStockage distribuÃ© (HDFS, S3)\n\n\nImpossible de charger en RAM\nTraitement par partitions\n\n\n\n\n\n\nâš¡ Velocity â€” La vitesse de gÃ©nÃ©ration et traitement\n\n\n\nType\nExemple\nLatence\n\n\n\n\nBatch\nRapport mensuel\nHeures\n\n\nNear real-time\nDashboard\nMinutes\n\n\nReal-time / Streaming\nDÃ©tection de fraude\nMillisecondes\n\n\n\n\n\n\nğŸ¨ Variety â€” La diversitÃ© des formats\n\nğŸ’¡ Rappel : Tu as dÃ©jÃ  vu Ã§a dans le module 06 !\n\n\n\n\n\n\n\n\n\nType\nFormat\nExemple\n\n\n\n\nStructurÃ©\nTables, colonnes fixes\nSQL, CSV\n\n\nSemi-structurÃ©\nSchÃ©ma flexible\nJSON, XML (MongoDB, Elasticsearch)\n\n\nNon-structurÃ©\nPas de schÃ©ma\nImages, vidÃ©os, texte libre\n\n\n\n\n\n\nâœ… Veracity â€” La fiabilitÃ© des donnÃ©es\n\n\n\nProblÃ¨me\nImpact\n\n\n\n\nDonnÃ©es manquantes\nRÃ©sultats biaisÃ©s\n\n\nDoublons\nComptages faux\n\n\nErreurs de saisie\nMauvaises dÃ©cisions\n\n\nDonnÃ©es obsolÃ¨tes\nAnalyses non pertinentes\n\n\n\n\nğŸ’¡ Câ€™est lÃ  que le Data Engineer intervient : nettoyer, valider, transformer !\n\n\n\n\nğŸ’ Value â€” La valeur extraite\nLes donnÃ©es nâ€™ont de valeur que si on peut en tirer des insights : - ğŸ“ˆ PrÃ©dictions (ML) - ğŸ“Š Dashboards - ğŸ”” Alertes - ğŸ’° Optimisation business",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš™ï¸ 3. Pourquoi le traitement distribuÃ© ?",
    "text": "âš™ï¸ 3. Pourquoi le traitement distribuÃ© ?\n\nğŸš« Limites dâ€™une machine unique\nImaginons que tu dois traiter 10 To de logs :\n\n\n\nRessource\nLimite typique\nProblÃ¨me\n\n\n\n\nRAM\n64-256 Go\n10 To ne tient pas en mÃ©moire\n\n\nCPU\n8-64 cÅ“urs\nTraitement sÃ©quentiel = trop lent\n\n\nDisque\n500 Mo/s lecture\n10 To = 5+ heures juste pour lire\n\n\nRÃ©seau\nGoulot dâ€™Ã©tranglement\nTransfÃ©rer 10 To = des heures\n\n\n\n\n\n\nğŸ“ˆ Scale-Up vs Scale-Out\nSCALE-UP (vertical)              SCALE-OUT (horizontal)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”\n   â”‚             â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚   MEGA      â”‚               â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚\n   â”‚  SERVEUR    â”‚      vs       â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚\n   â”‚   ğŸ’ªğŸ’ªğŸ’ª    â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚             â”‚               â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               \n                                 Cluster de serveurs\n   + Plus de RAM                 + Moins cher (commodity)\n   + Plus de CPU                 + ScalabilitÃ© infinie\n   - TrÃ¨s cher $$$               + TolÃ©rance aux pannes\n   - Limite physique             - Plus complexe\n\n\nâœ… Le Big Data utilise le Scale-Out !\nAu lieu dâ€™une machine surpuissante, on utilise un cluster de machines ordinaires.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ§  4. Concepts clÃ©s du traitement distribuÃ©",
    "text": "ğŸ§  4. Concepts clÃ©s du traitement distribuÃ©\n\nğŸ”„ ParallÃ©lisme vs Distribution\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nParallÃ©lisme\nPlusieurs tÃ¢ches en mÃªme temps sur une machine\nMulti-threading\n\n\nDistribution\nTÃ¢ches rÃ©parties sur plusieurs machines\nCluster Hadoop/Spark\n\n\n\n\n\n\nğŸ“ Data Locality â€” â€œAmener le code aux donnÃ©esâ€\nâŒ MAUVAIS : DÃ©placer les donnÃ©es vers le code\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â•â•10 Toâ•â•â–º    â”‚   Serveur 2    â”‚\nâ”‚   (donnÃ©es)    â”‚   rÃ©seau       â”‚    (calcul)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   lent !       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… BON : DÃ©placer le code vers les donnÃ©es\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â—„â•â•codeâ•â•     â”‚   Serveur 2    â”‚\nâ”‚ donnÃ©es+calcul â”‚   (petit)      â”‚    (master)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Câ€™est le principe fondamental de Hadoop et Spark !\n\n\n\n\nğŸ›¡ï¸ Fault Tolerance â€” TolÃ©rance aux pannes\nDans un cluster de 1000 machines, des pannes arrivent tous les jours !\n\n\n\n\n\n\n\nStratÃ©gie\nDescription\n\n\n\n\nRÃ©plication\nCopier les donnÃ©es sur plusieurs nÅ“uds (HDFS : 3 copies)\n\n\nCheckpointing\nSauvegarder lâ€™Ã©tat intermÃ©diaire\n\n\nLineage\nRecalculer les donnÃ©es perdues (Spark RDD)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ˜ 5. Lâ€™Ã©cosystÃ¨me Hadoop",
    "text": "ğŸ˜ 5. Lâ€™Ã©cosystÃ¨me Hadoop\nHadoop est un framework open-source crÃ©Ã© par Yahoo (2006), inspirÃ© des papiers de Google (GFS, MapReduce).\n\nğŸ—ï¸ Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Ã‰COSYSTÃˆME HADOOP                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚   â”‚  Hive   â”‚  â”‚   Pig   â”‚  â”‚  HBase  â”‚  â”‚  Sqoop  â”‚  ...  â”‚\nâ”‚   â”‚  (SQL)  â”‚  â”‚(scripts)â”‚  â”‚ (NoSQL) â”‚  â”‚ (import)â”‚       â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚\nâ”‚        â”‚            â”‚            â”‚            â”‚             â”‚\nâ”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚  MapReduce  â”‚  â—„â”€â”€ Traitement         â”‚\nâ”‚                    â”‚  (calcul)   â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    YARN     â”‚  â—„â”€â”€ Ressources         â”‚\nâ”‚                    â”‚ (scheduler) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    HDFS     â”‚  â—„â”€â”€ Stockage           â”‚\nâ”‚                    â”‚  (fichiers) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“ HDFS â€” Hadoop Distributed File System\nSystÃ¨me de fichiers distribuÃ© qui stocke les donnÃ©es sur plusieurs machines.\nFichier original : data.csv (300 Mo)\n                      â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â–¼             â–¼             â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ Block 1 â”‚   â”‚ Block 2 â”‚   â”‚ Block 3 â”‚   (128 Mo chacun)\n   â”‚ 128 Mo  â”‚   â”‚ 128 Mo  â”‚   â”‚  44 Mo  â”‚\n   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n        â”‚             â”‚             â”‚\n   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x\n\n\n\nCaractÃ©ristique\nValeur par dÃ©faut\n\n\n\n\nTaille de bloc\n128 Mo\n\n\nFacteur de rÃ©plication\n3\n\n\nType dâ€™accÃ¨s\nWrite once, read many\n\n\n\n\n\n\nğŸ›ï¸ YARN â€” Yet Another Resource Negotiator\nGestionnaire de ressources du cluster :\n\nAlloue CPU/RAM aux applications\nGÃ¨re la file dâ€™attente des jobs\nSurveille lâ€™Ã©tat des nÅ“uds",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ”„ 6. MapReduce â€” Le modÃ¨le de traitement",
    "text": "ğŸ”„ 6. MapReduce â€” Le modÃ¨le de traitement\nMapReduce est un modÃ¨le de programmation pour traiter de grandes quantitÃ©s de donnÃ©es en parallÃ¨le.\n\nğŸ“ Les 3 Ã©tapes\n\n\n\nÃ‰tape\nAction\nParallÃ©lisme\n\n\n\n\nMap\nTransformer chaque Ã©lÃ©ment\nâœ… ParallÃ¨le\n\n\nShuffle\nRegrouper par clÃ©\nâš ï¸ RÃ©seau\n\n\nReduce\nAgrÃ©ger les valeurs\nâœ… ParallÃ¨le\n\n\n\n\n\n\nğŸ“ Exemple : Word Count\nCompter les occurrences de chaque mot dans un texte.\nENTRÃ‰E : \"hello world hello\"\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                              MAP\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  \"hello world hello\"   \n         â”‚\n         â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  (hello, 1)  (world, 1)  (hello, 1)  â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            SHUFFLE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  Regrouper par clÃ© (mot) :\n  \n  hello â†’ [1, 1]\n  world â†’ [1]\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            REDUCE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  hello â†’ sum([1, 1]) â†’ 2\n  world â†’ sum([1])    â†’ 1\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSORTIE : { \"hello\": 2, \"world\": 1 }\n\n\n\nğŸ–¥ï¸ MapReduce sur un cluster\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚   MASTER    â”‚\n                         â”‚  (Driver)   â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â”‚                    â”‚                    â”‚\n           â–¼                    â–¼                    â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚   NODE 1    â”‚      â”‚   NODE 2    â”‚      â”‚   NODE 3    â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚\n    â”‚ \"hello\"     â”‚      â”‚ \"world\"     â”‚      â”‚ \"hello\"     â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ MAP:        â”‚      â”‚ MAP:        â”‚      â”‚ MAP:        â”‚\n    â”‚ (hello, 1)  â”‚      â”‚ (world, 1)  â”‚      â”‚ (hello, 1)  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                    â”‚                    â”‚\n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n                         SHUFFLE (rÃ©seau)\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â–¼                                         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  REDUCER 1  â”‚                           â”‚  REDUCER 2  â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ hello: [1,1]â”‚                           â”‚ world: [1]  â”‚\n    â”‚ â†’ hello: 2  â”‚                           â”‚ â†’ world: 1  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Chaque nÅ“ud traite ses donnÃ©es localement (data locality) !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš ï¸ 7. Limites de MapReduce",
    "text": "âš ï¸ 7. Limites de MapReduce\n\nğŸ˜“ ProblÃ¨mes de MapReduce\n\n\n\n\n\n\n\n\nProblÃ¨me\nCause\nImpact\n\n\n\n\nLent\nÃ‰crit sur disque entre chaque Ã©tape\nI/O intensif\n\n\nVerbose\nCode Java complexe\nProductivitÃ© basse\n\n\nBatch only\nPas de streaming\nPas de temps rÃ©el\n\n\nPas de cache\nRelit les donnÃ©es Ã  chaque job\nItÃ©rations lentes (ML)\n\n\n\n\n\nğŸ’¾ Le problÃ¨me du disque\nMapReduce : DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                 â†‘           â†‘              â†‘              â†‘\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              LENT ! (I/O disque)\n\nSpark :     DISQUE â†’ Map â†’ MÃ‰MOIRE â†’ Shuffle â†’ MÃ‰MOIRE â†’ Reduce\n                            â†‘                    â†‘\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              RAPIDE ! (in-memory)\n\n\n\nğŸ”¥ Exemple : Algorithme itÃ©ratif (ML)\nPour un algorithme qui fait 10 itÃ©rations sur les mÃªmes donnÃ©es :\n\n\n\nFramework\nComportement\nTemps\n\n\n\n\nMapReduce\nRelit les donnÃ©es du disque 10 fois\nğŸ˜“\n\n\nSpark\nGarde les donnÃ©es en mÃ©moire, itÃ¨re 10 fois\nâš¡",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš¡ 8. Spark â€” Lâ€™Ã©volution",
    "text": "âš¡ 8. Spark â€” Lâ€™Ã©volution\nApache Spark (2014) a Ã©tÃ© crÃ©Ã© pour rÃ©soudre les limitations de MapReduce.\n\nğŸ“Š Comparaison Hadoop MapReduce vs Spark\n\n\n\nCritÃ¨re\nHadoop MapReduce\nApache Spark\n\n\n\n\nVitesse\nLent (disque)\n100x plus rapide (mÃ©moire)\n\n\nFacilitÃ©\nJava verbeux\nPython, Scala, SQL\n\n\nTraitement\nBatch only\nBatch + Streaming\n\n\nItÃ©rations\nLent (relit le disque)\nRapide (cache en RAM)\n\n\nÃ‰cosystÃ¨me\nHive, Pig, etc.\nSpark SQL, MLlib, GraphX\n\n\nStockage\nHDFS\nHDFS, S3, Cassandra, etc.\n\n\n\n\n\n\nğŸ—ï¸ Architecture Spark\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      APACHE SPARK                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚ Spark SQL â”‚ â”‚ Streaming â”‚ â”‚   MLlib   â”‚ â”‚  GraphX   â”‚  â”‚\nâ”‚   â”‚  (SQL)    â”‚ â”‚ (temps    â”‚ â”‚   (ML)    â”‚ â”‚ (graphes) â”‚  â”‚\nâ”‚   â”‚           â”‚ â”‚   rÃ©el)   â”‚ â”‚           â”‚ â”‚           â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚         â”‚             â”‚             â”‚             â”‚        â”‚\nâ”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚                       â”‚ Spark Core  â”‚                      â”‚\nâ”‚                       â”‚   (RDD)     â”‚                      â”‚\nâ”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚         â–¼                    â–¼                    â–¼        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚   YARN   â”‚        â”‚   Mesos  â”‚        â”‚Standaloneâ”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Spark peut fonctionner sur YARN (cluster Hadoop existant) ou en mode standalone.\n\n\n\n\nğŸ”® Concepts Spark Ã  venir\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nRDD\nResilient Distributed Dataset â€” collection distribuÃ©e\n\n\nDataFrame\nComme un tableau avec colonnes (similaire Ã  Pandas)\n\n\nTransformation\nOpÃ©ration lazy (map, filter, groupBy)\n\n\nAction\nDÃ©clenche le calcul (collect, count, show)\n\n\nLazy Evaluation\nRien ne sâ€™exÃ©cute tant quâ€™une action nâ€™est pas appelÃ©e\n\n\nPartition\nMorceau de donnÃ©es traitÃ© par un worker\n\n\n\n\n\n\nğŸ¯ Ce que tu vas apprendre avec PySpark\n\nCrÃ©er et manipuler des DataFrames distribuÃ©s\nÃ‰crire des transformations SQL-like\nLire/Ã©crire des fichiers (CSV, Parquet, JSON)\nOptimiser les performances\nConstruire des pipelines de donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ¤” Pourquoi NoSQL ?",
    "text": "ğŸ¤” Pourquoi NoSQL ?\n\nLes limites des bases relationnelles face au Big Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PROBLÃˆMES DES BASES SQL CLASSIQUES                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  âŒ SchÃ©ma rigide â†’ Difficile de changer la structure           â”‚\nâ”‚  âŒ Scale-up only â†’ Un seul serveur (coÃ»teux)                   â”‚\nâ”‚  âŒ Jointures â†’ Lentes sur des milliards de lignes              â”‚\nâ”‚  âŒ ACID strict â†’ Latence Ã©levÃ©e                                â”‚\nâ”‚  âŒ DonnÃ©es variÃ©es â†’ JSON, graphes mal gÃ©rÃ©s                   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLa solution NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    AVANTAGES NoSQL                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  âœ… SchÃ©ma flexible â†’ S'adapte aux donnÃ©es                      â”‚\nâ”‚  âœ… Scale-out natif â†’ Ajout de serveurs facile                  â”‚\nâ”‚  âœ… Pas de jointures â†’ DonnÃ©es dÃ©normalisÃ©es, rapides           â”‚\nâ”‚  âœ… Haute disponibilitÃ© â†’ TolÃ©rance aux pannes                  â”‚\nâ”‚  âœ… ModÃ¨les variÃ©s â†’ Document, clÃ©-valeur, graphe...            â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ NoSQL = â€œNot Only SQLâ€ (pas seulement SQL), pas â€œNo SQLâ€ !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“Š Les 4 types de bases NoSQL",
    "text": "ğŸ“Š Les 4 types de bases NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         TYPES DE BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     ğŸ“„ DOCUMENT     â”‚   ğŸ”‘ CLÃ‰-VALEUR    â”‚    ğŸ“Š COLONNES      â”‚  ğŸ”— GRAPHE â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”‚  {                  â”‚   key1 â†’ value1     â”‚  Row1: col1, col2   â”‚   (A)â”€â”€â†’(B)â”‚\nâ”‚    \"nom\": \"Alice\", â”‚   key2 â†’ value2     â”‚  Row2: col1, col3   â”‚    â”‚       â”‚\nâ”‚    \"age\": 30        â”‚   key3 â†’ value3     â”‚  Row3: col2, col4   â”‚    â†“       â”‚\nâ”‚  }                  â”‚                     â”‚                     â”‚   (C)     â”‚\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  MongoDB            â”‚  Redis              â”‚  Cassandra          â”‚ Neo4j     â”‚\nâ”‚  Couchbase          â”‚  Memcached          â”‚  HBase              â”‚ Amazon    â”‚\nâ”‚  Firestore          â”‚  DynamoDB           â”‚  ScyllaDB           â”‚ Neptune   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“„ 1. Bases Document (MongoDB, Couchbase)\nStockent des documents JSON/BSON avec un schÃ©ma flexible.\n// Document MongoDB\n{\n  \"_id\": \"user123\",\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@example.com\",\n  \"adresses\": [\n    {\"type\": \"domicile\", \"ville\": \"Paris\"},\n    {\"type\": \"travail\", \"ville\": \"Lyon\"}\n  ],\n  \"commandes\": [\n    {\"id\": \"cmd001\", \"total\": 150.00},\n    {\"id\": \"cmd002\", \"total\": 89.99}\n  ]\n}\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nSchÃ©ma flexible\nPas de jointures natives\n\n\nDocuments imbriquÃ©s\nDonnÃ©es dupliquÃ©es\n\n\nRequÃªtes riches\nTransactions limitÃ©es\n\n\n\nCas dâ€™usage : Catalogues produits, profils utilisateurs, CMS, logs\n\n\n\nğŸ”‘ 2. Bases ClÃ©-Valeur (Redis, Memcached, DynamoDB)\nStockage ultra-simple : une clÃ© â†’ une valeur.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      CLÃ‰        â”‚          VALEUR            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  user:123       â”‚  {\"nom\": \"Alice\", ...}    â”‚\nâ”‚  session:abc    â”‚  {\"user_id\": 123, ...}    â”‚\nâ”‚  cache:page:42  â”‚  \"&lt;html&gt;...&lt;/html&gt;\"       â”‚\nâ”‚  counter:views  â”‚  1583920                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nUltra-rapide (RAM)\nRequÃªtes limitÃ©es\n\n\nSimple\nPas de structure\n\n\nScalable\nRecherche par clÃ© uniquement\n\n\n\nCas dâ€™usage : Cache, sessions, compteurs, files dâ€™attente\n\n\n\nğŸ“Š 3. Bases Colonnes (Cassandra, HBase, ScyllaDB)\nOptimisÃ©es pour les Ã©critures massives et les requÃªtes analytiques.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Row Key: user123                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  nom:Alice   â”‚  age:30      â”‚  ville:Paris â”‚  ...    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Row Key: user456                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  nom:Bob     â”‚  email:...   â”‚  (pas de ville)          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nÃ‰critures trÃ¨s rapides\nModÃ¨le complexe\n\n\nScalabilitÃ© linÃ©aire\nRequÃªtes limitÃ©es\n\n\nTolÃ©rance aux pannes\nPas de jointures\n\n\n\nCas dâ€™usage : IoT, time-series, logs, analytics\n\n\n\nğŸ”— 4. Bases Graphe (Neo4j, Amazon Neptune, ArangoDB)\nOptimisÃ©es pour les relations complexes entre entitÃ©s.\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  Alice  â”‚\n         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n              â”‚ KNOWS\n              â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       WORKS_AT       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚   Bob   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  TechCorp   â”‚\n         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              â”‚ FOLLOWS\n              â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Charlie â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nRelations naturelles\nScalabilitÃ© complexe\n\n\nRequÃªtes de traversÃ©e rapides\nCas dâ€™usage spÃ©cifique\n\n\nFlexibilitÃ©\nMoins mature\n\n\n\nCas dâ€™usage : RÃ©seaux sociaux, recommandations, fraude, connaissances",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ”º Le thÃ©orÃ¨me CAP",
    "text": "ğŸ”º Le thÃ©orÃ¨me CAP\nLe thÃ©orÃ¨me CAP (Eric Brewer, 2000) est fondamental pour comprendre les choix de design des bases distribuÃ©es.\n                         C\n                    Consistency\n                    (CohÃ©rence)\n                        /\\\n                       /  \\\n                      /    \\\n                     /  CA  \\\n                    /________\\\n                   /\\        /\\\n                  /  \\      /  \\\n                 / CP \\    / AP \\\n                /______\\  /______\\\n               A                    P\n          Availability          Partition\n         (DisponibilitÃ©)        Tolerance\n                              (TolÃ©rance aux\n                                partitions)\n\nğŸ“– Les 3 propriÃ©tÃ©s\n\n\n\n\n\n\n\n\nPropriÃ©tÃ©\nSignification\nExemple\n\n\n\n\nConsistency\nTous les nÅ“uds voient les mÃªmes donnÃ©es\nLecture aprÃ¨s Ã©criture identique\n\n\nAvailability\nLe systÃ¨me rÃ©pond toujours\nPas de timeout\n\n\nPartition Tolerance\nFonctionne malgrÃ© des pannes rÃ©seau\nServeurs isolÃ©s continuent\n\n\n\n\n\nâš ï¸ Le thÃ©orÃ¨me dit :\n\nEn cas de partition rÃ©seau, tu dois choisir entre CohÃ©rence et DisponibilitÃ©.\n\nTu ne peux avoir que 2 sur 3 !\n\n\n\nğŸ—‚ï¸ Classification des bases selon CAP\n\n\n\n\n\n\n\n\n\nType\nChoix CAP\nBases\nComportement\n\n\n\n\nCP\nCohÃ©rence + Partition\nMongoDB, HBase, Redis\nPeut refuser des requÃªtes si partition\n\n\nAP\nDisponibilitÃ© + Partition\nCassandra, DynamoDB, CouchDB\nRÃ©pond toujours, cohÃ©rence Ã©ventuelle\n\n\nCA\nCohÃ©rence + DisponibilitÃ©\nPostgreSQL, MySQL (single node)\nPas de tolÃ©rance aux partitions\n\n\n\n\n\n\nğŸ’¡ En pratique : CohÃ©rence Ã©ventuelle (Eventual Consistency)\nLa plupart des bases NoSQL utilisent la cohÃ©rence Ã©ventuelle :\nTemps T0: Ã‰criture sur NÅ“ud A\n          [A: nouvelle valeur] [B: ancienne] [C: ancienne]\n          \nTemps T1: RÃ©plication en cours\n          [A: nouvelle valeur] [B: nouvelle] [C: ancienne]\n          \nTemps T2: CohÃ©rence atteinte\n          [A: nouvelle valeur] [B: nouvelle] [C: nouvelle]\n\nğŸ’¡ Acceptable pour : likes, vues, logs. Pas pour : transactions bancaires !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "âš–ï¸ SQL vs NoSQL â€” Quand utiliser quoi ?",
    "text": "âš–ï¸ SQL vs NoSQL â€” Quand utiliser quoi ?\n\nğŸ“Š Tableau comparatif\n\n\n\nCritÃ¨re\nSQL (Relationnel)\nNoSQL\n\n\n\n\nSchÃ©ma\nFixe, dÃ©fini Ã  lâ€™avance\nFlexible, dynamique\n\n\nDonnÃ©es\nStructurÃ©es, normalisÃ©es\nSemi/non-structurÃ©es\n\n\nRelations\nJointures natives\nDonnÃ©es embarquÃ©es/dÃ©normalisÃ©es\n\n\nTransactions\nACID complet\nBASE (Ã©ventuel)\n\n\nScalabilitÃ©\nVerticale (scale-up)\nHorizontale (scale-out)\n\n\nRequÃªtes\nSQL standardisÃ©\nLangage spÃ©cifique\n\n\nMaturitÃ©\n40+ ans\n15+ ans\n\n\n\n\n\n\nâœ… Utilise SQL quandâ€¦\n\n\n\nSituation\nExemple\n\n\n\n\nDonnÃ©es trÃ¨s structurÃ©es\nComptabilitÃ©, RH\n\n\nRelations complexes\nERP, CRM\n\n\nTransactions critiques\nBanque, e-commerce\n\n\nRequÃªtes ad-hoc variÃ©es\nReporting, BI\n\n\nIntÃ©gritÃ© des donnÃ©es critique\nSantÃ©, finance\n\n\n\n\n\n\nâœ… Utilise NoSQL quandâ€¦\n\n\n\nSituation\nType NoSQL\nExemple\n\n\n\n\nSchÃ©ma variable\nDocument\nCatalogues, profils\n\n\nCache haute performance\nClÃ©-valeur\nSessions, cache\n\n\nÃ‰critures massives\nColonnes\nIoT, logs\n\n\nRelations complexes\nGraphe\nRÃ©seaux sociaux\n\n\nScale-out nÃ©cessaire\nTous\nBig Data\n\n\nDÃ©veloppement agile\nDocument\nStartups, MVPs\n\n\n\n\n\n\nğŸ¯ En rÃ©alitÃ© : on utilise les deux !\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  ARCHITECTURE POLYGLOTTE                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   [Application Web]                                             â”‚\nâ”‚         â”‚                                                       â”‚\nâ”‚         â”œâ”€â”€â–º PostgreSQL (utilisateurs, commandes, paiements)   â”‚\nâ”‚         â”‚                                                       â”‚\nâ”‚         â”œâ”€â”€â–º MongoDB (catalogues produits, reviews)            â”‚\nâ”‚         â”‚                                                       â”‚\nâ”‚         â”œâ”€â”€â–º Redis (sessions, cache, rate limiting)            â”‚\nâ”‚         â”‚                                                       â”‚\nâ”‚         â”œâ”€â”€â–º Elasticsearch (recherche full-text)               â”‚\nâ”‚         â”‚                                                       â”‚\nâ”‚         â””â”€â”€â–º Neo4j (recommandations, graphe social)            â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Polyglot Persistence : Utiliser la bonne base pour le bon cas dâ€™usage !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#bases-nosql-populaires",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#bases-nosql-populaires",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ—‚ï¸ Bases NoSQL populaires",
    "text": "ğŸ—‚ï¸ Bases NoSQL populaires\n\nğŸ“„ MongoDB â€” La plus populaire (Document)\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nType\nBase de documents JSON (BSON)\n\n\nLangage de requÃªte\nMQL (MongoDB Query Language)\n\n\nScalabilitÃ©\nSharding natif\n\n\nTransactions\nACID multi-documents (depuis v4.0)\n\n\nCloud\nMongoDB Atlas\n\n\n\n// Exemple de requÃªte MongoDB\ndb.users.find({\n  age: { $gte: 25 },\n  ville: \"Paris\"\n}).sort({ nom: 1 })\n\nğŸ’¡ Tu vas apprendre MongoDB en dÃ©tail dans le module suivant !\n\n\n\n\nâš¡ Redis â€” Ultra-rapide (ClÃ©-valeur)\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nType\nIn-memory key-value store\n\n\nPerformance\n&lt; 1ms de latence\n\n\nStructures\nStrings, Lists, Sets, Hashes, Streams\n\n\nPersistance\nOptionnelle (RDB, AOF)\n\n\n\n# Exemple avec Python\nimport redis\nr = redis.Redis()\n\nr.set(\"user:123:name\", \"Alice\")\nr.get(\"user:123:name\")  # b'Alice'\n\nr.incr(\"page:views\")  # Compteur atomique\n\n\n\nğŸŒ Cassandra â€” Ã‰critures massives (Colonnes)\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nType\nWide-column store\n\n\nOrigine\nFacebook (inbox search)\n\n\nScalabilitÃ©\nLinÃ©aire, peer-to-peer\n\n\nDisponibilitÃ©\nPas de single point of failure\n\n\n\n-- CQL (Cassandra Query Language)\nCREATE TABLE events (\n    device_id UUID,\n    timestamp TIMESTAMP,\n    value DOUBLE,\n    PRIMARY KEY (device_id, timestamp)\n);\n\n\n\nğŸ” Elasticsearch â€” Recherche (Document + Search)\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nType\nMoteur de recherche distribuÃ©\n\n\nBase\nApache Lucene\n\n\nUsage\nFull-text search, logs, analytics\n\n\nStack\nELK (Elasticsearch, Logstash, Kibana)\n\n\n\n\nğŸ’¡ Tu as vu Elasticsearch dans le module 10 !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#quiz-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#quiz-nosql",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ§  Quiz NoSQL",
    "text": "ğŸ§  Quiz NoSQL\n\nâ“ Q8. Que signifie NoSQL ?\n\nNo SQL (pas de SQL)\n\nNot Only SQL\n\nNew SQL\n\nNon-Structured Query Language\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” NoSQL = â€œNot Only SQLâ€ (pas seulement SQL). Ces bases peuvent parfois supporter du SQL !\n\n\n\n\nâ“ Q9. Quel type de base NoSQL est MongoDB ?\n\nClÃ©-valeur\n\nColonnes\n\nDocument\n\nGraphe\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” MongoDB est une base de documents (JSON/BSON).\n\n\n\n\nâ“ Q10. Selon le thÃ©orÃ¨me CAP, que sacrifie une base AP ?\n\nDisponibilitÃ©\n\nCohÃ©rence\n\nTolÃ©rance aux partitions\n\nPerformance\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Une base AP (Available + Partition tolerant) sacrifie la CohÃ©rence immÃ©diate. Elle utilise la â€œcohÃ©rence Ã©ventuelleâ€.\n\n\n\n\nâ“ Q11. Pour quel cas dâ€™usage Redis est-il le plus adaptÃ© ?\n\nStockage de documents JSON\n\nCache et sessions\n\nAnalyse de graphes sociaux\n\nStockage de vidÃ©os\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Redis est une base in-memory clÃ©-valeur, idÃ©ale pour le cache et les sessions.\n\n\n\n\nâ“ Q12. Quelle base NoSQL choisir pour des relations complexes (rÃ©seau social) ?\n\nMongoDB\n\nRedis\n\nCassandra\n\nNeo4j\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… d â€” Neo4j est une base de graphes, optimisÃ©e pour les relations complexes.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nLes 5V du Big Data\n\n\n\nV\nDÃ©fi\n\n\n\n\nVolume\nStocker et traiter des To/Po\n\n\nVelocity\nTraiter en temps rÃ©el\n\n\nVariety\nGÃ©rer tous les formats\n\n\nVeracity\nAssurer la qualitÃ©\n\n\nValue\nExtraire des insights\n\n\n\n\n\nTraitement distribuÃ©\n\n\n\nConcept\nRetenir\n\n\n\n\nScale-out\nCluster de machines ordinaires\n\n\nData locality\nAmener le code aux donnÃ©es\n\n\nFault tolerance\nRÃ©plication, recalcul\n\n\n\n\n\nHadoop vs Spark\n\n\n\n\nHadoop MR\nSpark\n\n\n\n\nStockage intermÃ©diaire\nDisque\nMÃ©moire\n\n\nVitesse\nLent\n100x plus rapide\n\n\nLangages\nJava\nPython, Scala, SQL\n\n\n\n\n\nSQL vs NoSQL\n\n\n\nType\nQuand utiliser\n\n\n\n\nSQL\nDonnÃ©es structurÃ©es, transactions, intÃ©gritÃ©\n\n\nNoSQL Document\nSchÃ©ma flexible, JSON (MongoDB)\n\n\nNoSQL ClÃ©-valeur\nCache, sessions (Redis)\n\n\nNoSQL Colonnes\nÃ‰critures massives, IoT (Cassandra)\n\n\nNoSQL Graphe\nRelations complexes (Neo4j)\n\n\n\n\n\nThÃ©orÃ¨me CAP\n\n\n\nChoix\nExemples\n\n\n\n\nCP\nMongoDB, HBase\n\n\nAP\nCassandra, DynamoDB\n\n\nCA\nPostgreSQL (single node)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quels sont les 3V originaux du Big Data ?\n\nVitesse, Valeur, VÃ©ritÃ©\n\nVolume, Velocity, Variety\n\nVolume, Validation, Visualisation\n\nVÃ©locitÃ©, VÃ©racitÃ©, Valorisation\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les 3V originaux (Doug Laney, 2001) sont Volume, Velocity, Variety.\n\n\n\n\nâ“ Q2. Que signifie â€œScale-outâ€ ?\n\nAugmenter la RAM dâ€™un serveur\n\nAjouter des machines au cluster\n\nCompresser les donnÃ©es\n\nRÃ©duire la taille du cluster\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Scale-out (horizontal) = ajouter des machines. Scale-up (vertical) = augmenter les ressources dâ€™une machine.\n\n\n\n\nâ“ Q3. Quel est le principe de â€œData Localityâ€ ?\n\nStocker les donnÃ©es localement sur son PC\n\nDÃ©placer les donnÃ©es vers le serveur de calcul\n\nDÃ©placer le code vers les donnÃ©es\n\nCompresser les donnÃ©es pour les transfÃ©rer\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” On envoie le code (petit) vers les donnÃ©es (grosses), pas lâ€™inverse.\n\n\n\n\nâ“ Q4. Quels sont les 3 composants principaux de Hadoop ?\n\nHDFS, Spark, Kafka\n\nHDFS, MapReduce, YARN\n\nHive, Pig, HBase\n\nMap, Shuffle, Reduce\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” HDFS (stockage), MapReduce (calcul), YARN (ressources).\n\n\n\n\nâ“ Q5. Pourquoi Spark est plus rapide que MapReduce ?\n\nIl utilise un meilleur algorithme\n\nIl stocke les donnÃ©es intermÃ©diaires en mÃ©moire\n\nIl compresse les donnÃ©es\n\nIl utilise plus de serveurs\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Spark garde les donnÃ©es en mÃ©moire (RAM) au lieu dâ€™Ã©crire sur disque entre chaque Ã©tape.\n\n\n\n\nâ“ Q6. Dans MapReduce, que fait lâ€™Ã©tape â€œShuffleâ€ ?\n\nTrier les donnÃ©es\n\nRegrouper les donnÃ©es par clÃ©\n\nSupprimer les doublons\n\nCompresser les rÃ©sultats\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Le Shuffle regroupe toutes les valeurs ayant la mÃªme clÃ© pour les envoyer au mÃªme Reducer.\n\n\n\n\nâ“ Q7. Combien de copies HDFS fait-il par dÃ©faut ?\n\n1\n\n2\n\n3\n\n5\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” HDFS rÃ©plique chaque bloc 3 fois par dÃ©faut pour la tolÃ©rance aux pannes.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nBig Data & Hadoop\n\nHadoop Documentation\nSpark Documentation\nThe Google File System (paper)\nMapReduce: Simplified Data Processing (paper)\n\n\n\nNoSQL\n\nMongoDB University â€” Cours gratuits\nRedis Documentation\nCassandra Documentation\nCAP Theorem Explained",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "title": "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu as maintenant les bases thÃ©oriques du Big Data, du traitement distribuÃ© et du NoSQL. Place Ã  la pratique avec MongoDB !\nğŸ‘‰ Module suivant : 09_mongodb_for_data_engineers.ipynb â€” MongoDB pour les Data Engineers\nTu apprendras Ã  : - Installer et utiliser MongoDB - CrÃ©er des collections et documents - Ã‰crire des requÃªtes MQL (MongoDB Query Language) - Utiliser PyMongo depuis Python - ModÃ©liser des donnÃ©es pour MongoDB\n\nğŸ‰ FÃ©licitations ! Tu comprends maintenant le Big Data, les systÃ¨mes distribuÃ©s et le NoSQL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ Introduction au Big Data & SystÃ¨mes DistribuÃ©s"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "",
    "text": "Ce module couvre les fondamentaux et concepts avancÃ©s de SQL, avec une pratique directe via Python + DuckDB.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "âš™ï¸ Approche de ce cours",
    "text": "âš™ï¸ Approche de ce cours\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ğŸ¯ FOCUS DU COURS                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Ce cours utilise Python + DuckDB pour apprendre SQL          â”‚\nâ”‚                                                                 â”‚\nâ”‚   âœ… SQL standard (compatible PostgreSQL, MySQL, etc.)          â”‚\nâ”‚   âœ… ExÃ©cution directe dans Jupyter (pas de serveur)            â”‚\nâ”‚   âœ… IntÃ©gration native avec Pandas                             â”‚\nâ”‚   âœ… CompÃ©tences transfÃ©rables Ã  tout SGBD                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Le SQL appris ici fonctionne sur PostgreSQL, MySQL, SQLite, BigQuery, Snowflake, etc. avec des variations mineures de syntaxe.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nComprendre les concepts de tables, colonnes, clÃ©s\n\n\nâœ… Requis\nBases de Python (module 04 et 05)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Ã‰crire des requÃªtes SELECT, WHERE, ORDER BY\nâœ… Utiliser les fonctions dâ€™agrÃ©gation (COUNT, SUM, AVG)\nâœ… MaÃ®triser GROUP BY et HAVING\nâœ… Faire des jointures (JOIN, LEFT JOIN)\nâœ… Utiliser les fonctions de date (DATE_TRUNC, EXTRACT)\nâœ… Utiliser CASE, les CTEs et les Window Functions\nâœ… ExÃ©cuter du SQL depuis Python avec DuckDB\n\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter les cellules de code directement !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§  Câ€™est quoi SQL ?",
    "text": "ğŸ§  Câ€™est quoi SQL ?\nSQL (Structured Query Language) est un langage de requÃªtes pour interagir avec des bases de donnÃ©es relationnelles.\n\n\n\nAction\nCommande SQL\n\n\n\n\nğŸ” Rechercher\nSELECT\n\n\nâœï¸ InsÃ©rer\nINSERT\n\n\nğŸ”„ Modifier\nUPDATE\n\n\nğŸ—‘ï¸ Supprimer\nDELETE\n\n\nğŸ“Š AgrÃ©ger\nCOUNT, SUM, AVG\n\n\nğŸ”— Joindre\nJOIN",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§° Pourquoi câ€™est essentiel pour un Data Engineer ?",
    "text": "ğŸ§° Pourquoi câ€™est essentiel pour un Data Engineer ?\n\nInterroger les data warehouses (BigQuery, Snowflake, Redshiftâ€¦)\nExtraire / filtrer les donnÃ©es pour les pipelines\nVÃ©rifier la qualitÃ© des donnÃ©es\nCrÃ©er des vues et agrÃ©gats pour les dashboards",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸŒ Outils en ligne pour tester du SQL",
    "text": "ğŸŒ Outils en ligne pour tester du SQL\nAvant de plonger dans Python + DuckDB, sache quâ€™il existe de nombreux outils en ligne gratuits pour tester des requÃªtes SQL sur diffÃ©rents SGBD. Câ€™est utile pour :\n\nğŸ§ª Tester rapidement une requÃªte\nğŸ”„ VÃ©rifier la compatibilitÃ© entre SGBD\nğŸ“š Sâ€™entraÃ®ner sans rien installer\nğŸ¤ Partager des exemples avec des collÃ¨gues\n\n\nğŸ› ï¸ Outils recommandÃ©s\n\n\n\nOutil\nURL\nSGBD supportÃ©s\nPoints forts\n\n\n\n\nDB Fiddle\ndb-fiddle.com\nPostgreSQL, MySQL, SQLite\nInterface claire, partage facile\n\n\nSQL Fiddle\nsqlfiddle.com\nMySQL, PostgreSQL, Oracle, SQL Server\nLe classique, multi-SGBD\n\n\nSQLite Online\nsqliteonline.com\nSQLite, PostgreSQL, MySQL\nTrÃ¨s simple, import CSV\n\n\nProgramiz SQL\nprogramiz.com/sql/online-compiler\nSQLite\nIdÃ©al dÃ©butants, tutoriels intÃ©grÃ©s\n\n\nOneCompiler\nonecompiler.com/mysql\nMySQL, PostgreSQL, SQL Server\nRapide, moderne\n\n\nReplit\nreplit.com\nSQLite, PostgreSQL\nEnvironnement complet, collaboratif\n\n\n\n\n\nğŸ’¡ Exemple avec DB Fiddle\n\nAller sur db-fiddle.com\nChoisir le SGBD (ex: PostgreSQL 15)\nDans le panneau gauche, crÃ©er le schÃ©ma :\n\nCREATE TABLE employes (\n    id INT PRIMARY KEY,\n    nom VARCHAR(50),\n    salaire INT\n);\n\nINSERT INTO employes VALUES \n(1, 'Alice', 50000),\n(2, 'Bob', 60000),\n(3, 'Charlie', 55000);\n\nDans le panneau droit, Ã©crire ta requÃªte :\n\nSELECT nom, salaire \nFROM employes \nWHERE salaire &gt; 52000;\n\nCliquer sur Run et voir le rÃ©sultat !\n\n\n\nâš ï¸ DiffÃ©rences entre SGBD\nLe SQL est standardisÃ©, mais chaque SGBD a ses particularitÃ©s :\n\n\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nPostgreSQL\nMySQL\nSQL Server\nSQLite\n\n\n\n\nConcatÃ©nation\n\\|\\| ou CONCAT()\nCONCAT()\n+ ou CONCAT()\n\\|\\|\n\n\nLimite rÃ©sultats\nLIMIT 10\nLIMIT 10\nTOP 10\nLIMIT 10\n\n\nAuto-increment\nSERIAL\nAUTO_INCREMENT\nIDENTITY\nAUTOINCREMENT\n\n\nDate actuelle\nCURRENT_DATE\nCURDATE()\nGETDATE()\nDATE('now')\n\n\nBoolÃ©ens\nTRUE/FALSE\n1/0\n1/0\n1/0\n\n\n\n\nğŸ’¡ DuckDB utilise une syntaxe proche de PostgreSQL, ce qui est idÃ©al car PostgreSQL est le standard de facto en Data Engineering.\n\n\n\n\nğŸ¯ Pourquoi on utilise Python + DuckDB dans ce cours ?\n\n\n\nAvantage\nExplication\n\n\n\n\nTout-en-un\nSQL + Python dans le mÃªme notebook\n\n\nPas dâ€™installation serveur\nDuckDB tourne en mÃ©moire\n\n\nIntÃ©gration Pandas\nRÃ©sultats directement en DataFrame\n\n\nFichiers directs\nLire CSV/Parquet/JSON sans import\n\n\nSyntaxe standard\nCompatible PostgreSQL â†’ transfÃ©rable\n\n\nPerformance\nOptimisÃ© pour lâ€™analytics (OLAP)\n\n\n\n# Exemple : SQL â†’ DataFrame en 1 ligne\ndf = con.execute(\"SELECT * FROM clients WHERE pays = 'France'\").df()\n\nğŸ“ Les compÃ©tences SQL apprises ici sont directement applicables Ã  PostgreSQL, BigQuery, Snowflake, Redshift, et tout autre SGBD en production.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¦† Python + DuckDB â€” SQL dans ton notebook !",
    "text": "ğŸ¦† Python + DuckDB â€” SQL dans ton notebook !\n\nQuâ€™est-ce que DuckDB ?\nDuckDB est une base de donnÃ©es analytique embarquÃ©e (comme SQLite, mais optimisÃ©e pour lâ€™analytics).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    POURQUOI DUCKDB ?                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  âœ… ZÃ©ro installation serveur (fonctionne en mÃ©moire)       â”‚\nâ”‚  âœ… Syntaxe SQL standard (PostgreSQL-like)                  â”‚\nâ”‚  âœ… IntÃ©gration native avec Pandas                         â”‚\nâ”‚  âœ… TrÃ¨s rapide pour l'analytics (colonnar storage)        â”‚\nâ”‚  âœ… Lit directement CSV, Parquet, JSON                      â”‚\nâ”‚  âœ… Parfait pour apprendre SQL dans un notebook             â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ“Š DuckDB vs autres bases\n\n\n\n\n\n\n\n\n\nCritÃ¨re\nSQLite\nDuckDB\nPostgreSQL\n\n\n\n\nInstallation\nEmbarquÃ©\nEmbarquÃ©\nServeur\n\n\nOptimisÃ© pour\nOLTP (transactions)\nOLAP (analytics)\nLes deux\n\n\nIntÃ©gration Pandas\nâš ï¸ Via SQLAlchemy\nâœ… Native\nâš ï¸ Via psycopg2\n\n\nFichiers CSV/Parquet\nâŒ Non\nâœ… Direct\nâŒ Non\n\n\nUsage\nMobile, embarquÃ©\nData Science, ETL\nProduction\n\n\n\n\n\nCode\n# ğŸ“¦ Installation de DuckDB\n!pip install duckdb --quiet\n\nprint(\"âœ… DuckDB installÃ© !\")\n\n\n\n\nCode\nimport duckdb\n\n# CrÃ©er une connexion en mÃ©moire\ncon = duckdb.connect(database=':memory:')\n\nprint(\"âœ… Connexion DuckDB crÃ©Ã©e !\")\nprint(f\"Version : {duckdb.__version__}\")\n\n\n\n\nCode\n# ğŸ“Š CrÃ©ation des tables de dÃ©monstration\n\n# Table clients\ncon.execute(\"\"\"\nCREATE TABLE clients (\n    id_client INTEGER PRIMARY KEY,\n    nom VARCHAR(50),\n    email VARCHAR(100),\n    pays VARCHAR(50),\n    date_inscription DATE\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO clients VALUES\n    (1, 'Alice', 'alice@mail.com', 'France', '2023-01-15'),\n    (2, 'Bob', 'bob@mail.com', 'France', '2023-02-20'),\n    (3, 'Charlie', 'charlie@mail.com', 'Allemagne', '2023-03-10'),\n    (4, 'Diana', 'diana@mail.com', 'Belgique', '2023-04-05'),\n    (5, 'Eve', 'eve@mail.com', 'France', '2023-05-12');\n\"\"\")\n\n# Table commandes\ncon.execute(\"\"\"\nCREATE TABLE commandes (\n    id_commande INTEGER PRIMARY KEY,\n    id_client INTEGER,\n    produit VARCHAR(50),\n    montant DECIMAL(10,2),\n    date_commande DATE,\n    FOREIGN KEY (id_client) REFERENCES clients(id_client)\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO commandes VALUES\n    (1, 1, 'Clavier', 50.00, '2023-07-12'),\n    (2, 1, 'Souris', 25.00, '2023-07-15'),\n    (3, 2, 'Ã‰cran', 120.00, '2023-08-01'),\n    (4, 1, 'Webcam', 45.00, '2023-08-20'),\n    (5, 4, 'Casque', 80.00, '2023-09-05'),\n    (6, 2, 'Clavier', 55.00, '2023-09-15'),\n    (7, 5, 'Souris', 30.00, '2023-10-01'),\n    (8, 4, 'Ã‰cran', 150.00, '2023-10-20');\n\"\"\")\n\nprint(\"âœ… Tables crÃ©Ã©es : clients (5 lignes), commandes (8 lignes)\")\n\n\n\n\nCode\n# ğŸ‘€ VÃ©rifier les donnÃ©es\nprint(\"ğŸ“‹ Table clients :\")\nprint(con.execute(\"SELECT * FROM clients\").fetchdf())\n\nprint(\"\\nğŸ“‹ Table commandes :\")\nprint(con.execute(\"SELECT * FROM commandes\").fetchdf())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“‹ Cheatsheet SQL â€” Commandes essentielles",
    "text": "ğŸ“‹ Cheatsheet SQL â€” Commandes essentielles\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\nExemple\n\n\n\n\nLecture\nSELECT\nSÃ©lectionner des colonnes\nSELECT nom, email FROM clients\n\n\n\nSELECT *\nToutes les colonnes\nSELECT * FROM clients\n\n\n\nDISTINCT\nValeurs uniques\nSELECT DISTINCT pays FROM clients\n\n\nFiltrage\nWHERE\nFiltrer les lignes\nWHERE pays = 'France'\n\n\n\nAND / OR\nConditions multiples\nWHERE age &gt; 18 AND pays = 'France'\n\n\n\nIN\nListe de valeurs\nWHERE pays IN ('France', 'Belgique')\n\n\n\nBETWEEN\nPlage de valeurs\nWHERE montant BETWEEN 50 AND 100\n\n\n\nLIKE\nRecherche pattern\nWHERE nom LIKE 'A%'\n\n\n\nIS NULL\nValeurs nulles\nWHERE email IS NULL\n\n\nTri\nORDER BY\nTrier les rÃ©sultats\nORDER BY nom ASC\n\n\n\nLIMIT\nLimiter le nombre\nLIMIT 10\n\n\nAgrÃ©gation\nCOUNT()\nCompter\nSELECT COUNT(*) FROM clients\n\n\n\nSUM()\nSomme\nSELECT SUM(montant) FROM commandes\n\n\n\nAVG()\nMoyenne\nSELECT AVG(montant) FROM commandes\n\n\n\nMIN() / MAX()\nMin / Max\nSELECT MAX(montant) FROM commandes\n\n\nGroupement\nGROUP BY\nRegrouper\nGROUP BY pays\n\n\n\nHAVING\nFiltrer aprÃ¨s agrÃ©gation\nHAVING COUNT(*) &gt; 5\n\n\nJointures\nJOIN\nJointure interne\nJOIN commandes ON ...\n\n\n\nLEFT JOIN\nJointure gauche\nLEFT JOIN commandes ON ...",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ” 1. SELECT â€” SÃ©lectionner des donnÃ©es",
    "text": "ğŸ” 1. SELECT â€” SÃ©lectionner des donnÃ©es\n\nSyntaxe de base\nSELECT colonne1, colonne2\nFROM table\nWHERE condition\nORDER BY colonne;\n\n\nCode\n# ğŸ” SELECT * â€” Toutes les colonnes\nquery = \"SELECT * FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” SELECT colonnes spÃ©cifiques\nquery = \"SELECT nom, email, pays FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” WHERE â€” Filtrer les lignes\nquery = \"\"\"\nSELECT nom, email, pays\nFROM clients\nWHERE pays = 'France'\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” ORDER BY + LIMIT\nquery = \"\"\"\nSELECT nom, montant, date_commande\nFROM commandes\nJOIN clients ON clients.id_client = commandes.id_client\nORDER BY montant DESC\nLIMIT 3\n\"\"\"\nprint(\"ğŸ† Top 3 des commandes les plus chÃ¨res :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“Š 2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX",
    "text": "ğŸ“Š 2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX\n\n\nCode\n# ğŸ“Š COUNT â€” Compter les lignes\nquery = \"SELECT COUNT(*) AS nb_clients FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“Š SUM, AVG, MIN, MAX\nquery = \"\"\"\nSELECT \n    COUNT(*) AS nb_commandes,\n    SUM(montant) AS total,\n    AVG(montant) AS moyenne,\n    MIN(montant) AS min,\n    MAX(montant) AS max\nFROM commandes\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“¦ 3. GROUP BY â€” Regrouper les donnÃ©es",
    "text": "ğŸ“¦ 3. GROUP BY â€” Regrouper les donnÃ©es\n\n\nCode\n# ğŸ“¦ GROUP BY â€” Nombre de clients par pays\nquery = \"\"\"\nSELECT pays, COUNT(*) AS nb_clients\nFROM clients\nGROUP BY pays\nORDER BY nb_clients DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“¦ Total des achats par client\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“¦ HAVING â€” Filtrer aprÃ¨s agrÃ©gation\n# Clients ayant dÃ©pensÃ© plus de 100â‚¬\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nHAVING SUM(cmd.montant) &gt; 100\nORDER BY total_achats DESC\n\"\"\"\nprint(\"ğŸ† Clients ayant dÃ©pensÃ© plus de 100â‚¬ :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ”— 4. JOIN â€” Combiner les tables",
    "text": "ğŸ”— 4. JOIN â€” Combiner les tables\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ INNER JOIN (JOIN)         â”‚ Seulement les correspondances      â”‚\nâ”‚ A âˆ© B                      â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ LEFT JOIN                 â”‚ Tout A + correspondances B         â”‚\nâ”‚ A + (A âˆ© B)               â”‚ (NULL si pas de correspondance)    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ RIGHT JOIN                â”‚ Tout B + correspondances A         â”‚\nâ”‚ B + (A âˆ© B)               â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ FULL OUTER JOIN           â”‚ Tout A + Tout B                    â”‚\nâ”‚ A âˆª B                      â”‚                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCode\n# ğŸ”— INNER JOIN â€” Clients avec leurs commandes\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ”— LEFT JOIN â€” Tous les clients, mÃªme sans commande\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\nprint(\"ğŸ‘€ Remarque : Charlie n'a pas de commande (NULL)\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ” Trouver les clients sans commande\nquery = \"\"\"\nSELECT c.nom, c.email\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nWHERE cmd.id_commande IS NULL\n\"\"\"\nprint(\"ğŸ˜´ Clients sans aucune commande :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“… 5. Fonctions de date",
    "text": "ğŸ“… 5. Fonctions de date\n\n\n\n\n\n\n\n\nFonction\nDescription\nExemple\n\n\n\n\nCURRENT_DATE\nDate du jour\n2024-01-15\n\n\nEXTRACT()\nExtraire une partie\nEXTRACT(YEAR FROM date) â†’ 2024\n\n\nDATE_TRUNC()\nTronquer Ã  une pÃ©riode\nDATE_TRUNC('month', date) â†’ 2024-01-01\n\n\nDATE_DIFF()\nDiffÃ©rence entre dates\nDATE_DIFF('day', date1, date2)\n\n\n\n\n\nCode\n# ğŸ“… Ventes par mois\nquery = \"\"\"\nSELECT \n    DATE_TRUNC('month', date_commande) AS mois,\n    SUM(montant) AS total_ventes,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois\n\"\"\"\nprint(\"ğŸ“Š Ventes mensuelles :\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“… Commandes par jour de la semaine\nquery = \"\"\"\nSELECT \n    EXTRACT(DOW FROM date_commande) AS jour_num,\n    CASE EXTRACT(DOW FROM date_commande)\n        WHEN 0 THEN 'Dimanche'\n        WHEN 1 THEN 'Lundi'\n        WHEN 2 THEN 'Mardi'\n        WHEN 3 THEN 'Mercredi'\n        WHEN 4 THEN 'Jeudi'\n        WHEN 5 THEN 'Vendredi'\n        WHEN 6 THEN 'Samedi'\n    END AS jour,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY EXTRACT(DOW FROM date_commande)\nORDER BY jour_num\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ”€ 6. CASE â€” Conditions dans les requÃªtes",
    "text": "ğŸ”€ 6. CASE â€” Conditions dans les requÃªtes\n\n\nCode\n# ğŸ”€ CASE â€” CatÃ©goriser les clients\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    CASE\n        WHEN SUM(cmd.montant) &gt;= 150 THEN 'ğŸ¥‡ Premium'\n        WHEN SUM(cmd.montant) &gt;= 100 THEN 'ğŸ¥ˆ Standard'\n        ELSE 'ğŸ¥‰ Basique'\n    END AS categorie\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“¦ 7. CTE (Common Table Expression) â€” RequÃªtes lisibles",
    "text": "ğŸ“¦ 7. CTE (Common Table Expression) â€” RequÃªtes lisibles\nLes CTEs permettent de crÃ©er des â€œtables temporairesâ€ pour clarifier les requÃªtes complexes.\nWITH nom_cte AS (\n    SELECT ...\n)\nSELECT * FROM nom_cte;\n\n\nCode\n# ğŸ“¦ CTE â€” Calcul intermÃ©diaire rÃ©utilisable\nquery = \"\"\"\nWITH total_par_client AS (\n    SELECT \n        c.id_client,\n        c.nom,\n        c.pays,\n        SUM(cmd.montant) AS total\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n    GROUP BY c.id_client, c.nom, c.pays\n)\nSELECT \n    nom,\n    pays,\n    total,\n    CASE WHEN total &gt; 100 THEN 'âœ… VIP' ELSE 'âŒ' END AS vip\nFROM total_par_client\nORDER BY total DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸªŸ 8. Window Functions â€” Calculs avancÃ©s",
    "text": "ğŸªŸ 8. Window Functions â€” Calculs avancÃ©s\nLes Window Functions permettent de faire des calculs sur un groupe de lignes sans les regrouper.\n\n\n\nFonction\nDescription\n\n\n\n\nROW_NUMBER()\nNumÃ©ro de ligne\n\n\nRANK()\nClassement (avec ex-aequo)\n\n\nDENSE_RANK()\nClassement sans saut\n\n\nLAG()\nValeur de la ligne prÃ©cÃ©dente\n\n\nLEAD()\nValeur de la ligne suivante\n\n\nSUM() OVER()\nSomme cumulative\n\n\n\n\n\nCode\n# ğŸ† RANK â€” Classement des clients par total d'achats\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    RANK() OVER (ORDER BY SUM(cmd.montant) DESC) AS rang\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“ˆ SUM OVER â€” Total cumulatif\nquery = \"\"\"\nSELECT \n    date_commande,\n    montant,\n    SUM(montant) OVER (ORDER BY date_commande) AS cumul\nFROM commandes\nORDER BY date_commande\n\"\"\"\nprint(\"ğŸ“ˆ Ã‰volution cumulative des ventes :\")\ncon.execute(query).fetchdf()\n\n\n\n\nCode\n# ğŸ“Š LAG â€” Comparer avec la pÃ©riode prÃ©cÃ©dente\nquery = \"\"\"\nWITH mensuel AS (\n    SELECT \n        DATE_TRUNC('month', date_commande) AS mois,\n        SUM(montant) AS total\n    FROM commandes\n    GROUP BY DATE_TRUNC('month', date_commande)\n)\nSELECT \n    mois,\n    total,\n    LAG(total) OVER (ORDER BY mois) AS total_precedent,\n    total - LAG(total) OVER (ORDER BY mois) AS evolution\nFROM mensuel\nORDER BY mois\n\"\"\"\nprint(\"ğŸ“Š Ã‰volution mensuelle :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¼ 9. IntÃ©gration avec Pandas",
    "text": "ğŸ¼ 9. IntÃ©gration avec Pandas\nDuckDB sâ€™intÃ¨gre parfaitement avec Pandas !\n\n\nCode\nimport pandas as pd\n\n# ğŸ¼ RÃ©cupÃ©rer le rÃ©sultat en DataFrame Pandas\nquery = \"\"\"\nSELECT c.nom, c.pays, SUM(cmd.montant) AS total\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom, c.pays\nORDER BY total DESC\n\"\"\"\n\ndf = con.execute(query).fetchdf()\nprint(type(df))  # C'est un DataFrame Pandas !\ndf\n\n\n\n\nCode\n# ğŸ¦† RequÃªter directement un DataFrame Pandas avec SQL !\ndf_exemple = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'A', 'B'],\n    'ventes': [100, 200, 150, 120, 180]\n})\n\n# DuckDB peut requÃªter le DataFrame directement\nresult = duckdb.query(\"\"\"\n    SELECT produit, SUM(ventes) AS total\n    FROM df_exemple\n    GROUP BY produit\n    ORDER BY total DESC\n\"\"\").fetchdf()\n\nprint(\"ğŸ”¥ SQL directement sur un DataFrame Pandas :\")\nresult\n\n\n\n\nCode\n# ğŸ“ DuckDB peut aussi lire directement des fichiers CSV/Parquet\n# Exemple (si tu as un fichier) :\n# duckdb.query(\"SELECT * FROM 'mon_fichier.csv' LIMIT 10\")\n# duckdb.query(\"SELECT * FROM 'data.parquet' WHERE date &gt; '2024-01-01'\")\n\nprint(\"ğŸ’¡ DuckDB peut lire directement :\")\nprint(\"   - CSV  : SELECT * FROM 'fichier.csv'\")\nprint(\"   - Parquet : SELECT * FROM 'fichier.parquet'\")\nprint(\"   - JSON : SELECT * FROM 'fichier.json'\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ¯ 10. Exercices pratiques",
    "text": "ğŸ¯ 10. Exercices pratiques\nEssaie de rÃ©soudre ces exercices dans les cellules de code ci-dessous !\n\n\nğŸ‹ï¸ Exercice 1 â€” Facile\nAfficher tous les clients de France, triÃ©s par nom.\n\n\nğŸ’¡ Solution\n\nSELECT * FROM clients WHERE pays = 'France' ORDER BY nom;\n\n\n\n\nğŸ‹ï¸ Exercice 2 â€” Facile\nCalculer le montant moyen des commandes.\n\n\nğŸ’¡ Solution\n\nSELECT AVG(montant) AS montant_moyen FROM commandes;\n\n\n\n\nğŸ‹ï¸ Exercice 3 â€” IntermÃ©diaire\nAfficher le total des achats par client, y compris ceux sans commande (afficher 0).\n\n\nğŸ’¡ Solution\n\nSELECT c.nom, COALESCE(SUM(cmd.montant), 0) AS total\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total DESC;\n\n\n\n\nğŸ‹ï¸ Exercice 4 â€” IntermÃ©diaire\nCalculer les ventes par mois avec DATE_TRUNC.\n\n\nğŸ’¡ Solution\n\nSELECT DATE_TRUNC('month', date_commande) AS mois, SUM(montant) AS total\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois;\n\n\n\n\nğŸ‹ï¸ Exercice 5 â€” AvancÃ©\nPour chaque client, afficher sa derniÃ¨re commande (produit et date). Indice : ROW_NUMBER()\n\n\nğŸ’¡ Solution\n\nWITH derniere AS (\n    SELECT c.nom, cmd.produit, cmd.date_commande,\n        ROW_NUMBER() OVER (PARTITION BY c.id_client ORDER BY cmd.date_commande DESC) AS rn\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n)\nSELECT nom, produit, date_commande FROM derniere WHERE rn = 1;\n\n\n\nCode\n# âœï¸ Espace pour tes exercices\nquery = \"\"\"\n-- Ã‰cris ta requÃªte ici\nSELECT * FROM clients LIMIT 5\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quelle commande affiche toutes les colonnes dâ€™une table clients ?\n\nSHOW * FROM clients;\n\nSELECT * FROM clients;\n\nLIST * FROM clients;\n\nDISPLAY * FROM clients;\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” SELECT * FROM clients; affiche toutes les colonnes.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre WHERE et HAVING ?\n\nAucune diffÃ©rence\n\nWHERE sâ€™utilise avant GROUP BY, HAVING aprÃ¨s\n\nHAVING est plus rapide\n\nWHERE ne peut filtrer que les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” WHERE filtre avant agrÃ©gation, HAVING filtre aprÃ¨s.\n\n\n\n\nâ“ Q3. Quel JOIN retourne UNIQUEMENT les lignes qui ont une correspondance ?\n\nLEFT JOIN\n\nRIGHT JOIN\n\nINNER JOIN\n\nFULL OUTER JOIN\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” INNER JOIN ne garde que les correspondances.\n\n\n\n\nâ“ Q4. Ã€ quoi sert une CTE (WITH) ?\n\nCrÃ©er une table permanente\n\nDÃ©finir une sous-requÃªte rÃ©utilisable\n\nSupprimer des donnÃ©es\n\nCrÃ©er un index\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les CTEs crÃ©ent des â€œtables temporairesâ€ pour clarifier les requÃªtes.\n\n\n\n\nâ“ Q5. Quelle Window Function permet de classer les lignes ?\n\nCOUNT()\n\nRANK()\n\nSUM()\n\nGROUP BY\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” RANK() OVER (ORDER BY ...) attribue un classement.\n\n\n\n\nâ“ Q6. Quel avantage de DuckDB pour un Data Engineer ?\n\nIl nÃ©cessite un serveur\n\nIl peut requÃªter directement des DataFrames Pandas\n\nIl ne supporte pas SQL standard\n\nIl ne lit pas les fichiers CSV\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” DuckDB peut requÃªter directement des DataFrames Pandas avec SQL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nğŸ® Pratiquer SQL en ligne\n\nSQLBolt â€” Tutoriel interactif\nMode SQL Tutorial â€” Cas business rÃ©els\nLeetCode SQL â€” DÃ©fis progressifs\nHackerRank SQL â€” Exercices variÃ©s\n\n\n\nğŸ¦† DuckDB\n\nDuckDB Documentation\nDuckDB SQL Reference\n\n\n\nğŸ“– Documentation SQL\n\nPostgreSQL Documentation\nW3Schools SQL",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ˜ SQL for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant SQL et sais lâ€™exÃ©cuter depuis Python ! DÃ©couvrons les concepts du Big Data et les bases NoSQL.\nğŸ‘‰ Module suivant : 08_intro_big_data_nosql.ipynb â€” Big Data, traitement distribuÃ© et NoSQL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module SQL pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ˜ SQL for Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "",
    "text": "Ce module couvre le traitement de donnÃ©es avancÃ© avec Python : Pandas, visualisation, APIs, et pipelines ETL.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 04_python_basics_for_data_engineers\n\n\nâœ… Requis\nMaÃ®triser les bases de Python (variables, fonctions, boucles)\n\n\nâœ… Requis\nSavoir utiliser pip et les environnements virtuels",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Manipuler des donnÃ©es avec Pandas (DataFrames, nettoyage, agrÃ©gations)\nâœ… Visualiser des donnÃ©es avec Matplotlib\nâœ… CrÃ©er des graphiques statistiques avec Seaborn\nâœ… Traiter du texte et utiliser les regex\nâœ… Consommer des APIs REST\nâœ… Valider la qualitÃ© des donnÃ©es\nâœ… Construire un pipeline ETL complet\nâœ… GÃ©rer les configurations et secrets",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“¦ Installation des dÃ©pendances",
    "text": "ğŸ“¦ Installation des dÃ©pendances\nAvant de commencer, assurons-nous dâ€™avoir toutes les librairies nÃ©cessaires.\n\n\nCode\n# Installation des packages (Ã  exÃ©cuter une seule fois)\n!pip install pandas numpy requests python-dotenv pytest pandera pyarrow openpyxl matplotlib seaborn\n\n\n\n\nCode\n# Imports de base\nimport pandas as pd\nimport numpy as np\nimport json\nimport requests\nfrom datetime import datetime\nimport time\nimport logging\nimport re\nfrom pathlib import Path\n\n# Configuration de l'affichage\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\n\nprint(\"âœ… Imports rÃ©ussis !\")\nprint(f\"Version Pandas : {pd.__version__}\")\nprint(f\"Version NumPy : {np.__version__}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.1 CrÃ©er et lire des donnÃ©es",
    "text": "1.1 CrÃ©er et lire des donnÃ©es\n\n\nCode\n# CrÃ©er un DataFrame simple\ndata = {\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'age': [25, 30, 35, None, 28],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon'],\n    'salaire': [45000, 55000, 60000, 50000, None]\n}\n\ndf = pd.DataFrame(data)\nprint(\"ğŸ“Š DataFrame crÃ©Ã© :\")\nprint(df)\n\n\n\n\nCode\n# Sauvegarder en CSV\ndf.to_csv('exemple_employes.csv', index=False)\nprint(\"âœ… Fichier CSV sauvegardÃ©\")\n\n# Lire depuis CSV\ndf_from_csv = pd.read_csv('exemple_employes.csv')\nprint(\"\\nğŸ“‚ Lecture depuis CSV :\")\nprint(df_from_csv.head())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.2 Exploration des donnÃ©es",
    "text": "1.2 Exploration des donnÃ©es\n\n\nCode\n# Informations gÃ©nÃ©rales\nprint(\"ğŸ“‹ Informations du DataFrame :\")\nprint(df.info())\nprint(\"\\n\" + \"=\"*50)\n\n# Statistiques descriptives\nprint(\"\\nğŸ“Š Statistiques descriptives :\")\nprint(df.describe())\n\n# PremiÃ¨res lignes\nprint(\"\\nğŸ” PremiÃ¨res lignes :\")\nprint(df.head(3))\n\n# DerniÃ¨res lignes\nprint(\"\\nğŸ”š DerniÃ¨res lignes :\")\nprint(df.tail(2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¤– Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es",
    "text": "ğŸ¤– Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es\nJulius.ai est une plateforme dâ€™IA qui permet dâ€™analyser des donnÃ©es en langage naturel, sans Ã©crire de code.\n\nğŸŒ AccÃ¨s\nğŸ‘‰ julius.ai â€” Gratuit avec limitations, plans payants disponibles\n\n\nâœ¨ FonctionnalitÃ©s\n\n\n\n\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nUpload de fichiers\nCSV, Excel, JSON, bases de donnÃ©es\n\n\nQuestions en franÃ§ais\nâ€œQuelle est la moyenne des salaires par ville ?â€\n\n\nGÃ©nÃ©ration de code\nPython/Pandas gÃ©nÃ©rÃ© automatiquement\n\n\nVisualisations\nGraphiques crÃ©Ã©s Ã  la demande\n\n\nExport\nCode Python, graphiques, rapports\n\n\n\n\n\nğŸ“ Exemples de questions Ã  poser\n- \"Montre-moi les 10 premiÃ¨res lignes\"\n- \"Combien de valeurs manquantes par colonne ?\"\n- \"CrÃ©e un graphique des ventes par mois\"\n- \"Quelle est la corrÃ©lation entre age et salaire ?\"\n- \"Nettoie les doublons et les valeurs aberrantes\"\n- \"GÃ©nÃ¨re un rapport de qualitÃ© des donnÃ©es\"\n\n\nğŸ’¡ Cas dâ€™usage Data Engineering\n\n\n\nSituation\nComment Julius aide\n\n\n\n\nNouveau dataset inconnu\nExploration rapide sans code\n\n\nRÃ©union avec non-techniques\nDÃ©mo interactive\n\n\nPrototypage rapide\nGÃ©nÃ©rer du code Pandas Ã  rÃ©utiliser\n\n\nDebugging\nâ€œPourquoi jâ€™ai des NaN dans cette colonne ?â€\n\n\n\n\n\nâš ï¸ Limitations\n\nDonnÃ©es envoyÃ©es dans le cloud (attention aux donnÃ©es sensibles)\nGratuit limitÃ© en nombre de requÃªtes\nPas adaptÃ© pour la production (utiliser le code gÃ©nÃ©rÃ© plutÃ´t)\n\n\nğŸ’¡ Astuce : Utilise Julius pour explorer, puis copie le code Python gÃ©nÃ©rÃ© dans ton pipeline !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š ydata-profiling â€” Rapport complet en 1 ligne",
    "text": "ğŸ“Š ydata-profiling â€” Rapport complet en 1 ligne\nydata-profiling (anciennement pandas-profiling) gÃ©nÃ¨re un rapport HTML interactif complet sur ton DataFrame.\n\nğŸ“¦ Installation\npip install ydata-profiling\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install ydata-profiling\n\nfrom ydata_profiling import ProfileReport\n\n# CrÃ©er un dataset d'exemple\ndf_exemple = pd.DataFrame({\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n    'age': [25, 30, 35, None, 28, 45, 32, 29],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris', 'Lyon', 'Paris'],\n    'salaire': [45000, 55000, 60000, 50000, None, 75000, 52000, 48000],\n    'experience': [2, 5, 8, 3, 4, 15, 7, 3],\n    'date_embauche': pd.to_datetime(['2022-01-15', '2019-06-20', '2016-03-10', \n                                      '2021-09-01', '2020-04-15', '2010-01-01',\n                                      '2017-08-20', '2021-11-30'])\n})\n\n# GÃ©nÃ©rer le rapport (mode minimal pour rapiditÃ©)\nprofile = ProfileReport(\n    df_exemple, \n    title=\"Rapport EmployÃ©s\",\n    minimal=True,  # Mode rapide\n    explorative=True\n)\n\n# Afficher dans le notebook\nprofile.to_notebook_iframe()\n\n# Ou sauvegarder en HTML\n# profile.to_file(\"rapport_employes.html\")\n\n\n\n\nğŸ“‹ Ce que contient le rapport\n\n\n\n\n\n\n\nSection\nContenu\n\n\n\n\nOverview\nNombre de lignes, colonnes, types, taille mÃ©moire\n\n\nVariables\nStats par colonne (min, max, mean, distribution)\n\n\nInteractions\nCorrÃ©lations entre variables\n\n\nCorrelations\nMatrices de corrÃ©lation (Pearson, Spearman)\n\n\nMissing values\nVisualisation des valeurs manquantes\n\n\nDuplicates\nDÃ©tection des doublons\n\n\nAlerts\nâš ï¸ Alertes automatiques (haute cardinalitÃ©, skewness, etc.)\n\n\n\n\n\nâš™ï¸ Options utiles\n# Rapport complet (plus lent)\nprofile = ProfileReport(df, minimal=False)\n\n# Comparer deux datasets\nprofile_train = ProfileReport(df_train, title=\"Train\")\nprofile_test = ProfileReport(df_test, title=\"Test\")\ncomparison = profile_train.compare(profile_test)\ncomparison.to_file(\"comparison.html\")\n\n# Exclure certaines analyses (plus rapide)\nprofile = ProfileReport(\n    df,\n    correlations=None,  # DÃ©sactiver les corrÃ©lations\n    interactions=None   # DÃ©sactiver les interactions\n)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¬ Sweetviz â€” Comparaison visuelle de datasets",
    "text": "ğŸ¬ Sweetviz â€” Comparaison visuelle de datasets\nSweetviz est spÃ©cialisÃ© dans la comparaison de datasets (train vs test, avant vs aprÃ¨s nettoyage).\n\nğŸ“¦ Installation\npip install sweetviz\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install sweetviz\n\nimport sweetviz as sv\n\n# Rapport simple\nreport = sv.analyze(df_exemple)\nreport.show_notebook()  # Afficher dans le notebook\n# report.show_html(\"sweetviz_report.html\")  # Ou sauvegarder\n\n\n\n\nCode\n# Comparaison de deux datasets (ex: train vs test)\ndf_train = df_exemple.iloc[:5]\ndf_test = df_exemple.iloc[5:]\n\n# GÃ©nÃ©rer le rapport de comparaison\ncomparison_report = sv.compare([df_train, \"Train\"], [df_test, \"Test\"])\ncomparison_report.show_notebook()\n\n# Analyse avec variable cible (pour ML)\n# report = sv.analyze(df, target_feat=\"salaire\")\n\n\n\n\nâœ¨ Points forts de Sweetviz\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nComparaison cÃ´te Ã  cÃ´te\nVoir les diffÃ©rences entre 2 datasets\n\n\nVariable cible\nAnalyse par rapport Ã  une target (ML)\n\n\nDesign moderne\nRapports visuellement attractifs\n\n\nRapide\nPlus lÃ©ger que ydata-profiling\n\n\n\n\n\nğŸ†š ydata-profiling vs Sweetviz\n\n\n\nCritÃ¨re\nydata-profiling\nSweetviz\n\n\n\n\nProfondeur dâ€™analyse\nâ­â­â­ TrÃ¨s dÃ©taillÃ©\nâ­â­ Essentiel\n\n\nVitesse\nğŸ¢ Plus lent\nğŸ‡ Plus rapide\n\n\nComparaison\nâœ… Possible\nâ­â­â­ Excellent\n\n\nDesign\nClassique\nModerne\n\n\nAlertes\nâœ… Oui\nâŒ Non",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ–¥ï¸ D-Tale â€” Exploration interactive (comme Excel)",
    "text": "ğŸ–¥ï¸ D-Tale â€” Exploration interactive (comme Excel)\nD-Tale lance une interface web interactive pour explorer tes donnÃ©es comme dans Excel/Google Sheets, mais avec la puissance de Python derriÃ¨re.\n\nğŸ“¦ Installation\npip install dtale\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install dtale\n\nimport dtale\n\n# Lancer D-Tale\nd = dtale.show(df_exemple)\n\n# Afficher dans le notebook (ou ouvre un nouvel onglet)\nd.notebook()\n\n\n\n\nâœ¨ FonctionnalitÃ©s D-Tale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  D-Tale                                         [Export] [Code]â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  [Filters] [Sort] [Charts] [Correlations] [Describe] [Missing] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    nom    â”‚  age  â”‚  ville   â”‚ salaire â”‚ experience â”‚          â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚\nâ”‚  Alice    â”‚  25   â”‚  Paris   â”‚  45000  â”‚     2      â”‚          â”‚\nâ”‚  Bob      â”‚  30   â”‚  Lyon    â”‚  55000  â”‚     5      â”‚          â”‚\nâ”‚  Charlie  â”‚  35   â”‚  Paris   â”‚  60000  â”‚     8      â”‚          â”‚\nâ”‚  ...      â”‚  ...  â”‚  ...     â”‚  ...    â”‚    ...     â”‚          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nAction\nComment\n\n\n\n\nFiltrer\nCliquer sur une colonne â†’ Filter\n\n\nTrier\nCliquer sur lâ€™en-tÃªte de colonne\n\n\nGraphiques\nMenu Charts â†’ choisir le type\n\n\nStats\nMenu Describe â†’ stats par colonne\n\n\nExporter le code\nBouton â€œCode Exportâ€ â†’ copier le Pandas gÃ©nÃ©rÃ©\n\n\n\n\nğŸ’¡ Killer feature : D-Tale gÃ©nÃ¨re le code Pandas de toutes tes manipulations !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“ˆ Pygwalker â€” Interface Tableau dans Jupyter",
    "text": "ğŸ“ˆ Pygwalker â€” Interface Tableau dans Jupyter\nPygwalker transforme ton DataFrame en une interface drag & drop comme Tableau/Power BI, directement dans Jupyter.\n\nğŸ“¦ Installation\npip install pygwalker\n\n\nCode\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install pygwalker\n\nimport pygwalker as pyg\n\n# Lancer l'interface interactive\nwalker = pyg.walk(df_exemple)\n\n\n\n\nâœ¨ Comment utiliser Pygwalker\n\nGlisser-dÃ©poser les colonnes sur les axes X, Y, Color, Size\nChoisir le type de graphique (bar, line, scatter, heatmapâ€¦)\nFiltrer les donnÃ©es visuellement\nExporter la configuration pour la rÃ©utiliser\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Pygwalker                                                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   FIELDS         â”‚                                              â”‚\nâ”‚                  â”‚         [Graphique interactif]               â”‚\nâ”‚   ğŸ“Š nom         â”‚                                              â”‚\nâ”‚   ğŸ“Š age         â”‚              â–ˆâ–ˆâ–ˆâ–ˆ                            â”‚\nâ”‚   ğŸ“Š ville       â”‚         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚\nâ”‚   ğŸ“Š salaire     â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚\nâ”‚   ğŸ“Š experience  â”‚                                              â”‚\nâ”‚                  â”‚                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  X: ville        â”‚  Y: salaire    Color: experience             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Cas dâ€™usage\n\nExploration visuelle rapide sans Ã©crire de code matplotlib\nPrÃ©sentation Ã  des non-techniques\nPrototypage de dashboards avant de coder",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“‹ RÃ©capitulatif â€” Quel outil choisir ?",
    "text": "ğŸ“‹ RÃ©capitulatif â€” Quel outil choisir ?\n\n\n\n\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nPremier aperÃ§u rapide dâ€™un dataset\nydata-profiling (minimal=True)\n\n\nComparer data1 vs data2\nSweetviz\n\n\nExploration interactive (comme Excel)\nD-Tale\n\n\nCrÃ©er des graphiques sans code\nPygwalker\n\n\nPoser des questions en franÃ§ais\nJulius.ai\n\n\nDonnÃ©es sensibles (pas de cloud)\nD-Tale ou ydata-profiling (tout local)\n\n\nGÃ©nÃ©rer du code Pandas\nJulius.ai ou D-Tale\n\n\n\n\nğŸ“¦ Installation complÃ¨te\npip install ydata-profiling sweetviz dtale pygwalker\n\n\nâš ï¸ Bonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nUtiliser ces outils pour explorer\nLes utiliser en production\n\n\nCopier le code gÃ©nÃ©rÃ© dans ton pipeline\nDÃ©pendre de lâ€™interface pour le traitement\n\n\nPartager les rapports HTML avec lâ€™Ã©quipe\nEnvoyer des donnÃ©es sensibles sur Julius.ai\n\n\nCombiner plusieurs outils\nSe limiter Ã  un seul\n\n\n\n\nğŸ’¡ Workflow recommandÃ© : 1. Julius.ai pour les premiÃ¨res questions 2. ydata-profiling pour un rapport complet 3. D-Tale pour explorer interactivement 4. Copier le code dans ton pipeline Pandas",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.3 Nettoyage des donnÃ©es",
    "text": "1.3 Nettoyage des donnÃ©es\n\n\nCode\n# DÃ©tecter les valeurs manquantes\nprint(\"â“ Valeurs manquantes par colonne :\")\nprint(df.isnull().sum())\nprint(f\"\\nTotal de valeurs manquantes : {df.isnull().sum().sum()}\")\n\n# Visualiser les lignes avec des valeurs manquantes\nprint(\"\\nğŸ” Lignes avec des NaN :\")\nprint(df[df.isnull().any(axis=1)])\n\n\n\n\nCode\n# StratÃ©gies de gestion des valeurs manquantes\n\n# 1. Supprimer les lignes avec des NaN\ndf_drop = df.dropna()\nprint(\"ğŸ—‘ï¸ AprÃ¨s suppression des lignes avec NaN :\")\nprint(df_drop)\n\n# 2. Remplir avec une valeur par dÃ©faut\ndf_fill = df.fillna({\n    'age': df['age'].median(),\n    'salaire': df['salaire'].mean()\n})\nprint(\"\\nâœ¨ AprÃ¨s remplissage des NaN :\")\nprint(df_fill)\n\n# 3. Forward fill (propager la valeur prÃ©cÃ©dente)\ndf_ffill = df.fillna(method='ffill')\nprint(\"\\nâ¡ï¸ AprÃ¨s forward fill :\")\nprint(df_ffill)\n\n\n\n\nCode\n# Supprimer les doublons\ndf_with_duplicates = pd.DataFrame({\n    'id': [1, 2, 3, 2, 4],\n    'nom': ['Alice', 'Bob', 'Charlie', 'Bob', 'David']\n})\n\nprint(\"Avant suppression des doublons :\")\nprint(df_with_duplicates)\n\ndf_no_duplicates = df_with_duplicates.drop_duplicates()\nprint(\"\\nAprÃ¨s suppression :\")\nprint(df_no_duplicates)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.4 SÃ©lection et filtrage",
    "text": "1.4 SÃ©lection et filtrage\n\n\nCode\n# Utilisons le DataFrame nettoyÃ©\ndf_clean = df_fill.copy()\n\n# SÃ©lectionner une colonne\nprint(\"ğŸ“Œ Colonne 'nom' :\")\nprint(df_clean['nom'])\n\n# SÃ©lectionner plusieurs colonnes\nprint(\"\\nğŸ“Œ Colonnes 'nom' et 'ville' :\")\nprint(df_clean[['nom', 'ville']])\n\n# Filtrer les lignes\nprint(\"\\nğŸ” EmployÃ©s de Paris :\")\nprint(df_clean[df_clean['ville'] == 'Paris'])\n\n# Filtres multiples\nprint(\"\\nğŸ” EmployÃ©s de Paris avec salaire &gt; 50000 :\")\nprint(df_clean[(df_clean['ville'] == 'Paris') & (df_clean['salaire'] &gt; 50000)])\n\n\n\n\nCode\n# Indexation avancÃ©e avec loc et iloc\n\n# loc : par label/nom\nprint(\"ğŸ“ loc[0, 'nom'] :\")\nprint(df_clean.loc[0, 'nom'])\n\n# iloc : par position numÃ©rique\nprint(\"\\nğŸ“ iloc[0, 0] (premiÃ¨re ligne, premiÃ¨re colonne) :\")\nprint(df_clean.iloc[0, 0])\n\n# SÃ©lection de plages\nprint(\"\\nğŸ“ loc[0:2, ['nom', 'age']] :\")\nprint(df_clean.loc[0:2, ['nom', 'age']])",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.5 GroupBy et agrÃ©gations",
    "text": "1.5 GroupBy et agrÃ©gations\n\n\nCode\n# Grouper par ville et calculer des statistiques\nprint(\"ğŸ“Š Statistiques par ville :\")\ngrouped = df_clean.groupby('ville').agg({\n    'nom': 'count',\n    'age': ['mean', 'min', 'max'],\n    'salaire': ['mean', 'sum']\n})\nprint(grouped)\n\n# Renommer les colonnes pour plus de clartÃ©\nprint(\"\\nğŸ“Š Salaire moyen par ville :\")\nsalaire_moyen = df_clean.groupby('ville')['salaire'].mean().round(2)\nprint(salaire_moyen)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.6 Apply vs Vectorisation",
    "text": "1.6 Apply vs Vectorisation\n\n\nCode\n# CrÃ©er une colonne calculÃ©e\n\n# MÃ©thode 1 : Apply (plus lent mais flexible)\ndef categoriser_age(age):\n    if age &lt; 30:\n        return 'Junior'\n    elif age &lt; 40:\n        return 'Senior'\n    else:\n        return 'Expert'\n\ndf_clean['categorie_apply'] = df_clean['age'].apply(categoriser_age)\n\n# MÃ©thode 2 : Vectorisation (plus rapide)\ndf_clean['categorie_vect'] = pd.cut(\n    df_clean['age'],\n    bins=[0, 30, 40, 100],\n    labels=['Junior', 'Senior', 'Expert']\n)\n\nprint(\"ğŸ”§ Colonnes calculÃ©es :\")\nprint(df_clean[['nom', 'age', 'categorie_apply', 'categorie_vect']])\n\n\n\n\nCode\n# Comparaison de performance (sur un grand dataset)\nimport time\n\n# CrÃ©er un grand DataFrame\nbig_df = pd.DataFrame({\n    'valeur': np.random.randint(1, 100, 100000)\n})\n\n# MÃ©thode Apply\nstart = time.time()\nbig_df['double_apply'] = big_df['valeur'].apply(lambda x: x * 2)\ntime_apply = time.time() - start\n\n# MÃ©thode VectorisÃ©e\nstart = time.time()\nbig_df['double_vect'] = big_df['valeur'] * 2\ntime_vect = time.time() - start\n\nprint(f\"â±ï¸ Temps Apply : {time_apply:.4f}s\")\nprint(f\"â±ï¸ Temps Vectorisation : {time_vect:.4f}s\")\nprint(f\"ğŸš€ Vectorisation est {time_apply/time_vect:.1f}x plus rapide !\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.7 Gestion de la mÃ©moire",
    "text": "1.7 Gestion de la mÃ©moire\n\n\nCode\n# VÃ©rifier l'utilisation mÃ©moire\nprint(\"ğŸ’¾ Utilisation mÃ©moire par colonne :\")\nprint(df_clean.memory_usage(deep=True))\nprint(f\"\\nTotal : {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n\n\n\nCode\n# Optimiser les types de donnÃ©es\ndf_optimized = df_clean.copy()\n\n# Avant optimisation\nprint(\"Avant optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n# Convertir en types plus efficaces\ndf_optimized['age'] = df_optimized['age'].astype('int8')\ndf_optimized['salaire'] = df_optimized['salaire'].astype('int32')\ndf_optimized['ville'] = df_optimized['ville'].astype('category')\n\nprint(\"\\nAprÃ¨s optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.8 Manipulation de dates",
    "text": "1.8 Manipulation de dates\n\n\nCode\n# CrÃ©er un DataFrame avec des dates\ndf_dates = pd.DataFrame({\n    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],\n    'montant': [1000, 1500, 1200, 1800]\n})\n\n# Convertir en datetime\ndf_dates['date'] = pd.to_datetime(df_dates['date_str'])\n\n# Extraire des composantes\ndf_dates['annee'] = df_dates['date'].dt.year\ndf_dates['mois'] = df_dates['date'].dt.month\ndf_dates['jour'] = df_dates['date'].dt.day\ndf_dates['nom_mois'] = df_dates['date'].dt.month_name()\ndf_dates['jour_semaine'] = df_dates['date'].dt.day_name()\n\nprint(\"ğŸ“… DataFrame avec dates extraites :\")\nprint(df_dates)\n\n\n\n\nCode\n# Calculs avec les dates\ndf_dates['jours_depuis_debut'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n# Ajouter/soustraire des pÃ©riodes\ndf_dates['date_plus_30j'] = df_dates['date'] + pd.Timedelta(days=30)\ndf_dates['date_moins_1mois'] = df_dates['date'] - pd.DateOffset(months=1)\n\nprint(\"ğŸ“… Calculs de dates :\")\nprint(df_dates[['date', 'jours_depuis_debut', 'date_plus_30j', 'date_moins_1mois']])",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "1.9 Export de donnÃ©es",
    "text": "1.9 Export de donnÃ©es\n\n\nCode\n# Export CSV\ndf_clean.to_csv('employes_clean.csv', index=False)\nprint(\"âœ… Export CSV rÃ©ussi\")\n\n# Export JSON\ndf_clean.to_json('employes_clean.json', orient='records', indent=2)\nprint(\"âœ… Export JSON rÃ©ussi\")\n\n# Export Parquet (format columnar, trÃ¨s efficace)\ndf_clean.to_parquet('employes_clean.parquet', index=False)\nprint(\"âœ… Export Parquet rÃ©ussi\")\n\n# Export Excel\ndf_clean.to_excel('employes_clean.xlsx', index=False, sheet_name='EmployÃ©s')\nprint(\"âœ… Export Excel rÃ©ussi\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 1 : Pandas",
    "text": "ğŸ¯ Exercice Pratique 1 : Pandas\nObjectif : Analyser un fichier de ventes\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, quantitÃ©, prix, date)\nCalculer le chiffre dâ€™affaires total\nTrouver le produit le plus vendu\nCalculer les ventes mensuelles\nExporter le rÃ©sultat en CSV\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques linÃ©aires (Line Plots)",
    "text": "ğŸ“Š Graphiques linÃ©aires (Line Plots)\n\n\nCode\n# DonnÃ©es pour un graphique linÃ©aire\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# CrÃ©ation du graphique\nplt.figure(figsize=(12, 6))\nplt.plot(x, y1, label='Sin(x)', color='blue', linewidth=2)\nplt.plot(x, y2, label='Cos(x)', color='red', linestyle='--', linewidth=2)\n\n# Personnalisation\nplt.title('Fonctions trigonomÃ©triques', fontsize=16, fontweight='bold')\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques Ã  barres (Bar Plots)",
    "text": "ğŸ“Š Graphiques Ã  barres (Bar Plots)\n\n\nCode\n# DonnÃ©es de ventes par mois\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\nventes_2023 = [1200, 1500, 1800, 1600, 2000, 2200]\nventes_2024 = [1400, 1700, 1900, 1800, 2300, 2500]\n\nx = np.arange(len(mois))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Barres groupÃ©es\nbars1 = ax.bar(x - width/2, ventes_2023, width, label='2023', color='steelblue')\nbars2 = ax.bar(x + width/2, ventes_2024, width, label='2024', color='coral')\n\n# Personnalisation\nax.set_title('Comparaison des ventes 2023 vs 2024', fontsize=16, fontweight='bold')\nax.set_xlabel('Mois', fontsize=12)\nax.set_ylabel('Ventes (â‚¬)', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(mois)\nax.legend()\n\n# Ajouter les valeurs sur les barres\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height}', xy=(bar.get_x() + bar.get_width()/2, height),\n                xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Nuages de points (Scatter Plots)",
    "text": "ğŸ“Š Nuages de points (Scatter Plots)\n\n\nCode\n# DonnÃ©es alÃ©atoires avec corrÃ©lation\nnp.random.seed(42)\nx = np.random.randn(100)\ny = 2 * x + np.random.randn(100) * 0.5\ncolors = np.random.rand(100)\nsizes = np.random.rand(100) * 200\n\n# Scatter plot avec couleurs et tailles variables\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(x, y, c=colors, s=sizes, alpha=0.6, cmap='viridis')\n\n# Ajouter une ligne de tendance\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x, p(x), 'r--', linewidth=2, label=f'Tendance: y = {z[0]:.2f}x + {z[1]:.2f}')\n\nplt.colorbar(scatter, label='Valeur')\nplt.title('Nuage de points avec ligne de tendance', fontsize=16, fontweight='bold')\nplt.xlabel('Variable X')\nplt.ylabel('Variable Y')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Histogrammes",
    "text": "ğŸ“Š Histogrammes\n\n\nCode\n# DonnÃ©es de distribution\nnp.random.seed(42)\ndata_normal = np.random.normal(loc=50, scale=10, size=1000)\ndata_skewed = np.random.exponential(scale=10, size=1000)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme distribution normale\naxes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\naxes[0].set_title('Distribution Normale', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Valeur')\naxes[0].set_ylabel('FrÃ©quence')\naxes[0].legend()\n\n# Histogramme distribution exponentielle\naxes[1].hist(data_skewed, bins=30, color='coral', edgecolor='black', alpha=0.7)\naxes[1].axvline(data_skewed.mean(), color='red', linestyle='--', label=f'Moyenne: {data_skewed.mean():.1f}')\naxes[1].set_title('Distribution Exponentielle', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Valeur')\naxes[1].set_ylabel('FrÃ©quence')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques circulaires (Pie Charts)",
    "text": "ğŸ“Š Graphiques circulaires (Pie Charts)\n\n\nCode\n# DonnÃ©es de rÃ©partition\ncategories = ['Produit A', 'Produit B', 'Produit C', 'Produit D', 'Autres']\nparts = [35, 25, 20, 15, 5]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\nexplode = (0.05, 0, 0, 0, 0)  # Mettre en Ã©vidence le premier segment\n\nplt.figure(figsize=(10, 8))\nwedges, texts, autotexts = plt.pie(parts, labels=categories, colors=colors, explode=explode,\n                                    autopct='%1.1f%%', startangle=90, shadow=True)\n\n# AmÃ©liorer l'apparence du texte\nfor autotext in autotexts:\n    autotext.set_fontsize(11)\n    autotext.set_fontweight('bold')\n\nplt.title('RÃ©partition des ventes par produit', fontsize=16, fontweight='bold')\nplt.axis('equal')  # Assure que le cercle est bien rond\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Sous-graphiques (Subplots)",
    "text": "ğŸ“Š Sous-graphiques (Subplots)\n\n\nCode\n# CrÃ©er une grille de sous-graphiques\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# DonnÃ©es\nx = np.linspace(0, 10, 50)\n\n# Graphique 1: Ligne\naxes[0, 0].plot(x, np.sin(x), 'b-', linewidth=2)\naxes[0, 0].set_title('Graphique linÃ©aire')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Sin(X)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Graphique 2: Barres\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 56, 78]\naxes[0, 1].bar(categories, values, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\naxes[0, 1].set_title('Graphique Ã  barres')\n\n# Graphique 3: Scatter\nx_scatter = np.random.rand(50)\ny_scatter = np.random.rand(50)\naxes[1, 0].scatter(x_scatter, y_scatter, c='purple', alpha=0.6, s=100)\naxes[1, 0].set_title('Nuage de points')\n\n# Graphique 4: Histogramme\ndata = np.random.randn(1000)\naxes[1, 1].hist(data, bins=30, color='orange', edgecolor='black', alpha=0.7)\naxes[1, 1].set_title('Histogramme')\n\nplt.suptitle('Tableau de bord - Vue d\\'ensemble', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ’¾ Sauvegarder des graphiques",
    "text": "ğŸ’¾ Sauvegarder des graphiques\n\n\nCode\n# CrÃ©er un graphique Ã  sauvegarder\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), 'b-', linewidth=2, label='Sin(x)')\nax.set_title('Graphique Ã  exporter', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Sauvegarder dans diffÃ©rents formats\nfig.savefig('graphique.png', dpi=300, bbox_inches='tight')\nfig.savefig('graphique.pdf', bbox_inches='tight')\nfig.savefig('graphique.svg', bbox_inches='tight')\n\nprint(\"âœ… Graphiques sauvegardÃ©s en PNG, PDF et SVG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique : Matplotlib",
    "text": "ğŸ¯ Exercice Pratique : Matplotlib\nObjectif : CrÃ©er un tableau de bord de visualisation\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, mois, ventes, profit)\nCrÃ©er 4 sous-graphiques montrant :\n\nÃ‰volution des ventes mensuelles (ligne)\nVentes par produit (barres)\nRelation ventes/profit (scatter)\nDistribution des profits (histogramme)\n\nPersonnaliser les couleurs et ajouter des titres\nSauvegarder le rÃ©sultat en PNG\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“¦ Charger des jeux de donnÃ©es intÃ©grÃ©s",
    "text": "ğŸ“¦ Charger des jeux de donnÃ©es intÃ©grÃ©s\n\n\nCode\n# Seaborn propose des jeux de donnÃ©es pour s'entraÃ®ner\ntips = sns.load_dataset('tips')\nprint(\"ğŸ“Š Dataset 'tips' :\")\nprint(tips.head())\nprint(f\"\\nDimensions : {tips.shape}\")\nprint(f\"\\nColonnes : {list(tips.columns)}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des distributions",
    "text": "ğŸ“Š Visualisation des distributions\n\n\nCode\n# Histogramme avec KDE (Kernel Density Estimation)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme simple\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0], color='steelblue')\naxes[0].set_title('Distribution du montant total', fontsize=14, fontweight='bold')\n\n# Histogramme avec hue (groupement)\nsns.histplot(data=tips, x='total_bill', hue='time', kde=True, ax=axes[1])\naxes[1].set_title('Distribution par moment de la journÃ©e', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# KDE plot (densitÃ© de probabilitÃ©)\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=tips, x='total_bill', hue='day', fill=True, alpha=0.5)\nplt.title('DensitÃ© du montant total par jour', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des donnÃ©es catÃ©gorielles",
    "text": "ğŸ“Š Visualisation des donnÃ©es catÃ©gorielles\n\n\nCode\n# Box plot - Distribution par catÃ©gorie\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Box plot simple\nsns.boxplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Set2')\naxes[0].set_title('Montant total par jour', fontsize=14, fontweight='bold')\n\n# Box plot avec hue\nsns.boxplot(data=tips, x='day', y='total_bill', hue='sex', ax=axes[1], palette='Set1')\naxes[1].set_title('Montant total par jour et sexe', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Violin plot - Combine box plot et KDE\nplt.figure(figsize=(12, 6))\nsns.violinplot(data=tips, x='day', y='total_bill', hue='sex', split=True, palette='muted')\nplt.title('Distribution du montant par jour et sexe (Violin Plot)', fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nCode\n# Bar plot avec estimation statistique\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Bar plot avec intervalle de confiance\nsns.barplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Blues_d', errorbar='ci')\naxes[0].set_title('Montant moyen par jour (avec IC 95%)', fontsize=14, fontweight='bold')\n\n# Count plot (compte les occurrences)\nsns.countplot(data=tips, x='day', hue='time', ax=axes[1], palette='Set2')\naxes[1].set_title('Nombre de repas par jour', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisation des relations entre variables",
    "text": "ğŸ“Š Visualisation des relations entre variables\n\n\nCode\n# Scatter plot avec rÃ©gression\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Scatter plot simple avec rÃ©gression\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[0], color='coral')\naxes[0].set_title('Relation montant/pourboire avec rÃ©gression', fontsize=14, fontweight='bold')\n\n# Scatter plot avec hue et style\nsns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', style='sex', \n                size='size', sizes=(50, 200), ax=axes[1], palette='Set1')\naxes[1].set_title('Relation multidimensionnelle', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# lmplot - RÃ©gression avec facettes\ng = sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', col='time', \n               height=5, aspect=1.2, palette='Set1')\ng.fig.suptitle('RÃ©gression par moment et statut fumeur', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Heatmaps et matrices de corrÃ©lation",
    "text": "ğŸ“Š Heatmaps et matrices de corrÃ©lation\n\n\nCode\n# Matrice de corrÃ©lation\n# SÃ©lectionner uniquement les colonnes numÃ©riques\nnumeric_cols = tips.select_dtypes(include=[np.number])\ncorrelation_matrix = numeric_cols.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', linewidths=0.5, square=True)\nplt.title('Matrice de corrÃ©lation', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Heatmap de donnÃ©es pivotÃ©es\npivot_data = tips.pivot_table(values='tip', index='day', columns='time', aggfunc='mean')\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)\nplt.title('Pourboire moyen par jour et moment', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Pair plots (visualisation multivariÃ©e)",
    "text": "ğŸ“Š Pair plots (visualisation multivariÃ©e)\n\n\nCode\n# Pair plot - Toutes les combinaisons de variables\ng = sns.pairplot(tips, hue='time', palette='Set1', diag_kind='kde', \n                 plot_kws={'alpha': 0.6}, height=2.5)\ng.fig.suptitle('Pair Plot - Dataset Tips', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š FacetGrid - Graphiques multi-facettes",
    "text": "ğŸ“Š FacetGrid - Graphiques multi-facettes\n\n\nCode\n# CrÃ©er une grille de facettes\ng = sns.FacetGrid(tips, col='time', row='smoker', height=4, aspect=1.2)\ng.map_dataframe(sns.histplot, x='total_bill', kde=True)\ng.add_legend()\ng.fig.suptitle('Distribution du montant par temps et statut fumeur', y=1.02, \n               fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Joint plots - Distributions jointes",
    "text": "ğŸ“Š Joint plots - Distributions jointes\n\n\nCode\n# Joint plot avec distributions marginales\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', \n                  height=8, ratio=4, color='coral')\ng.fig.suptitle('Distribution jointe montant/pourboire', y=1.02, fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nCode\n# Joint plot avec hexbin (pour grandes quantitÃ©s de donnÃ©es)\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='hex', \n                  height=8, ratio=4, cmap='Blues')\ng.fig.suptitle('Distribution jointe (Hexbin)', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¨ Personnalisation des styles",
    "text": "ğŸ¨ Personnalisation des styles\n\n\nCode\n# Explorer diffÃ©rents styles\nstyles = ['white', 'dark', 'whitegrid', 'darkgrid', 'ticks']\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\n\nfor ax, style in zip(axes, styles):\n    with sns.axes_style(style):\n        sns.histplot(tips['total_bill'], ax=ax, color='steelblue')\n        ax.set_title(f\"Style: {style}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Palettes de couleurs\npalettes = ['deep', 'muted', 'bright', 'pastel', 'dark', 'colorblind']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor ax, palette in zip(axes, palettes):\n    sns.barplot(data=tips, x='day', y='total_bill', palette=palette, ax=ax)\n    ax.set_title(f\"Palette: {palette}\", fontsize=12)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Exemple : Tableau de bord complet",
    "text": "ğŸ“Š Exemple : Tableau de bord complet\n\n\nCode\n# CrÃ©er un tableau de bord d'analyse complet\nsns.set_theme(style='whitegrid')\n\nfig = plt.figure(figsize=(16, 12))\n\n# CrÃ©er une grille personnalisÃ©e\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Distribution des montants\nax1 = fig.add_subplot(gs[0, 0])\nsns.histplot(data=tips, x='total_bill', kde=True, ax=ax1, color='steelblue')\nax1.set_title('Distribution des montants', fontweight='bold')\n\n# 2. Distribution des pourboires\nax2 = fig.add_subplot(gs[0, 1])\nsns.histplot(data=tips, x='tip', kde=True, ax=ax2, color='coral')\nax2.set_title('Distribution des pourboires', fontweight='bold')\n\n# 3. Relation montant/pourboire\nax3 = fig.add_subplot(gs[0, 2])\nsns.regplot(data=tips, x='total_bill', y='tip', ax=ax3, color='purple', scatter_kws={'alpha':0.5})\nax3.set_title('Montant vs Pourboire', fontweight='bold')\n\n# 4. Boxplot par jour\nax4 = fig.add_subplot(gs[1, 0])\nsns.boxplot(data=tips, x='day', y='total_bill', ax=ax4, palette='Set2')\nax4.set_title('Montants par jour', fontweight='bold')\n\n# 5. Violin plot par temps\nax5 = fig.add_subplot(gs[1, 1])\nsns.violinplot(data=tips, x='time', y='total_bill', hue='sex', split=True, ax=ax5, palette='muted')\nax5.set_title('Distribution par temps et sexe', fontweight='bold')\n\n# 6. Count plot\nax6 = fig.add_subplot(gs[1, 2])\nsns.countplot(data=tips, x='day', hue='time', ax=ax6, palette='Set1')\nax6.set_title('Nombre de repas', fontweight='bold')\n\n# 7. Heatmap de corrÃ©lation (grande)\nax7 = fig.add_subplot(gs[2, :])\npivot = tips.pivot_table(values='tip', index='day', columns='size', aggfunc='mean')\nsns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax7, linewidths=0.5)\nax7.set_title('Pourboire moyen par jour et taille de groupe', fontweight='bold')\n\nplt.suptitle('ğŸ“Š Tableau de bord - Analyse des pourboires', fontsize=18, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.savefig('dashboard_seaborn.png', dpi=300, bbox_inches='tight')\nprint(\"âœ… Dashboard sauvegardÃ© en PNG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique : Seaborn",
    "text": "ğŸ¯ Exercice Pratique : Seaborn\nObjectif : Analyser le dataset â€˜titanicâ€™ de Seaborn\n\nCharger le dataset avec sns.load_dataset('titanic')\nCrÃ©er un tableau de bord avec :\n\nDistribution des Ã¢ges par classe (violin plot)\nTaux de survie par sexe et classe (bar plot)\nMatrice de corrÃ©lation des variables numÃ©riques (heatmap)\nRelation Ã¢ge/tarif avec survie en couleur (scatter plot)\n\nUtiliser FacetGrid pour analyser les survivants par sexe et classe\nSauvegarder votre tableau de bord\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Charger le dataset\ntitanic = sns.load_dataset('titanic')\nprint(titanic.head())\nprint(f\"\\nDimensions : {titanic.shape}\")\n\n# Votre code de visualisation ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-de-base-avec-matplotlib",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-de-base-avec-matplotlib",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Graphiques de base avec Matplotlib",
    "text": "ğŸ“Š Graphiques de base avec Matplotlib\n\n\nCode\n# DonnÃ©es pour les graphiques\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\nventes = [1200, 1500, 1800, 1600, 2000, 2200]\n\n# CrÃ©er une figure avec 2x2 sous-graphiques\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Graphique linÃ©aire\naxes[0, 0].plot(mois, ventes, marker='o', linewidth=2, color='steelblue')\naxes[0, 0].set_title('ğŸ“ˆ Ã‰volution des ventes', fontweight='bold')\naxes[0, 0].set_xlabel('Mois')\naxes[0, 0].set_ylabel('Ventes (â‚¬)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2. Graphique Ã  barres\ncolors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7', '#dfe6e9']\naxes[0, 1].bar(mois, ventes, color=colors, edgecolor='black')\naxes[0, 1].set_title('ğŸ“Š Ventes par mois', fontweight='bold')\naxes[0, 1].set_xlabel('Mois')\naxes[0, 1].set_ylabel('Ventes (â‚¬)')\n\n# 3. Graphique circulaire\naxes[1, 0].pie(ventes, labels=mois, autopct='%1.1f%%', colors=colors)\naxes[1, 0].set_title('ğŸ¥§ RÃ©partition des ventes', fontweight='bold')\n\n# 4. Nuage de points\nnp.random.seed(42)\nx = np.random.randn(50)\ny = 2 * x + np.random.randn(50) * 0.5\naxes[1, 1].scatter(x, y, c='coral', alpha=0.7, s=100)\naxes[1, 1].set_title('ğŸ“ Nuage de points', fontweight='bold')\naxes[1, 1].set_xlabel('Variable X')\naxes[1, 1].set_ylabel('Variable Y')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('graphiques_matplotlib.png', dpi=150)\nprint(\"âœ… Graphique sauvegardÃ© en PNG\")\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes-et-distributions",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes-et-distributions",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Histogrammes et distributions",
    "text": "ğŸ“Š Histogrammes et distributions\n\n\nCode\n# GÃ©nÃ©rer des donnÃ©es de distribution\nnp.random.seed(42)\ndata_normal = np.random.normal(loc=50, scale=10, size=1000)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme simple\naxes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\naxes[0].set_title('Distribution des donnÃ©es', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Valeur')\naxes[0].set_ylabel('FrÃ©quence')\naxes[0].legend()\n\n# Box plot\ndata_multi = [np.random.normal(50, 10, 100), np.random.normal(60, 15, 100), np.random.normal(45, 8, 100)]\nbp = axes[1].boxplot(data_multi, labels=['Groupe A', 'Groupe B', 'Groupe C'], patch_artist=True)\ncolors = ['#ff6b6b', '#4ecdc4', '#45b7d1']\nfor patch, color in zip(bp['boxes'], colors):\n    patch.set_facecolor(color)\naxes[1].set_title('Comparaison des groupes', fontsize=14, fontweight='bold')\naxes[1].set_ylabel('Valeur')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisations-avec-seaborn",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisations-avec-seaborn",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“Š Visualisations avec Seaborn",
    "text": "ğŸ“Š Visualisations avec Seaborn\n\n\nCode\n# CrÃ©er un tableau de bord avec Seaborn\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Distribution avec KDE\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0, 0], color='steelblue')\naxes[0, 0].set_title('Distribution du montant', fontweight='bold')\n\n# 2. Box plot par catÃ©gorie\nsns.boxplot(data=tips, x='day', y='total_bill', palette='Set2', ax=axes[0, 1])\naxes[0, 1].set_title('Montant par jour', fontweight='bold')\n\n# 3. Scatter plot avec rÃ©gression\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[1, 0], color='coral')\naxes[1, 0].set_title('Relation montant/pourboire', fontweight='bold')\n\n# 4. Count plot\nsns.countplot(data=tips, x='day', hue='time', palette='Set1', ax=axes[1, 1])\naxes[1, 1].set_title('Nombre de repas par jour', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCode\n# Heatmap de corrÃ©lation\nplt.figure(figsize=(8, 6))\nnumeric_cols = tips.select_dtypes(include=[np.number])\ncorrelation = numeric_cols.corr()\n\nsns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', linewidths=0.5, square=True)\nplt.title('Matrice de corrÃ©lation', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-visualisation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-visualisation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice : Visualisation",
    "text": "ğŸ¯ Exercice : Visualisation\nCrÃ©er un tableau de bord de visualisation avec les donnÃ©es df_clean (employÃ©s) :\n\nDistribution des salaires (histogramme)\nSalaire moyen par ville (bar plot)\nRelation Ã¢ge/salaire (scatter plot)\nRÃ©partition par ville (pie chart)\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# CrÃ©ez votre tableau de bord ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.1 Nettoyage de base",
    "text": "2.1 Nettoyage de base\n\n\nCode\n# DonnÃ©es textuelles brutes\ntextes = pd.DataFrame({\n    'texte': [\n        '  BONJOUR   ',\n        'Salut tout le monde!',\n        'Python_est_gÃ©nial',\n        'Data-Engineering-2024'\n    ]\n})\n\n# Nettoyage basique\ntextes['clean'] = textes['texte'].str.strip()  # Supprimer espaces\ntextes['lower'] = textes['texte'].str.lower()  # Minuscules\ntextes['upper'] = textes['texte'].str.upper()  # Majuscules\ntextes['replace'] = textes['texte'].str.replace('_', ' ')  # Remplacer\n\nprint(\"ğŸ§¹ Nettoyage de texte :\")\nprint(textes)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.2 MÃ©thodes Pandas string (.str accessor)",
    "text": "2.2 MÃ©thodes Pandas string (.str accessor)\n\n\nCode\n# DonnÃ©es d'exemple\ndf_text = pd.DataFrame({\n    'email': ['alice@example.com', 'bob@test.org', 'charlie@mail.fr'],\n    'nom_complet': ['Jean Dupont', 'Marie Martin', 'Pierre Durand'],\n    'telephone': ['0612345678', '06-98-76-54-32', '06 11 22 33 44']\n})\n\n# VÃ©rifier si contient\ndf_text['email_gmail'] = df_text['email'].str.contains('gmail')\n\n# Commencer/finir par\ndf_text['email_com'] = df_text['email'].str.endswith('.com')\n\n# Extraire le domaine\ndf_text['domaine'] = df_text['email'].str.split('@').str[1]\n\n# SÃ©parer nom et prÃ©nom\ndf_text[['prenom', 'nom']] = df_text['nom_complet'].str.split(' ', expand=True)\n\n# Longueur\ndf_text['longueur_nom'] = df_text['nom_complet'].str.len()\n\nprint(\"ğŸ”¤ MÃ©thodes string :\")\nprint(df_text)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.3 Expressions rÃ©guliÃ¨res (Regex)",
    "text": "2.3 Expressions rÃ©guliÃ¨res (Regex)\n\n\nCode\nimport re\n\n# Exemples de regex courantes\ntexte_test = \"\"\"\nContact: alice@example.com ou bob@test.org\nTÃ©lÃ©phones: 06.12.34.56.78, 01-23-45-67-89\nURL: https://www.example.com\nPrix: 29.99â‚¬, 15.50â‚¬, 100â‚¬\n\"\"\"\n\n# Extraire les emails\nemails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', texte_test)\nprint(\"ğŸ“§ Emails trouvÃ©s :\")\nprint(emails)\n\n# Extraire les tÃ©lÃ©phones\ntelephones = re.findall(r'\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}', texte_test)\nprint(\"\\nğŸ“ TÃ©lÃ©phones trouvÃ©s :\")\nprint(telephones)\n\n# Extraire les URLs\nurls = re.findall(r'https?://[^\\s]+', texte_test)\nprint(\"\\nğŸ”— URLs trouvÃ©es :\")\nprint(urls)\n\n# Extraire les prix\nprix = re.findall(r'\\d+\\.?\\d*â‚¬', texte_test)\nprint(\"\\nğŸ’° Prix trouvÃ©s :\")\nprint(prix)\n\n\n\n\nCode\n# Validation avec regex\ndef valider_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\n# Test\nemails_test = ['alice@example.com', 'bob@invalid', 'charlie.fr', 'david@test.org']\nfor email in emails_test:\n    valide = \"âœ…\" if valider_email(email) else \"âŒ\"\n    print(f\"{valide} {email}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs",
    "text": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs\n\n\nCode\n# Exemple de logs Apache/Nginx\nlogs = \"\"\"\n192.168.1.1 - - [01/Dec/2024:10:15:30 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.2 - - [01/Dec/2024:10:16:45 +0000] \"POST /api/login HTTP/1.1\" 401 567\n192.168.1.3 - - [01/Dec/2024:10:17:20 +0000] \"GET /api/products HTTP/1.1\" 200 8901\n\"\"\"\n\n# Pattern pour parser les logs\npattern = r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)'\n\n# Extraire les informations\nmatches = re.findall(pattern, logs)\n\n# CrÃ©er un DataFrame\ndf_logs = pd.DataFrame(matches, columns=[\n    'ip', 'timestamp', 'methode', 'endpoint', 'protocole', 'status', 'bytes'\n])\n\n# Convertir les types\ndf_logs['status'] = df_logs['status'].astype(int)\ndf_logs['bytes'] = df_logs['bytes'].astype(int)\n\nprint(\"ğŸ“‹ Logs parsÃ©s :\")\nprint(df_logs)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "2.5 Gestion de lâ€™encodage",
    "text": "2.5 Gestion de lâ€™encodage\n\n\nCode\n# CrÃ©er un fichier avec encodage spÃ©cifique\ntexte_accentue = \"Voici du texte avec des accents : Ã©Ã Ã¹Ã´ Ã§Ã±\"\n\n# Sauvegarder en UTF-8\nwith open('test_utf8.txt', 'w', encoding='utf-8') as f:\n    f.write(texte_accentue)\n\n# Sauvegarder en Latin-1\nwith open('test_latin1.txt', 'w', encoding='latin-1') as f:\n    f.write(texte_accentue)\n\n# Lire avec le bon encodage\nprint(\"âœ… Lecture UTF-8 :\")\nwith open('test_utf8.txt', 'r', encoding='utf-8') as f:\n    print(f.read())\n\nprint(\"\\nâœ… Lecture Latin-1 :\")\nwith open('test_latin1.txt', 'r', encoding='latin-1') as f:\n    print(f.read())\n\n\n\n\nCode\n# DÃ©tecter l'encodage automatiquement\n!pip install chardet\n\nimport chardet\n\n# DÃ©tecter l'encodage d'un fichier\nwith open('test_utf8.txt', 'rb') as f:\n    result = chardet.detect(f.read())\n    print(f\"Encodage dÃ©tectÃ© : {result['encoding']} (confiance: {result['confidence']*100:.1f}%)\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 2 : Texte et Regex",
    "text": "ğŸ¯ Exercice Pratique 2 : Texte et Regex\nObjectif : Nettoyer et valider des donnÃ©es clients\n\nCrÃ©er un DataFrame avec nom, email, tÃ©lÃ©phone\nNettoyer les noms (trim, capitaliser)\nValider les emails avec regex\nNormaliser les numÃ©ros de tÃ©lÃ©phone (format uniforme)\nExporter les donnÃ©es valides uniquement\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.1 Manipulation de JSON",
    "text": "3.1 Manipulation de JSON\n\n\nCode\nimport json\n\n# CrÃ©er un dictionnaire Python\ndata = {\n    \"nom\": \"Alice\",\n    \"age\": 30,\n    \"competences\": [\"Python\", \"SQL\", \"Pandas\"],\n    \"actif\": True\n}\n\n# Convertir en JSON\njson_str = json.dumps(data, indent=2)\nprint(\"ğŸ“„ JSON formatÃ© :\")\nprint(json_str)\n\n# Reconvertir en dictionnaire\ndata_reloaded = json.loads(json_str)\nprint(\"\\nğŸ”„ RechargÃ© :\")\nprint(data_reloaded)\n\n\n\n\nCode\n# Sauvegarder et lire des fichiers JSON\n\n# Sauvegarder\nwith open('data.json', 'w', encoding='utf-8') as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\nprint(\"âœ… JSON sauvegardÃ©\")\n\n# Lire\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data_loaded = json.load(f)\nprint(\"\\nğŸ“‚ JSON chargÃ© :\")\nprint(data_loaded)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.2 Appels API avec requests",
    "text": "3.2 Appels API avec requests\n\n\nCode\nimport requests\n\n# API publique gratuite : JSONPlaceholder\nurl = \"https://jsonplaceholder.typicode.com/users\"\n\n# GET Request\nresponse = requests.get(url)\n\n# VÃ©rifier le statut\nprint(f\"Status code: {response.status_code}\")\n\nif response.status_code == 200:\n    users = response.json()\n    print(f\"\\nâœ… {len(users)} utilisateurs rÃ©cupÃ©rÃ©s\")\n    print(\"\\nPremier utilisateur :\")\n    print(json.dumps(users[0], indent=2))\nelse:\n    print(\"âŒ Erreur lors de la requÃªte\")\n\n\n\n\nCode\n# Convertir en DataFrame\ndf_users = pd.json_normalize(users)\nprint(\"ğŸ‘¥ DataFrame des utilisateurs :\")\nprint(df_users.head())\nprint(f\"\\nColonnes : {df_users.columns.tolist()}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.3 Gestion des erreurs HTTP",
    "text": "3.3 Gestion des erreurs HTTP\n\n\nCode\ndef fetch_data_safe(url):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec gestion d'erreurs\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # LÃ¨ve une exception si statut &gt;= 400\n        return response.json()\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : le serveur met trop de temps Ã  rÃ©pondre\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        print(f\"âŒ Erreur HTTP : {e}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        print(f\"âŒ Erreur de connexion : {e}\")\n        return None\n\n# Test avec une URL valide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"âœ… DonnÃ©es rÃ©cupÃ©rÃ©es :\")\n    print(json.dumps(data, indent=2))\n\n# Test avec une URL invalide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/invalid\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.4 Authentification API",
    "text": "3.4 Authentification API\n\n\nCode\n# Exemple 1 : API Key dans les headers\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY_HERE\",\n    \"Content-Type\": \"application/json\"\n}\n\n# response = requests.get(url, headers=headers)\n\n# Exemple 2 : API Key dans les paramÃ¨tres\nparams = {\n    \"api_key\": \"YOUR_API_KEY_HERE\",\n    \"format\": \"json\"\n}\n\n# response = requests.get(url, params=params)\n\n# Exemple 3 : Basic Auth\nfrom requests.auth import HTTPBasicAuth\n\n# response = requests.get(url, auth=HTTPBasicAuth('username', 'password'))\n\nprint(\"ğŸ’¡ Les exemples ci-dessus montrent diffÃ©rentes mÃ©thodes d'authentification\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.5 Pagination dâ€™APIs",
    "text": "3.5 Pagination dâ€™APIs\n\n\nCode\ndef fetch_all_pages(base_url, max_pages=5):\n    \"\"\"RÃ©cupÃ¨re toutes les pages d'une API paginÃ©e\"\"\"\n    all_data = []\n    \n    for page in range(1, max_pages + 1):\n        url = f\"{base_url}?_page={page}&_limit=10\"\n        print(f\"ğŸ“„ RÃ©cupÃ©ration page {page}...\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            if not data:  # Plus de donnÃ©es\n                break\n            all_data.extend(data)\n        else:\n            print(f\"âŒ Erreur page {page}\")\n            break\n    \n    return all_data\n\n# Test avec JSONPlaceholder\nposts = fetch_all_pages(\"https://jsonplaceholder.typicode.com/posts\", max_pages=3)\nprint(f\"\\nâœ… Total rÃ©cupÃ©rÃ© : {len(posts)} posts\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.6 Rate Limiting et Retry Logic",
    "text": "3.6 Rate Limiting et Retry Logic\n\n\nCode\nimport time\nfrom datetime import datetime\n\ndef fetch_with_retry(url, max_retries=3, delay=2):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec retry et backoff exponentiel\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"ğŸ”„ Tentative {attempt + 1}/{max_retries}\")\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            print(f\"âŒ Erreur : {e}\")\n            if attempt &lt; max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Backoff exponentiel\n                print(f\"â³ Attente de {wait_time}s avant nouvelle tentative...\")\n                time.sleep(wait_time)\n            else:\n                print(\"âŒ Ã‰chec aprÃ¨s toutes les tentatives\")\n                return None\n\n# Test\ndata = fetch_with_retry(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"\\nâœ… SuccÃ¨s !\")\n\n\n\n\nCode\n# Rate limiting simple\ndef fetch_with_rate_limit(urls, requests_per_second=2):\n    \"\"\"RÃ©cupÃ¨re plusieurs URLs en respectant un rate limit\"\"\"\n    delay = 1.0 / requests_per_second\n    results = []\n    \n    for url in urls:\n        start = time.time()\n        print(f\"â¬ RÃ©cupÃ©ration : {url}\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            results.append(response.json())\n        \n        elapsed = time.time() - start\n        sleep_time = max(0, delay - elapsed)\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n    \n    return results\n\n# Test\nurls = [\n    \"https://jsonplaceholder.typicode.com/users/1\",\n    \"https://jsonplaceholder.typicode.com/users/2\",\n    \"https://jsonplaceholder.typicode.com/users/3\"\n]\n\nstart_time = time.time()\nresults = fetch_with_rate_limit(urls, requests_per_second=1)\ntotal_time = time.time() - start_time\n\nprint(f\"\\nâœ… {len(results)} URLs rÃ©cupÃ©rÃ©es en {total_time:.2f}s\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "3.7 JSON imbriquÃ© complexe",
    "text": "3.7 JSON imbriquÃ© complexe\n\n\nCode\n# JSON complexe imbriquÃ©\ncomplex_json = {\n    \"id\": 1,\n    \"nom\": \"Entreprise A\",\n    \"employes\": [\n        {\n            \"id\": 101,\n            \"nom\": \"Alice\",\n            \"competences\": [\"Python\", \"SQL\"],\n            \"adresse\": {\"ville\": \"Paris\", \"code_postal\": \"75001\"}\n        },\n        {\n            \"id\": 102,\n            \"nom\": \"Bob\",\n            \"competences\": [\"Java\", \"Docker\"],\n            \"adresse\": {\"ville\": \"Lyon\", \"code_postal\": \"69001\"}\n        }\n    ]\n}\n\n# Normaliser avec json_normalize\ndf_complex = pd.json_normalize(\n    complex_json,\n    record_path='employes',\n    meta=['nom'],\n    meta_prefix='entreprise_'\n)\n\nprint(\"ğŸ”„ JSON normalisÃ© :\")\nprint(df_complex)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 3 : APIs",
    "text": "ğŸ¯ Exercice Pratique 3 : APIs\nObjectif : RÃ©cupÃ©rer et analyser des donnÃ©es dâ€™une API publique\n\nUtiliser lâ€™API JSONPlaceholder pour rÃ©cupÃ©rer les posts\nConvertir en DataFrame\nCompter le nombre de posts par utilisateur\nRÃ©cupÃ©rer les dÃ©tails des 5 utilisateurs les plus actifs\nExporter le rÃ©sultat en JSON\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.1 VÃ©rifications basiques",
    "text": "4.1 VÃ©rifications basiques\n\n\nCode\n# CrÃ©er des donnÃ©es de test\ntest_data = pd.DataFrame({\n    'user_id': [1, 2, 3, 2, 5],\n    'email': ['alice@test.com', 'bob@test', None, 'bob@test', 'eve@test.com'],\n    'age': [25, 150, -5, 30, 28],\n    'salaire': [45000, 55000, 60000, 55000, None]\n})\n\nprint(\"ğŸ“Š DonnÃ©es de test :\")\nprint(test_data)\n\n# VÃ©rifications\nprint(\"\\nğŸ” VÃ©rifications :\")\nprint(f\"Colonnes manquantes : {set(['user_id', 'email', 'age', 'salaire']) - set(test_data.columns)}\")\nprint(f\"Valeurs nulles : {test_data.isnull().sum().sum()}\")\nprint(f\"Doublons : {test_data.duplicated().sum()}\")\nprint(f\"Types : \\n{test_data.dtypes}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.2 Classe de validation complÃ¨te",
    "text": "4.2 Classe de validation complÃ¨te\n\n\nCode\nclass DataValidator:\n    \"\"\"Validateur simple pour DataFrames\"\"\"\n    \n    def __init__(self, df):\n        self.df = df\n        self.errors = []\n    \n    def check_columns(self, required_columns):\n        \"\"\"VÃ©rifie prÃ©sence des colonnes requises\"\"\"\n        missing = set(required_columns) - set(self.df.columns)\n        if missing:\n            self.errors.append(f\"Colonnes manquantes: {missing}\")\n            return False\n        return True\n    \n    def check_nulls(self, max_null_pct=10):\n        \"\"\"VÃ©rifie le pourcentage de valeurs nulles\"\"\"\n        null_pct = (self.df.isnull().sum() / len(self.df)) * 100\n        violations = null_pct[null_pct &gt; max_null_pct]\n        if not violations.empty:\n            self.errors.append(f\"Trop de nulls: {violations.to_dict()}\")\n            return False\n        return True\n    \n    def check_range(self, column, min_val, max_val):\n        \"\"\"VÃ©rifie que les valeurs sont dans une plage\"\"\"\n        if column in self.df.columns:\n            violations = self.df[(self.df[column] &lt; min_val) | (self.df[column] &gt; max_val)]\n            if len(violations) &gt; 0:\n                self.errors.append(f\"{column}: {len(violations)} valeurs hors plage [{min_val}, {max_val}]\")\n                return False\n        return True\n    \n    def check_duplicates(self, subset=None):\n        \"\"\"VÃ©rifie les doublons\"\"\"\n        duplicates = self.df.duplicated(subset=subset).sum()\n        if duplicates &gt; 0:\n            self.errors.append(f\"{duplicates} doublons trouvÃ©s\")\n            return False\n        return True\n    \n    def check_types(self, column, expected_type):\n        \"\"\"VÃ©rifie le type d'une colonne\"\"\"\n        if column in self.df.columns:\n            if self.df[column].dtype != expected_type:\n                self.errors.append(f\"{column}: type attendu {expected_type}, obtenu {self.df[column].dtype}\")\n                return False\n        return True\n    \n    def validate(self):\n        \"\"\"Retourne True si valide, False sinon\"\"\"\n        return len(self.errors) == 0\n    \n    def report(self):\n        \"\"\"GÃ©nÃ¨re un rapport de validation\"\"\"\n        return {\n            'is_valid': self.validate(),\n            'total_errors': len(self.errors),\n            'errors': self.errors\n        }\n\n# Utilisation\nvalidator = DataValidator(test_data)\nvalidator.check_columns(['user_id', 'email', 'age'])\nvalidator.check_nulls(max_null_pct=15)\nvalidator.check_range('age', 0, 120)\nvalidator.check_duplicates(subset=['user_id', 'email'])\n\nreport = validator.report()\nprint(\"\\nğŸ“‹ Rapport de validation:\")\nprint(json.dumps(report, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "4.3 Validation avec schÃ©ma",
    "text": "4.3 Validation avec schÃ©ma\n\n\nCode\n# DÃ©finir un schÃ©ma de validation\nschema = {\n    'user_id': {'type': 'int64', 'nullable': False, 'unique': True},\n    'email': {'type': 'object', 'nullable': False, 'pattern': r'.+@.+\\..+'},\n    'age': {'type': 'int64', 'nullable': False, 'min': 0, 'max': 120},\n    'salaire': {'type': 'int64', 'nullable': True, 'min': 0}\n}\n\ndef validate_schema(df, schema):\n    \"\"\"Valide un DataFrame contre un schÃ©ma\"\"\"\n    errors = []\n    \n    for column, rules in schema.items():\n        # VÃ©rifier si la colonne existe\n        if column not in df.columns:\n            errors.append(f\"Colonne manquante: {column}\")\n            continue\n        \n        # VÃ©rifier les nulls\n        if not rules.get('nullable', True) and df[column].isnull().any():\n            errors.append(f\"{column}: contient des valeurs nulles\")\n        \n        # VÃ©rifier l'unicitÃ©\n        if rules.get('unique', False) and df[column].duplicated().any():\n            errors.append(f\"{column}: contient des doublons\")\n        \n        # VÃ©rifier la plage\n        if 'min' in rules:\n            violations = df[df[column] &lt; rules['min']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &lt; {rules['min']}\")\n        \n        if 'max' in rules:\n            violations = df[df[column] &gt; rules['max']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &gt; {rules['max']}\")\n        \n        # VÃ©rifier le pattern (pour les strings)\n        if 'pattern' in rules:\n            pattern = rules['pattern']\n            invalid = df[column].dropna()[~df[column].dropna().str.match(pattern)]\n            if len(invalid) &gt; 0:\n                errors.append(f\"{column}: {len(invalid)} valeurs ne matchent pas le pattern\")\n    \n    return {\n        'is_valid': len(errors) == 0,\n        'errors': errors\n    }\n\n# Test\nresult = validate_schema(test_data, schema)\nprint(\"\\nğŸ“‹ Validation avec schÃ©ma :\")\nprint(json.dumps(result, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Pratique 4 : Validation",
    "text": "ğŸ¯ Exercice Pratique 4 : Validation\nObjectif : CrÃ©er un validateur pour des transactions\n\nCrÃ©er un DataFrame de transactions (id, date, montant, type)\nDÃ©finir un schÃ©ma de validation\nValider que toutes les transactions ont un montant positif\nVÃ©rifier quâ€™il nâ€™y a pas de doublons dâ€™ID\nGÃ©nÃ©rer un rapport de qualitÃ©\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.1 Architecture du pipeline",
    "text": "5.1 Architecture du pipeline\n\n\nCode\n# CrÃ©er la structure de dossiers\nfrom pathlib import Path\n\ndirs = ['data/raw', 'data/processed', 'data/output', 'logs']\nfor dir_path in dirs:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"âœ… Structure de dossiers crÃ©Ã©e\")\nprint(\"\\nğŸ“ Structure :\")\nprint(\"\"\"\nproject/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”œâ”€â”€ processed/\nâ”‚   â””â”€â”€ output/\nâ””â”€â”€ logs/\n\"\"\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.2 Configuration et Logging",
    "text": "5.2 Configuration et Logging\n\n\nCode\nimport logging\nfrom datetime import datetime\n\n# Configuration du logging\nlog_file = f\"logs/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger('ETL_Pipeline')\nlogger.info(\"ğŸš€ Pipeline dÃ©marrÃ©\")\n\n\n\n\nCode\n# Configuration centralisÃ©e\nclass Config:\n    \"\"\"Configuration du pipeline\"\"\"\n    # Chemins\n    RAW_DATA_DIR = 'data/raw'\n    PROCESSED_DATA_DIR = 'data/processed'\n    OUTPUT_DIR = 'data/output'\n    \n    # API\n    API_URL = 'https://jsonplaceholder.typicode.com'\n    API_TIMEOUT = 10\n    API_MAX_RETRIES = 3\n    \n    # Validation\n    MAX_NULL_PCT = 10\n    \n    # Export\n    EXPORT_FORMATS = ['csv', 'parquet', 'json']\n\nconfig = Config()\nlogger.info(\"âš™ï¸ Configuration chargÃ©e\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.3 Ã‰tape 1 : Extract",
    "text": "5.3 Ã‰tape 1 : Extract\n\n\nCode\ndef extract_from_api(url, max_retries=3):\n    \"\"\"Extrait des donnÃ©es depuis une API\"\"\"\n    logger.info(f\"ğŸ“¥ Extraction depuis {url}\")\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=config.API_TIMEOUT)\n            response.raise_for_status()\n            data = response.json()\n            logger.info(f\"âœ… {len(data)} enregistrements extraits\")\n            return data\n        except Exception as e:\n            logger.warning(f\"âš ï¸ Tentative {attempt + 1}/{max_retries} Ã©chouÃ©e: {e}\")\n            if attempt == max_retries - 1:\n                logger.error(\"âŒ Extraction Ã©chouÃ©e\")\n                raise\n            time.sleep(2 ** attempt)\n\n# Test extraction\nusers_data = extract_from_api(f\"{config.API_URL}/users\")\ndf_raw = pd.DataFrame(users_data)\n\n# Sauvegarder les donnÃ©es brutes\nraw_file = f\"{config.RAW_DATA_DIR}/users_raw_{datetime.now().strftime('%Y%m%d')}.csv\"\ndf_raw.to_csv(raw_file, index=False)\nlogger.info(f\"ğŸ’¾ DonnÃ©es brutes sauvegardÃ©es: {raw_file}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.4 Ã‰tape 2 : Transform",
    "text": "5.4 Ã‰tape 2 : Transform\n\n\nCode\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    logger.info(\"ğŸ”„ DÃ©but de la transformation\")\n    \n    df_transformed = df.copy()\n    \n    # 1. Normaliser les colonnes imbriquÃ©es\n    if 'address' in df.columns:\n        address_df = pd.json_normalize(df['address'])\n        address_df.columns = ['address_' + col for col in address_df.columns]\n        df_transformed = pd.concat([df_transformed.drop('address', axis=1), address_df], axis=1)\n        logger.info(\"âœ… Colonnes adresse normalisÃ©es\")\n    \n    # 2. Nettoyer les noms de colonnes\n    df_transformed.columns = df_transformed.columns.str.lower().str.replace('.', '_')\n    logger.info(\"âœ… Noms de colonnes nettoyÃ©s\")\n    \n    # 3. GÃ©rer les valeurs manquantes\n    null_counts = df_transformed.isnull().sum()\n    if null_counts.sum() &gt; 0:\n        logger.warning(f\"âš ï¸ {null_counts.sum()} valeurs manquantes dÃ©tectÃ©es\")\n        df_transformed = df_transformed.dropna()\n        logger.info(\"âœ… Valeurs manquantes supprimÃ©es\")\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    if 'name' in df_transformed.columns:\n        df_transformed['name_length'] = df_transformed['name'].str.len()\n        logger.info(\"âœ… Colonne dÃ©rivÃ©e 'name_length' crÃ©Ã©e\")\n    \n    # 5. Ajouter metadata\n    df_transformed['processed_at'] = datetime.now().isoformat()\n    \n    logger.info(f\"âœ… Transformation terminÃ©e: {len(df_transformed)} lignes\")\n    return df_transformed\n\n# Test transformation\ndf_transformed = transform_data(df_raw)\nprint(\"\\nğŸ“Š DonnÃ©es transformÃ©es :\")\nprint(df_transformed.head())\nprint(f\"\\nColonnes: {df_transformed.columns.tolist()}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.5 Ã‰tape 3 : Validate",
    "text": "5.5 Ã‰tape 3 : Validate\n\n\nCode\ndef validate_data(df):\n    \"\"\"Valide la qualitÃ© des donnÃ©es\"\"\"\n    logger.info(\"ğŸ” DÃ©but de la validation\")\n    \n    validator = DataValidator(df)\n    \n    # DÃ©finir les rÃ¨gles de validation\n    required_columns = ['id', 'name', 'email']\n    validator.check_columns(required_columns)\n    validator.check_nulls(max_null_pct=config.MAX_NULL_PCT)\n    validator.check_duplicates(subset=['id'])\n    \n    # GÃ©nÃ©rer le rapport\n    report = validator.report()\n    \n    if report['is_valid']:\n        logger.info(\"âœ… Validation rÃ©ussie\")\n    else:\n        logger.error(f\"âŒ Validation Ã©chouÃ©e: {report['total_errors']} erreurs\")\n        for error in report['errors']:\n            logger.error(f\"  - {error}\")\n    \n    return report\n\n# Test validation\nvalidation_report = validate_data(df_transformed)\nprint(\"\\nğŸ“‹ Rapport de validation :\")\nprint(json.dumps(validation_report, indent=2))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.6 Ã‰tape 4 : Load",
    "text": "5.6 Ã‰tape 4 : Load\n\n\nCode\ndef load_data(df, base_filename):\n    \"\"\"Exporte les donnÃ©es dans plusieurs formats\"\"\"\n    logger.info(\"ğŸ’¾ DÃ©but de l'export\")\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    files_created = []\n    \n    for format_type in config.EXPORT_FORMATS:\n        filename = f\"{config.OUTPUT_DIR}/{base_filename}_{timestamp}.{format_type}\"\n        \n        try:\n            if format_type == 'csv':\n                df.to_csv(filename, index=False)\n            elif format_type == 'parquet':\n                df.to_parquet(filename, index=False)\n            elif format_type == 'json':\n                df.to_json(filename, orient='records', indent=2)\n            \n            file_size = Path(filename).stat().st_size / 1024  # KB\n            logger.info(f\"âœ… Export {format_type.upper()}: {filename} ({file_size:.2f} KB)\")\n            files_created.append(filename)\n        except Exception as e:\n            logger.error(f\"âŒ Erreur export {format_type}: {e}\")\n    \n    return files_created\n\n# Test export\nexported_files = load_data(df_transformed, 'users_processed')\nprint(\"\\nğŸ“¦ Fichiers exportÃ©s :\")\nfor file in exported_files:\n    print(f\"  - {file}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "5.7 Pipeline complet",
    "text": "5.7 Pipeline complet\n\n\nCode\ndef run_pipeline():\n    \"\"\"ExÃ©cute le pipeline complet\"\"\"\n    start_time = time.time()\n    logger.info(\"=\"*50)\n    logger.info(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE\")\n    logger.info(\"=\"*50)\n    \n    try:\n        # EXTRACT\n        logger.info(\"\\nğŸ“¥ PHASE 1: EXTRACTION\")\n        data = extract_from_api(f\"{config.API_URL}/users\")\n        df = pd.DataFrame(data)\n        logger.info(f\"Lignes extraites: {len(df)}\")\n        \n        # TRANSFORM\n        logger.info(\"\\nğŸ”„ PHASE 2: TRANSFORMATION\")\n        df_clean = transform_data(df)\n        logger.info(f\"Lignes aprÃ¨s transformation: {len(df_clean)}\")\n        \n        # VALIDATE\n        logger.info(\"\\nğŸ” PHASE 3: VALIDATION\")\n        validation = validate_data(df_clean)\n        \n        if not validation['is_valid']:\n            logger.error(\"âŒ Validation Ã©chouÃ©e, arrÃªt du pipeline\")\n            return False\n        \n        # LOAD\n        logger.info(\"\\nğŸ’¾ PHASE 4: EXPORT\")\n        files = load_data(df_clean, 'users_final')\n        \n        # STATISTIQUES\n        duration = time.time() - start_time\n        logger.info(\"\\n\" + \"=\"*50)\n        logger.info(\"ğŸ“Š STATISTIQUES DU PIPELINE\")\n        logger.info(\"=\"*50)\n        logger.info(f\"DurÃ©e totale: {duration:.2f}s\")\n        logger.info(f\"Lignes traitÃ©es: {len(df_clean)}\")\n        logger.info(f\"Fichiers crÃ©Ã©s: {len(files)}\")\n        logger.info(f\"Taux de rÃ©ussite: 100%\")\n        logger.info(\"=\"*50)\n        logger.info(\"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        logger.info(\"=\"*50)\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"âŒ ERREUR FATALE: {e}\")\n        logger.exception(\"Stack trace:\")\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ¯ Exercice Final : Pipeline Complet",
    "text": "ğŸ¯ Exercice Final : Pipeline Complet\nObjectif : CrÃ©er votre propre pipeline ETL\n\nExtraire des donnÃ©es de posts depuis JSONPlaceholder\nEnrichir avec les donnÃ©es utilisateurs\nCalculer des statistiques (posts par utilisateur, mots par post, etc.)\nValider la qualitÃ©\nExporter dans tous les formats\nAjouter un logging complet\n\n\n\nCode\n# Ã€ VOUS DE JOUER ! ğŸ®\n# CrÃ©ez votre pipeline complet ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "6ï¸âƒ£ Gestion des configurations",
    "text": "6ï¸âƒ£ Gestion des configurations\n\n\nCode\n# Installer python-dotenv\n!pip install python-dotenv\n\n# CrÃ©er un fichier .env (Ã  ne JAMAIS commiter)\nenv_content = \"\"\"\nAPI_KEY=votre_cle_api_secrete\nDATABASE_URL=postgresql://user:password@localhost:5432/db\nENVIRONMENT=development\n\"\"\"\n\nwith open('.env', 'w') as f:\n    f.write(env_content)\n\nprint(\"âœ… Fichier .env crÃ©Ã©\")\nprint(\"âš ï¸ N'oubliez pas d'ajouter .env Ã  votre .gitignore !\")\n\n\n\n\nCode\nfrom dotenv import load_dotenv\nimport os\n\n# Charger les variables d'environnement\nload_dotenv()\n\n# AccÃ©der aux variables\napi_key = os.getenv('API_KEY')\ndb_url = os.getenv('DATABASE_URL')\nenv = os.getenv('ENVIRONMENT')\n\nprint(f\"ğŸ”‘ API Key: {api_key[:10]}...\")\nprint(f\"ğŸ—„ï¸ Database URL: {db_url[:30]}...\")\nprint(f\"ğŸŒ Environment: {env}\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "7ï¸âƒ£ Tests unitaires basiques",
    "text": "7ï¸âƒ£ Tests unitaires basiques\n\n\nCode\n# Exemple de fonction Ã  tester\ndef calculer_age_moyen(df, colonne='age'):\n    \"\"\"Calcule l'Ã¢ge moyen d'un DataFrame\"\"\"\n    if colonne not in df.columns:\n        raise ValueError(f\"Colonne '{colonne}' introuvable\")\n    return df[colonne].mean()\n\n# Tests\ndef test_calculer_age_moyen():\n    # Test avec donnÃ©es valides\n    df_test = pd.DataFrame({'age': [20, 30, 40]})\n    assert calculer_age_moyen(df_test) == 30, \"Test 1 Ã©chouÃ©\"\n    print(\"âœ… Test 1: donnÃ©es valides\")\n    \n    # Test avec colonne manquante\n    try:\n        calculer_age_moyen(pd.DataFrame({'nom': ['Alice']}), 'age')\n        print(\"âŒ Test 2 Ã©chouÃ©: devrait lever une exception\")\n    except ValueError:\n        print(\"âœ… Test 2: exception levÃ©e correctement\")\n    \n    # Test avec valeurs nulles\n    df_null = pd.DataFrame({'age': [20, None, 40]})\n    result = calculer_age_moyen(df_null)\n    assert result == 30, \"Test 3 Ã©chouÃ©\"\n    print(\"âœ… Test 3: gestion des nulls\")\n    \n    print(\"\\nğŸ‰ Tous les tests passent !\")\n\n# ExÃ©cuter les tests\ntest_calculer_age_moyen()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "Ce que tu as appris âœ…",
    "text": "Ce que tu as appris âœ…\n\n\n\n\n\n\n\nSection\nCompÃ©tences acquises\n\n\n\n\nPandas\nManipulation de donnÃ©es, nettoyage, agrÃ©gations, merges\n\n\nMatplotlib\nGraphiques de base, personnalisation, export\n\n\nSeaborn\nVisualisations statistiques, heatmaps, pair plots\n\n\nTexte & Regex\nNettoyage, parsing de logs, expressions rÃ©guliÃ¨res\n\n\nAPIs\nAppels REST, pagination, retry logic\n\n\nValidation\nSchÃ©mas, checks de qualitÃ©\n\n\nPipeline ETL\nArchitecture complÃ¨te Extract-Transform-Load\n\n\nBonnes pratiques\nLogging, configuration, tests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nDocumentation officielle\n\nPandas Documentation\nMatplotlib Documentation\nSeaborn Documentation\nRequests Documentation\n\n\n\nTutoriels et cours\n\nReal Python - Pandas\nKaggle Learn\nDataCamp\n\n\n\nOutils avancÃ©s Ã  explorer\n\nPolars â€” Alternative plus rapide Ã  Pandas\nGreat Expectations â€” Validation de donnÃ©es avancÃ©e\nPandera â€” SchÃ©mas de validation pour DataFrames",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“Š Python for Data Processing - Complete Guide",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises le traitement de donnÃ©es, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 06_intro_databases.ipynb â€” Introduction aux bases de donnÃ©es\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Data Processing pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“Š Python for Data Processing - Complete Guide"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html",
    "href": "notebooks/beginner/06_intro_relational_databases.html",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "",
    "text": "Ce module prÃ©sente les concepts fondamentaux des bases de donnÃ©es relationnelles.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 05_python_data_processing\n\n\nâœ… Requis\nComprendre les structures de donnÃ©es (listes, dictionnaires)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "href": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Expliquer ce quâ€™est une base de donnÃ©es\nâœ… Comprendre le modÃ¨le relationnel (tables, colonnes, lignes)\nâœ… DÃ©finir les clÃ©s primaires et Ã©trangÃ¨res\nâœ… Identifier les types de relations (1-1, 1-N, N-N)\nâœ… Comprendre les principes de normalisation\nâœ… Expliquer les propriÃ©tÃ©s ACID\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique SQL viendra au module suivant !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ¯ 1. Câ€™est quoi une Base de DonnÃ©es ?",
    "text": "ğŸ¯ 1. Câ€™est quoi une Base de DonnÃ©es ?\n\nğŸ“– DÃ©finition\nUne base de donnÃ©es est un systÃ¨me organisÃ© pour :\n\n\n\nFonction\nDescription\n\n\n\n\nğŸ’¾ Stocker\nConserver des informations de faÃ§on permanente\n\n\nğŸ” Rechercher\nRetrouver rapidement nâ€™importe quelle donnÃ©e\n\n\nâœï¸ Modifier\nMettre Ã  jour les informations\n\n\nğŸ”’ SÃ©curiser\nContrÃ´ler lâ€™accÃ¨s aux donnÃ©es sensibles\n\n\nğŸ”— Relier\nConnecter diffÃ©rentes informations entre elles\n\n\n\n\n\n\nğŸŒŸ En une phrase\n\nâ€œUne base de donnÃ©es, câ€™est comme un classeur numÃ©rique gÃ©ant, ultra-organisÃ© et intelligent, capable de retrouver nâ€™importe quelle information parmi des milliards de donnÃ©es.â€\n\n\n\n\nğŸ“ Fichiers vs Base de donnÃ©es\nPourquoi ne pas simplement utiliser des fichiers CSV ou Excel ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nFichiers (CSV, Excel)\nBase de donnÃ©es\n\n\n\n\nAccÃ¨s concurrent\nâŒ Conflits si plusieurs utilisateurs\nâœ… GÃ©rÃ© automatiquement\n\n\nVolume\nâŒ Lent au-delÃ  de ~100K lignes\nâœ… Millions/milliards de lignes\n\n\nIntÃ©gritÃ©\nâŒ Pas de validation\nâœ… Contraintes, types\n\n\nRelations\nâŒ Difficile Ã  gÃ©rer\nâœ… Jointures natives\n\n\nSÃ©curitÃ©\nâŒ Tout ou rien\nâœ… Permissions fines\n\n\nSauvegarde\nâŒ Manuelle\nâœ… Automatique",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ–¥ï¸ 2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es",
    "text": "ğŸ–¥ï¸ 2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es\nUn SGBD (ou DBMS en anglais) est le logiciel qui gÃ¨re la base de donnÃ©es.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        APPLICATION                          â”‚\nâ”‚                  (Python, Java, Web...)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ RequÃªtes SQL\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          SGBD                               â”‚\nâ”‚              (PostgreSQL, MySQL, Oracle...)                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\nâ”‚  â”‚   Parser    â”‚  â”‚  Optimizer  â”‚  â”‚   Engine    â”‚         â”‚\nâ”‚  â”‚   (SQL)     â”‚  â”‚  (requÃªtes) â”‚  â”‚  (stockage) â”‚         â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     FICHIERS DISQUE                         â”‚\nâ”‚                   (donnÃ©es stockÃ©es)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ† SGBD Relationnels populaires\n\n\n\n\n\n\n\n\n\nSGBD\nType\nPoints forts\nCas dâ€™usage\n\n\n\n\nPostgreSQL\nOpen source\nPuissant, extensible, SQL avancÃ©\nProduction, analytics\n\n\nMySQL\nOpen source\nSimple, rapide, trÃ¨s rÃ©pandu\nWeb, startups\n\n\nSQLite\nEmbarquÃ©\nLÃ©ger, fichier unique, zÃ©ro config\nMobile, prototypage\n\n\nOracle\nCommercial\nEntreprise, haute disponibilitÃ©\nBanques, grandes entreprises\n\n\nSQL Server\nCommercial\nIntÃ©gration Microsoft\nEntreprises Windows\n\n\n\n\nğŸ’¡ Pour ce cours, on utilisera PostgreSQL â€” le plus complet et gratuit.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“Š 3. Le ModÃ¨le Relationnel",
    "text": "ğŸ“Š 3. Le ModÃ¨le Relationnel\nLe modÃ¨le relationnel a Ã©tÃ© inventÃ© par Edgar F. Codd (IBM) en 1970. Câ€™est le modÃ¨le le plus utilisÃ© depuis 50 ans !\n\nğŸ§± Vocabulaire de base\nBASE DE DONNÃ‰ES : ma_boutique\nâ”‚\nâ”œâ”€â”€ TABLE : clients\nâ”‚   â”‚\nâ”‚   â”‚    COLONNES (attributs)\nâ”‚   â”‚    â†“      â†“         â†“\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   â”‚  â”‚ id  â”‚  nom   â”‚     email       â”‚  â† EN-TÃŠTE\nâ”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   â”‚  â”‚  1  â”‚ Alice  â”‚ alice@mail.com  â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  2  â”‚ Bob    â”‚ bob@mail.com    â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  3  â”‚ Charlieâ”‚ charlie@mail.comâ”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚   â”‚    â†‘\nâ”‚   â”‚    CELLULE (valeur)\nâ”‚\nâ”œâ”€â”€ TABLE : produits\nâ”‚   â””â”€â”€ ...\nâ”‚\nâ””â”€â”€ TABLE : commandes\n    â””â”€â”€ ...\n\n\n\nğŸ“š Terminologie\n\n\n\nTerme technique\nTerme courant\nDescription\n\n\n\n\nRelation\nTable\nEnsemble de donnÃ©es du mÃªme type\n\n\nTuple\nLigne / Enregistrement\nUne entrÃ©e (ex: un client)\n\n\nAttribut\nColonne / Champ\nUne propriÃ©tÃ© (ex: nom, email)\n\n\nDomaine\nType\nValeurs possibles (INTEGER, VARCHARâ€¦)\n\n\nSchÃ©ma\nStructure\nDÃ©finition des colonnes et types",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”¢ 4. Types de donnÃ©es",
    "text": "ğŸ”¢ 4. Types de donnÃ©es\nChaque colonne a un type qui dÃ©finit les valeurs autorisÃ©es.\n\nğŸ“‹ Types courants (PostgreSQL)\n\n\n\nCatÃ©gorie\nType\nDescription\nExemple\n\n\n\n\nEntiers\nINTEGER\nNombre entier\n42\n\n\n\nBIGINT\nGrand entier\n9223372036854775807\n\n\n\nSMALLINT\nPetit entier\n32767 max\n\n\nDÃ©cimaux\nDECIMAL(p,s)\nPrÃ©cision exacte\n19.99\n\n\n\nFLOAT\nApproximatif\n3.14159\n\n\nTexte\nVARCHAR(n)\nTexte variable (max n)\n'Alice'\n\n\n\nTEXT\nTexte illimitÃ©\n'Long texte...'\n\n\n\nCHAR(n)\nTexte fixe (n caractÃ¨res)\n'FR'\n\n\nBoolÃ©en\nBOOLEAN\nVrai/Faux\nTRUE, FALSE\n\n\nDate/Heure\nDATE\nDate seule\n'2024-01-15'\n\n\n\nTIMESTAMP\nDate + heure\n'2024-01-15 14:30:00'\n\n\n\nTIME\nHeure seule\n'14:30:00'\n\n\nAutres\nUUID\nIdentifiant unique\n'a0eebc99-9c0b...'\n\n\n\nJSON\nDonnÃ©es JSON\n'{\"key\": \"value\"}'\n\n\n\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nDECIMAL pour lâ€™argent\nFLOAT pour lâ€™argent (imprÃ©cis)\n\n\nVARCHAR(255) pour emails\nTEXT partout (pas de limite)\n\n\nDATE pour les dates\nVARCHAR pour les dates\n\n\nTypes les plus petits possibles\nTypes trop grands â€œau cas oÃ¹â€",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”‘ 5. ClÃ© Primaire (Primary Key)",
    "text": "ğŸ”‘ 5. ClÃ© Primaire (Primary Key)\nLa clÃ© primaire (PK) identifie de faÃ§on unique chaque ligne dâ€™une table.\n\nğŸ“ RÃ¨gles\n\n\n\nRÃ¨gle\nDescription\n\n\n\n\nUnique\nDeux lignes ne peuvent pas avoir la mÃªme valeur\n\n\nNon NULL\nLa valeur doit toujours Ãªtre prÃ©sente\n\n\nImmuable\nNe devrait jamais changer\n\n\n\n\n\n\nğŸ“Š Exemple\nTable : clients\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚      email      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚ alice@mail.com  â”‚\nâ”‚    2    â”‚  Bob     â”‚ bob@mail.com    â”‚\nâ”‚    3    â”‚  Charlie â”‚ charlie@mail.comâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â†‘\n     ClÃ© primaire : garantit l'unicitÃ©\n\n\n\nğŸ”¢ Types de clÃ©s primaires\n\n\n\n\n\n\n\n\nType\nDescription\nExemple\n\n\n\n\nAuto-incrÃ©mentÃ©e\nGÃ©nÃ©rÃ©e automatiquement (1, 2, 3â€¦)\nSERIAL en PostgreSQL\n\n\nUUID\nIdentifiant universel unique\na0eebc99-9c0b-4ef8...\n\n\nNaturelle\nDonnÃ©e existante unique\nNumÃ©ro de sÃ©curitÃ© sociale\n\n\nComposite\nPlusieurs colonnes combinÃ©es\n(pays, code_postal)\n\n\n\n\nğŸ’¡ Recommandation : Utiliser SERIAL (auto-increment) ou UUID plutÃ´t quâ€™une clÃ© naturelle.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”— 6. ClÃ© Ã‰trangÃ¨re (Foreign Key)",
    "text": "ğŸ”— 6. ClÃ© Ã‰trangÃ¨re (Foreign Key)\nLa clÃ© Ã©trangÃ¨re (FK) crÃ©e un lien entre deux tables.\n\nğŸ“ Principe\nTable : clients                    Table : commandes\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚            â”‚ id (PK) â”‚ client_id(FK)â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    1    â”‚      1      â”‚ Clavier â”‚\nâ”‚    2    â”‚  Bob     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    2    â”‚      2      â”‚ Souris  â”‚\nâ”‚    3    â”‚  Charlie â”‚            â”‚    3    â”‚      1      â”‚ Ã‰cran   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                              â†‘\n                                    ClÃ© Ã©trangÃ¨re : rÃ©fÃ©rence clients.id\n\n\n\nâœ… Ce que garantit la FK\n\n\n\n\n\n\n\nGarantie\nDescription\n\n\n\n\nIntÃ©gritÃ© rÃ©fÃ©rentielle\nImpossible de rÃ©fÃ©rencer un client inexistant\n\n\nCohÃ©rence\nSi on supprime un client, que faire des commandes ?\n\n\n\n\n\n\nğŸ—‘ï¸ Actions en cascade\nQue se passe-t-il si on supprime ou modifie la ligne rÃ©fÃ©rencÃ©e ?\n\n\n\nAction\nComportement\n\n\n\n\nCASCADE\nSupprime/modifie aussi les lignes liÃ©es\n\n\nSET NULL\nMet la FK Ã  NULL\n\n\nSET DEFAULT\nMet une valeur par dÃ©faut\n\n\nRESTRICT\nInterdit la suppression/modification\n\n\nNO ACTION\nComme RESTRICT (vÃ©rification diffÃ©rÃ©e)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ”€ 7. Types de Relations",
    "text": "ğŸ”€ 7. Types de Relations\n\n1ï¸âƒ£ Relation Un-Ã -Un (1:1)\nUne ligne dans A correspond Ã  exactement une ligne dans B.\nutilisateurs              profils\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  email  â”‚         â”‚ id â”‚ user_id â”‚   bio     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ a@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1  â”‚    1    â”‚ Dev...    â”‚\nâ”‚ 2  â”‚ b@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2  â”‚    2    â”‚ Designer..â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : SÃ©parer des donnÃ©es rarement utilisÃ©es (optimisation).\n\n\n\n1ï¸âƒ£â¡ï¸ğŸ”¢ Relation Un-Ã -Plusieurs (1:N)\nUne ligne dans A peut correspondre Ã  plusieurs lignes dans B.\nclients                   commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚         â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚  Alice  â”‚â—„â”€â”€â”€â”€â”¬â”€â”€â”€â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚  Bob    â”‚â—„â”€â”€â” â””â”€â”€â”€â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : Client â†’ Commandes, Auteur â†’ Articles, Pays â†’ Villes.\n\n\n\nğŸ”¢â†”ï¸ï¸ğŸ”¢ Relation Plusieurs-Ã -Plusieurs (N:N)\nPlusieurs lignes dans A correspondent Ã  plusieurs lignes dans B.\nNÃ©cessite une table de jonction !\netudiants          inscriptions         cours\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  nom  â”‚    â”‚ etudiant_idâ”‚ cours_id â”‚    â”‚ id â”‚   nom   â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice â”‚â—„â”€â”€â”€â”‚     1      â”‚    1     â”‚â”€â”€â”€â–ºâ”‚ 1  â”‚  Maths  â”‚\nâ”‚ 2  â”‚ Bob   â”‚â—„â”€â”¬â”€â”‚     1      â”‚    2     â”‚â”€â”¬â”€â–ºâ”‚ 2  â”‚ Python  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚     2      â”‚    1     â”‚ â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â””â”€â”‚     2      â”‚    2     â”‚â”€â”˜\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                       Table de jonction\nCas dâ€™usage : Ã‰tudiants â†”ï¸ Cours, Produits â†”ï¸ Tags, Acteurs â†”ï¸ Films.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "href": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“ 8. Normalisation",
    "text": "ğŸ“ 8. Normalisation\nLa normalisation consiste Ã  organiser les donnÃ©es pour : - âŒ Ã‰viter la redondance (donnÃ©es dupliquÃ©es) - âŒ Ã‰viter les anomalies (incohÃ©rences lors de modifications) - âœ… Garantir lâ€™intÃ©gritÃ© des donnÃ©es\n\n\nâŒ Exemple NON normalisÃ©\nTable : commandes (MAUVAIS)\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚ client  â”‚ client_email  â”‚ produit â”‚    ville     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com   â”‚ Clavier â”‚    Paris     â”‚\nâ”‚ 2  â”‚ Alice   â”‚ alice@m.com   â”‚ Souris  â”‚    Paris     â”‚  â† Redondance !\nâ”‚ 3  â”‚ Bob     â”‚ bob@m.com     â”‚ Ã‰cran   â”‚    Lyon      â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ProblÃ¨mes :\n  - Si Alice change d'email â†’ modifier TOUTES les lignes\n  - Risque d'incohÃ©rence si on oublie une ligne\n  - Espace gaspillÃ©\n\n\n\nâœ… Exemple normalisÃ©\nTable : clients               Table : commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚    email    â”‚   â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com â”‚   â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚ Bob     â”‚ bob@m.com   â”‚   â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                                 â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… Avantages :\n  - Email modifiÃ© une seule fois\n  - Pas de redondance\n  - DonnÃ©es cohÃ©rentes\n\n\n\nğŸ“Š Formes normales (rÃ©sumÃ©)\n\n\n\nForme\nRÃ¨gle principale\n\n\n\n\n1NF\nChaque cellule contient une seule valeur (pas de listes)\n\n\n2NF\n1NF + chaque colonne dÃ©pend de TOUTE la clÃ© primaire\n\n\n3NF\n2NF + pas de dÃ©pendance entre colonnes non-clÃ©s\n\n\n\n\nğŸ’¡ En pratique, la 3NF est gÃ©nÃ©ralement suffisante.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "href": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ” 9. PropriÃ©tÃ©s ACID",
    "text": "ğŸ” 9. PropriÃ©tÃ©s ACID\nACID garantit la fiabilitÃ© des transactions dans une base relationnelle.\n\nğŸ“Š Les 4 propriÃ©tÃ©s\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   TRANSACTION   â”‚\n                    â”‚   (ex: virement â”‚\n                    â”‚    bancaire)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ ATOMICITÃ‰ â”‚      â”‚ COHÃ‰RENCE â”‚      â”‚ ISOLATION â”‚\n   â”‚  Tout ou  â”‚      â”‚   Ã‰tat    â”‚      â”‚Transactionsâ”‚\n   â”‚   rien    â”‚      â”‚  valide   â”‚      â”‚ sÃ©parÃ©es  â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ DURABILITÃ‰â”‚\n                      â”‚ Permanent â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“‹ DÃ©tail de chaque propriÃ©tÃ©\n\n\n\n\n\n\n\n\n\nLettre\nPropriÃ©tÃ©\nDescription\nExemple\n\n\n\n\nA\nAtomicitÃ©\nTout ou rien â€” si une partie Ã©choue, tout est annulÃ©\nVirement : dÃ©bit ET crÃ©dit rÃ©ussissent ou rien\n\n\nC\nCohÃ©rence\nLa base reste dans un Ã©tat valide\nLe solde ne peut pas Ãªtre nÃ©gatif\n\n\nI\nIsolation\nLes transactions concurrentes ne sâ€™interfÃ¨rent pas\nDeux virements simultanÃ©s ne se mÃ©langent pas\n\n\nD\nDurabilitÃ©\nUne fois validÃ©e, la transaction est permanente\nMÃªme aprÃ¨s un crash, le virement est enregistrÃ©\n\n\n\n\n\n\nğŸ’° Exemple : Virement bancaire\nTRANSACTION : Virer 100â‚¬ de Alice vers Bob\n\n  1. DÃ©biter 100â‚¬ du compte Alice\n  2. CrÃ©diter 100â‚¬ sur le compte Bob\n\nATOMICITÃ‰ :\n  âœ… Les deux opÃ©rations rÃ©ussissent â†’ COMMIT\n  âŒ Une opÃ©ration Ã©choue â†’ ROLLBACK (rien ne change)\n\nCOHÃ‰RENCE :\n  âœ… Alice : 500â‚¬ â†’ 400â‚¬\n  âœ… Bob   : 200â‚¬ â†’ 300â‚¬\n  âœ… Total : 700â‚¬ â†’ 700â‚¬ (inchangÃ©)\n\nISOLATION :\n  Un autre virement simultanÃ© ne voit pas l'Ã©tat intermÃ©diaire\n\nDURABILITÃ‰ :\n  MÃªme si le serveur crash juste aprÃ¨s le COMMIT,\n  le virement sera toujours lÃ  au redÃ©marrage",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne",
    "href": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ—ï¸ 10. Exemple complet : SchÃ©ma dâ€™une boutique en ligne",
    "text": "ğŸ—ï¸ 10. Exemple complet : SchÃ©ma dâ€™une boutique en ligne\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        BASE DE DONNÃ‰ES : ma_boutique                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚    CLIENTS      â”‚       â”‚    COMMANDES    â”‚       â”‚   PRODUITS    â”‚ â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\nâ”‚  â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)    â”‚ â”‚\nâ”‚  â”‚ nom             â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ ğŸ”— client_id(FK)â”‚       â”‚ nom           â”‚ â”‚\nâ”‚  â”‚ email           â”‚       â”‚ date            â”‚       â”‚ prix          â”‚ â”‚\nâ”‚  â”‚ telephone       â”‚       â”‚ statut          â”‚       â”‚ stock         â”‚ â”‚\nâ”‚  â”‚ created_at      â”‚       â”‚ total           â”‚       â”‚ categorie     â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                     â”‚                        â”‚         â”‚\nâ”‚                                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                                     â”‚    â”‚                             â”‚\nâ”‚                                     â–¼    â–¼                             â”‚\nâ”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                            â”‚   LIGNES_COMMANDE   â”‚                     â”‚\nâ”‚                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚\nâ”‚                            â”‚ ğŸ”‘ id (PK)          â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— commande_id (FK) â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— produit_id (FK)  â”‚                     â”‚\nâ”‚                            â”‚ quantite            â”‚                     â”‚\nâ”‚                            â”‚ prix_unitaire       â”‚                     â”‚\nâ”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRELATIONS :\n  â€¢ clients (1) â”€â”€â”€â–º (N) commandes        : Un client a plusieurs commandes\n  â€¢ commandes (1) â”€â”€â”€â–º (N) lignes_commande : Une commande a plusieurs lignes\n  â€¢ produits (1) â”€â”€â”€â–º (N) lignes_commande  : Un produit dans plusieurs lignes",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“‹ RÃ©sumÃ©",
    "text": "ğŸ“‹ RÃ©sumÃ©\n\nğŸ§± Vocabulaire\n\n\n\nTerme\nDescription\n\n\n\n\nTable\nCollection de donnÃ©es structurÃ©es\n\n\nColonne\nAttribut (nom, email, prixâ€¦)\n\n\nLigne\nUn enregistrement (un client, une commandeâ€¦)\n\n\nClÃ© Primaire (PK)\nIdentifiant unique dâ€™une ligne\n\n\nClÃ© Ã‰trangÃ¨re (FK)\nRÃ©fÃ©rence vers une autre table\n\n\nSGBD\nLogiciel de gestion (PostgreSQL, MySQLâ€¦)\n\n\n\n\n\nğŸ”€ Relations\n\n\n\nType\nNotation\nExemple\n\n\n\n\nUn-Ã -Un\n1:1\nUtilisateur â†”ï¸ Profil\n\n\nUn-Ã -Plusieurs\n1:N\nClient â†’ Commandes\n\n\nPlusieurs-Ã -Plusieurs\nN:N\nÃ‰tudiants â†”ï¸ Cours\n\n\n\n\n\nğŸ” ACID\n\n\n\nLettre\nPropriÃ©tÃ©\n\n\n\n\nA\nAtomicitÃ© â€” Tout ou rien\n\n\nC\nCohÃ©rence â€” Ã‰tat valide\n\n\nI\nIsolation â€” Transactions sÃ©parÃ©es\n\n\nD\nDurabilitÃ© â€” Permanent",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "href": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ§  Quiz",
    "text": "ğŸ§  Quiz\n\n\nâ“ Q1. Quâ€™est-ce quâ€™une clÃ© primaire ?\n\nUne clÃ© de chiffrement\n\nUn identifiant unique pour chaque ligne\n\nLe nom de la premiÃ¨re colonne\n\nUn mot de passe\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© primaire identifie de faÃ§on unique chaque ligne dâ€™une table.\n\n\n\n\nâ“ Q2. Ã€ quoi sert une clÃ© Ã©trangÃ¨re ?\n\nÃ€ chiffrer les donnÃ©es\n\nÃ€ crÃ©er un lien entre deux tables\n\nÃ€ indexer les colonnes\n\nÃ€ supprimer des lignes\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© Ã©trangÃ¨re rÃ©fÃ©rence la clÃ© primaire dâ€™une autre table pour crÃ©er une relation.\n\n\n\n\nâ“ Q3. Quelle relation nÃ©cessite une table de jonction ?\n\nUn-Ã -Un (1:1)\n\nUn-Ã -Plusieurs (1:N)\n\nPlusieurs-Ã -Plusieurs (N:N)\n\nAucune\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Les relations N:N nÃ©cessitent une table intermÃ©diaire (jonction) pour stocker les associations.\n\n\n\n\nâ“ Q4. Que signifie le A de ACID ?\n\nAuthentification\n\nAtomicitÃ©\n\nAutomatisation\n\nArchivage\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” AtomicitÃ© : une transaction est indivisible (tout rÃ©ussit ou tout Ã©choue).\n\n\n\n\nâ“ Q5. Pourquoi normaliser une base de donnÃ©es ?\n\nPour la rendre plus rapide\n\nPour Ã©viter la redondance et les incohÃ©rences\n\nPour ajouter du chiffrement\n\nPour compresser les donnÃ©es\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La normalisation Ã©vite les donnÃ©es dupliquÃ©es et garantit la cohÃ©rence.\n\n\n\n\nâ“ Q6. Quel SGBD est open source ET trÃ¨s complet ?\n\nOracle\n\nSQL Server\n\nPostgreSQL\n\nAccess\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” PostgreSQL est open source, gratuit et trÃ¨s complet (le plus recommandÃ©).\n\n\n\n\nâ“ Q7. Quel type utiliser pour stocker des montants en euros ?\n\nFLOAT\n\nINTEGER\n\nDECIMAL\n\nVARCHAR\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” DECIMAL garantit une prÃ©cision exacte pour les montants financiers (FLOAT est imprÃ©cis).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "href": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nPostgreSQL Documentation\nSQLBolt â€” Tutoriel interactif\nDB Diagram â€” CrÃ©er des schÃ©mas visuels\nDatabase Normalization (Wikipedia)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "title": "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu connais maintenant la thÃ©orie des bases de donnÃ©es relationnelles. Passons Ã  la pratique !\nğŸ‘‰ Module suivant : 07_sql_for_data_engineers.ipynb â€” Ã‰crire des requÃªtes SQL\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises les concepts fondamentaux des bases relationnelles.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ—„ï¸ Introduction aux Bases de DonnÃ©es Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html",
    "href": "notebooks/beginner/03_git_for_data_engineers.html",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "",
    "text": "Objectif : Comprendre Git, GitHub et GitLab, savoir versionner ses scripts, notebooks et configurations.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 02_bash_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal\n\n\nâœ… Requis\nAvoir un compte GitHub (gratuit) â€” voir section ci-dessous",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les concepts de versioning\nInitialiser et configurer un dÃ©pÃ´t Git\nMaÃ®triser le workflow : add â†’ commit â†’ push\nTravailler avec les branches\nCollaborer efficacement avec une Ã©quipe\nUtiliser .gitignore pour protÃ©ger les donnÃ©es sensibles",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ§° Pourquoi Git est essentiel pour un Data Engineer ?",
    "text": "ğŸ§° Pourquoi Git est essentiel pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nVersionner les pipelines\nSuivre lâ€™Ã©volution de tes scripts ETL\n\n\nCollaborer en Ã©quipe\nTravailler Ã  plusieurs sur le mÃªme projet data\n\n\nRevenir en arriÃ¨re\nRestaurer une version qui fonctionnait aprÃ¨s un bug\n\n\nCode review\nValider les modifications avant mise en production\n\n\nCI/CD\nDÃ©clencher automatiquement des tests et dÃ©ploiements\n\n\nDocumentation\nHistorique complet de qui a fait quoi et pourquoi\n\n\n\n\nğŸ’¡ En bref : Git est le systÃ¨me nerveux de tout projet data moderne. Sans Git, pas de collaboration efficace ni de traÃ§abilitÃ©.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab",
    "text": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab\n\nğŸ§© Quâ€™est-ce que Git ?\nGit est un logiciel de gestion de versions distribuÃ©. Il permet de :\n\nSuivre lâ€™Ã©volution de vos fichiers au fil du temps\nCrÃ©er des points de restauration (commits)\nTravailler en parallÃ¨le sur diffÃ©rentes fonctionnalitÃ©s (branches)\nFusionner le travail de plusieurs personnes\n\nğŸ“ Ton projet\n    â”‚\n    â”œâ”€â”€ ğŸ“„ pipeline.py      â† VersionnÃ© par Git\n    â”œâ”€â”€ ğŸ“„ config.yaml      â† VersionnÃ© par Git  \n    â”œâ”€â”€ ğŸ“ .git/            â† Dossier cachÃ© contenant l'historique\n    â””â”€â”€ ğŸ“ data/            â† âš ï¸ Ã€ NE PAS versionner !\n\n\nâ˜ï¸ Quâ€™est-ce que GitHub ?\nGitHub est une plateforme cloud (propriÃ©tÃ© de Microsoft) qui hÃ©berge vos dÃ©pÃ´ts Git.\n\nInterface web moderne\nTrÃ¨s populaire pour lâ€™open source\nGitHub Actions pour la CI/CD\n\n\n\nğŸ—ï¸ Quâ€™est-ce que GitLab ?\nGitLab est une alternative open source Ã  GitHub :\n\nPeut Ãªtre auto-hÃ©bergÃ© (on-premise)\nCI/CD intÃ©grÃ© trÃ¨s puissant\nSouvent utilisÃ© en entreprise",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif",
    "text": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nGitHub\nGitLab\n\n\n\n\nğŸ  HÃ©bergement\nCloud (Microsoft)\nCloud ou auto-hÃ©bergÃ©\n\n\nâš™ï¸ CI/CD\nGitHub Actions\nGitLab CI/CD (intÃ©grÃ©)\n\n\nğŸ‘¥ CommunautÃ©\nTrÃ¨s vaste (open source)\nOrientÃ© entreprise\n\n\nğŸ¨ Interface\nModerne, simple\nComplÃ¨te, personnalisable\n\n\nğŸ’° Prix\nGratuit + plans payants\nGratuit + plans payants\n\n\nğŸ”’ SÃ©curitÃ©\nDÃ©pend du plan\nPeut Ãªtre auto-hÃ©bergÃ©\n\n\nğŸ“Š Usage recommandÃ©\nProjets publics, open source\nProjets internes, entreprise\n\n\n\n\nğŸ’¡ Pour ce cours, tu peux utiliser lâ€™un ou lâ€™autre. Les commandes Git sont identiques !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ†“ CrÃ©er un compte GitHub gratuit",
    "text": "ğŸ†“ CrÃ©er un compte GitHub gratuit\n\nğŸ“ Ã‰tape 1 : Inscription\n\nAller sur github.com\nCliquer sur â€œSign upâ€ (en haut Ã  droite)\nRemplir le formulaire :\n\nEmail : ton adresse email\nPassword : un mot de passe fort\nUsername : ton pseudo (visible publiquement)\n\nRÃ©soudre le puzzle de vÃ©rification\nValider lâ€™email (vÃ©rifier ta boÃ®te mail)\n\nâœ… Câ€™est gratuit ! Le plan gratuit inclut : - DÃ©pÃ´ts publics illimitÃ©s - DÃ©pÃ´ts privÃ©s illimitÃ©s - 500 Mo de stockage pour GitHub Packages - 2000 minutes/mois de GitHub Actions\n\n\n\nğŸ” Ã‰tape 2 : Configurer lâ€™authentification\nDepuis 2021, GitHub nâ€™accepte plus les mots de passe pour git push. Tu dois utiliser :\n\n\n\nMÃ©thode\nDifficultÃ©\nRecommandation\n\n\n\n\nHTTPS + Token\nâ­ Facile\nâœ… Pour dÃ©buter\n\n\nSSH\nâ­â­ Moyen\nâœ… Pour usage rÃ©gulier\n\n\n\n\n\n\nğŸ”‘ Option A : HTTPS + Personal Access Token (PAT)\nCrÃ©er un token :\n\nConnecte-toi sur github.com\nClique sur ta photo de profil â†’ Settings\nDans le menu gauche, descends jusquâ€™Ã  Developer settings\nClique sur Personal access tokens â†’ Tokens (classic)\nClique sur Generate new token â†’ Generate new token (classic)\nConfigure le token :\n\nNote : git-access (ou un nom descriptif)\nExpiration : 90 days (ou plus)\nScopes : cocher repo (accÃ¨s complet aux dÃ©pÃ´ts)\n\nClique sur Generate token\nâš ï¸ COPIE LE TOKEN MAINTENANT â€” tu ne pourras plus le voir aprÃ¨s !\n\nUtiliser le token :\n# Quand Git demande ton mot de passe, colle le TOKEN (pas ton mot de passe !)\ngit push origin main\nUsername: ton-username\nPassword: ghp_xxxxxxxxxxxxxxxxxxxx   # â† Coller le token ici\nSauvegarder le token (optionnel) :\n# MÃ©moriser les credentials pour 1 heure\ngit config --global credential.helper cache\n\n# Ou mÃ©moriser indÃ©finiment (moins sÃ©curisÃ©)\ngit config --global credential.helper store\n\n\n\nğŸ” Option B : ClÃ© SSH (recommandÃ© pour usage rÃ©gulier)\n1. GÃ©nÃ©rer une clÃ© SSH :\n# GÃ©nÃ©rer une paire de clÃ©s (appuie sur EntrÃ©e pour les valeurs par dÃ©faut)\nssh-keygen -t ed25519 -C \"ton.email@exemple.com\"\n\n# DÃ©marrer l'agent SSH\neval \"$(ssh-agent -s)\"\n\n# Ajouter la clÃ© Ã  l'agent\nssh-add ~/.ssh/id_ed25519\n2. Ajouter la clÃ© Ã  GitHub :\n# Copier la clÃ© publique\ncat ~/.ssh/id_ed25519.pub\n# Copie le rÃ©sultat (commence par ssh-ed25519...)\n\nSur GitHub : Settings â†’ SSH and GPG keys â†’ New SSH key\nColler la clÃ© publique et sauvegarder\n\n3. Tester la connexion :\nssh -T git@github.com\n# RÃ©ponse attendue : \"Hi username! You've successfully authenticated...\"\n4. Utiliser SSH pour cloner :\n# Cloner avec SSH (au lieu de HTTPS)\ngit clone git@github.com:username/repo.git\n\n# Ou changer un dÃ©pÃ´t existant vers SSH\ngit remote set-url origin git@github.com:username/repo.git\n\n\n\nğŸ“¦ Ã‰tape 3 : CrÃ©er ton premier dÃ©pÃ´t sur GitHub\nVia lâ€™interface web :\n\nConnecte-toi sur github.com\nClique sur â€œ+â€ (en haut Ã  droite) â†’ New repository\nConfigure le dÃ©pÃ´t :\n\nRepository name : mon-projet-data\nDescription : â€œMon premier projet Data Engineeringâ€\nVisibility : Public ou Private\nâŒ Ne PAS cocher â€œAdd a README fileâ€ (on le fera localement)\n\nClique sur Create repository\nGitHub affiche les commandes Ã  exÃ©cuter â€” copie-les !\n\nLier ton projet local au dÃ©pÃ´t GitHub :\n# Si tu as dÃ©jÃ  un projet local avec des commits\ncd mon_projet_data\ngit remote add origin https://github.com/ton-username/mon-projet-data.git\ngit branch -M main\ngit push -u origin main\n# Ou cloner un dÃ©pÃ´t existant\ngit clone https://github.com/ton-username/mon-projet-data.git\ncd mon-projet-data\n\n\n\nğŸ“Š RÃ©capitulatif : Workflow complet\n1. CrÃ©er compte GitHub â”€â”€â”€â”€â–º github.com/signup\n                                    â”‚\n2. Configurer auth â”€â”€â”€â”€â”€â”€â”€â”€â–º Token (HTTPS) ou SSH\n                                    â”‚\n3. CrÃ©er dÃ©pÃ´t sur GitHub â”€â–º github.com â†’ New repository\n                                    â”‚\n4. Lier projet local â”€â”€â”€â”€â”€â”€â–º git remote add origin ...\n                                    â”‚\n5. Pousser le code â”€â”€â”€â”€â”€â”€â”€â”€â–º git push -u origin main\n                                    â”‚\n                              âœ… Code sur GitHub !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "3ï¸âƒ£ Installation et configuration de Git",
    "text": "3ï¸âƒ£ Installation et configuration de Git\n\nğŸ§° Installation\n\n\n\nSystÃ¨me\nCommande\n\n\n\n\nğŸªŸ Windows\nTÃ©lÃ©charger depuis git-scm.com\n\n\nğŸ macOS\nbrew install git\n\n\nğŸ§ Linux (Debian/Ubuntu)\nsudo apt install git\n\n\nğŸ§ Linux (Fedora)\nsudo dnf install git\n\n\n\n\n\nCode\n%%bash\n# VÃ©rifier la version installÃ©e\ngit --version\n\n# Configuration obligatoire (identitÃ©)\ngit config --global user.name \"Ton Nom\"\ngit config --global user.email \"ton.email@exemple.com\"\n\n# Configuration recommandÃ©e\ngit config --global init.defaultBranch main    # Branche par dÃ©faut\ngit config --global core.editor \"code --wait\"  # Ã‰diteur (VS Code)\ngit config --global pull.rebase false          # Merge par dÃ©faut lors du pull\n\n# VÃ©rifier la configuration\ngit config --list",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "4ï¸âƒ£ Comprendre le workflow Git",
    "text": "4ï¸âƒ£ Comprendre le workflow Git\n\nğŸ“Š Les 4 zones de Git\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git add     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Working Dir    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Staging Area   â”‚\nâ”‚  (tes fichiers) â”‚                â”‚  (index)        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                            â”‚ git commit\n                                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git push    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Remote         â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Local Repo     â”‚\nâ”‚  (GitHub/GitLab)â”‚                â”‚  (.git)         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ”„ Cycle de vie dâ€™un fichier\n\n\n\nÃ‰tat\nDescription\nCommande pour passer Ã  lâ€™Ã©tat suivant\n\n\n\n\nUntracked\nNouveau fichier, non suivi\ngit add &lt;fichier&gt;\n\n\nStaged\nPrÃªt Ã  Ãªtre commitÃ©\ngit commit -m \"message\"\n\n\nCommitted\nEnregistrÃ© localement\ngit push\n\n\nPushed\nEnvoyÃ© sur le serveur distant\nâœ… TerminÃ©",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ–¥ï¸ Clients Git (alternatives aux commandes)",
    "text": "ğŸ–¥ï¸ Clients Git (alternatives aux commandes)\nTu nâ€™es pas obligÃ© dâ€™utiliser le terminal ! Il existe des interfaces graphiques (GUI) pour Git qui facilitent la visualisation et certaines opÃ©rations.\n\nğŸ“± Clients populaires\n\n\n\nClient\nPlateforme\nPoints forts\nPrix\n\n\n\n\nGitHub Desktop\nWindows, Mac\nSimple, parfait pour dÃ©buter, intÃ©gration GitHub\nGratuit\n\n\nGitKraken\nWindows, Mac, Linux\nInterface visuelle puissante, graphe des branches\nGratuit (public)\n\n\nSourcetree\nWindows, Mac\nComplet, supporte Git et Mercurial\nGratuit\n\n\nVS Code\nTous\nGit intÃ©grÃ© + extension GitLens\nGratuit\n\n\nPyCharm / IntelliJ\nTous\nGit intÃ©grÃ© dans lâ€™IDE\nGratuit / Payant\n\n\n\n\n\nğŸ¤” Terminal vs GUI : quand utiliser quoi ?\n\n\n\n\n\n\n\nSituation\nRecommandation\n\n\n\n\nVisualiser lâ€™historique et les branches\nğŸ–¥ï¸ GUI â€” Plus clair visuellement\n\n\nRÃ©soudre des conflits de merge\nğŸ–¥ï¸ GUI â€” Comparaison cÃ´te Ã  cÃ´te\n\n\nOpÃ©rations quotidiennes (add, commit, push)\nâŒ¨ï¸ Terminal ou ğŸ–¥ï¸ GUI â€” Au choix\n\n\nScripts et automatisation (CI/CD)\nâŒ¨ï¸ Terminal â€” Obligatoire\n\n\nServeurs distants (SSH)\nâŒ¨ï¸ Terminal â€” Pas de GUI disponible\n\n\nApprendre Git en profondeur\nâŒ¨ï¸ Terminal â€” Comprendre ce qui se passe\n\n\n\n\nğŸ’¡ Conseil : Apprends dâ€™abord les commandes pour comprendre Git, puis utilise un client GUI pour gagner en productivitÃ© au quotidien.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "5ï¸âƒ£ Utilisation de Git pas Ã  pas",
    "text": "5ï¸âƒ£ Utilisation de Git pas Ã  pas\n\nÃ‰tape 1 : CrÃ©er un projet et initialiser Git\n\n\nCode\n%%bash\n# CrÃ©er un dossier de projet\nmkdir mon_projet_data\ncd mon_projet_data\n\n# Initialiser Git (crÃ©e le dossier .git)\ngit init\n\n# VÃ©rifier le statut\ngit status\n\n\n\n\nÃ‰tape 2 : Ajouter des fichiers et commiter\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un fichier Python\necho \"print('Hello Data Engineering!')\" &gt; main.py\n\n# Voir le statut (fichier untracked)\ngit status\n\n# Ajouter le fichier Ã  la staging area\ngit add main.py\n\n# Voir le statut (fichier staged)\ngit status\n\n# Commiter avec un message descriptif\ngit commit -m \"feat: ajouter script principal\"\n\n# Voir l'historique\ngit log --oneline\n\n\n\n\nğŸ“ Conventions de commits (Conventional Commits)\nUtilise des prÃ©fixes standardisÃ©s pour des messages clairs :\n\n\n\n\n\n\n\n\nPrÃ©fixe\nUsage\nExemple\n\n\n\n\nfeat:\nNouvelle fonctionnalitÃ©\nfeat: ajouter extraction API\n\n\nfix:\nCorrection de bug\nfix: corriger parsing dates\n\n\ndocs:\nDocumentation\ndocs: mettre Ã  jour README\n\n\nrefactor:\nRefactoring (sans changer le comportement)\nrefactor: simplifier fonction ETL\n\n\ntest:\nAjout/modification de tests\ntest: ajouter tests unitaires\n\n\nchore:\nMaintenance, config\nchore: mettre Ã  jour dÃ©pendances\n\n\n\nExemple de bon message :\nfeat: ajouter pipeline d'extraction des donnÃ©es clients\n\n- Connexion Ã  l'API CRM\n- Transformation des donnÃ©es JSON\n- Export en format Parquet",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers",
    "text": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers\nLe .gitignore indique Ã  Git quels fichiers NE PAS versionner.\n\nâš ï¸ Ne JAMAIS versionner :\n\nğŸ“Š Fichiers de donnÃ©es (CSV, Parquet, JSON volumineux)\nğŸ”‘ Secrets et credentials (mots de passe, clÃ©s API)\nğŸ“¦ DÃ©pendances (node_modules, venv)\nğŸ—‘ï¸ Fichiers temporaires (cache, logs)\n\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un .gitignore pour projet Data Engineering\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# ==== DONNÃ‰ES ====\n*.csv\n*.parquet\n*.json\n*.xlsx\ndata/\nraw/\nprocessed/\n\n# ==== SECRETS ====\n.env\n*.pem\n*.key\ncredentials.json\nsecrets.yaml\n\n# ==== PYTHON ====\n__pycache__/\n*.py[cod]\nvenv/\n.venv/\n*.egg-info/\n.pytest_cache/\n\n# ==== JUPYTER ====\n.ipynb_checkpoints/\n*.ipynb_checkpoints\n\n# ==== IDE ====\n.idea/\n.vscode/\n*.swp\n\n# ==== LOGS ====\n*.log\nlogs/\n\n# ==== OS ====\n.DS_Store\nThumbs.db\nEOF\n\necho \"âœ… .gitignore crÃ©Ã©\"\ncat .gitignore\n\n\n\n\nğŸ’¡ Astuces .gitignore\n# Ignorer un dossier\ndata/\n\n# Ignorer tous les .csv sauf un\n*.csv\n!schema.csv\n\n# Ignorer les fichiers dans tous les sous-dossiers\n**/*.log\n\n# VÃ©rifier ce qui est ignorÃ©\ngit status --ignored\n\nğŸ”— GÃ©nÃ©rateur de .gitignore : gitignore.io",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)",
    "text": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)\n\nÃ‰tape 1 : CrÃ©er un dÃ©pÃ´t sur GitHub/GitLab\n\nVa sur github.com ou gitlab.com\nClique sur â€œNew repositoryâ€ / â€œNew projectâ€\nDonne un nom (ex: mon_projet_data)\nNe coche PAS â€œInitialize with READMEâ€ (on a dÃ©jÃ  un repo local)\nCopie lâ€™URL HTTPS\n\n\n\nÃ‰tape 2 : Connecter le dÃ©pÃ´t local\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# Ajouter le dÃ©pÃ´t distant (remplace par ton URL)\ngit remote add origin https://github.com/ton-username/mon_projet_data.git\n\n# VÃ©rifier les remotes\ngit remote -v\n\n# S'assurer d'Ãªtre sur la branche main\ngit branch -M main\n\n# Pousser le code (premiÃ¨re fois : -u pour lier la branche)\ngit push -u origin main\n\n\n\n\nğŸ“¥ Cloner un projet existant\n# Cloner un dÃ©pÃ´t\ngit clone https://github.com/username/projet.git\n\n# Cloner dans un dossier spÃ©cifique\ngit clone https://github.com/username/projet.git mon_dossier\n\n\nğŸ”„ Synchroniser avec le distant\n# RÃ©cupÃ©rer les modifications (fetch + merge)\ngit pull\n\n# Voir les modifications distantes sans les appliquer\ngit fetch\ngit log origin/main --oneline",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "8ï¸âƒ£ Travailler avec les branches",
    "text": "8ï¸âƒ£ Travailler avec les branches\nLes branches permettent de travailler sur des fonctionnalitÃ©s en parallÃ¨le sans affecter le code principal.\nmain         â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—  (code stable)\n                  â”‚               â†‘\nfeature/etl       â””â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”˜     (nouvelle fonctionnalitÃ©)\n\n\nCode\n%%bash\ncd mon_projet_data\n\n# Voir les branches existantes\ngit branch\n\n# CrÃ©er une nouvelle branche\ngit branch feature/add-etl\n\n# Basculer sur la branche\ngit switch feature/add-etl\n\n# OU crÃ©er + basculer en une commande\ngit switch -c feature/add-validation\n\n# Faire des modifications\necho \"def validate(df): pass\" &gt; validation.py\ngit add validation.py\ngit commit -m \"feat: ajouter module de validation\"\n\n# Revenir sur main\ngit switch main\n\n# Fusionner la branche\ngit merge feature/add-validation\n\n# Supprimer la branche fusionnÃ©e\ngit branch -d feature/add-validation\n\n\n\nğŸŒ¿ Workflow de branches recommandÃ© pour Data Engineers\nmain (production)\n  â”‚\n  â”œâ”€â”€ develop (intÃ©gration)\n  â”‚     â”‚\n  â”‚     â”œâ”€â”€ feature/etl-clients\n  â”‚     â”œâ”€â”€ feature/dashboard-ventes  \n  â”‚     â””â”€â”€ fix/bug-parsing-dates\n  â”‚\n  â””â”€â”€ hotfix/critical-fix (urgences)\n\n\n\nBranche\nUsage\n\n\n\n\nmain\nCode en production, toujours stable\n\n\ndevelop\nIntÃ©gration des features avant release\n\n\nfeature/*\nNouvelles fonctionnalitÃ©s\n\n\nfix/*\nCorrections de bugs\n\n\nhotfix/*\nCorrections urgentes en production",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "9ï¸âƒ£ RÃ©soudre les conflits de merge",
    "text": "9ï¸âƒ£ RÃ©soudre les conflits de merge\nUn conflit survient quand deux personnes modifient la mÃªme ligne.\n\nğŸ” Ã€ quoi ressemble un conflit ?\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef process_data(df):\n    return df.dropna()\n=======\ndef process_data(dataframe):\n    return dataframe.fillna(0)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/autre-branche\n\n\nâœ… Comment rÃ©soudre ?\n\nOuvrir le fichier et choisir la bonne version\nSupprimer les marqueurs (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nTester que le code fonctionne\nCommiter la rÃ©solution\n\n# AprÃ¨s avoir Ã©ditÃ© le fichier\ngit add fichier_resolu.py\ngit commit -m \"fix: rÃ©soudre conflit sur process_data\"\n\nğŸ’¡ Astuce : Utilise un outil visuel comme VS Code pour rÃ©soudre les conflits plus facilement.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ”§ Commandes utiles avancÃ©es",
    "text": "ğŸ”§ Commandes utiles avancÃ©es\n\nğŸ“¦ git stash â€” Mettre de cÃ´tÃ© temporairement\n# Sauvegarder les modifications en cours\ngit stash\n\n# Voir les stash\ngit stash list\n\n# RÃ©cupÃ©rer le dernier stash\ngit stash pop\n\n# RÃ©cupÃ©rer un stash spÃ©cifique\ngit stash apply stash@{0}\n\n\nâ†©ï¸ï¸ Annuler des changements\n# Annuler les modifications d'un fichier (non commitÃ©)\ngit checkout -- fichier.py\n\n# Retirer un fichier de la staging area\ngit reset HEAD fichier.py\n\n# Annuler le dernier commit (garde les fichiers)\ngit reset --soft HEAD~1\n\n# Annuler le dernier commit (supprime les fichiers)\ngit reset --hard HEAD~1  # âš ï¸ DANGEREUX\n\n# CrÃ©er un commit qui annule un commit prÃ©cÃ©dent\ngit revert &lt;commit-hash&gt;\n\n\nğŸ” Inspecter lâ€™historique\n# Historique compact\ngit log --oneline\n\n# Historique graphique\ngit log --oneline --graph --all\n\n# Voir les modifications d'un commit\ngit show &lt;commit-hash&gt;\n\n# Voir qui a modifiÃ© chaque ligne\ngit blame fichier.py\n\n# Chercher un commit par message\ngit log --grep=\"ETL\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“‹ Cheatsheet â€” Commandes essentielles",
    "text": "ğŸ“‹ Cheatsheet â€” Commandes essentielles\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nSetup\ngit init\nInitialiser un dÃ©pÃ´t\n\n\n\ngit clone &lt;url&gt;\nCloner un dÃ©pÃ´t distant\n\n\n\ngit config --global user.name\nConfigurer son nom\n\n\nBasique\ngit status\nVoir lâ€™Ã©tat des fichiers\n\n\n\ngit add &lt;fichier&gt;\nAjouter Ã  la staging area\n\n\n\ngit add .\nAjouter tous les fichiers\n\n\n\ngit commit -m \"msg\"\nEnregistrer les modifications\n\n\nHistorique\ngit log --oneline\nVoir lâ€™historique compact\n\n\n\ngit diff\nVoir les modifications\n\n\n\ngit blame &lt;fichier&gt;\nVoir qui a modifiÃ© quoi\n\n\nBranches\ngit branch\nLister les branches\n\n\n\ngit switch -c &lt;nom&gt;\nCrÃ©er et basculer\n\n\n\ngit merge &lt;branche&gt;\nFusionner une branche\n\n\n\ngit branch -d &lt;nom&gt;\nSupprimer une branche\n\n\nRemote\ngit remote add origin &lt;url&gt;\nAjouter un dÃ©pÃ´t distant\n\n\n\ngit push\nEnvoyer sur le serveur\n\n\n\ngit pull\nRÃ©cupÃ©rer du serveur\n\n\n\ngit fetch\nVÃ©rifier les changements\n\n\nAnnuler\ngit stash\nMettre de cÃ´tÃ©\n\n\n\ngit reset --soft HEAD~1\nAnnuler dernier commit\n\n\n\ngit revert &lt;hash&gt;\nCrÃ©er un commit dâ€™annulation\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Git Cheatsheet officiel (PDF)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Git est un outil de :\n\nDesign graphique\n\nGestion de versions\n\nStockage cloud\n\nDÃ©ploiement automatique\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Git est un systÃ¨me de gestion de versions distribuÃ©.\n\n\n\n\nâ“ Q2. Quelle commande initialise un dÃ©pÃ´t Git ?\n\ngit start\n\ngit init\n\ngit create\n\ngit new\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” git init crÃ©e un nouveau dÃ©pÃ´t Git.\n\n\n\n\nâ“ Q3. Quelle est la diffÃ©rence entre git commit et git push ?\n\ncommit enregistre localement, push envoie au serveur distant\n\ncommit supprime des fichiers\n\npush crÃ©e un dÃ©pÃ´t local\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” commit sauvegarde localement, push synchronise avec le serveur.\n\n\n\n\nâ“ Q4. Que faut-il mettre dans le .gitignore pour un projet Data ?\n\nLes fichiers Python\n\nLes fichiers de donnÃ©es (CSV, Parquet) et les secrets\n\nLe README\n\nTous les fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Ne jamais versionner les donnÃ©es volumineuses ni les credentials !\n\n\n\n\nâ“ Q5. Quelle commande crÃ©e une nouvelle branche et bascule dessus ?\n\ngit branch new\n\ngit create-branch nom\n\ngit switch -c nom\n\ngit branch -m nom\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” git switch -c nom crÃ©e et bascule sur la branche.\n\n\n\n\nâ“ Q6. Comment annuler le dernier commit tout en gardant les fichiers ?\n\ngit delete commit\n\ngit reset --hard HEAD~1\n\ngit reset --soft HEAD~1\n\ngit undo\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” --soft garde les fichiers, --hard les supprime.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ’» Exercice pratique",
    "text": "ğŸ’» Exercice pratique\n\nğŸ¯ Objectif\nCrÃ©er un projet Data Engineering versionnÃ© et le pousser sur GitHub/GitLab.\n\n\nğŸ“ Instructions\n\nCrÃ©er un dossier projet_etl\nInitialiser Git\nCrÃ©er un .gitignore appropriÃ©\nCrÃ©er un fichier etl.py avec un script simple\nCrÃ©er un fichier README.md\nFaire un premier commit\nCrÃ©er une branche feature/add-config\nAjouter un fichier config.yaml sur cette branche\nMerger dans main\nCrÃ©er un dÃ©pÃ´t sur GitHub et pousser le code\n\n\n\nâœ… Solution\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n# 1. CrÃ©er le dossier\nmkdir projet_etl && cd projet_etl\n\n# 2. Initialiser Git\ngit init\n\n# 3. CrÃ©er le .gitignore\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# DonnÃ©es\n*.csv\n*.parquet\ndata/\n\n# Secrets\n.env\ncredentials.json\n\n# Python\n__pycache__/\nvenv/\nEOF\n\n# 4. CrÃ©er le script ETL\ncat &lt;&lt; 'EOF' &gt; etl.py\n#!/usr/bin/env python3\n\"\"\"Simple ETL Pipeline\"\"\"\n\ndef extract():\n    print(\"ğŸ“¥ Extracting data...\")\n    return {\"data\": [1, 2, 3]}\n\ndef transform(data):\n    print(\"ğŸ”„ Transforming data...\")\n    return {\"data\": [x * 2 for x in data[\"data\"]]}\n\ndef load(data):\n    print(\"ğŸ’¾ Loading data...\")\n    print(f\"Result: {data}\")\n\nif __name__ == \"__main__\":\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n    print(\"âœ… ETL completed!\")\nEOF\n\n# 5. CrÃ©er le README\ncat &lt;&lt; 'EOF' &gt; README.md\n# Projet ETL\n\nUn pipeline ETL simple pour apprendre Git.\n\n## Usage\n\n```bash\npython etl.py\nEOF",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ® Apprendre en pratiquant\n\nLearn Git Branching â€” Tutoriel interactif visuel\nOh My Git! â€” Jeu pour apprendre Git\nGit Katas â€” Exercices pratiques\n\n\n\nğŸ“– Documentation\n\nPro Git Book â€” Livre gratuit (en franÃ§ais)\nGitHub Docs\nGitLab Docs\n\n\n\nğŸ› ï¸ Outils\n\nGitHub Desktop â€” Interface graphique\nGitKraken â€” Client Git visuel\nConventional Commits â€” Standard de messages",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "âœ… Conclusion",
    "text": "âœ… Conclusion\nTu sais maintenant :\n\nâœ… Ce quâ€™est Git, GitHub et GitLab\nâœ… Comment initialiser et configurer un projet\nâœ… Le workflow : add â†’ commit â†’ push\nâœ… Travailler avec les branches\nâœ… Utiliser .gitignore pour protÃ©ger les donnÃ©es sensibles\nâœ… RÃ©soudre les conflits de merge\nâœ… Les commandes avancÃ©es (stash, reset, revert)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“˜ Git pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu sais versionner ton code, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 04_python_basics_for_data_engineers.ipynb â€” Les fondamentaux de Python\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Git pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Git pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "",
    "text": "Ce notebook donne les bases de Python nÃ©cessaires pour la suite du parcours Data Engineering From Zero to Hero.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Version Python recommandÃ©e",
    "text": "âš ï¸ Version Python recommandÃ©e\n\n\n\n\n\n\n\n\nVersion\nStatut\nRecommandation\n\n\n\n\nPython 3.12\nâœ… DerniÃ¨re stable\nRecommandÃ©e pour nouveaux projets\n\n\nPython 3.11\nâœ… Stable\nExcellent choix, trÃ¨s performant\n\n\nPython 3.10\nâœ… SupportÃ©e\nMinimum pour les type hints modernes\n\n\nPython 3.9\nâš ï¸ Maintenance\nÃ‰viter pour nouveaux projets\n\n\nPython 3.8 et avant\nâŒ ObsolÃ¨te\nNe pas utiliser\n\n\n\n\nğŸ’¡ Pour ce cours, utilise Python 3.11 ou 3.12.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 03_git_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal (Bash)\n\n\nğŸŸ¡ Optionnel\nNotions de programmation dans un autre langage",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Installer et configurer ton environnement Python\nâœ… CrÃ©er et gÃ©rer des environnements virtuels\nâœ… Manipuler les types de base (nombres, chaÃ®nes, listes, dictionnaires)\nâœ… Utiliser les conditions et boucles\nâœ… Ã‰crire des fonctions et des classes simples\nâœ… GÃ©rer les erreurs avec try / except\nâœ… Lire et Ã©crire des fichiers (texte, JSON)\nâœ… Utiliser le module logging pour tracer un script",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ Pourquoi Python pour le Data Engineering ?",
    "text": "ğŸ Pourquoi Python pour le Data Engineering ?\n\n\n\n\n\n\n\nRaison\nDÃ©tail\n\n\n\n\nğŸ“š Ã‰cosystÃ¨me riche\nPandas, PySpark, Airflow, dbt, FastAPIâ€¦\n\n\nğŸ”§ Polyvalent\nScripts, APIs, pipelines, ML, automation\n\n\nğŸ¤ Standard de lâ€™industrie\nUtilisÃ© par Netflix, Spotify, Airbnbâ€¦\n\n\nğŸ“– Facile Ã  apprendre\nSyntaxe claire et lisible\n\n\nğŸ”— IntÃ©grations\nConnecteurs pour toutes les bases de donnÃ©es",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0. Installation & Environnement ğŸ› ï¸",
    "text": "0. Installation & Environnement ğŸ› ï¸\nCette section explique comment installer Python, VS Code, Jupyter et vÃ©rifier que tout fonctionne. Les commandes sont donnÃ©es pour Windows, Linux et macOS.\n\n0.1 Installer Python\n\nSous Windows\n\nAller sur le site officiel : https://www.python.org/downloads/\nTÃ©lÃ©charger la derniÃ¨re version stable de Python 3.x.\nLors de lâ€™installation :\n\nCocher â€œAdd Python to PATHâ€ en bas de la premiÃ¨re fenÃªtre ;\npuis cliquer sur Install Now.\n\n\n\n\nSous Linux (Ubuntu / Debian)\nsudo apt update\nsudo apt install -y python3 python3-pip\n\n\nSous macOS\n\nOption 1 : paquet officiel :\n\nTÃ©lÃ©charger un .pkg depuis https://www.python.org/downloads/mac-osx/\nInstaller comme une application classique.\n\nOption 2 (si Homebrew est installÃ©) :\n\nbrew install python\n\n\n\n0.2 VÃ©rifier lâ€™installation de Python\nOuvrir un terminal (ou PowerShell sous Windows) puis taper :\npython --version   # ou parfois: python3 --version\nTu dois voir une version du type : Python 3.12.x.\nâš ï¸ Erreurs frÃ©quentes : - python nâ€™est pas reconnu â†’ Python nâ€™est pas dans le PATH ; - sur Linux/macOS, il faut parfois utiliser python3 au lieu de python.\n\n\n0.3 Installer Visual Studio Code (VS Code)\n\nTÃ©lÃ©charger VS Code : https://code.visualstudio.com/\nInstaller la version adaptÃ©e Ã  ton systÃ¨me (Windows, Linux, macOS).\nLancer VS Code.\n\nVS Code servira Ã  : - Ã©diter des scripts .py ; - ouvrir des notebooks Jupyter (.ipynb) ; - organiser un projet de data engineering complet.\n\n\n0.4 Extensions VS Code : Python & Jupyter\nDans VS Code, aller dans lâ€™onglet Extensions (icÃ´ne de blocs Ã  gauche), puis :\n\nRechercher â€œPythonâ€ (Ã©diteur : Microsoft) et lâ€™installer ;\nRechercher â€œJupyterâ€ (Ã©diteur : Microsoft) et lâ€™installer.\n\nEnsuite, ouvrir un fichier .py ou .ipynb : VS Code proposera de sÃ©lectionner un interprÃ©teur Python (en bas Ã  droite). Choisir ton installation Python 3.x.\n\n\n0.5 Jupyter Notebook â€” Installation et utilisation\nJupyter Notebook est un environnement interactif qui permet dâ€™Ã©crire du code, de lâ€™exÃ©cuter, et de voir les rÃ©sultats immÃ©diatement. Câ€™est lâ€™outil idÃ©al pour apprendre, explorer des donnÃ©es et documenter son travail.\n\nğŸ’¡ Ce cours est lui-mÃªme un Notebook Jupyter (fichier .ipynb) !\n\n\n\nğŸ“¦ Installation\n# Installer Jupyter\npip install notebook\n\n# Ou avec Anaconda (dÃ©jÃ  inclus)\n# Rien Ã  faire, c'est installÃ© par dÃ©faut\n\n\n\nğŸš€ Lancer Jupyter Notebook\n# Dans ton terminal, place-toi dans ton dossier de travail\ncd /chemin/vers/mon/projet\n\n# Lancer Jupyter\njupyter notebook\nCe qui se passe :\n\nUn serveur local dÃ©marre\nTon navigateur sâ€™ouvre automatiquement sur http://localhost:8888\nTu vois la liste des fichiers de ton dossier\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Jupyter                                    [Quit] [Logout]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Files    Running    Clusters                                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  ğŸ“ data/                                                       â”‚\nâ”‚  ğŸ“„ 01_intro.ipynb                                              â”‚\nâ”‚  ğŸ“„ 02_basics.ipynb                                             â”‚\nâ”‚  ğŸ“„ script.py                                                   â”‚\nâ”‚                                                                 â”‚\nâ”‚  [New â–¼]  â† Cliquer ici pour crÃ©er un nouveau notebook          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ“ CrÃ©er un nouveau Notebook\n\nCliquer sur New (en haut Ã  droite)\nSÃ©lectionner Python 3 (ou Python 3 (ipykernel))\nUn nouveau notebook sâ€™ouvre : Untitled.ipynb\nCliquer sur â€œUntitledâ€ pour le renommer (ex: mon_premier_notebook.ipynb)\n\n\n\n\nğŸ–¥ï¸ Lâ€™interface Jupyter\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  mon_notebook.ipynb                              [Trusted]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  File  Edit  View  Insert  Cell  Kernel  Help                   â”‚\nâ”‚  [ğŸ’¾] [+] [âœ‚ï¸] [ğŸ“‹] [â–¶ï¸ Run] [â¹ï¸] [ğŸ”„]    | Code â–¼ |             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  In [1]: â–ˆ                              â† Cellule de code       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nÃ‰lÃ©ment\nDescription\n\n\n\n\nCellule\nBloc oÃ¹ tu Ã©cris du code ou du texte\n\n\nIn [1]:\nNumÃ©ro dâ€™exÃ©cution de la cellule\n\n\nâ–¶ï¸ Run\nExÃ©cute la cellule sÃ©lectionnÃ©e\n\n\nCode â–¼\nType de cellule (Code ou Markdown)\n\n\n\n\n\n\nâŒ¨ï¸ Raccourcis clavier essentiels\n\n\n\nRaccourci\nAction\n\n\n\n\nShift + Enter\nâ–¶ï¸ ExÃ©cuter la cellule et passer Ã  la suivante\n\n\nCtrl + Enter\nExÃ©cuter la cellule (rester dessus)\n\n\nEsc\nPasser en mode commande (cellule bleue)\n\n\nEnter\nPasser en mode Ã©dition (cellule verte)\n\n\nA (mode commande)\nInsÃ©rer une cellule au-dessus\n\n\nB (mode commande)\nInsÃ©rer une cellule en-dessous\n\n\nDD (mode commande)\nSupprimer la cellule\n\n\nM (mode commande)\nConvertir en cellule Markdown\n\n\nY (mode commande)\nConvertir en cellule Code\n\n\nCtrl + S\nSauvegarder le notebook\n\n\n\n\nğŸ’¡ Astuce : Apprends Shift + Enter en premier â€” câ€™est le raccourci que tu utiliseras le plus !\n\n\n\n\nğŸ§ª Premier test dans Jupyter\n\nDans une cellule, tape :\n\nprint(\"Hello Data Engineer !\")\n\nAppuie sur Shift + Enter\nTu dois voir :\n\nIn [1]: print(\"Hello Data Engineer !\")\n\nHello Data Engineer !\n\n\n\nğŸ“„ Types de cellules\n\n\n\nType\nUsage\nExemple\n\n\n\n\nCode\nExÃ©cuter du Python\nprint(\"Hello\")\n\n\nMarkdown\nDocumenter, titres, explications\n# Mon titre\n\n\n\n# Titre principal\n## Sous-titre\n\nDu texte en **gras** et en *italique*.\n\n- Liste Ã  puces\n- Autre Ã©lÃ©ment\n\n\n\nğŸ›‘ ArrÃªter Jupyter\n\nSauvegarder ton notebook (Ctrl + S)\nFermer lâ€™onglet du navigateur\nDans le terminal oÃ¹ Jupyter tourne : appuyer sur Ctrl + C deux fois\n\n^C\nShutdown this notebook server (y/[n])? y\n\n\n\nğŸ’¡ Alternative : Jupyter dans VS Code\nTu peux aussi ouvrir des notebooks directement dans VS Code (avec lâ€™extension Jupyter installÃ©e) :\n\nOuvrir VS Code\nFile &gt; Open File â†’ sÃ©lectionner un fichier .ipynb\nOu crÃ©er un nouveau fichier avec lâ€™extension .ipynb\n\nCâ€™est souvent plus pratique car tu as tout dans le mÃªme Ã©diteur !\n\n\n\n0.6 Premier test Python\nPython peut sâ€™utiliser de 3 faÃ§ons diffÃ©rentes :\n\n\n\nMode\nUsage\nCommande\n\n\n\n\nInteractif\nTester rapidement du code\npython ou python3\n\n\nScript\nExÃ©cuter un fichier .py\npython mon_script.py\n\n\nNotebook\nExploration, visualisation\nJupyter / VS Code\n\n\n\n\n\nğŸ–¥ï¸ Mode interactif (REPL)\nREPL = Read-Eval-Print Loop (Lire-Ã‰valuer-Afficher en boucle)\n1. Lancer lâ€™interprÃ©teur Python :\n# Dans ton terminal (PowerShell, CMD, Bash...)\npython\n# ou sur Linux/macOS\npython3\n2. Tu verras apparaÃ®tre le prompt &gt;&gt;&gt; :\nPython 3.12.0 (main, Oct  2 2024, 12:00:00)\n[GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n3. Taper du code Python directement :\n&gt;&gt;&gt; print(\"Hello Data Engineer\")\nHello Data Engineer\n\n&gt;&gt;&gt; 2 + 3\n5\n\n&gt;&gt;&gt; nom = \"Alice\"\n&gt;&gt;&gt; print(f\"Bonjour {nom}\")\nBonjour Alice\n4. Quitter lâ€™interprÃ©teur :\n&gt;&gt;&gt; exit()\nOu utiliser le raccourci clavier : - Windows : Ctrl + Z puis EntrÃ©e - Linux/macOS : Ctrl + D\n\n\n\nğŸ’¡ Quand utiliser le mode interactif ?\n\n\n\nâœ… Bon usage\nâŒ Mauvais usage\n\n\n\n\nTester une syntaxe rapidement\nÃ‰crire un programme complet\n\n\nVÃ©rifier le rÃ©sultat dâ€™une expression\nCode quâ€™on veut sauvegarder\n\n\nExplorer une librairie (help(fonction))\nPipeline de production\n\n\nCalculatrice avancÃ©e\nTravail collaboratif\n\n\n\n\nğŸ’¡ Pour ce cours, tu utiliseras surtout les Notebooks Jupyter (exploration) et les scripts .py (production). Le mode interactif est utile pour des tests rapides.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0.7 Environnements virtuels ğŸ”’",
    "text": "0.7 Environnements virtuels ğŸ”’\nUn environnement virtuel isole les dÃ©pendances de chaque projet. Câ€™est indispensable en Data Engineering pour Ã©viter les conflits de versions.\n\nPourquoi utiliser un environnement virtuel ?\n\n\n\n\n\n\n\nâŒ Sans environnement virtuel\nâœ… Avec environnement virtuel\n\n\n\n\nTous les projets partagent les mÃªmes packages\nChaque projet a ses propres packages\n\n\nConflits de versions\nIsolation complÃ¨te\n\n\nDifficile Ã  reproduire\nReproductible avec requirements.txt\n\n\n\n\n\nOption 1 : venv (intÃ©grÃ© Ã  Python)\n# CrÃ©er un environnement virtuel\npython -m venv mon_env\n\n# Activer l'environnement\n# Windows\nmon_env\\Scripts\\activate\n\n# Linux / macOS\nsource mon_env/bin/activate\n\n# Tu verras (mon_env) au dÃ©but de ta ligne de commande\n\n# DÃ©sactiver l'environnement\ndeactivate\n\n\nOption 2 : conda (Anaconda/Miniconda)\n# CrÃ©er un environnement\nconda create -n mon_projet python=3.11\n\n# Activer\nconda activate mon_projet\n\n# DÃ©sactiver\nconda deactivate\n\n# Lister les environnements\nconda env list\n\n\nğŸ’¡ Recommandation pour Data Engineers\n\n\n\nOutil\nQuand lâ€™utiliser\n\n\n\n\nvenv\nProjets Python purs, lÃ©gers, CI/CD\n\n\nconda\nData Science, dÃ©pendances complexes (NumPy, Spark)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "0.8 Gestion des packages avec pip ğŸ“¦",
    "text": "0.8 Gestion des packages avec pip ğŸ“¦\npip est le gestionnaire de packages Python. Tu lâ€™utiliseras pour installer les librairies Data Engineering.\n\nCommandes essentielles\n# Installer un package\npip install pandas\n\n# Installer une version spÃ©cifique\npip install pandas==2.0.0\n\n# Installer plusieurs packages\npip install pandas numpy requests\n\n# Mettre Ã  jour un package\npip install --upgrade pandas\n\n# DÃ©sinstaller\npip uninstall pandas\n\n# Lister les packages installÃ©s\npip list\n\n# Voir les infos d'un package\npip show pandas\n\n\nLe fichier requirements.txt\nCe fichier liste toutes les dÃ©pendances dâ€™un projet. Indispensable pour la reproductibilitÃ©.\n# GÃ©nÃ©rer le fichier Ã  partir de l'environnement actuel\npip freeze &gt; requirements.txt\n\n# Installer toutes les dÃ©pendances d'un projet\npip install -r requirements.txt\n\n\nExemple de requirements.txt pour Data Engineering\n# Data Processing\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\npyarrow&gt;=12.0.0\n\n# APIs\nrequests&gt;=2.28.0\nfastapi&gt;=0.100.0\n\n# Database\nsqlalchemy&gt;=2.0.0\npsycopg2-binary&gt;=2.9.0\n\n# Testing\npytest&gt;=7.0.0\n\nğŸ’¡ Bonne pratique : Toujours travailler dans un environnement virtuel avant dâ€™installer des packages !",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "1. Variables et Types ğŸ”¤",
    "text": "1. Variables et Types ğŸ”¤\nUne variable est un nom qui rÃ©fÃ©rence une valeur en mÃ©moire.\nPython possÃ¨de plusieurs types intÃ©grÃ©s (builtins). Voici ceux Ã  maÃ®triser absolument en Data Engineering :\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nType\nExemple\nUsage Data Engineering\n\n\n\n\nNumÃ©rique\nint\n3, 42, -5\nComptage, index, tailles, IDs\n\n\n\nfloat\n3.14, 0.99\nPrix, mesures, statistiques\n\n\nTexte\nstr\n\"Abidjan\"\nParsing CSV/JSON, nettoyage, logs\n\n\nBoolÃ©en\nbool\nTrue, False\nConditions, filtres, validation\n\n\nSÃ©quences ordonnÃ©es\nlist\n[1,2,3]\nLignes CSV, sÃ©ries numÃ©riques\n\n\n\ntuple\n(200, \"OK\")\nValeurs fixes, clÃ©s composites\n\n\nMapping\ndict\n{\"id\":1,\"ville\":\"Paris\"}\nJSON, API, MongoDB\n\n\nEnsemble (unique)\nset\n{\"python\",\"data\"}\nDÃ©duplication (emails, tags)\n\n\nBinaire\nbytes\nb\"abc\"\nFichiers binaires, images, rÃ©seau\n\n\n\n\n\nExemples de variables\nage = 30              # int\npi = 3.14             # float\nnom = \"Alice\"         # str\nest_data_engineer = True  # bool\n\nprint(age, type(age))\nprint(pi, type(pi))\nprint(nom, type(nom))\nprint(est_data_engineer, type(est_data_engineer))\n\n\nConversion de types\nIl est frÃ©quent de convertir des chaÃ®nes en nombres, par exemple aprÃ¨s lecture dâ€™un fichier.\nage_str = \"25\"\nage_int = int(age_str)\nâš ï¸ Erreurs frÃ©quentes : - Essayer de convertir une chaÃ®ne non numÃ©rique : int(\"abc\") â†’ ValueError ; - Additionner directement un str et un int : - \"25\" + 3 âŒ - int(\"25\") + 3 âœ”ï¸",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” 2. Conditions et Boucles",
    "text": "ğŸ” 2. Conditions et Boucles\nLes conditions permettent de prendre des dÃ©cisions dans le code.\nLes boucles permettent de rÃ©pÃ©ter des actions automatiquement.\n\nğŸ’¡ En Data Engineering, ces structures sont essentielles pour : - Filtrer des donnÃ©es (conditions) - Traiter plusieurs fichiers (boucles) - Valider des rÃ¨gles mÃ©tier (conditions) - Parcourir des bases de donnÃ©es (boucles)\n\n\n\nğŸ¯ 2.1 Conditions (if / elif / else)\nLes conditions testent des critÃ¨res et exÃ©cutent du code selon le rÃ©sultat.\n\nSyntaxe de base\nage = 20\n\nif age &lt; 18:\n    print(\"Mineur\")\nelif age == 18:\n    print(\"Tout juste majeur\")\nelse:\n    print(\"Majeur\")\nFonctionnement :\n\nif : PremiÃ¨re condition testÃ©e\nelif : Condition alternative (si if est fausse)\nelse : Cas par dÃ©faut (si toutes les conditions sont fausses)\n\n\n\nOpÃ©rateurs de comparaison\n\n\n\nOpÃ©rateur\nSignification\nExemple\n\n\n\n\n==\nÃ‰gal Ã \nage == 18\n\n\n!=\nDiffÃ©rent de\nage != 18\n\n\n&lt;\nInfÃ©rieur Ã \nage &lt; 18\n\n\n&lt;=\nInfÃ©rieur ou Ã©gal\nage &lt;= 18\n\n\n&gt;\nSupÃ©rieur Ã \nage &gt; 18\n\n\n&gt;=\nSupÃ©rieur ou Ã©gal\nage &gt;= 18\n\n\n\n\n\nConditions multiples\nage = 25\npays = \"France\"\n\n# OpÃ©rateur AND (et)\nif age &gt;= 18 and pays == \"France\":\n    print(\"Peut voter en France\")\n\n# OpÃ©rateur OR (ou)\nif age &lt; 18 or age &gt; 65:\n    print(\"Tarif rÃ©duit\")\n\n# OpÃ©rateur NOT (nÃ©gation)\nif not (age &lt; 18):\n    print(\"Majeur\")\n\n\n\n\nğŸ”„ 2.2 Boucles (for et while)\nLes boucles permettent dâ€™exÃ©cuter du code de maniÃ¨re rÃ©pÃ©tÃ©e.\n\nBoucle for : Parcourir une sÃ©quence\n# Parcourir une liste\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\n\nfor nom in noms:\n    print(f\"Bonjour {nom}\")\n\n# RÃ©sultat :\n# Bonjour Alice\n# Bonjour Bob\n# Bonjour Charlie\n\n\nBoucle avec range()\n# range(n) gÃ©nÃ¨re les nombres de 0 Ã  n-1\nfor i in range(5):\n    print(f\"ItÃ©ration {i}\")\n\n# RÃ©sultat : 0, 1, 2, 3, 4\n\n# range(dÃ©but, fin, pas)\nfor i in range(0, 10, 2):\n    print(i)  # 0, 2, 4, 6, 8\n\n\nBoucle while : RÃ©pÃ©ter tant quâ€™une condition est vraie\ncompteur = 0\n\nwhile compteur &lt; 3:\n    print(f\"Compteur = {compteur}\")\n    compteur += 1  # âš ï¸ IMPORTANT : incrÃ©mentation obligatoire\n\n# RÃ©sultat :\n# Compteur = 0\n# Compteur = 1\n# Compteur = 2\n\n\nContrÃ´le de flux : break et continue\n# break : Sortir immÃ©diatement de la boucle\nfor i in range(10):\n    if i == 5:\n        break  # ArrÃªte la boucle Ã  5\n    print(i)  # Affiche 0, 1, 2, 3, 4\n\n# continue : Passer Ã  l'itÃ©ration suivante\nfor i in range(5):\n    if i == 2:\n        continue  # Saute l'itÃ©ration quand i=2\n    print(i)  # Affiche 0, 1, 3, 4\n\n\nğŸ“Œ Cas dâ€™usage Data Engineering\n# Exemple 1 : Traitement de fichiers multiples\nfichiers = [\"users_2024_01.csv\", \"users_2024_02.csv\", \"users_2024_03.csv\"]\n\nfor fichier in fichiers:\n    print(f\"Traitement de {fichier}...\")\n    # Ici : logique de lecture/transformation\n    # df = pd.read_csv(fichier)\n    # process(df)\n\n# Exemple 2 : Nettoyage de donnÃ©es\nposts = [\n    {\"text\": \"Hello\", \"likes\": 10},\n    {\"text\": \"\", \"likes\": 5},       # âš ï¸ Texte vide\n    {\"text\": \"Python\", \"likes\": 20}\n]\n\nposts_valides = []\n\nfor post in posts:\n    # Skip les posts vides\n    if not post[\"text\"].strip():\n        continue\n    \n    # Nettoyer et garder\n    post[\"text\"] = post[\"text\"].strip().lower()\n    posts_valides.append(post)\n\nprint(posts_valides)\n# [{'text': 'hello', 'likes': 10}, {'text': 'python', 'likes': 20}]\n\n# Exemple 3 : Retry logic (tentatives multiples)\nmax_tentatives = 3\ntentative = 0\nsucces = False\n\nwhile tentative &lt; max_tentatives and not succes:\n    print(f\"Tentative {tentative + 1}...\")\n    \n    # Simulation d'une connexion\n    # succes = tenter_connexion()\n    \n    tentative += 1\n    \n    if not succes and tentative &lt; max_tentatives:\n        print(\"Ã‰chec, nouvelle tentative...\")\n\n\n\n\nğŸ” 2.3 Boucles avancÃ©es : enumerate() et zip()\n\nenumerate() : Obtenir lâ€™index ET la valeur\nfruits = [\"pomme\", \"banane\", \"orange\"]\n\n# Sans enumerate (moins pratique)\nfor i in range(len(fruits)):\n    print(f\"{i}: {fruits[i]}\")\n\n# Avec enumerate (recommandÃ©)\nfor index, fruit in enumerate(fruits):\n    print(f\"{index}: {fruit}\")\n\n# Avec enumerate dÃ©marrant Ã  1\nfor num, fruit in enumerate(fruits, start=1):\n    print(f\"Fruit #{num}: {fruit}\")\n\n\nzip() : Parcourir plusieurs listes simultanÃ©ment\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 35]\nvilles = [\"Paris\", \"Lyon\", \"Marseille\"]\n\nfor nom, age, ville in zip(noms, ages, villes):\n    print(f\"{nom} a {age} ans et habite Ã  {ville}\")\n\n# RÃ©sultat :\n# Alice a 25 ans et habite Ã  Paris\n# Bob a 30 ans et habite Ã  Lyon\n# Charlie a 35 ans et habite Ã  Marseille\n\n\n\n\nâš ï¸ Erreurs frÃ©quentes et bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ Erreur\nâœ… Correction\nğŸ’¡ Explication\n\n\n\n\nOublier : aprÃ¨s if/for/while\nif age &gt; 18:\nSyntaxe obligatoire\n\n\nMauvaise indentation\nUtiliser 4 espaces\nPython est sensible Ã  lâ€™indentation\n\n\nBoucle infinie while\nToujours incrÃ©menter\ncompteur += 1\n\n\nModifier liste pendant for\nCrÃ©er nouvelle liste\nÃ‰vite comportements imprÃ©visibles\n\n\n= au lieu de ==\nif age == 18:\n= assigne, == compare\n\n\n\nExemples dâ€™erreurs :\n# âŒ Erreur 1 : Oublier le :\nif age &gt; 18\n    print(\"Majeur\")  # SyntaxError\n\n# âœ… Correction\nif age &gt; 18:\n    print(\"Majeur\")\n\n# âŒ Erreur 2 : Boucle infinie\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    # Oubli d'incrÃ©menter â†’ boucle infinie !\n\n# âœ… Correction\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    compteur += 1\n\n# âŒ Erreur 3 : Modifier liste pendant boucle\nnombres = [1, 2, 3, 4, 5]\nfor n in nombres:\n    if n % 2 == 0:\n        nombres.remove(n)  # âš ï¸ Comportement imprÃ©visible\n\n# âœ… Correction : List comprehension\nnombres = [1, 2, 3, 4, 5]\nnombres_impairs = [n for n in nombres if n % 2 != 0]\n\n\n\nğŸ“š RÃ©capitulatif\n\n\n\nStructure\nUsage\nExemple\n\n\n\n\nif/elif/else\nPrendre des dÃ©cisions\nValidation, filtrage\n\n\nfor\nParcourir sÃ©quences\nTraiter fichiers, lignes\n\n\nwhile\nRÃ©pÃ©ter tant queâ€¦\nRetry logic, polling\n\n\nbreak\nSortir de boucle\nArrÃªt prÃ©maturÃ©\n\n\ncontinue\nSauter itÃ©ration\nSkip donnÃ©es invalides\n\n\nenumerate()\nIndex + valeur\nNumÃ©rotation\n\n\nzip()\nCombiner listes\nJoindre donnÃ©es parallÃ¨les\n\n\n\n\n\n\nğŸ¯ Points clÃ©s Ã  retenir\n\nConditions : Utilisent == pour comparer (pas =)\nIndentation : 4 espaces obligatoires aprÃ¨s :\nwhile : Toujours prÃ©voir une sortie de boucle\nfor : PrÃ©fÃ©rer enumerate() si besoin de lâ€™index\nList comprehension : Alternative Ã©lÃ©gante aux boucles simples",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "3. Structures de donnÃ©es ğŸ§±",
    "text": "3. Structures de donnÃ©es ğŸ§±\nPython propose plusieurs structures trÃ¨s utilisÃ©es en data engineering.\n\n\n\nStructure\nOrdonnÃ©\nModifiable\nDuplicats\nAccÃ¨s principal\n\n\n\n\nlist\nâœ”ï¸\nâœ”ï¸\nâœ”ï¸\nindex (0, 1, 2â€¦)\n\n\ndict\nâœ”ï¸ (3.7+)\nâœ”ï¸\nclÃ©s uniques\nclÃ© (\"nom\")\n\n\ntuple\nâœ”ï¸\nâŒ\nâœ”ï¸\nindex\n\n\nset\nâŒ\nâœ”ï¸\nâŒ\nappartenance (in)\n\n\n\n\n3.1 Listes (list)\nUne liste est une sÃ©quence ordonnÃ©e et modifiable.\n# CrÃ©ation d'une liste\nnombres = [10, 20, 30, 40]\n\n# AccÃ¨s par index\nprint(nombres[0])   # 10\nprint(nombres[2])   # 30\n\n# Ajout en fin de liste\nnombres.append(50)\nprint(\"AprÃ¨s append :\", nombres)\n\n# Insertion Ã  une position prÃ©cise\nnombres.insert(1, 15)\nprint(\"AprÃ¨s insert :\", nombres)\n\n# Modification d'un Ã©lÃ©ment\nnombres[0] = 5\nprint(\"AprÃ¨s modification :\", nombres)\n\n# Suppression par valeur\nnombres.remove(30)\nprint(\"AprÃ¨s remove :\", nombres)\n\n# Suppression par index\ndel nombres[0]\nprint(\"AprÃ¨s del :\", nombres)\n\nCrÃ©ation dynamique de listes\nOn crÃ©e trÃ¨s souvent des listes Ã  partir dâ€™autres listes, avec une boucle ou une list comprehension.\nnombres = [1, 2, 3, 4, 5, 6]\n\n# Version avec boucle\npairs = []\nfor n in nombres:\n    if n % 2 == 0:\n        pairs.append(n)\n\nprint(\"Pairs (boucle) :\", pairs)\n\n# Version list comprehension\npairs2 = [n for n in nombres if n % 2 == 0]\nprint(\"Pairs (list comprehension) :\", pairs2)\nâš ï¸ Erreurs frÃ©quentes avec les listes : - nombres[10] alors que la liste a moins dâ€™Ã©lÃ©ments â†’ IndexError ; - nombres.remove(999) alors que 999 nâ€™est pas dans la liste â†’ ValueError.\n\n\n\n3.2 Dictionnaires (dict)\nUn dictionnaire stocke des paires clÃ© â†’ valeur. Câ€™est lâ€™Ã©quivalent naturel des objets JSON, trÃ¨s utilisÃ© pour les APIs et NoSQL.\nutilisateur = {\n    \"id\": 1,\n    \"nom\": \"Alice\",\n    \"ville\": \"Abidjan\"\n}\n\n# AccÃ¨s Ã  une valeur par clÃ©\nprint(utilisateur[\"nom\"])  # Alice\n\n# Ajout / modification\nutilisateur[\"age\"] = 30\nutilisateur[\"ville\"] = \"BouakÃ©\"\n\n# Suppression\ndel utilisateur[\"id\"]\n\nprint(utilisateur)\n\nAccÃ¨s sÃ©curisÃ© avec .get()\nUtiliser dict.get() permet dâ€™Ã©viter un KeyError si la clÃ© nâ€™existe pas.\nprint(utilisateur.get(\"email\"))           # None\nprint(utilisateur.get(\"email\", \"Inconnu\"))  # Inconnu\n\n\nCrÃ©ation dynamique : comptage dâ€™occurrences\nnoms = [\"bob\", \"alice\", \"bob\", \"charlie\", \"alice\"]\ncompte = {}\n\nfor n in noms:\n    compte[n] = compte.get(n, 0) + 1\n\nprint(compte)  # {'bob': 2, 'alice': 2, 'charlie': 1}\nâš ï¸ Erreurs frÃ©quentes avec les dictionnaires : - utilisateur[\"email\"] alors que la clÃ© nâ€™existe pas â†’ KeyError ; - supposer quâ€™un dictionnaire est indexÃ© comme une liste (utilisateur[0]).\n\n\n\n3.3 Tuples (tuple)\nUn tuple est comme une liste non modifiable (immutable). On lâ€™utilise pour reprÃ©senter des collections fixes de valeurs : coordonnÃ©es, dates, etc.\ncoord = (5.0, 10.0)\nprint(coord[0])  # 5.0\nprint(coord[1])  # 10.0\n\n# coord[0] = 20.0  # âŒ TypeError : un tuple n'est pas modifiable\nEn data engineering, les tuples sont utiles pour : - retourner plusieurs valeurs depuis une fonction ; - reprÃ©senter des clÃ©s composites (ex : (annÃ©e, mois)).\n\n\n3.4 Ensembles (set)\nUn ensemble (set) contient des valeurs uniques, sans ordre garanti. TrÃ¨s utile pour dÃ©dupliquer une liste.\ntags = {\"python\", \"data\", \"python\"}\nprint(tags)  # 'python' n'apparaÃ®t qu'une seule fois\n\n# Ajout\ntags.add(\"engineer\")\n\n# Suppression (erreur si absent)\ntags.remove(\"data\")\n\n# Suppression sans erreur si absent\ntags.discard(\"ai\")\n\nprint(tags)\nğŸ’¡ Exemple de dÃ©duplication :\nemails = [\"a@test.com\", \"b@test.com\", \"a@test.com\"]\nemails_uniques = list(set(emails))\nprint(emails_uniques)\nâš ï¸ Erreurs frÃ©quentes : - Compter sur lâ€™ordre dâ€™un set (lâ€™ordre nâ€™est pas garanti) ; - Utiliser remove() pour un Ã©lÃ©ment possiblement absent â†’ prÃ©fÃ©rer discard().\n\n\nğŸ§± 3.5 Mini-exercice â€” Structures de donnÃ©es\nğŸ“Œ DonnÃ©es\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\nğŸ¯ Objectifs 1. CrÃ©er la liste de tous les utilisateurs (list) 2. CrÃ©er la liste unique des utilisateurs (set) 3. CrÃ©er un dictionnaire qui compte le nombre de requÃªtes par utilisateur (dict)\n# Ã€ toi de jouer ğŸ˜Š\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []            # TODO\nutilisateurs_uniques = set() # TODO\ncompte_par_user = {}         # TODO\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []\nutilisateurs_uniques = set()\ncompte_par_user = {}\n\nfor log in logs:\n    user = log[\"user\"]\n    utilisateurs.append(user)\n    utilisateurs_uniques.add(user)\n    compte_par_user[user] = compte_par_user.get(user, 0) + 1\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸš€ Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es",
    "text": "ğŸš€ Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es\nLes comprehensions sont une syntaxe Python Ã©lÃ©gante et performante pour crÃ©er des listes, dictionnaires ou sets en une seule ligne. Câ€™est fondamental en Data Engineering pour transformer des donnÃ©es efficacement.\n\n\nğŸ“‹ List Comprehension\nSyntaxe : [expression for item in iterable if condition]\n# âŒ MÃ©thode classique (verbose)\nnombres = [1, 2, 3, 4, 5]\ncarres = []\nfor n in nombres:\n    carres.append(n ** 2)\nprint(carres)  # [1, 4, 9, 16, 25]\n\n# âœ… List comprehension (recommandÃ©)\ncarres = [n ** 2 for n in nombres]\nprint(carres)  # [1, 4, 9, 16, 25]\n\nAvec condition (filtre)\n# Garder seulement les nombres pairs\nnombres = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npairs = [n for n in nombres if n % 2 == 0]\nprint(pairs)  # [2, 4, 6, 8, 10]\n\n# Exemple Data Engineering : filtrer les emails valides\nemails = [\"alice@test.com\", \"invalid\", \"bob@company.org\", \"\"]\nemails_valides = [e for e in emails if \"@\" in e and e]\nprint(emails_valides)  # ['alice@test.com', 'bob@company.org']\n\n\nCas dâ€™usage Data Engineering\n# Extraire les noms d'une liste de dictionnaires\nusers = [\n    {\"nom\": \"Alice\", \"actif\": True},\n    {\"nom\": \"Bob\", \"actif\": False},\n    {\"nom\": \"Charlie\", \"actif\": True}\n]\n\n# Noms des utilisateurs actifs en majuscules\nnoms_actifs = [u[\"nom\"].upper() for u in users if u[\"actif\"]]\nprint(noms_actifs)  # ['ALICE', 'CHARLIE']\n\n# Nettoyer une liste de fichiers\nfichiers = [\"data.csv\", \"readme.txt\", \"users.csv\", \"config.yaml\"]\ncsv_files = [f for f in fichiers if f.endswith(\".csv\")]\nprint(csv_files)  # ['data.csv', 'users.csv']\n\n\n\n\nğŸ“– Dict Comprehension\nSyntaxe : {key: value for item in iterable if condition}\n# CrÃ©er un dictionnaire Ã  partir de deux listes\nnoms = [\"alice\", \"bob\", \"charlie\"]\nages = [25, 30, 35]\n\nusers_dict = {nom: age for nom, age in zip(noms, ages)}\nprint(users_dict)  # {'alice': 25, 'bob': 30, 'charlie': 35}\n\n# Inverser un dictionnaire\noriginal = {\"a\": 1, \"b\": 2, \"c\": 3}\ninverse = {v: k for k, v in original.items()}\nprint(inverse)  # {1: 'a', 2: 'b', 3: 'c'}\n\nCas dâ€™usage Data Engineering\n# Transformer des donnÃ©es JSON\nraw_data = [\n    {\"id\": 1, \"value\": \"100\"},\n    {\"id\": 2, \"value\": \"200\"},\n    {\"id\": 3, \"value\": \"300\"}\n]\n\n# CrÃ©er un mapping id -&gt; valeur (convertie en int)\nid_to_value = {d[\"id\"]: int(d[\"value\"]) for d in raw_data}\nprint(id_to_value)  # {1: 100, 2: 200, 3: 300}\n\n\n\n\nğŸ¯ Set Comprehension\nSyntaxe : {expression for item in iterable if condition}\n# Valeurs uniques\nnombres = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\nuniques = {n for n in nombres}\nprint(uniques)  # {1, 2, 3, 4}\n\n# Domaines uniques des emails\nemails = [\"alice@gmail.com\", \"bob@yahoo.com\", \"charlie@gmail.com\"]\ndomaines = {e.split(\"@\")[1] for e in emails}\nprint(domaines)  # {'gmail.com', 'yahoo.com'}\n\n\n\nâš ï¸ Bonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nComprehensions simples et lisibles\nComprehensions imbriquÃ©es complexes\n\n\nUne seule transformation\nPlusieurs opÃ©rations chaÃ®nÃ©es\n\n\nUtiliser pour crÃ©er des collections\nUtiliser pour des effets de bord\n\n\n\n\n\n\nğŸ§ª Mini-exercice\nTransformer cette liste de transactions :\ntransactions = [\n    {\"id\": 1, \"montant\": 100, \"devise\": \"EUR\"},\n    {\"id\": 2, \"montant\": 50, \"devise\": \"USD\"},\n    {\"id\": 3, \"montant\": 200, \"devise\": \"EUR\"},\n    {\"id\": 4, \"montant\": 75, \"devise\": \"USD\"}\n]\n\nCrÃ©er une liste des montants en EUR uniquement\nCrÃ©er un dict {id: montant} pour les transactions &gt; 60\n\n\n\nğŸ’¡ Solution\n\n# 1. Montants EUR\nmontants_eur = [t[\"montant\"] for t in transactions if t[\"devise\"] == \"EUR\"]\nprint(montants_eur)  # [100, 200]\n\n# 2. Dict id -&gt; montant (&gt; 60)\nid_montant = {t[\"id\"]: t[\"montant\"] for t in transactions if t[\"montant\"] &gt; 60}\nprint(id_montant)  # {1: 100, 3: 200, 4: 75}",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "4. Fonctions",
    "text": "4. Fonctions\nUne fonction permet de :\n\nrÃ©utiliser du code ;\nencapsuler de la logique mÃ©tier (nettoyage, validationâ€¦) ;\nsÃ©curiser la qualitÃ© de donnÃ©es (type, structureâ€¦).\n\nSyntaxe gÃ©nÃ©rale :\ndef nom(param1, param2=valeur_par_defaut) -&gt; type_retour:\n    # traitement\n    return resultat\nExemple :\ndef somme(a: int, b: int) -&gt; int:\n    \"\"\"Retourne la somme de deux entiers.\"\"\"\n    return a + b\n\nresultat = somme(3, 5)\nprint(\"RÃ©sultat :\", resultat)\n\nâœ”ï¸ 4.1 Fonction pure (sans effet externe)\ndef normaliser_nom(nom: str) -&gt; str:\n    \"\"\"Nettoie un nom : supprime espaces, met en minuscule et capitalise.\"\"\"\n    return nom.strip().lower().capitalize()\n\nprint(normaliser_nom(\"  aLiCe  \"))  # Alice\n\n\nâš™ï¸ 4.2 ParamÃ¨tres + valeurs par dÃ©faut\ndef calculer_total(prix: float, quantite: int = 1, taxe: float = 0.18) -&gt; float:\n    \"\"\"Retourne le prix total avec taxe.\"\"\"\n    return prix * quantite * (1 + taxe)\n\nprint(calculer_total(2000))  \nprint(calculer_total(2000, quantite=3, taxe=0.09))\n\n\nğŸ“Š 4.3 Retourner plusieurs valeurs (tuple)\ndef stats_notes(notes: list[int]) -&gt; tuple[float, float]:\n    \"\"\"Retourne moyenne et maximum d'une liste de notes.\"\"\"\n    moyenne = sum(notes) / len(notes)\n    maxi = max(notes)\n    return moyenne, maxi\n\nm, mx = stats_notes([14, 9, 18])\nprint(\"Moyenne:\", m, \"Max:\", mx)\n\n\nğŸ”— 4.4 Fonctions qui manipulent un dictionnaire\ndef extraire_champ(data: dict, champ: str, default=None):\n    \"\"\"RÃ©cupÃ¨re un champ d'un dict, Ã©vite KeyError.\"\"\"\n    return data.get(champ, default)\n\nuser = {\"nom\": \"Sara\", \"ville\": \"Paris\"}\nprint(extraire_champ(user, \"nom\"))         # Sara\nprint(extraire_champ(user, \"age\", \"N/A\"))  # N/A\nâš ï¸ Erreurs frÃ©quentes : - Oublier les parenthÃ¨ses lors de lâ€™appel : somme au lieu de somme(3, 5) ; - Oublier return â†’ la fonction retourne None ; - Ne pas respecter le nombre dâ€™arguments attendus.\nâŒ Mauvaise pratique :\ndef ajouter(element, liste=[]):  # liste partagÃ©e !\n    liste.append(element)\n    return liste\nâœ”ï¸ Correct :\ndef ajouter(element, liste=None):\n    if liste is None: liste = []\n    liste.append(element)\n    return liste\n\n\nğŸ§ª Mini-exercice â€” Fonctions sur des posts\n\nğŸ“Œ Dataset simulÃ©\nposts = [\n  {\"user\": \"alice\", \"text\": \"  Hello World  \"},\n  {\"user\": \"bob\", \"text\": \"Data Engineer ici\"},\n  {\"user\": \"alice\", \"text\": \"Python est top \"}\n]\n\n\nğŸ¯ Instructions\nCrÃ©er 3 fonctions :\n\nnettoyer_texte(text: str) -&gt; str\nNettoie le texte (supprime espaces, convertit en minuscules)\nlongueur_post(post: dict) -&gt; int\nRetourne la longueur du texte nettoyÃ© dâ€™un post\nstats_posts(posts: list[dict]) -&gt; tuple[float, int, int]\nRetour attendu : (moyenne, maximum, minimum) des longueurs de texte\n\n# Ã€ toi de jouer ğŸ˜Š\n\n# TODO\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\ndef longueur_post(post: dict) -&gt; int:\n    return len(nettoyer_texte(post[\"text\"]))\n\ndef stats_posts(posts: list[dict]) -&gt; tuple[float, int, int]:\n    longueurs = [longueur_post(p) for p in posts]\n    return (sum(longueurs)/len(longueurs), max(longueurs), min(longueurs))\n\nprint(stats_posts(posts))\nRÃ©sultat attendu : (15.333333333333334, 19, 11)\n\n\n\n\nğŸ”§ 4.5 *args et **kwargs â€” Fonctions flexibles\nCes syntaxes permettent de crÃ©er des fonctions qui acceptent un nombre variable dâ€™arguments. TrÃ¨s utilisÃ© pour crÃ©er des wrappers, des dÃ©corateurs, ou des fonctions gÃ©nÃ©riques.\n\n\n*args â€” Arguments positionnels variables\ndef somme(*args):\n    \"\"\"Accepte n'importe quel nombre d'arguments.\"\"\"\n    print(f\"Arguments reÃ§us : {args}\")  # C'est un tuple\n    return sum(args)\n\nprint(somme(1, 2))           # 3\nprint(somme(1, 2, 3, 4, 5))  # 15\nprint(somme())               # 0\n\n\n\n**kwargs â€” Arguments nommÃ©s variables\ndef afficher_info(**kwargs):\n    \"\"\"Accepte n'importe quel argument nommÃ©.\"\"\"\n    print(f\"Arguments reÃ§us : {kwargs}\")  # C'est un dict\n    for cle, valeur in kwargs.items():\n        print(f\"  {cle} = {valeur}\")\n\nafficher_info(nom=\"Alice\", age=30, ville=\"Paris\")\n# Arguments reÃ§us : {'nom': 'Alice', 'age': 30, 'ville': 'Paris'}\n#   nom = Alice\n#   age = 30\n#   ville = Paris\n\n\n\nCombiner les deux\ndef fonction_flexible(obligatoire, *args, option=\"defaut\", **kwargs):\n    \"\"\"Ordre : obligatoire, *args, avec dÃ©faut, **kwargs\"\"\"\n    print(f\"Obligatoire : {obligatoire}\")\n    print(f\"Args : {args}\")\n    print(f\"Option : {option}\")\n    print(f\"Kwargs : {kwargs}\")\n\nfonction_flexible(\"premier\", 1, 2, 3, option=\"custom\", extra=\"valeur\")\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\ndef log_etl(etape: str, *messages, niveau: str = \"INFO\", **metadata):\n    \"\"\"Logger flexible pour pipeline ETL.\"\"\"\n    print(f\"[{niveau}] {etape}\")\n    for msg in messages:\n        print(f\"  - {msg}\")\n    if metadata:\n        print(f\"  Metadata: {metadata}\")\n\n# Utilisation\nlog_etl(\n    \"EXTRACT\",\n    \"Connexion Ã©tablie\",\n    \"1000 lignes lues\",\n    niveau=\"INFO\",\n    source=\"postgres\",\n    table=\"users\"\n)\n# Wrapper pour ajouter du logging Ã  n'importe quelle fonction\ndef avec_logging(func):\n    def wrapper(*args, **kwargs):\n        print(f\"Appel de {func.__name__} avec args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"RÃ©sultat : {result}\")\n        return result\n    return wrapper",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤",
    "text": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\nu = Utilisateur(1, \"Alice\", \"Abidjan\")\nprint(u.presentation())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.2 Ajouter une mÃ©thode utile ğŸ™ï¸",
    "text": "5.2 Ajouter une mÃ©thode utile ğŸ™ï¸\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\n\n    def changer_ville(self, nouvelle_ville: str) -&gt; None:\n        self.ville = nouvelle_ville",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s ğŸ”’",
    "text": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s ğŸ”’\nclass Compte:\n    def __init__(self, solde: float):\n        self._solde = solde        # usage interne\n        self.__secret = \"XYZ123\"   # privÃ© via name mangling",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.4 ReprÃ©sentation textuelle (str) ğŸ“",
    "text": "5.4 ReprÃ©sentation textuelle (str) ğŸ“\nclass Utilisateur:\n    def __init__(self, identifiant, nom, ville):\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def __str__(self) -&gt; str:\n        return f\"[{self.identifiant}] {self.nom} ({self.ville})\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.5 Composition dâ€™objets (objet dans un objet) ğŸ§±",
    "text": "5.5 Composition dâ€™objets (objet dans un objet) ğŸ§±\nclass Adresse:\n    def __init__(self, rue: str, ville: str, pays: str) -&gt; None:\n        self.rue = rue\n        self.ville = ville\n        self.pays = pays\n\n    def __str__(self) -&gt; str:\n        return f\"{self.rue}, {self.ville} ({self.pays})\"\n\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, adresse: Adresse) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.adresse = adresse\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.adresse}.\"",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.6 Pourquoi utiliser des classes en Data Engineering ? âš™ï¸",
    "text": "5.6 Pourquoi utiliser des classes en Data Engineering ? âš™ï¸\nclass Transaction:\n    def __init__(self, id, montant, devise, timestamp):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n        self.timestamp = timestamp\n\nclass Transaction:\n    def __init__(self, id, montant, devise):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n\n    def montant_fcfa(self):\n        taux = {\"EUR\": 655, \"USD\": 600}\n        return self.montant * taux.get(self.devise, 1)\n\nclass ExtracteurCSV:\n    def __init__(self, chemin):\n        self.chemin = chemin\n\n    def extract(self):\n        with open(self.chemin) as f:\n            return f.readlines()\n\nevent = EventHubRecord(payload)\nevent.clean()\nevent.validate()\nevent.to_parquet()",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "5.7 Erreurs frÃ©quentes âš ï¸",
    "text": "5.7 Erreurs frÃ©quentes âš ï¸\n\nOublier self dans les mÃ©thodes.\nAccÃ©der Ã  un attribut avant de lâ€™avoir crÃ©Ã©.\nUtiliser une valeur mutable dans __init__ (list, dict).\nConfondre composition et hÃ©ritage.\nFaire une classe Â« God Object Â» avec trop de responsabilitÃ©s.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“¦ Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es",
    "text": "ğŸ“¦ Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es\nLes dataclasses (Python 3.7+) simplifient la crÃ©ation de classes qui servent principalement Ã  stocker des donnÃ©es. Câ€™est le standard moderne en Data Engineering pour modÃ©liser des structures de donnÃ©es.\n\n\nğŸ¤” ProblÃ¨me avec les classes classiques\n# âŒ Classe classique â€” verbose et rÃ©pÃ©titif\nclass User:\n    def __init__(self, id: int, nom: str, email: str, actif: bool = True):\n        self.id = id\n        self.nom = nom\n        self.email = email\n        self.actif = actif\n    \n    def __repr__(self):\n        return f\"User(id={self.id}, nom='{self.nom}', email='{self.email}', actif={self.actif})\"\n    \n    def __eq__(self, other):\n        return self.id == other.id and self.nom == other.nom\n\n\n\nâœ… Solution avec dataclass\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n    actif: bool = True  # Valeur par dÃ©faut\n\n# Utilisation\nuser = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user)  # User(id=1, nom='Alice', email='alice@test.com', actif=True)\n\n# Comparaison automatique\nuser2 = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user == user2)  # True\n\n\n\nğŸ¯ Avantages des dataclasses\n\n\n\nAvantage\nDescription\n\n\n\n\n__init__ auto-gÃ©nÃ©rÃ©\nPlus besoin dâ€™Ã©crire le constructeur\n\n\n__repr__ auto-gÃ©nÃ©rÃ©\nAffichage lisible pour le debug\n\n\n__eq__ auto-gÃ©nÃ©rÃ©\nComparaison par valeur\n\n\nType hints intÃ©grÃ©s\nDocumentation et validation IDE\n\n\nValeurs par dÃ©faut\nSyntaxe simple\n\n\n\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass Transaction:\n    id: int\n    montant: float\n    devise: str\n    timestamp: str\n    metadata: dict = field(default_factory=dict)  # Pour les types mutables\n\n# CrÃ©er une transaction\ntx = Transaction(\n    id=1001,\n    montant=150.50,\n    devise=\"EUR\",\n    timestamp=\"2024-01-15T10:30:00Z\"\n)\nprint(tx)\n\n# AccÃ©der aux attributs\nprint(f\"Montant: {tx.montant} {tx.devise}\")\n\nDataclass immuable (frozen)\n@dataclass(frozen=True)  # Immuable comme un tuple\nclass Config:\n    host: str\n    port: int\n    database: str\n\nconfig = Config(host=\"localhost\", port=5432, database=\"warehouse\")\n# config.port = 3306  # âŒ Erreur : FrozenInstanceError\n\n\n\n\nğŸ”„ Convertir en dict/tuple\nfrom dataclasses import asdict, astuple\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n\nuser = User(1, \"Alice\", \"alice@test.com\")\n\n# Conversion en dict (utile pour JSON)\nuser_dict = asdict(user)\nprint(user_dict)  # {'id': 1, 'nom': 'Alice', 'email': 'alice@test.com'}\n\n# Conversion en tuple\nuser_tuple = astuple(user)\nprint(user_tuple)  # (1, 'Alice', 'alice@test.com')\n\n\n\nâš ï¸ Attention : valeurs par dÃ©faut mutables\n# âŒ ERREUR : liste partagÃ©e entre instances\n@dataclass\nclass BadExample:\n    items: list = []  # ValueError!\n\n# âœ… CORRECT : utiliser field(default_factory=...)\nfrom dataclasses import field\n\n@dataclass\nclass GoodExample:\n    items: list = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n\n\n\nğŸ’¡ Quand utiliser dataclass vs classe classique ?\n\n\n\nSituation\nRecommandation\n\n\n\n\nStocker des donnÃ©es (models, records)\nâœ… @dataclass\n\n\nLogique mÃ©tier complexe\nClasse classique\n\n\nConfiguration, paramÃ¨tres\nâœ… @dataclass(frozen=True)\n\n\nValidation avancÃ©e\nPydantic (module suivant)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "6. Gestion dâ€™erreurs ğŸ›‘ â€” (Indispensable en Data Engineering)",
    "text": "6. Gestion dâ€™erreurs ğŸ›‘ â€” (Indispensable en Data Engineering)\nEn Data Engineering, les erreurs sont inÃ©vitables :\n\nfichiers manquants\n\nAPI indisponible\n\nJSON mal formÃ©\n\ndivision par zÃ©ro\n\nconnexion BD Ã©chouÃ©e\n\ntypage incorrect\n\nğŸ‘‰ La bonne pratique consiste Ã  capturer, expliquer, puis continuer proprement.\nCâ€™est le rÃ´le de try / except.\n\n\nğŸ¯ Exemple simple â€” division sÃ©curisÃ©e\ndef division(a: float, b: float) -&gt; float | None:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        print(\"âŒ Erreur : division par zÃ©ro.\")\n        return None\nprint(division(10, 2))  # OK\nprint(division(10, 0))  # Erreur gÃ©rÃ©e\n\n\n\nğŸ§° Exemple plus rÃ©aliste â€” lecture de fichier\ndef lire_csv(chemin: str) -&gt; list | None:\n    try:\n        with open(chemin, \"r\") as f:\n            return f.readlines()\n    except FileNotFoundError:\n        print(f\"âŒ Fichier introuvable : {chemin}\")\n        return None\n\n\n\nğŸ’¡ else et finally\nOn peut amÃ©liorer la lisibilitÃ© avec else et finally :\ntry:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print(\"Erreur\")\nelse:\n    print(\"Aucune erreur, rÃ©sultat =\", result)\nfinally:\n    print(\"Bloc exÃ©cutÃ© dans tous les cas\")\n\n\n\nğŸ—ï¸ Exemple Data Engineering : appel API sÃ©curisÃ©\nimport requests\n\ndef fetch_json(url: str) -&gt; dict | None:\n    try:\n        response = requests.get(url, timeout=3)\n        response.raise_for_status()   # GÃ©nÃ¨re une erreur HTTP si code â‰  200\n        return response.json()\n    except requests.exceptions.HTTPError as e:\n        print(\"âŒ Erreur HTTP :\", e)\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : serveur trop lent\")\n    except ValueError:\n        print(\"âŒ JSON mal formÃ©\")\n    except Exception as e:\n        print(\"âš ï¸ Erreur inconnue :\", e)\n    return None\n\n\n\nâš ï¸ Erreurs frÃ©quentes Ã  Ã©viter\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nâœ… Bonne pratique\n\n\n\n\nexcept: (attrape tout)\nSpÃ©cifier lâ€™erreur : except ValueError:\n\n\nCacher lâ€™erreur sans message\nFournir un contexte ğŸ—ƒï¸\n\n\nRetourner nâ€™importe quoi\nRetour cohÃ©rent (None ou valeur par dÃ©faut)\n\n\nMettre trop de logique dans try\nLimiter au strict nÃ©cessaire\n\n\nIgnorer les erreurs silencieusement\nLogguer ou notifier\n\n\n\n\n\n\nğŸš€ Conseil pro (trÃ¨s utile en Data Engineering)\nUtiliser raise pour propager lâ€™erreur si elle doit Ãªtre traitÃ©e ailleurs :\ndef parse_json(data: str) -&gt; dict:\n    try:\n        return json.loads(data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"JSON invalide : {e}\")\n\n\n\nğŸ“Œ Ã€ retenir\n\nToujours attraper le type exact dâ€™erreur.\n\nToujours expliquer lâ€™erreur (message clair).\n\nToujours garder un comportement cohÃ©rent (retour None ou valeur dÃ©faut).\n\nLes erreurs silencieuses sont pires que les erreurs visibles.\n\nLes pipelines cassent souvent â€” gÃ©rer les erreurs = Ãªtre un vrai Data Engineer.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "7. Modules et Imports ğŸ“¦ â€” Structurer son code comme un Data Engineer",
    "text": "7. Modules et Imports ğŸ“¦ â€” Structurer son code comme un Data Engineer\nEn Python :\n\nUn module = un fichier .py\nUn package = un dossier contenant plusieurs modules + un fichier __init__.py\n\nğŸ‘‰ Cela permet dâ€™organiser un projet data en blocs logiques :\ningestion, nettoyage, transformation, validation, etc.\n\n\nğŸ—‚ï¸ Exemple de structure de projet (propre & professionnelle)\nproject/\nâ”œâ”€â”€ utils/               â† Package (outils rÃ©utilisables)\nâ”‚   â”œâ”€â”€ __init__.py      â† Indique que `utils` est un package\nâ”‚   â””â”€â”€ math_utils.py     â† Module\nâ”œâ”€â”€ processors/           â† Package pour le traitement\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ text_cleaner.py   â† Module\nâ””â”€â”€ main.py               â† Point dâ€™entrÃ©e du projet\n\n\n\nğŸ“Œ Contenu du module : utils/math_utils.py\ndef somme(a, b):\n    return a + b\n\n\n\nğŸ“Œ Contenu dâ€™un autre module : processors/text_cleaner.py\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\n\n\nğŸ“Œ Importer dans main.py\nfrom utils.math_utils import somme\nfrom processors.text_cleaner import nettoyer_texte\n\nprint(somme(2, 3))\nprint(nettoyer_texte(\"  Hello WORLD  \"))",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” ğŸ’¡ Import absolu vs import relatif",
    "text": "ğŸ” ğŸ’¡ Import absolu vs import relatif\n\nâœ”ï¸ Import absolu (recommandÃ©)\nfrom utils.math_utils import somme\nâ¡ï¸ Le plus lisible, idÃ©al pour projets pro / Data Engineering.\n\n\nâœ”ï¸ Import relatif (utile dans les packages)\nfrom .math_utils import somme\nâ¡ï¸ Courant dans les gros projets et dans les packages pip.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§  Pourquoi câ€™est important en Data Engineering ?",
    "text": "ğŸ§  Pourquoi câ€™est important en Data Engineering ?\n\nCrÃ©er des modules = rendre ton code rÃ©utilisable (API, pipelines, notebooks)\nStructurer ton projet = Ã©viter le â€œscript spaghettiâ€\nFaciliter les tests unitaires\nFaciliter la maintenance dâ€™un pipeline data\nRÃ©duire les erreurs de duplication de logique",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & solutions",
    "text": "âš ï¸ Erreurs frÃ©quentes & solutions\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nModuleNotFoundError\nexÃ©cuter Python depuis le mauvais dossier\nToujours lancer Python depuis la racine du projet\n\n\nImport relatif impossible\nabsence du fichier __init__.py\nAjouter __init__.py dans le dossier\n\n\nModules dupliquÃ©s\nfichiers ayant le mÃªme nom dans deux dossiers\nRenommer ou structurer les packages\n\n\nimport *\nimports imprÃ©cis\nToujours importer explicitement\n\n\n\n\n\nâœ”ï¸ Astuce pro : vÃ©rifier ton PYTHONPATH\nimport sys\nprint(sys.path)\nCela indique oÃ¹ Python cherche les modules.\n\n\n\nâ­ Rappel essentiel\n\nImporter = RÃ©utiliser.\nStructurer = Devenir un vrai Data Engineer.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8. Manipulation de fichiers ğŸ“ â€” CompÃ©tence essentielle du Data Engineer",
    "text": "8. Manipulation de fichiers ğŸ“ â€” CompÃ©tence essentielle du Data Engineer\nDans un pipeline Data, on manipule en permanence des fichiers :\n\nfichiers texte (logs, outputs)\nfichiers CSV (exports mÃ©tier, ingestion)\nfichiers JSON (APIs, NoSQL, Ã©vÃ©nements Kafka)\ndossiers qui contiennent les donnÃ©es\n\nPython fournit des outils natifs trÃ¨s puissants pour cela :\n\npathlib.Path â†’ gÃ©rer les chemins et dossiers\n\nopen() â†’ lire/Ã©crire des fichiers texte\n\njson â†’ sÃ©rialiser/dÃ©sÃ©rialiser des donnÃ©es JSON\n\n\n\nğŸ—‚ï¸ Initialisation du dossier data/\nfrom pathlib import Path\nimport json\n\n# CrÃ©ation du dossier de travail\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.1 ğŸ“„ Manipulation dâ€™un fichier texte (logs, outputsâ€¦)",
    "text": "8.1 ğŸ“„ Manipulation dâ€™un fichier texte (logs, outputsâ€¦)\ntexte_path = DATA_DIR / \"exemple.txt\"\n\n# Ã‰criture\nwith open(texte_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Bonjour Data Engineer\\n\")\n\n# Lecture\nwith open(texte_path, \"r\", encoding=\"utf-8\") as f:\n    contenu = f.read()\n\nprint(\"Contenu du fichier :\", contenu)\nğŸ” Bonnes pratiques :\n\nToujours utiliser encoding=\"utf-8\"\n\nToujours utiliser with ... (fermeture automatique du fichier)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.2 ğŸ§© Manipulation dâ€™un fichier JSON",
    "text": "8.2 ğŸ§© Manipulation dâ€™un fichier JSON\nFormat le plus utilisÃ© dans : - APIs REST - MongoDB - Events Kafka / Kinesis - Configurations de job\njson_path = DATA_DIR / \"utilisateur.json\"\nutilisateur = {\"nom\": \"Alice\", \"age\": 30}\n\n# Ã‰criture JSON\nwith open(json_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(utilisateur, f, ensure_ascii=False, indent=2)\n\n# Lecture JSON\nwith open(json_path, \"r\", encoding=\"utf-8\") as f:\n    utilisateur_charge = json.load(f)\n\nprint(\"Utilisateur chargÃ© :\", utilisateur_charge)\nğŸ’¡ ensure_ascii=False permet dâ€™Ã©crire proprement les accents.\nğŸ’¡ indent=2 rend le JSON lisible (log, debug, auditsâ€¦).",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "8.3 ğŸ“Š (Optionnel) Manipulation dâ€™un CSV avec Pandas",
    "text": "8.3 ğŸ“Š (Optionnel) Manipulation dâ€™un CSV avec Pandas\nSouvent utilisÃ© en ingestion de donnÃ©es.\nimport pandas as pd\n\ncsv_path = DATA_DIR / \"exemple.csv\"\n\ndf = pd.DataFrame({\n    \"nom\": [\"Alice\", \"Bob\"],\n    \"age\": [30, 25]\n})\n\ndf.to_csv(csv_path, index=False)\n\ndf_loaded = pd.read_csv(csv_path)\nprint(df_loaded)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nFileNotFoundError\nMauvais chemin\nToujours utiliser Path() et vÃ©rifier path.exists()\n\n\nAccents cassÃ©s (Ã©, Ã â€¦)\nMauvais encoding\nToujours encoding=\"utf-8\"\n\n\nFichier non fermÃ©\nopen() sans contexte\nToujours utiliser with open(...)\n\n\nJSON mal formÃ©\nÃ©crit Ã  la main\nToujours utiliser json.dump / json.load\n\n\nChemins relatifs non fiables\nmauvais dossier dâ€™exÃ©cution\nUtiliser Path(__file__).resolve().parent dans un projet rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Points clÃ©s Ã  retenir",
    "text": "ğŸ“ Points clÃ©s Ã  retenir\n\npathlib.Path simplifie la gestion des chemins.\n\nwith open(...) est obligatoire pour Ã©viter les fuites de fichier.\n\nJSON = format standard du Data Engineering (MongoDB, API, logsâ€¦).\n\nToujours contrÃ´ler lâ€™encoding lors de la lecture/Ã©criture.\n\nLe dossier data/ centralise vos fichiers dans un projet propre.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "9. Packages et pip ğŸ“¦ â€” GÃ©rer les dÃ©pendances comme un Data Engineer",
    "text": "9. Packages et pip ğŸ“¦ â€” GÃ©rer les dÃ©pendances comme un Data Engineer\nPython devient puissant grÃ¢ce Ã  ses packages externes :\nPandas, Requests, SQLAlchemy, PyMongo, Polars, FastAPI, etc.\nPour installer ces packages, on utilise pip, le gestionnaire officiel de Python.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”§ Installer un package avec pip",
    "text": "ğŸ”§ Installer un package avec pip\nExemple : installer requests pour faire des appels HTTP (API REST).\npython -m pip install requests\nğŸ‘‰ Pourquoi python -m pip et pas juste pip install ?\nParce que cela garantit quâ€™on utilise le pip liÃ© Ã  la bonne version de Python, surtout si plusieurs versions sont installÃ©es.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“Œ Utilisation dans un script",
    "text": "ğŸ“Œ Utilisation dans un script\nimport requests\n\nresponse = requests.get(\"https://api.github.com\")\nprint(response.status_code)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Installer plusieurs packages â€” requirements.txt",
    "text": "ğŸ“ Installer plusieurs packages â€” requirements.txt\nDans un vrai projet Data Engineering, on liste les dÃ©pendances dans un fichier :\nrequests\npandas\nsqlalchemy\npymongo\nPuis on installe tout dâ€™un coup :\npython -m pip install -r requirements.txt",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â™»ï¸ Mettre Ã  jour un package",
    "text": "â™»ï¸ Mettre Ã  jour un package\npython -m pip install --upgrade requests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§¹ DÃ©sinstaller un package",
    "text": "ğŸ§¹ DÃ©sinstaller un package\npython -m pip uninstall requests",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“¦ OÃ¹ sont installÃ©s les packages ?",
    "text": "ğŸ“¦ OÃ¹ sont installÃ©s les packages ?\npython -m pip show requests\nDonne : version, emplacement, dÃ©pendances, etc.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques",
    "text": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\npip: command not found\nPython mal installÃ©\nUtiliser python -m pip\n\n\nInstaller dans le mauvais Python\nPlusieurs versions installÃ©es\nToujours python -m pip install\n\n\nVersion incompatible\nConflit de dÃ©pendances\nSpÃ©cifier une version : requests==2.31.0\n\n\nInstaller globalement\nRisque de casser le systÃ¨me\nUtiliser un venv (python -m venv .venv)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce Pro : figer les versions pour la production",
    "text": "â­ Astuce Pro : figer les versions pour la production\npython -m pip freeze &gt; requirements.txt\nâ¡ï¸ Cela capture exactement les versions utilisÃ©es dans ton environnement.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”„ GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire",
    "text": "ğŸ”„ GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire\nLes gÃ©nÃ©rateurs produisent des valeurs une par une, sans tout charger en mÃ©moire. Câ€™est essentiel en Data Engineering pour traiter des fichiers volumineux ou des flux de donnÃ©es.\n\n\nğŸ¤” ProblÃ¨me : charger tout en mÃ©moire\n# âŒ Charge TOUT le fichier en mÃ©moire\ndef lire_fichier_complet(chemin):\n    with open(chemin) as f:\n        return f.readlines()  # Liste de TOUTES les lignes\n\n# Si le fichier fait 10 Go... ğŸ’¥ MemoryError\nlignes = lire_fichier_complet(\"huge_file.csv\")\n\n\n\nâœ… Solution : gÃ©nÃ©rateur avec yield\n# âœ… Lit UNE ligne Ã  la fois\ndef lire_fichier_ligne_par_ligne(chemin):\n    with open(chemin) as f:\n        for ligne in f:\n            yield ligne.strip()  # Retourne et \"pause\"\n\n# Utilisation : ne charge qu'une ligne Ã  la fois\nfor ligne in lire_fichier_ligne_par_ligne(\"huge_file.csv\"):\n    process(ligne)  # Traite ligne par ligne\n\n\n\nğŸ¯ DiffÃ©rence return vs yield\n\n\n\n\n\n\n\nreturn\nyield\n\n\n\n\nRetourne une valeur et termine\nRetourne une valeur et pause\n\n\nFonction normale\nGÃ©nÃ©rateur\n\n\nTout en mÃ©moire\nUne valeur Ã  la fois\n\n\n\n# Fonction normale\ndef carres_liste(n):\n    result = []\n    for i in range(n):\n        result.append(i ** 2)\n    return result  # Retourne toute la liste\n\n# GÃ©nÃ©rateur\ndef carres_generateur(n):\n    for i in range(n):\n        yield i ** 2  # Retourne un par un\n\n# Test mÃ©moire\nimport sys\nliste = carres_liste(1000000)\ngen = carres_generateur(1000000)\n\nprint(sys.getsizeof(liste))  # ~8 Mo\nprint(sys.getsizeof(gen))    # ~200 octets !\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\n\nLire un CSV volumineux\ndef lire_csv_en_chunks(chemin, chunk_size=1000):\n    \"\"\"Lit un CSV par lots de chunk_size lignes.\"\"\"\n    with open(chemin) as f:\n        header = next(f).strip().split(\",\")\n        chunk = []\n        \n        for ligne in f:\n            valeurs = ligne.strip().split(\",\")\n            row = dict(zip(header, valeurs))\n            chunk.append(row)\n            \n            if len(chunk) &gt;= chunk_size:\n                yield chunk\n                chunk = []\n        \n        if chunk:  # Dernier lot\n            yield chunk\n\n# Utilisation\nfor batch in lire_csv_en_chunks(\"users.csv\", chunk_size=500):\n    print(f\"Traitement de {len(batch)} lignes...\")\n    # insert_to_database(batch)\n\n\nPipeline de transformation\ndef extraire(source):\n    for item in source:\n        yield item\n\ndef transformer(items):\n    for item in items:\n        item[\"nom\"] = item[\"nom\"].upper()\n        yield item\n\ndef filtrer(items, condition):\n    for item in items:\n        if condition(item):\n            yield item\n\n# Pipeline chaÃ®nÃ© â€” Ã©valuation paresseuse (lazy)\nsource = [{\"nom\": \"alice\"}, {\"nom\": \"bob\"}, {\"nom\": \"charlie\"}]\npipeline = filtrer(\n    transformer(extraire(source)),\n    lambda x: len(x[\"nom\"]) &gt; 3\n)\n\nfor item in pipeline:\n    print(item)  # {'nom': 'ALICE'}, {'nom': 'CHARLIE'}\n\n\n\n\nğŸš€ Generator Expressions (syntaxe courte)\n# List comprehension â€” crÃ©e une liste en mÃ©moire\ncarres_liste = [x**2 for x in range(1000000)]\n\n# Generator expression â€” crÃ©e un gÃ©nÃ©rateur\ncarres_gen = (x**2 for x in range(1000000))  # ParenthÃ¨ses !\n\n# Utile pour les agrÃ©gations\ntotal = sum(x**2 for x in range(1000000))  # Efficient",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ–¥ï¸ CrÃ©er et exÃ©cuter un script Python (.py)",
    "text": "ğŸ–¥ï¸ CrÃ©er et exÃ©cuter un script Python (.py)\nJusquâ€™ici, tu as travaillÃ© dans un Notebook Jupyter. Mais en Data Engineering, on Ã©crit surtout des scripts .py qui sâ€™exÃ©cutent depuis le terminal ou sont orchestrÃ©s par Airflow, cron, etc.\n\n\nğŸ““ Notebook vs Script â€” Quand utiliser quoi ?\n\n\n\nCritÃ¨re\nğŸ““ Notebook (.ipynb)\nğŸ“„ Script (.py)\n\n\n\n\nExploration\nâœ… IdÃ©al\nâŒ Pas adaptÃ©\n\n\nVisualisation\nâœ… Graphiques inline\nâš ï¸ Doit sauvegarder\n\n\nProduction\nâŒ Difficile Ã  orchestrer\nâœ… Standard\n\n\nTests unitaires\nâŒ CompliquÃ©\nâœ… Facile avec pytest\n\n\nGit / Code review\nâš ï¸ Diffs illisibles\nâœ… Diffs propres\n\n\nCI/CD\nâŒ Pas adaptÃ©\nâœ… Standard\n\n\nAirflow / Orchestration\nâš ï¸ Possible mais pas idÃ©al\nâœ… Natif\n\n\n\n\nğŸ’¡ RÃ¨gle : Notebook pour explorer, Script pour produire.\n\n\n\n\nğŸ“ CrÃ©er ton premier script\n\nOuvre VS Code\nCrÃ©e un nouveau fichier : File &gt; New File\nSauvegarde-le avec lâ€™extension .py : mon_script.py\n\n\nExemple : hello.py\n# hello.py\nprint(\"Hello, Data Engineer!\")\n\n\n\n\nâ–¶ï¸ ExÃ©cuter depuis le terminal\n# Se placer dans le dossier du script\ncd /chemin/vers/mon/projet\n\n# ExÃ©cuter le script\npython hello.py\n# ou\npython3 hello.py\nRÃ©sultat :\nHello, Data Engineer!\n\n\n\nğŸ¯ Structure standard : if __name__ == \"__main__\"\nCâ€™est LA structure que tu verras dans tous les scripts Python professionnels.\n# etl_simple.py\n\ndef extract():\n    \"\"\"Extrait les donnÃ©es.\"\"\"\n    print(\"ğŸ“¥ Extraction...\")\n    return [{\"id\": 1, \"nom\": \"Alice\"}, {\"id\": 2, \"nom\": \"Bob\"}]\n\ndef transform(data):\n    \"\"\"Transforme les donnÃ©es.\"\"\"\n    print(\"ğŸ”„ Transformation...\")\n    return [{**d, \"nom\": d[\"nom\"].upper()} for d in data]\n\ndef load(data):\n    \"\"\"Charge les donnÃ©es.\"\"\"\n    print(\"ğŸ’¾ Chargement...\")\n    for row in data:\n        print(f\"  InsÃ©rÃ© : {row}\")\n\ndef main():\n    \"\"\"Point d'entrÃ©e principal.\"\"\"\n    print(\"ğŸš€ DÃ©marrage du pipeline ETL\")\n    \n    data = extract()\n    data = transform(data)\n    load(data)\n    \n    print(\"âœ… Pipeline terminÃ© !\")\n\n# Ce bloc s'exÃ©cute SEULEMENT si le script est lancÃ© directement\nif __name__ == \"__main__\":\n    main()\n\nPourquoi if __name__ == \"__main__\" ?\n\n\n\nSituation\n__name__ vaut\nLe bloc sâ€™exÃ©cute ?\n\n\n\n\npython etl_simple.py\n\"__main__\"\nâœ… Oui\n\n\nimport etl_simple\n\"etl_simple\"\nâŒ Non\n\n\n\nCela permet de : - RÃ©utiliser les fonctions dans dâ€™autres scripts (import etl_simple) - ExÃ©cuter le script directement (python etl_simple.py)\n\n\n\n\nğŸ“Œ Passer des arguments au script\n\nMÃ©thode simple : sys.argv\n# script_args.py\nimport sys\n\ndef main():\n    print(f\"Arguments reÃ§us : {sys.argv}\")\n    \n    # sys.argv[0] = nom du script\n    # sys.argv[1], sys.argv[2], ... = arguments\n    \n    if len(sys.argv) &lt; 2:\n        print(\"Usage : python script_args.py &lt;fichier&gt;\")\n        sys.exit(1)\n    \n    fichier = sys.argv[1]\n    print(f\"Traitement de : {fichier}\")\n\nif __name__ == \"__main__\":\n    main()\npython script_args.py data.csv\n# Arguments reÃ§us : ['script_args.py', 'data.csv']\n# Traitement de : data.csv\n\n\nMÃ©thode pro : argparse\n# etl_avec_args.py\nimport argparse\n\ndef main():\n    # CrÃ©er le parser\n    parser = argparse.ArgumentParser(\n        description=\"Pipeline ETL simple\"\n    )\n    \n    # DÃ©finir les arguments\n    parser.add_argument(\n        \"--input\", \"-i\",\n        required=True,\n        help=\"Fichier d'entrÃ©e (CSV)\"\n    )\n    parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.csv\",\n        help=\"Fichier de sortie (dÃ©faut: output.csv)\"\n    )\n    parser.add_argument(\n        \"--verbose\", \"-v\",\n        action=\"store_true\",\n        help=\"Mode verbeux\"\n    )\n    \n    # Parser les arguments\n    args = parser.parse_args()\n    \n    # Utiliser les arguments\n    print(f\"Input  : {args.input}\")\n    print(f\"Output : {args.output}\")\n    print(f\"Verbose: {args.verbose}\")\n\nif __name__ == \"__main__\":\n    main()\n# Afficher l'aide\npython etl_avec_args.py --help\n\n# ExÃ©cuter avec arguments\npython etl_avec_args.py --input data.csv --output result.csv --verbose\npython etl_avec_args.py -i data.csv -o result.csv -v\n\n\n\n\nğŸ“ Structure dâ€™un projet Data Engineering\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â”œâ”€â”€ transform.py\nâ”‚   â””â”€â”€ load.py\nâ”œâ”€â”€ scripts/\nâ”‚   â””â”€â”€ run_etl.py      â† Point d'entrÃ©e\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ data/               â† âš ï¸ Dans .gitignore\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ .gitignore\nâ””â”€â”€ README.md\n\n\n\nğŸ§ª Exercice pratique\nCrÃ©er un script compter_lignes.py qui : 1. Prend un fichier en argument 2. Compte le nombre de lignes 3. Affiche le rÃ©sultat\n\n\nğŸ’¡ Solution\n\n# compter_lignes.py\nimport argparse\nfrom pathlib import Path\n\ndef compter_lignes(chemin: str) -&gt; int:\n    \"\"\"Compte les lignes d'un fichier.\"\"\"\n    path = Path(chemin)\n    if not path.exists():\n        raise FileNotFoundError(f\"Fichier non trouvÃ© : {chemin}\")\n    \n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return sum(1 for _ in f)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Compte les lignes d'un fichier\")\n    parser.add_argument(\"fichier\", help=\"Chemin du fichier Ã  analyser\")\n    args = parser.parse_args()\n    \n    try:\n        nb_lignes = compter_lignes(args.fichier)\n        print(f\"Le fichier '{args.fichier}' contient {nb_lignes} lignes.\")\n    except FileNotFoundError as e:\n        print(f\"Erreur : {e}\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()\npython compter_lignes.py mon_fichier.csv",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” Context Managers â€” GÃ©rer les ressources proprement",
    "text": "ğŸ” Context Managers â€” GÃ©rer les ressources proprement\nTu as dÃ©jÃ  utilisÃ© with open(...) pour les fichiers. Câ€™est un context manager ! Ce pattern garantit que les ressources sont toujours libÃ©rÃ©es, mÃªme en cas dâ€™erreur.\n\n\nğŸ¤” Le problÃ¨me sans context manager\n# âŒ Risque : fichier non fermÃ© si erreur\nf = open(\"data.txt\", \"r\")\ndata = f.read()\n# Si une erreur se produit ici...\nprocess(data)  # Le fichier reste ouvert !\nf.close()\n# âœ… Avec context manager : fermeture garantie\nwith open(\"data.txt\", \"r\") as f:\n    data = f.read()\n    process(data)\n# Fichier automatiquement fermÃ©, mÃªme en cas d'erreur\n\n\n\nğŸ“Š Cas dâ€™usage Data Engineering\n\n\n\nRessource\nPourquoi un context manager\n\n\n\n\nFichiers\nFermer aprÃ¨s lecture/Ã©criture\n\n\nConnexions DB\nLibÃ©rer la connexion\n\n\nTransactions\nCommit ou rollback\n\n\nFichiers temporaires\nSupprimer aprÃ¨s usage\n\n\nVerrous (locks)\nLibÃ©rer le verrou\n\n\n\n\n\n\nğŸ› ï¸ CrÃ©er son propre context manager\n\nMÃ©thode 1 : avec une classe\nclass DatabaseConnection:\n    def __init__(self, host: str, database: str):\n        self.host = host\n        self.database = database\n        self.connection = None\n    \n    def __enter__(self):\n        \"\"\"AppelÃ© au dÃ©but du bloc 'with'.\"\"\"\n        print(f\"ğŸ”Œ Connexion Ã  {self.host}/{self.database}...\")\n        self.connection = f\"Connection to {self.database}\"  # Simule une connexion\n        return self.connection\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"AppelÃ© Ã  la fin du bloc 'with' (mÃªme si erreur).\"\"\"\n        print(f\"ğŸ”Œ Fermeture de la connexion...\")\n        self.connection = None\n        # Retourner False pour propager les exceptions\n        return False\n\n# Utilisation\nwith DatabaseConnection(\"localhost\", \"warehouse\") as conn:\n    print(f\"Connexion active : {conn}\")\n    # Faire des requÃªtes...\n# Connexion automatiquement fermÃ©e ici\n\n\nMÃ©thode 2 : avec @contextmanager (plus simple)\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(nom: str):\n    \"\"\"Mesure le temps d'exÃ©cution d'un bloc.\"\"\"\n    import time\n    start = time.time()\n    print(f\"â±ï¸ DÃ©but : {nom}\")\n    \n    yield  # Le bloc 'with' s'exÃ©cute ici\n    \n    elapsed = time.time() - start\n    print(f\"â±ï¸ Fin : {nom} ({elapsed:.2f}s)\")\n\n# Utilisation\nwith timer(\"Traitement ETL\"):\n    # Simuler un traitement long\n    import time\n    time.sleep(1)\n    print(\"Traitement en cours...\")\n\n# Output:\n# â±ï¸ DÃ©but : Traitement ETL\n# Traitement en cours...\n# â±ï¸ Fin : Traitement ETL (1.00s)\n\n\n\n\nğŸ¯ Exemple complet : Transaction DB\nfrom contextlib import contextmanager\n\n@contextmanager\ndef transaction(connection):\n    \"\"\"GÃ¨re une transaction avec commit/rollback automatique.\"\"\"\n    try:\n        print(\"ğŸ”„ DÃ©but de la transaction\")\n        yield connection\n        print(\"âœ… COMMIT\")\n        # connection.commit()\n    except Exception as e:\n        print(f\"âŒ ROLLBACK : {e}\")\n        # connection.rollback()\n        raise\n\n# Utilisation\nwith transaction(\"ma_connexion\") as conn:\n    print(\"Insertion de donnÃ©es...\")\n    # Si une erreur ici â†’ rollback automatique\n\n\n\nğŸ’¡ Context managers utiles de la stdlib\nfrom contextlib import suppress, redirect_stdout\nimport tempfile\n\n# Ignorer silencieusement une erreur\nwith suppress(FileNotFoundError):\n    os.remove(\"fichier_peut_etre_absent.txt\")\n\n# Fichier temporaire (supprimÃ© automatiquement)\nwith tempfile.NamedTemporaryFile(mode=\"w\", delete=True) as tmp:\n    tmp.write(\"donnÃ©es temporaires\")\n    print(f\"Fichier temp : {tmp.name}\")\n# Fichier supprimÃ© ici",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "10. Logging ğŸ§¾ â€” TraÃ§abilitÃ© indispensable en Data Engineering",
    "text": "10. Logging ğŸ§¾ â€” TraÃ§abilitÃ© indispensable en Data Engineering\nDans un vrai pipeline Data (ingestion, nettoyage, transformationâ€¦), on doit suivre ce quâ€™il se passe :\n\nFichier introuvable ?\nAPI trop lente ?\nFormat JSON invalide ?\nDonnÃ©es anormales ?\nETL en retard ?\n\nâ¡ï¸ print() ne suffit pas, car il ne permet ni filtrage, ni niveaux, ni logs dans un fichier.\nâ¡ï¸ Le module logging est le standard utilisÃ© en entreprise.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ” 10.1 Pourquoi utiliser logging ?",
    "text": "ğŸ” 10.1 Pourquoi utiliser logging ?\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nğŸšï¸ Niveaux de log\nDEBUG, INFO, WARNING, ERROR, CRITICAL\n\n\nğŸ¯ Filtrage des messages\nOn peut afficher seulement WARNING+\n\n\nğŸ’¾ Logs dans un fichier\nIndispensable en production\n\n\nğŸ§© Standard Python\nCompatible Airflow, FastAPI, ETL, microservices\n\n\nğŸ§µ Thread-safe\nFonctionne mÃªme avec du multithreading",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”§ Configuration minimale recommandÃ©e",
    "text": "ğŸ”§ Configuration minimale recommandÃ©e\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\nExplications : - level=INFO â†’ DEBUG est ignorÃ© - %(asctime)s â†’ timestamp (important dans les pipelines) - %(levelname)s â†’ niveau (INFO, ERRORâ€¦) - %(message)s â†’ contenu du message",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§ª Exemple dâ€™utilisation",
    "text": "ğŸ§ª Exemple dâ€™utilisation\nlogging.debug(\"Message DEBUG (non affichÃ© en mode INFO)\")\nlogging.info(\"DÃ©marrage du mini-script\")\nlogging.warning(\"Attention : donnÃ©es manquantes\")\nlogging.error(\"Erreur : Ã©chec de connexion API\")\nlogging.critical(\"CRITIQUE : le pipeline doit s'arrÃªter !\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ 10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)",
    "text": "ğŸ“ 10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"pipeline.log\",\n    filemode=\"a\",  # append au lieu de rÃ©Ã©crire\n)\nâ¡ï¸ TrÃ¨s utilisÃ© dans : - scripts Airflow - traitements batch - pipelines de production",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§° 10.3 Exemple typique dans une fonction",
    "text": "ğŸ§° 10.3 Exemple typique dans une fonction\ndef charger_json(chemin: str) -&gt; dict | None:\n    logging.info(f\"Chargement du fichier : {chemin}\")\n    try:\n        with open(chemin, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except FileNotFoundError:\n        logging.error(f\"Fichier introuvable : {chemin}\")\n    except json.JSONDecodeError:\n        logging.error(\"JSON invalide\")\n    return None",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nğŸ’¡ Pourquoi\nâœ… Bonne pratique\n\n\n\n\nUtiliser print() partout\nimpossible Ã  filtrer/logguer\nUtiliser logging.info()\n\n\nAppeler logging.basicConfig() plusieurs fois\nne fonctionne que la 1Ã¨re fois\nConfigurer un seul logger global\n\n\nLogger des donnÃ©es sensibles\nfuite de secrets/mots de passe\nFiltrer/masquer les champs sensibles\n\n\nLogs trop verbeux\nralentissent les pipelines\nUtiliser DEBUG seulement en dev\n\n\nLogs insuffisants\ndifficile de diagnostiquer\nLogger les erreurs + contexte",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce pro : Logger dans la console et dans un fichier",
    "text": "â­ Astuce pro : Logger dans la console et dans un fichier\nlogger = logging.getLogger(\"pipeline\")\nlogger.setLevel(logging.INFO)\n\n# Handler console\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\n# Handler fichier\nfile = logging.FileHandler(\"pipeline.log\")\nfile.setLevel(logging.INFO)\n\n# Format\nfmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nconsole.setFormatter(fmt)\nfile.setFormatter(fmt)\n\n# Ajouter handlers\nlogger.addHandler(console)\nlogger.addHandler(file)\n\nlogger.info(\"Pipeline dÃ©marrÃ©\")",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“ Ã€ retenir",
    "text": "ğŸ“ Ã€ retenir\n\nlogging = indispensable en Data Engineering\n\nToujours configurer : niveau + format\n\nUtiliser un fichier log en production\n\nJamais de print() dans un vrai pipeline\n\nBien choisir le niveau (INFO, WARNING, ERRORâ€¦)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "11. Exercices pratiques ğŸ§ª",
    "text": "11. Exercices pratiques ğŸ§ª\nEssaie de rÃ©soudre ces exercices avant dâ€™afficher les corrections.\n\nExercice 1 â€“ Validation dâ€™Ã¢ge\nÃ‰crire une fonction est_majeur(age: int) -&gt; bool qui : - retourne True si age est supÃ©rieur ou Ã©gal Ã  18 ; - sinon retourne False.\n# Ã€ toi de jouer\ndef est_majeur(age: int) -&gt; bool:\n    # TODO: complÃ©ter\n    pass\n\nprint(est_majeur(15))  # attendu: False\nprint(est_majeur(18))  # attendu: True\n\n\nAfficher une solution possible\n\ndef est_majeur(age: int) -&gt; bool:\n    return age &gt;= 18\n\nprint(est_majeur(15))  # False\nprint(est_majeur(18))  # True\n\n\n\nExercice 2 â€“ Compter les posts par utilisateur\nOn dispose dâ€™une liste de dictionnaires reprÃ©sentant des posts :\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\nÃ‰crire une fonction compter_posts_par_utilisateur(posts) qui retourne :\n{\"alice\": 2, \"bob\": 1}\n# Ã€ toi de jouer\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    # TODO: complÃ©ter\n    result = {}\n    return result\n\nprint(compter_posts_par_utilisateur(posts))\n\n\nAfficher une solution possible\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    result = {}\n    for p in posts:\n        user = p[\"user\"]\n        if user not in result:\n            result[user] = 0\n        result[user] += 1\n    return result\n\nprint(compter_posts_par_utilisateur(posts))  # {'alice': 2, 'bob': 1}\n\n\n\nExercice 3 â€“ Classe Post\nCrÃ©er une classe Post avec : - attributs : auteur (str), texte (str) ; - mÃ©thode longueur() qui retourne la longueur du texte.\n# Ã€ toi de jouer\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        # TODO: stocker les attributs\n        pass\n\n    def longueur(self) -&gt; int:\n        # TODO: retourner la longueur du texte\n        return 0\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())  # attendu: longueur de la phrase\n\n\nAfficher une solution possible\n\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        self.auteur = auteur\n        self.texte = texte\n\n    def longueur(self) -&gt; int:\n        return len(self.texte)\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ§ª Introduction aux tests avec pytest",
    "text": "ğŸ§ª Introduction aux tests avec pytest\nTester son code est essentiel en Data Engineering. Un bug dans un pipeline peut corrompre des millions de lignes de donnÃ©es ! pytest est le framework de test standard en Python.\n\n\nğŸ“¦ Installation\npip install pytest\n\n\n\nğŸ¯ Premier test avec assert\nassert vÃ©rifie quâ€™une condition est vraie. Si elle est fausse â†’ erreur.\n# test_basics.py\n\ndef addition(a, b):\n    return a + b\n\ndef test_addition():\n    \"\"\"Test simple avec assert.\"\"\"\n    assert addition(2, 3) == 5\n    assert addition(0, 0) == 0\n    assert addition(-1, 1) == 0\n\ndef test_addition_floats():\n    \"\"\"Test avec des floats.\"\"\"\n    result = addition(0.1, 0.2)\n    assert abs(result - 0.3) &lt; 0.0001  # Comparaison floats\n# ExÃ©cuter les tests\npytest test_basics.py\n\n# Avec plus de dÃ©tails\npytest test_basics.py -v\n\n\n\nğŸ“Š Tester des fonctions Data Engineering\n# src/transform.py\ndef nettoyer_email(email: str) -&gt; str:\n    \"\"\"Nettoie un email : minuscules, strip.\"\"\"\n    if not email or \"@\" not in email:\n        raise ValueError(\"Email invalide\")\n    return email.strip().lower()\n\ndef filtrer_actifs(users: list[dict]) -&gt; list[dict]:\n    \"\"\"Garde uniquement les users actifs.\"\"\"\n    return [u for u in users if u.get(\"actif\", False)]\n# tests/test_transform.py\nimport pytest\nfrom src.transform import nettoyer_email, filtrer_actifs\n\nclass TestNettoyerEmail:\n    \"\"\"Tests pour nettoyer_email.\"\"\"\n    \n    def test_email_valide(self):\n        assert nettoyer_email(\"Alice@Test.COM\") == \"alice@test.com\"\n    \n    def test_email_avec_espaces(self):\n        assert nettoyer_email(\"  bob@test.com  \") == \"bob@test.com\"\n    \n    def test_email_invalide_sans_arobase(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"invalid_email\")\n    \n    def test_email_vide(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"\")\n\n\nclass TestFiltrerActifs:\n    \"\"\"Tests pour filtrer_actifs.\"\"\"\n    \n    def test_filtre_users_actifs(self):\n        users = [\n            {\"nom\": \"Alice\", \"actif\": True},\n            {\"nom\": \"Bob\", \"actif\": False},\n            {\"nom\": \"Charlie\", \"actif\": True}\n        ]\n        result = filtrer_actifs(users)\n        assert len(result) == 2\n        assert all(u[\"actif\"] for u in result)\n    \n    def test_liste_vide(self):\n        assert filtrer_actifs([]) == []\n    \n    def test_tous_inactifs(self):\n        users = [{\"nom\": \"X\", \"actif\": False}]\n        assert filtrer_actifs(users) == []\n\n\n\nğŸƒ ExÃ©cuter les tests\n# Tous les tests du dossier\npytest\n\n# Un fichier spÃ©cifique\npytest tests/test_transform.py\n\n# Une classe spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail\n\n# Un test spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail::test_email_valide\n\n# Avec couverture de code\npip install pytest-cov\npytest --cov=src\n\n\n\nğŸ“ Structure recommandÃ©e\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â””â”€â”€ transform.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ test_extract.py\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ pytest.ini          # Configuration pytest (optionnel)\nâ””â”€â”€ requirements.txt\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nPratique\nExplication\n\n\n\n\nNommer test_*.py\npytest les dÃ©tecte automatiquement\n\n\nUn assert par test\nPlus facile Ã  dÃ©buguer\n\n\nTester les cas limites\nListes vides, None, valeurs nÃ©gatives\n\n\nTester les erreurs\npytest.raises(Exception)\n\n\nExÃ©cuter avant chaque commit\nÃ‰vite les rÃ©gressions",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "12. Quiz final ğŸ¯",
    "text": "12. Quiz final ğŸ¯\nTeste tes connaissances ! RÃ©ponds mentalement puis vÃ©rifie les rÃ©ponses.\n\n\nâ“ Q1. Quel type correspond Ã  une chaÃ®ne de caractÃ¨res ?\n\nstr\n\ntext\n\nchar\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” str est le type pour les chaÃ®nes de caractÃ¨res.\n\n\n\n\nâ“ Q2. Quelle structure est la plus adaptÃ©e pour reprÃ©senter un objet JSON ?\n\nlist\n\ndict\n\ntuple\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” dict avec des paires clÃ©/valeur, comme JSON.\n\n\n\n\nâ“ Q3. Laquelle de ces boucles risque le plus de devenir infinie ?\n\nfor\n\nwhile\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” while continue tant que la condition est vraie.\n\n\n\n\nâ“ Q4. Quel mot-clÃ© permet de gÃ©rer une erreur ?\n\nerror\n\nexcept\n\ncatch\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” try/except pour gÃ©rer les exceptions.\n\n\n\n\nâ“ Q5. Quel module est utilisÃ© pour le logging ?\n\nlogs\n\nlogging\n\nlogger\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” import logging\n\n\n\n\nâ“ Q6. Comment crÃ©er un environnement virtuel avec venv ?\n\npython -m venv mon_env\n\npip create venv mon_env\n\npython --venv mon_env\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” python -m venv nom_environnement\n\n\n\n\nâ“ Q7. Quel fichier liste les dÃ©pendances dâ€™un projet Python ?\n\npackages.txt\n\nrequirements.txt\n\ndependencies.json\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” requirements.txt avec pip freeze",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Documentation officielle\n\nPython.org Documentation â€” RÃ©fÃ©rence complÃ¨te\nPython Tutorial â€” Tutoriel officiel\n\n\n\nğŸ“ Cours et tutoriels\n\nReal Python â€” Tutoriels de qualitÃ©\nPython for Data Engineering (DataCamp) â€” Cours interactifs\nAutomate the Boring Stuff â€” Livre gratuit\n\n\n\nğŸ› ï¸ Outils recommandÃ©s\n\nPyPI â€” Repository de packages Python\nRuff â€” Linter ultra-rapide\nBlack â€” Formateur de code\nmypy â€” VÃ©rification des types\n\n\n\nğŸ“Š Packages Data Engineering essentiels\n\n\n\nPackage\nUsage\n\n\n\n\npandas\nManipulation de donnÃ©es tabulaires\n\n\nnumpy\nCalcul numÃ©rique\n\n\nrequests\nAppels API HTTP\n\n\nsqlalchemy\nORM et connexions bases de donnÃ©es\n\n\npydantic\nValidation de donnÃ©es\n\n\npytest\nTests unitaires\n\n\nclick\nCLI (Command Line Interface)",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ“˜ Python â€“ Bases pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les bases de Python, passons au traitement de donnÃ©es !\nğŸ‘‰ Module suivant : 05_python_data_processing_for_data_engineers.ipynb â€” Pandas, Matplotlib, Seaborn et ETL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Basics pour Data Engineers.",
    "crumbs": [
      "DÃ©butant",
      "ğŸ“˜ Python â€“ Bases pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "",
    "text": "Ce module prÃ©sente PySpark, lâ€™API Python pour Apache Spark â€” le moteur de traitement distribuÃ© le plus utilisÃ© en Big Data.",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“‹ PrÃ©requis",
    "text": "ğŸ“‹ PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre MapReduce et ses limites\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nMaÃ®triser SQL (module 07)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nâœ… Comprendre lâ€™architecture Spark (Driver, Executors, Cluster Manager)\nâœ… CrÃ©er et manipuler des DataFrames distribuÃ©s\nâœ… Ã‰crire des transformations et actions\nâœ… Utiliser Spark SQL\nâœ… Optimiser les performances (partitioning, caching, broadcast)\nâœ… Lire/Ã©crire des fichiers (CSV, JSON, Parquet)\nâœ… DÃ©couvrir le streaming temps rÃ©el",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ¯ PySpark dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ PySpark dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 que Spark a remplacÃ© MapReduce comme moteur de traitement Big Data. Voici pourquoi :\n\nRappel : MapReduce vs Spark\nMapReduce :  DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                  â†‘           â†‘              â†‘              â†‘\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               LENT ! (I/O disque)\n\nSpark :      DISQUE â†’ Transformations â†’ MÃ‰MOIRE â†’ ... â†’ MÃ‰MOIRE â†’ Action\n                                          â†‘                â†‘\n                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           RAPIDE ! (in-memory)\n\n\nRappel : Les 5V et Spark\n\n\n\nV\nComment Spark rÃ©pond\n\n\n\n\nVolume\nTraitement distribuÃ© sur cluster (To â†’ Po)\n\n\nVelocity\nSpark Streaming pour le temps rÃ©el\n\n\nVariety\nLit CSV, JSON, Parquet, JDBC, Avroâ€¦\n\n\nVeracity\nTransformations pour nettoyer les donnÃ©es\n\n\nValue\nSpark SQL, MLlib pour extraire de la valeur\n\n\n\n\n\nPosition dans lâ€™Ã©cosystÃ¨me\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Ã‰COSYSTÃˆME BIG DATA                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources           Traitement              Stockage            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚                                                                 â”‚\nâ”‚   Kafka    â”€â”                         â”Œâ”€â–º  Data Lake (S3)      â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  âš¡ SPARK âš¡  â”€â”€â”€â”€â”€â”€â”¼â”€â–º  Data Warehouse       â”‚\nâ”‚   JDBC     â”€â”¤     (PySpark)          â”œâ”€â–º  NoSQL (MongoDB)     â”‚\nâ”‚   APIs     â”€â”˜                         â””â”€â–º  Elasticsearch       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter toutes les cellules de code !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“¦ Installation et Setup",
    "text": "ğŸ“¦ Installation et Setup\nPySpark nÃ©cessite Java. VÃ©rifions dâ€™abord lâ€™installation.\n\n\nCode\n# Installation de PySpark\n!pip install pyspark pandas numpy pyarrow\n\n\n\n\nCode\n# VÃ©rifier Java\n!java -version\n\n\n\n\nCode\n# Imports de base\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nprint(\"âœ… Imports rÃ©ussis !\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Quâ€™est-ce que Spark ?",
    "text": "Quâ€™est-ce que Spark ?\nApache Spark est un moteur de traitement distribuÃ© ultra-rapide pour le Big Data.\n\nğŸ”‘ Concepts clÃ©s\n\nSparkSession : Point dâ€™entrÃ©e de toute application Spark\nDataFrame : Collection distribuÃ©e de donnÃ©es organisÃ©es en colonnes\nRDD : Resilient Distributed Dataset (bas niveau)\nTransformations : OpÃ©rations lazy (map, filter, select, etc.)\nActions : DÃ©clenchent lâ€™exÃ©cution (count, collect, show, etc.)\n\n\n\nğŸš€ Avantages de Spark\n\nVitesse : 100x plus rapide que MapReduce\nScalabilitÃ© : De quelques MB Ã  plusieurs PB\nSimplicitÃ© : API unifiÃ©e (Python, Scala, Java, R)\nVersatilitÃ© : Batch, Streaming, ML, Graph processing",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.1 CrÃ©er une SparkSession",
    "text": "1.1 CrÃ©er une SparkSession\n\n\nCode\n# CrÃ©er une SparkSession\nspark = SparkSession.builder \\\n    .appName(\"PySpark Data Engineering Tutorial\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n    .getOrCreate()\n\nprint(\"âœ… SparkSession crÃ©Ã©e\")\nprint(f\"Version Spark : {spark.version}\")\nprint(f\"Application : {spark.sparkContext.appName}\")\nprint(f\"Master : {spark.sparkContext.master}\")\n\n\n\n\nCode\n# Configuration du logging\nspark.sparkContext.setLogLevel(\"ERROR\")\nprint(\"âœ… Logging configurÃ© sur ERROR\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.2 Premiers DataFrames",
    "text": "1.2 Premiers DataFrames\n\n\nCode\n# MÃ©thode 1 : Depuis une liste Python\ndata = [\n    (1, \"Alice\", 25, \"Paris\", 45000),\n    (2, \"Bob\", 30, \"Lyon\", 55000),\n    (3, \"Charlie\", 35, \"Paris\", 60000),\n    (4, \"David\", 28, \"Marseille\", 50000),\n    (5, \"Eve\", 32, \"Lyon\", 58000)\n]\n\ncolumns = [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"]\n\ndf = spark.createDataFrame(data, columns)\n\nprint(\"ğŸ“Š Premier DataFrame crÃ©Ã© :\")\ndf.show()\n\n\n\n\nCode\n# MÃ©thode 2 : Depuis un Pandas DataFrame\npandas_df = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'D'],\n    'prix': [10.5, 20.0, 15.75, 30.0],\n    'quantite': [100, 50, 75, 25]\n})\n\nspark_df = spark.createDataFrame(pandas_df)\n\nprint(\"ğŸ“Š DataFrame depuis Pandas :\")\nspark_df.show()\n\n\n\n\nCode\n# MÃ©thode 3 : Avec un schÃ©ma explicite\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"nom\", StringType(), False),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"ville\", StringType(), True),\n    StructField(\"salaire\", IntegerType(), True)\n])\n\ndf_with_schema = spark.createDataFrame(data, schema)\n\nprint(\"ğŸ“Š DataFrame avec schÃ©ma explicite :\")\ndf_with_schema.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "1.3 Explorer un DataFrame",
    "text": "1.3 Explorer un DataFrame\n\n\nCode\n# Afficher le schÃ©ma\nprint(\"ğŸ“‹ SchÃ©ma du DataFrame :\")\ndf.printSchema()\n\n# Afficher les premiÃ¨res lignes\nprint(\"\\nğŸ” PremiÃ¨res lignes :\")\ndf.show(3)\n\n# Compter les lignes\nprint(f\"\\nğŸ“ Nombre de lignes : {df.count()}\")\n\n# Colonnes\nprint(f\"\\nğŸ“‹ Colonnes : {df.columns}\")\n\n# Types de donnÃ©es\nprint(\"\\nğŸ”¤ Types de donnÃ©es :\")\nprint(df.dtypes)\n\n\n\n\nCode\n# Statistiques descriptives\nprint(\"ğŸ“Š Statistiques descriptives :\")\ndf.describe().show()\n\n# Statistiques sur colonnes spÃ©cifiques\nprint(\"\\nğŸ“Š Statistiques sur 'age' et 'salaire' :\")\ndf.select('age', 'salaire').describe().show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.1 SÃ©lection de colonnes",
    "text": "2.1 SÃ©lection de colonnes\n\n\nCode\n# SÃ©lectionner des colonnes\nprint(\"ğŸ“Œ SÃ©lection de colonnes :\")\ndf.select(\"nom\", \"ville\").show()\n\n# Avec alias\nprint(\"\\nğŸ“Œ Avec alias :\")\ndf.select(\n    F.col(\"nom\").alias(\"employee_name\"),\n    F.col(\"salaire\").alias(\"salary\")\n).show()\n\n# SÃ©lectionner avec expressions\nprint(\"\\nğŸ“Œ Avec expressions :\")\ndf.select(\n    \"nom\",\n    (F.col(\"salaire\") * 12).alias(\"salaire_annuel\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.2 Filtrage",
    "text": "2.2 Filtrage\n\n\nCode\n# Filtrer les lignes\nprint(\"ğŸ” EmployÃ©s de Paris :\")\ndf.filter(F.col(\"ville\") == \"Paris\").show()\n\n# Filtres multiples avec AND\nprint(\"\\nğŸ” EmployÃ©s de Paris avec salaire &gt; 50000 :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") & \n    (F.col(\"salaire\") &gt; 50000)\n).show()\n\n# Filtres avec OR\nprint(\"\\nğŸ” EmployÃ©s de Paris OU Lyon :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") | \n    (F.col(\"ville\") == \"Lyon\")\n).show()\n\n# Filtrer avec IN\nprint(\"\\nğŸ” Villes avec IN :\")\ndf.filter(F.col(\"ville\").isin([\"Paris\", \"Lyon\"])).show()\n\n\n\n\nCode\n# Filtres avancÃ©s\nprint(\"ğŸ” Noms commenÃ§ant par 'A' :\")\ndf.filter(F.col(\"nom\").startswith(\"A\")).show()\n\nprint(\"\\nğŸ” Noms contenant 'li' :\")\ndf.filter(F.col(\"nom\").contains(\"li\")).show()\n\nprint(\"\\nğŸ” Age entre 25 et 30 :\")\ndf.filter(F.col(\"age\").between(25, 30)).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.3 Ajouter et modifier des colonnes",
    "text": "2.3 Ajouter et modifier des colonnes\n\n\nCode\n# Ajouter une nouvelle colonne\ndf_with_bonus = df.withColumn(\n    \"bonus\",\n    F.col(\"salaire\") * 0.1\n)\n\nprint(\"â• Ajout de la colonne 'bonus' :\")\ndf_with_bonus.show()\n\n# Modifier une colonne existante\ndf_modified = df.withColumn(\n    \"salaire\",\n    F.col(\"salaire\") * 1.05  # Augmentation de 5%\n)\n\nprint(\"\\nâœï¸ Salaire augmentÃ© de 5% :\")\ndf_modified.show()\n\n\n\n\nCode\n# Ajouter plusieurs colonnes\ndf_enriched = df \\\n    .withColumn(\"salaire_mensuel\", F.col(\"salaire\")) \\\n    .withColumn(\"salaire_annuel\", F.col(\"salaire\") * 12) \\\n    .withColumn(\"bonus\", F.col(\"salaire\") * 0.1) \\\n    .withColumn(\"total_annuel\", F.col(\"salaire_annuel\") + F.col(\"bonus\"))\n\nprint(\"ğŸ“Š DataFrame enrichi :\")\ndf_enriched.select(\"nom\", \"salaire_mensuel\", \"salaire_annuel\", \"bonus\", \"total_annuel\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.4 Renommer et supprimer des colonnes",
    "text": "2.4 Renommer et supprimer des colonnes\n\n\nCode\n# Renommer une colonne\ndf_renamed = df.withColumnRenamed(\"nom\", \"employee_name\")\nprint(\"âœï¸ Colonne renommÃ©e :\")\ndf_renamed.show(3)\n\n# Supprimer des colonnes\ndf_dropped = df.drop(\"age\", \"ville\")\nprint(\"\\nğŸ—‘ï¸ Colonnes supprimÃ©es :\")\ndf_dropped.show(3)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "2.5 Tri",
    "text": "2.5 Tri\n\n\nCode\n# Trier par salaire (ascendant)\nprint(\"ğŸ“Š Tri par salaire (croissant) :\")\ndf.orderBy(\"salaire\").show()\n\n# Trier par salaire (descendant)\nprint(\"\\nğŸ“Š Tri par salaire (dÃ©croissant) :\")\ndf.orderBy(F.col(\"salaire\").desc()).show()\n\n# Tri multiple\nprint(\"\\nğŸ“Š Tri par ville puis salaire :\")\ndf.orderBy(\"ville\", F.col(\"salaire\").desc()).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.1 AgrÃ©gations simples",
    "text": "3.1 AgrÃ©gations simples\n\n\nCode\n# Statistiques de base\nprint(\"ğŸ“Š Statistiques simples :\")\ndf.select(\n    F.count(\"*\").alias(\"total\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\"),\n    F.sum(\"salaire\").alias(\"salaire_total\")\n).show()\n\n\n\n\nCode\n# AgrÃ©gations multiples\nfrom pyspark.sql.functions import stddev, variance\n\nprint(\"ğŸ“Š Statistiques avancÃ©es :\")\ndf.agg(\n    F.count(\"*\").alias(\"count\"),\n    F.avg(\"age\").alias(\"age_moyen\"),\n    F.stddev(\"salaire\").alias(\"salaire_stddev\"),\n    F.variance(\"salaire\").alias(\"salaire_variance\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.2 GroupBy",
    "text": "3.2 GroupBy\n\n\nCode\n# Grouper par ville\nprint(\"ğŸ“Š Statistiques par ville :\")\ndf.groupBy(\"ville\").agg(\n    F.count(\"*\").alias(\"nb_employes\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\")\n).orderBy(\"ville\").show()\n\n\n\n\nCode\n# CrÃ©er un DataFrame plus complexe pour les exemples\ndata_ventes = [\n    (\"2024-01\", \"Paris\", \"Produit A\", 100, 1500),\n    (\"2024-01\", \"Paris\", \"Produit B\", 50, 2000),\n    (\"2024-01\", \"Lyon\", \"Produit A\", 75, 1200),\n    (\"2024-02\", \"Paris\", \"Produit A\", 120, 1800),\n    (\"2024-02\", \"Lyon\", \"Produit B\", 60, 2400),\n    (\"2024-02\", \"Marseille\", \"Produit A\", 90, 1350),\n]\n\ncolumns_ventes = [\"mois\", \"ville\", \"produit\", \"quantite\", \"montant\"]\ndf_ventes = spark.createDataFrame(data_ventes, columns_ventes)\n\nprint(\"ğŸ“Š DonnÃ©es de ventes :\")\ndf_ventes.show()\n\n\n\n\nCode\n# GroupBy multiple\nprint(\"ğŸ“Š Ventes par mois et ville :\")\ndf_ventes.groupBy(\"mois\", \"ville\").agg(\n    F.sum(\"quantite\").alias(\"total_quantite\"),\n    F.sum(\"montant\").alias(\"total_montant\"),\n    F.count(\"*\").alias(\"nb_transactions\")\n).orderBy(\"mois\", \"ville\").show()\n\n\n\n\nCode\n# AgrÃ©gations conditionnelles\nprint(\"ğŸ“Š AgrÃ©gations conditionnelles :\")\ndf_ventes.groupBy(\"ville\").agg(\n    F.sum(\"montant\").alias(\"total\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit A\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_a\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit B\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_b\")\n).show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "3.3 Window Functions",
    "text": "3.3 Window Functions\n\n\nCode\n# Ranking dans chaque ville\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"ville\").orderBy(F.col(\"salaire\").desc())\n\ndf_ranked = df.withColumn(\n    \"rank\",\n    F.row_number().over(window_spec)\n)\n\nprint(\"ğŸ† Ranking des salaires par ville :\")\ndf_ranked.orderBy(\"ville\", \"rank\").show()\n\n\n\n\nCode\n# Calculs cumulatifs\nwindow_cumul = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\ndf_cumul = df.withColumn(\n    \"salaire_cumul\",\n    F.sum(\"salaire\").over(window_cumul)\n)\n\nprint(\"ğŸ“ˆ Salaire cumulÃ© par ville :\")\ndf_cumul.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_cumul\").orderBy(\"ville\", \"id\").show()\n\n\n\n\nCode\n# Calcul de moyennes mobiles\nwindow_rolling = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(-1, 1)\n\ndf_rolling = df.withColumn(\n    \"salaire_avg_3\",\n    F.avg(\"salaire\").over(window_rolling)\n)\n\nprint(\"ğŸ“Š Moyenne mobile sur 3 lignes :\")\ndf_rolling.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_avg_3\").orderBy(\"ville\", \"id\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "4.1 CrÃ©er des DataFrames pour les exemples",
    "text": "4.1 CrÃ©er des DataFrames pour les exemples\n\n\nCode\n# DataFrame employÃ©s\nemployes = spark.createDataFrame([\n    (1, \"Alice\", \"IT\"),\n    (2, \"Bob\", \"Finance\"),\n    (3, \"Charlie\", \"IT\"),\n    (4, \"David\", \"HR\")\n], [\"emp_id\", \"nom\", \"dept_id\"])\n\n# DataFrame dÃ©partements\ndepartements = spark.createDataFrame([\n    (\"IT\", \"Information Technology\", \"Paris\"),\n    (\"Finance\", \"Finance Department\", \"Lyon\"),\n    (\"HR\", \"Human Resources\", \"Marseille\"),\n    (\"Marketing\", \"Marketing Department\", \"Paris\")\n], [\"dept_id\", \"dept_name\", \"location\"])\n\nprint(\"ğŸ‘¥ EmployÃ©s :\")\nemployes.show()\n\nprint(\"\\nğŸ¢ DÃ©partements :\")\ndepartements.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "4.2 Types de jointures",
    "text": "4.2 Types de jointures\n\n\nCode\n# INNER JOIN (par dÃ©faut)\nprint(\"ğŸ”— INNER JOIN :\")\nemployes.join(departements, \"dept_id\", \"inner\").show()\n\n# LEFT JOIN\nprint(\"\\nğŸ”— LEFT JOIN :\")\nemployes.join(departements, \"dept_id\", \"left\").show()\n\n# RIGHT JOIN\nprint(\"\\nğŸ”— RIGHT JOIN :\")\nemployes.join(departements, \"dept_id\", \"right\").show()\n\n# FULL OUTER JOIN\nprint(\"\\nğŸ”— FULL OUTER JOIN :\")\nemployes.join(departements, \"dept_id\", \"outer\").show()\n\n\n\n\nCode\n# Jointure avec colonnes diffÃ©rentes\nemployes_alt = employes.withColumnRenamed(\"dept_id\", \"department\")\n\nprint(\"ğŸ”— Jointure avec colonnes diffÃ©rentes :\")\nemployes_alt.join(\n    departements,\n    employes_alt.department == departements.dept_id,\n    \"inner\"\n).select(\n    employes_alt[\"*\"],\n    departements.dept_name,\n    departements.location\n).show()\n\n\n\n\nCode\n# Jointures multiples\nsalaires = spark.createDataFrame([\n    (1, 45000),\n    (2, 55000),\n    (3, 50000),\n    (4, 48000)\n], [\"emp_id\", \"salaire\"])\n\nprint(\"ğŸ”— Jointures multiples :\")\nresult = employes \\\n    .join(departements, \"dept_id\", \"inner\") \\\n    .join(salaires, \"emp_id\", \"inner\")\n\nresult.select(\"nom\", \"dept_name\", \"location\", \"salaire\").show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.1 CSV",
    "text": "5.1 CSV\n\n\nCode\n# CrÃ©er des donnÃ©es de test\nimport os\nos.makedirs('data', exist_ok=True)\n\n# Ã‰crire en CSV\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\"âœ… CSV Ã©crit\")\n\n# Lire le CSV\ndf_from_csv = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\"\\nğŸ“‚ CSV lu :\")\ndf_from_csv.show(3)\ndf_from_csv.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.2 JSON",
    "text": "5.2 JSON\n\n\nCode\n# Ã‰crire en JSON\ndf.write \\\n    .mode(\"overwrite\") \\\n    .json(\"data/employes.json\")\n\nprint(\"âœ… JSON Ã©crit\")\n\n# Lire le JSON\ndf_from_json = spark.read.json(\"data/employes.json\")\n\nprint(\"\\nğŸ“‚ JSON lu :\")\ndf_from_json.show(3)\ndf_from_json.printSchema()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.3 Parquet (Format recommandÃ©)",
    "text": "5.3 Parquet (Format recommandÃ©)\n\n\nCode\n# Ã‰crire en Parquet\ndf.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"data/employes.parquet\")\n\nprint(\"âœ… Parquet Ã©crit\")\n\n# Lire le Parquet\ndf_from_parquet = spark.read.parquet(\"data/employes.parquet\")\n\nprint(\"\\nğŸ“‚ Parquet lu :\")\ndf_from_parquet.show(3)\ndf_from_parquet.printSchema()\n\n\n\n\nCode\n# Parquet avec partitionnement\ndf.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"ville\") \\\n    .parquet(\"data/employes_partitioned.parquet\")\n\nprint(\"âœ… Parquet partitionnÃ© Ã©crit\")\n\n# Lire avec filtre de partition (trÃ¨s performant)\ndf_paris = spark.read \\\n    .parquet(\"data/employes_partitioned.parquet\") \\\n    .filter(F.col(\"ville\") == \"Paris\")\n\nprint(\"\\nğŸ“‚ Parquet partitionnÃ© lu (ville=Paris) :\")\ndf_paris.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "5.4 Options dâ€™Ã©criture",
    "text": "5.4 Options dâ€™Ã©criture\n\n\nCode\n# Mode d'Ã©criture\n# - \"overwrite\" : Ã‰crase les donnÃ©es existantes\n# - \"append\" : Ajoute aux donnÃ©es existantes\n# - \"ignore\" : Ne fait rien si le fichier existe\n# - \"error\" (default) : Erreur si le fichier existe\n\n# Compression\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"compression\", \"snappy\") \\\n    .parquet(\"data/employes_compressed.parquet\")\n\nprint(\"âœ… Parquet compressÃ© Ã©crit\")\n\n# ContrÃ´ler le nombre de fichiers\ndf.coalesce(1).write \\\n    .mode(\"overwrite\") \\\n    .csv(\"data/employes_single_file.csv\")\n\nprint(\"âœ… CSV en un seul fichier Ã©crit\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "6.1 CrÃ©er des vues temporaires",
    "text": "6.1 CrÃ©er des vues temporaires\n\n\nCode\n# CrÃ©er une vue temporaire\ndf.createOrReplaceTempView(\"employes\")\ndf_ventes.createOrReplaceTempView(\"ventes\")\n\nprint(\"âœ… Vues temporaires crÃ©Ã©es\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "6.2 RequÃªtes SQL",
    "text": "6.2 RequÃªtes SQL\n\n\nCode\n# RequÃªte SQL simple\nresult = spark.sql(\"\"\"\n    SELECT nom, ville, salaire\n    FROM employes\n    WHERE salaire &gt; 50000\n    ORDER BY salaire DESC\n\"\"\")\n\nprint(\"ğŸ“Š EmployÃ©s avec salaire &gt; 50000 :\")\nresult.show()\n\n\n\n\nCode\n# AgrÃ©gation avec SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        ville,\n        COUNT(*) as nb_employes,\n        AVG(salaire) as salaire_moyen,\n        MIN(salaire) as salaire_min,\n        MAX(salaire) as salaire_max\n    FROM employes\n    GROUP BY ville\n    ORDER BY salaire_moyen DESC\n\"\"\")\n\nprint(\"ğŸ“Š Statistiques par ville :\")\nresult.show()\n\n\n\n\nCode\n# Window functions en SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        nom,\n        ville,\n        salaire,\n        ROW_NUMBER() OVER (PARTITION BY ville ORDER BY salaire DESC) as rank_ville,\n        DENSE_RANK() OVER (ORDER BY salaire DESC) as rank_global\n    FROM employes\n    ORDER BY ville, rank_ville\n\"\"\")\n\nprint(\"ğŸ† Ranking avec SQL :\")\nresult.show()\n\n\n\n\nCode\n# CTE (Common Table Expression)\nresult = spark.sql(\"\"\"\n    WITH stats_ville AS (\n        SELECT \n            ville,\n            AVG(salaire) as salaire_moyen\n        FROM employes\n        GROUP BY ville\n    )\n    SELECT \n        e.nom,\n        e.ville,\n        e.salaire,\n        s.salaire_moyen,\n        ROUND(e.salaire - s.salaire_moyen, 2) as diff_moyenne\n    FROM employes e\n    JOIN stats_ville s ON e.ville = s.ville\n    ORDER BY e.ville, e.salaire DESC\n\"\"\")\n\nprint(\"ğŸ“Š Comparaison Ã  la moyenne :\")\nresult.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.1 Partitionnement",
    "text": "9.1 Partitionnement\n\n\nCode\n# VÃ©rifier le nombre de partitions\nprint(f\"Nombre de partitions : {df.rdd.getNumPartitions()}\")\n\n# Repartitionner (shuffle)\ndf_repartitioned = df.repartition(4)\nprint(f\"AprÃ¨s repartition : {df_repartitioned.rdd.getNumPartitions()}\")\n\n# Coalesce (pas de shuffle, moins coÃ»teux)\ndf_coalesced = df.coalesce(2)\nprint(f\"AprÃ¨s coalesce : {df_coalesced.rdd.getNumPartitions()}\")\n\n\n\n\nCode\n# Repartitionner par colonne (utile avant les groupBy)\ndf_repartitioned_by_ville = df.repartition(\"ville\")\n\n# Maintenant les groupBy sur 'ville' seront plus efficaces\ndf_repartitioned_by_ville.groupBy(\"ville\").count().show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.2 Caching",
    "text": "9.2 Caching\n\n\nCode\n# Cache un DataFrame en mÃ©moire\ndf_cached = df.cache()\n\n# PremiÃ¨re action : calcul complet\nprint(\"PremiÃ¨re action (calcul complet) :\")\ndf_cached.count()\n\n# DeuxiÃ¨me action : utilise le cache (beaucoup plus rapide)\nprint(\"\\nDeuxiÃ¨me action (utilise le cache) :\")\ndf_cached.show()\n\n# LibÃ©rer le cache\ndf_cached.unpersist()\nprint(\"\\nâœ… Cache libÃ©rÃ©\")\n\n\n\n\nCode\n# Persist avec diffÃ©rents niveaux de stockage\nfrom pyspark import StorageLevel\n\n# MEMORY_ONLY : En mÃ©moire uniquement\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n# MEMORY_AND_DISK : MÃ©moire + disque si nÃ©cessaire\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n\n# DISK_ONLY : Disque uniquement\ndf.persist(StorageLevel.DISK_ONLY)\n\nprint(\"âœ… DiffÃ©rents niveaux de persistance disponibles\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.3 Broadcast Joins",
    "text": "9.3 Broadcast Joins\n\n\nCode\n# Pour les petits DataFrames (&lt; 10MB), utilisez broadcast\nfrom pyspark.sql.functions import broadcast\n\n# departements est petit, on le broadcast\nresult = employes.join(\n    broadcast(departements),\n    \"dept_id\",\n    \"inner\"\n)\n\nprint(\"ğŸš€ Broadcast join :\")\nresult.show()\n\n# Ã‰vite le shuffle, beaucoup plus rapide !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.4 Ã‰viter les UDFs quand possible",
    "text": "9.4 Ã‰viter les UDFs quand possible\n\n\nCode\n# âŒ Avec UDF (lent)\ndef add_ten(x):\n    return x + 10\n\nadd_ten_udf = udf(add_ten, IntegerType())\ndf.withColumn(\"salaire_plus_10_udf\", add_ten_udf(F.col(\"salaire\")))\n\n# âœ… Avec fonction native (rapide)\ndf.withColumn(\"salaire_plus_10\", F.col(\"salaire\") + 10)\n\nprint(\"âœ… Les fonctions natives sont toujours plus rapides !\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "9.5 Explain Plans",
    "text": "9.5 Explain Plans\n\n\nCode\n# Voir le plan d'exÃ©cution\nprint(\"ğŸ“Š Plan d'exÃ©cution :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain()\n\n# Plan dÃ©taillÃ©\nprint(\"\\nğŸ“Š Plan d'exÃ©cution dÃ©taillÃ© :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain(extended=True)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.1 Extract",
    "text": "11.1 Extract\n\n\nCode\ndef extract_data(spark, path):\n    \"\"\"Extrait des donnÃ©es depuis plusieurs sources\"\"\"\n    print(f\"ğŸ“¥ Extraction depuis {path}\")\n    \n    # CrÃ©er des donnÃ©es de test\n    data = [\n        (1, \"2024-01-15\", \"Paris\", \"Produit A\", 100, 1500, \"online\"),\n        (2, \"2024-01-15\", \"Lyon\", \"Produit B\", 50, 2000, \"store\"),\n        (3, \"2024-01-16\", \"Paris\", \"Produit A\", 75, 1200, \"online\"),\n        (4, \"2024-01-16\", \"Marseille\", \"Produit C\", 120, 1800, \"online\"),\n        (5, \"2024-01-17\", \"Lyon\", \"Produit B\", 60, 2400, \"store\"),\n        (6, \"2024-01-17\", None, \"Produit A\", 90, None, \"online\"),  # DonnÃ©es sales\n    ]\n    \n    columns = [\"id\", \"date\", \"ville\", \"produit\", \"quantite\", \"montant\", \"canal\"]\n    df = spark.createDataFrame(data, columns)\n    \n    # Sauvegarder les donnÃ©es brutes\n    df.write.mode(\"overwrite\").parquet(f\"{path}/ventes_raw.parquet\")\n    \n    print(f\"âœ… {df.count()} lignes extraites\")\n    return df\n\n# Test\ndf_raw = extract_data(spark, \"spark_pipeline/raw\")\ndf_raw.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.2 Transform",
    "text": "11.2 Transform\n\n\nCode\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    print(\"ğŸ”„ Transformation des donnÃ©es\")\n    \n    # 1. Convertir la date\n    df = df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n    \n    # 2. GÃ©rer les valeurs manquantes\n    df = df.fillna({\n        \"ville\": \"Inconnu\",\n        \"montant\": 0\n    })\n    \n    # 3. Filtrer les donnÃ©es invalides\n    df = df.filter(\n        (F.col(\"quantite\") &gt; 0) & \n        (F.col(\"montant\") &gt;= 0)\n    )\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    df = df.withColumn(\n        \"prix_unitaire\",\n        F.when(F.col(\"quantite\") &gt; 0, F.col(\"montant\") / F.col(\"quantite\")).otherwise(0)\n    )\n    \n    df = df.withColumn(\n        \"annee\",\n        F.year(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"mois\",\n        F.month(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"jour_semaine\",\n        F.dayofweek(F.col(\"date\"))\n    )\n    \n    # 5. CatÃ©goriser\n    df = df.withColumn(\n        \"categorie_montant\",\n        F.when(F.col(\"montant\") &lt; 1500, \"Faible\")\n         .when(F.col(\"montant\") &lt; 2000, \"Moyen\")\n         .otherwise(\"Ã‰levÃ©\")\n    )\n    \n    # 6. Ajouter metadata\n    df = df.withColumn(\"processed_at\", F.current_timestamp())\n    \n    print(f\"âœ… {df.count()} lignes transformÃ©es\")\n    return df\n\n# Test\ndf_transformed = transform_data(df_raw)\ndf_transformed.show()",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.3 Aggregate",
    "text": "11.3 Aggregate\n\n\nCode\ndef aggregate_data(df):\n    \"\"\"CrÃ©e des agrÃ©gations mÃ©tier\"\"\"\n    print(\"ğŸ“Š AgrÃ©gation des donnÃ©es\")\n    \n    # AgrÃ©gation par ville et produit\n    agg_ville_produit = df.groupBy(\"ville\", \"produit\").agg(\n        F.sum(\"quantite\").alias(\"total_quantite\"),\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.avg(\"prix_unitaire\").alias(\"prix_moyen\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"ville\", \"produit\")\n    \n    print(\"\\nğŸ“Š AgrÃ©gation par ville et produit :\")\n    agg_ville_produit.show()\n    \n    # AgrÃ©gation par canal\n    agg_canal = df.groupBy(\"canal\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\"),\n        F.avg(\"montant\").alias(\"montant_moyen\")\n    )\n    \n    print(\"\\nğŸ“Š AgrÃ©gation par canal :\")\n    agg_canal.show()\n    \n    # AgrÃ©gation temporelle\n    agg_temporelle = df.groupBy(\"annee\", \"mois\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"annee\", \"mois\")\n    \n    print(\"\\nğŸ“Š AgrÃ©gation temporelle :\")\n    agg_temporelle.show()\n    \n    return {\n        \"ville_produit\": agg_ville_produit,\n        \"canal\": agg_canal,\n        \"temporelle\": agg_temporelle\n    }\n\n# Test\naggregations = aggregate_data(df_transformed)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.4 Load",
    "text": "11.4 Load\n\n\nCode\ndef load_data(df, aggregations, output_path):\n    \"\"\"Charge les donnÃ©es dans le datalake\"\"\"\n    print(\"ğŸ’¾ Chargement des donnÃ©es\")\n    \n    # 1. DonnÃ©es transformÃ©es (partitionnÃ©es par date)\n    df.write \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"annee\", \"mois\") \\\n        .parquet(f\"{output_path}/ventes_transformed\")\n    print(\"âœ… DonnÃ©es transformÃ©es sauvegardÃ©es\")\n    \n    # 2. AgrÃ©gations\n    for name, agg_df in aggregations.items():\n        agg_df.write \\\n            .mode(\"overwrite\") \\\n            .parquet(f\"{output_path}/agg_{name}\")\n        print(f\"âœ… AgrÃ©gation '{name}' sauvegardÃ©e\")\n    \n    # 3. Export CSV pour l'analyse\n    df.coalesce(1).write \\\n        .mode(\"overwrite\") \\\n        .option(\"header\", \"true\") \\\n        .csv(f\"{output_path}/ventes_export.csv\")\n    print(\"âœ… Export CSV crÃ©Ã©\")\n\n# Test\nload_data(df_transformed, aggregations, \"spark_pipeline/output\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "11.5 Pipeline complet orchestrÃ©",
    "text": "11.5 Pipeline complet orchestrÃ©\n\n\nCode\ndef run_pipeline(spark):\n    \"\"\"ExÃ©cute le pipeline ETL complet\"\"\"\n    import time\n    \n    start_time = time.time()\n    print(\"=\"*60)\n    print(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE PYSPARK\")\n    print(\"=\"*60)\n    \n    try:\n        # EXTRACT\n        print(\"\\nğŸ“¥ PHASE 1: EXTRACTION\")\n        df_raw = extract_data(spark, \"spark_pipeline/raw\")\n        \n        # TRANSFORM\n        print(\"\\nğŸ”„ PHASE 2: TRANSFORMATION\")\n        df_transformed = transform_data(df_raw)\n        \n        # Cache pour les performances\n        df_transformed.cache()\n        \n        # AGGREGATE\n        print(\"\\nğŸ“Š PHASE 3: AGRÃ‰GATION\")\n        aggregations = aggregate_data(df_transformed)\n        \n        # LOAD\n        print(\"\\nğŸ’¾ PHASE 4: CHARGEMENT\")\n        load_data(df_transformed, aggregations, \"spark_pipeline/output\")\n        \n        # STATISTICS\n        duration = time.time() - start_time\n        print(\"\\n\" + \"=\"*60)\n        print(\"ğŸ“Š STATISTIQUES DU PIPELINE\")\n        print(\"=\"*60)\n        print(f\"DurÃ©e totale: {duration:.2f}s\")\n        print(f\"Lignes traitÃ©es: {df_transformed.count()}\")\n        print(f\"Partitions: {df_transformed.rdd.getNumPartitions()}\")\n        print(\"=\"*60)\n        print(\"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        print(\"=\"*60)\n        \n        # LibÃ©rer le cache\n        df_transformed.unpersist()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"\\nâŒ ERREUR: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline(spark)",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Ce que vous avez appris âœ…",
    "text": "Ce que vous avez appris âœ…\n\nFondamentaux Spark : Architecture, concepts, SparkSession\nDataFrames : CrÃ©ation, transformations, actions\nTransformations : Select, filter, withColumn, orderBy\nAgrÃ©gations : GroupBy, agrÃ©gations complexes, window functions\nJointures : Inner, left, right, outer, broadcast\nI/O : CSV, JSON, Parquet avec partitionnement\nSpark SQL : RequÃªtes SQL, CTEs, window functions\nOptimisation : Partitionnement, caching, broadcast joins\nUDFs : Fonctions personnalisÃ©es\nStreaming : Traitement temps rÃ©el (introduction)\nPipeline ETL : Architecture complÃ¨te production-ready",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "DiffÃ©rences clÃ©s Pandas vs PySpark ğŸ”„",
    "text": "DiffÃ©rences clÃ©s Pandas vs PySpark ğŸ”„\n\n\n\n\n\n\n\n\nAspect\nPandas\nPySpark\n\n\n\n\nExÃ©cution\nEager (immÃ©diate)\nLazy (diffÃ©rÃ©e)\n\n\nDonnÃ©es\nEn mÃ©moire (single machine)\nDistribuÃ©es (cluster)\n\n\nScalabilitÃ©\nLimitÃ© Ã  la RAM\nQuasi illimitÃ©\n\n\nAPI\ndf[df['col'] &gt; 5]\ndf.filter(F.col('col') &gt; 5)\n\n\nPerformances\nRapide pour petites donnÃ©es\nRapide pour Big Data",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Quand utiliser PySpark ? ğŸ¤”",
    "text": "Quand utiliser PySpark ? ğŸ¤”\nâœ… Utilisez PySpark si : - DonnÃ©es &gt; 10 GB - Besoin de parallÃ©lisation - Traitement distribuÃ© nÃ©cessaire - Streaming en temps rÃ©el\nâŒ Utilisez Pandas si : - DonnÃ©es &lt; 10 GB - Prototypage rapide - Analyses exploratoires - Machine learning local",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Prochaines Ã©tapes ğŸš€",
    "text": "Prochaines Ã©tapes ğŸš€\n\nPratiquer : CrÃ©er des pipelines avec vos propres donnÃ©es\nApprofondir :\n\nMLlib (Machine Learning)\nGraphX (Graph processing)\nDelta Lake (ACID transactions)\n\nProduction :\n\nDatabricks\nAWS EMR\nAzure Synapse\nGoogle Dataproc\n\nOrchestration :\n\nApache Airflow\nPrefect\nDagster",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "Ressources ğŸ“š",
    "text": "Ressources ğŸ“š\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy\n\n\nFÃ©licitations ! ğŸ‰ Vous maÃ®trisez maintenant les fondamentaux de PySpark !",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#votre-score",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#votre-score",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert PySpark ! PrÃªt pour la production\n8-9/10 : ğŸŒŸ Excellent ! Pratiquez les concepts avancÃ©s\n6-7/10 : ğŸ’ª Bon niveau ! Revoyez les optimisations\n&lt; 6/10 : ğŸ“š Relisez le notebook et pratiquez les exemples",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy â€” Cours gratuits\n\n\nğŸ­ Plateformes Cloud\n\n\n\nPlateforme\nService Spark\n\n\n\n\nDatabricks\nDatabricks Lakehouse\n\n\nAWS\nEMR (Elastic MapReduce)\n\n\nAzure\nSynapse Analytics, HDInsight\n\n\nGCP\nDataproc",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "title": "âš¡ PySpark for Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant le traitement Big Data avec PySpark !\nPour continuer ton parcours Data Engineering,\nğŸ‘‰ Module suivant : 12_orchestration_pipelines.ipynb â€” Orchestration de pipelines\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module PySpark et le parcours sur les bases de donnÃ©es et le Big Data !\n\n\nCode\n# Fermer la SparkSession\nspark.stop()\nprint(\"âœ… SparkSession fermÃ©e\")",
    "crumbs": [
      "DÃ©butant",
      "âš¡ PySpark for Data Engineering"
    ]
  }
]