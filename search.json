[
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html",
    "title": "Polars pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir Polars, la bibliothÃ¨que DataFrame ultra-rapide qui rÃ©volutionne le traitement de donnÃ©es en Python. Tu apprendras pourquoi Polars surpasse Pandas, comment exploiter son moteur dâ€™exÃ©cution lazy, et comment construire des pipelines ETL performants !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#prÃ©requis",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#prÃ©requis",
    "title": "Polars pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nConnaissances de base en Python\n\n\nâœ… Requis\nAvoir utilisÃ© Pandas (mÃªme basiquement)\n\n\nğŸ’¡ RecommandÃ©\nAvoir suivi les modules prÃ©cÃ©dents du bootcamp",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#objectifs-du-module",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#objectifs-du-module",
    "title": "Polars pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre pourquoi Polars est 5-100x plus rapide que Pandas\nMaÃ®triser lâ€™architecture columnar et le format Apache Arrow\nUtiliser les expressions Polars pour des transformations efficaces\nExploiter lâ€™exÃ©cution Lazy pour des pipelines optimisÃ©s\nMigrer du code Pandas vers Polars\nConstruire des pipelines ETL performants en production",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#polars-vs-pandas-pourquoi-changer",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#polars-vs-pandas-pourquoi-changer",
    "title": "Polars pour Data Engineers",
    "section": "1. Polars vs Pandas : Pourquoi changer ?",
    "text": "1. Polars vs Pandas : Pourquoi changer ?\nAvant de plonger dans Polars, comprenons pourquoi cette bibliothÃ¨que existe et ce quâ€™elle apporte.\n\n1.1 Les limitations de Pandas\nPandas est formidable pour lâ€™exploration de donnÃ©es, mais il a des limitations structurelles :\n\n\n\n\n\n\n\n\nLimitation\nExplication\nImpact\n\n\n\n\nSingle-threaded\nLe GIL Python bloque le parallÃ©lisme\nNâ€™utilise quâ€™1 CPU\n\n\nRow-based en mÃ©moire\nDonnÃ©es stockÃ©es par ligne\nCache CPU inefficace\n\n\nEager execution\nChaque opÃ©ration sâ€™exÃ©cute immÃ©diatement\nPas dâ€™optimisation globale\n\n\nCopies frÃ©quentes\nBeaucoup dâ€™opÃ©rations copient les donnÃ©es\nRAM x2 ou x3\n\n\nMÃ©moire gourmande\n~5-10x la taille du fichier\nLimite les gros datasets\n\n\n\n\n\n1.2 Les forces de Polars\n\n\n\nAspect\nPandas\nPolars\n\n\n\n\nBackend\nNumPy (C)\nRust ğŸ¦€\n\n\nThreading\nSingle (GIL)\nMulti-threaded\n\n\nMÃ©moire\nRow-based\nColumnar (Arrow)\n\n\nExecution\nEager only\nEager + Lazy\n\n\nVitesse\nBaseline\n5-100x plus rapide\n\n\nOut-of-core\nâŒ\nâœ… (streaming)\n\n\nOptimiseur\nâŒ\nâœ… Query planner\n\n\n\n\nğŸ’¡ En rÃ©sumÃ© : Polars est conÃ§u dÃ¨s le dÃ©part pour la performance et les gros volumes, lÃ  oÃ¹ Pandas a Ã©tÃ© conÃ§u pour lâ€™exploration interactive.\n\n\nâ„¹ï¸ Le savais-tu ?\nPolars a Ã©tÃ© crÃ©Ã© en 2020 par Ritchie Vink, un ingÃ©nieur nÃ©erlandais frustrÃ© par la lenteur de Pandas.\nLe nom â€œPolarsâ€ fait rÃ©fÃ©rence Ã  lâ€™ours polaire (ğŸ»â€â„ï¸) â€” un clin dâ€™Å“il Ã  Pandas (ğŸ¼) tout en Ã©tant plus rapide et adaptÃ© aux environnements â€œfroidsâ€ (haute performance).\nPolars est Ã©crit en Rust, un langage rÃ©putÃ© pour sa vitesse et sa sÃ©curitÃ© mÃ©moire.\nğŸ“– Site officiel Polars\n\n\n\n1.3 Benchmark concret\nComparons Pandas et Polars sur une opÃ©ration simple : lire un CSV et faire une agrÃ©gation.\n\n\nVoir le code\n# CrÃ©er un fichier de test\nimport random\nimport csv\nimport os\n\nos.makedirs(\"data\", exist_ok=True)\n\n# GÃ©nÃ©rer 500K lignes\ncategories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\nn_rows = 500_000\n\nwith open(\"data/benchmark.csv\", \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"id\", \"category\", \"amount\", \"quantity\"])\n    for i in range(n_rows):\n        writer.writerow([\n            i,\n            random.choice(categories),\n            round(random.uniform(10, 1000), 2),\n            random.randint(1, 100)\n        ])\n\nprint(f\"âœ… Fichier crÃ©Ã© : data/benchmark.csv ({n_rows:,} lignes)\")\n\n\n\n\nVoir le code\nimport pandas as pd\nimport time\n\n# Benchmark Pandas\nstart = time.time()\n\ndf_pandas = pd.read_csv(\"data/benchmark.csv\")\nresult_pandas = (\n    df_pandas\n    .groupby(\"category\")\n    .agg({\"amount\": \"sum\", \"quantity\": \"mean\"})\n    .reset_index()\n)\n\npandas_time = time.time() - start\nprint(f\"ğŸ¼ Pandas : {pandas_time:.3f} secondes\")\nprint(result_pandas)\n\n\n\n\nVoir le code\nimport polars as pl\nimport time\n\n# Benchmark Polars (Eager)\nstart = time.time()\n\ndf_polars = pl.read_csv(\"data/benchmark.csv\")\nresult_polars = (\n    df_polars\n    .group_by(\"category\")\n    .agg(\n        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n    )\n)\n\npolars_time = time.time() - start\nprint(f\"ğŸ»â€â„ï¸ Polars : {polars_time:.3f} secondes\")\nprint(f\"âš¡ Polars est {pandas_time/polars_time:.1f}x plus rapide !\")\nprint(result_polars)\n\n\n\n\nVoir le code\n# Benchmark Polars (Lazy) - encore plus rapide !\nstart = time.time()\n\nresult_lazy = (\n    pl.scan_csv(\"data/benchmark.csv\")  # Lazy !\n    .group_by(\"category\")\n    .agg(\n        pl.col(\"amount\").sum().alias(\"amount_sum\"),\n        pl.col(\"quantity\").mean().alias(\"quantity_mean\")\n    )\n    .collect()  # ExÃ©cution optimisÃ©e\n)\n\nlazy_time = time.time() - start\nprint(f\"ğŸš€ Polars Lazy : {lazy_time:.3f} secondes\")\nprint(f\"âš¡ Polars Lazy est {pandas_time/lazy_time:.1f}x plus rapide que Pandas !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#comprendre-larchitecture-de-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#comprendre-larchitecture-de-polars",
    "title": "Polars pour Data Engineers",
    "section": "2. Comprendre lâ€™architecture de Polars",
    "text": "2. Comprendre lâ€™architecture de Polars\nPour bien utiliser Polars, il faut comprendre pourquoi il est si rapide.\n\n2.1 Columnar vs Row-based\nROW-BASED (Pandas)                    COLUMNAR (Polars)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id  â”‚ name â”‚ age â”‚                  â”‚ id:   [1, 2, 3]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  1  â”‚ Ana  â”‚ 25  â”‚  â† Ligne 1       â”‚ name: [A, B, C]   â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  2  â”‚ Bob  â”‚ 30  â”‚  â† Ligne 2       â”‚ age:  [25, 30, 22]â”‚\nâ”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚  3  â”‚ Cat  â”‚ 22  â”‚  â† Ligne 3              â†‘\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                   Colonnes contiguÃ«s\n        â†‘                              en mÃ©moire\n  Lignes contiguÃ«s\n  en mÃ©moire\nPourquoi columnar est plus rapide ?\n\n\n\nAvantage\nExplication\n\n\n\n\nCache CPU\nDonnÃ©es contiguÃ«s = moins de cache misses\n\n\nSIMD\nOpÃ©rations vectorisÃ©es sur colonnes entiÃ¨res\n\n\nCompression\nColonnes homogÃ¨nes = meilleure compression\n\n\nSÃ©lection\nLire seulement les colonnes nÃ©cessaires\n\n\n\n\n\n2.2 Apache Arrow : le format sous-jacent\nPolars utilise Apache Arrow comme format mÃ©moire :\n\n\n\nAvantage\nDescription\n\n\n\n\nZero-copy\nPartage de donnÃ©es sans copie\n\n\nInteropÃ©rabilitÃ©\nCompatible Spark, DuckDB, PyArrow\n\n\nStandardisÃ©\nFormat ouvert et documentÃ©\n\n\n\n\n\n2.3 Eager vs Lazy execution\n\n\n\n\n\n\n\n\nMode\nDescription\nQuand lâ€™utiliser\n\n\n\n\nEager\nExÃ©cute immÃ©diatement chaque opÃ©ration\nExploration, debug, petits datasets\n\n\nLazy\nConstruit un plan, optimise, puis exÃ©cute\nProduction, gros fichiers, pipelines\n\n\n\n# Eager : rÃ©sultat immÃ©diat\ndf = pl.read_csv(\"data.csv\")        # Lit maintenant\ndf = df.filter(pl.col(\"x\") &gt; 5)     # Filtre maintenant\n\n# Lazy : plan d'exÃ©cution\nlf = pl.scan_csv(\"data.csv\")        # CrÃ©e un plan\nlf = lf.filter(pl.col(\"x\") &gt; 5)     # Ajoute au plan\ndf = lf.collect()                    # ExÃ©cute tout (optimisÃ©)\n\n\n2.4 Query Optimizer\nLe Query Optimizer de Polars applique automatiquement des optimisations :\n\n\n\nOptimisation\nDescription\n\n\n\n\nPredicate pushdown\nFiltres appliquÃ©s le plus tÃ´t possible\n\n\nProjection pruning\nColonnes inutiles jamais lues\n\n\nCommon subexpression\nCalculs redondants factorisÃ©s\n\n\nParallelization\nOpÃ©rations distribuÃ©es sur tous les CPUs\n\n\n\nPLAN ORIGINAL                    PLAN OPTIMISÃ‰\nâ•â•â•â•â•â•â•â•â•â•â•â•â•                    â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nscan_csv(all cols)               scan_csv(only needed cols)\n      â”‚                                â”‚\n      â–¼                                â–¼\nwith_columns(...)                filter(amount &gt; 100)  â† Pushdown!\n      â”‚                                â”‚\n      â–¼                                â–¼\nfilter(amount &gt; 100)             with_columns(...)\n      â”‚                                â”‚\n      â–¼                                â–¼\n   result                           result",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#installation-configuration",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#installation-configuration",
    "title": "Polars pour Data Engineers",
    "section": "3. Installation & Configuration",
    "text": "3. Installation & Configuration\n\nInstallation\n# Installation de base\npip install polars\n\n# Avec toutes les features (recommandÃ©)\npip install 'polars[all]'\n\n# Features spÃ©cifiques\npip install 'polars[pyarrow,pandas,numpy,fsspec]'\n\n\nVÃ©rification\n\n\nVoir le code\nimport polars as pl\n\nprint(f\"âœ… Polars version : {pl.__version__}\")\n\n# Configuration de l'affichage\npl.Config.set_tbl_rows(10)           # Lignes affichÃ©es\npl.Config.set_tbl_cols(12)           # Colonnes affichÃ©es\npl.Config.set_fmt_str_lengths(50)    # Longueur des strings\n\n# Voir le nombre de threads utilisÃ©s\nprint(f\"ğŸ”§ Threads disponibles : {pl.thread_pool_size()}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#charger-exporter-des-donnÃ©es",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#charger-exporter-des-donnÃ©es",
    "title": "Polars pour Data Engineers",
    "section": "4. Charger & Exporter des donnÃ©es",
    "text": "4. Charger & Exporter des donnÃ©es\n\n4.1 Formats supportÃ©s\n\n\n\n\n\n\n\n\n\nFormat\nRead (Eager)\nScan (Lazy)\nWrite\n\n\n\n\nCSV\nread_csv()\nscan_csv()\nwrite_csv()\n\n\nParquet\nread_parquet()\nscan_parquet()\nwrite_parquet()\n\n\nJSON\nread_json()\nscan_ndjson()\nwrite_json()\n\n\nExcel\nread_excel()\nâŒ\nwrite_excel()\n\n\nDatabase\nread_database()\nâŒ\nâŒ\n\n\nIPC/Feather\nread_ipc()\nscan_ipc()\nwrite_ipc()\n\n\n\n\n\n4.2 Lecture Eager vs Lazy\n\n\nVoir le code\nimport polars as pl\n\n# ============ EAGER (tout en mÃ©moire) ============\ndf = pl.read_csv(\"data/benchmark.csv\")\nprint(\"Eager - Type:\", type(df))\nprint(df.head(3))\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# ============ LAZY (plan d'exÃ©cution) ============\nlf = pl.scan_csv(\"data/benchmark.csv\")\nprint(\"Lazy - Type:\", type(lf))\nprint(lf)  # Affiche le plan, pas les donnÃ©es\n\n\n\n\nVoir le code\n# CrÃ©er plusieurs fichiers pour l'exemple\nimport os\nos.makedirs(\"data/multi\", exist_ok=True)\n\nfor i in range(3):\n    pl.DataFrame({\n        \"id\": range(i*100, (i+1)*100),\n        \"value\": [i*10 + j for j in range(100)]\n    }).write_csv(f\"data/multi/file_{i}.csv\")\n\nprint(\"âœ… Fichiers crÃ©Ã©s\")\n\n# Lire plusieurs fichiers avec glob pattern\nlf = pl.scan_csv(\"data/multi/*.csv\")\nprint(f\"\\nNombre de lignes : {lf.collect().height}\")\n\n\n\n\nVoir le code\n# Ã‰criture\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"Paris\", \"Lyon\", \"Marseille\"]\n})\n\n# CSV\ndf.write_csv(\"data/output.csv\")\n\n# Parquet (recommandÃ© pour la production)\ndf.write_parquet(\"data/output.parquet\")\n\n# JSON\ndf.write_json(\"data/output.json\")\n\nprint(\"âœ… Fichiers exportÃ©s\")\n\n# VÃ©rifier avec Parquet\ndf_parquet = pl.read_parquet(\"data/output.parquet\")\nprint(df_parquet)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#expressions-polars-le-cÅ“ur-du-moteur",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#expressions-polars-le-cÅ“ur-du-moteur",
    "title": "Polars pour Data Engineers",
    "section": "5. Expressions Polars â€” Le cÅ“ur du moteur",
    "text": "5. Expressions Polars â€” Le cÅ“ur du moteur\n\nLes expressions sont ce qui rend Polars si puissant. Câ€™est un changement de paradigme par rapport Ã  Pandas.\n\n\n5.1 Philosophie : tout est expression\n# âŒ Pandas : opÃ©rations sur colonnes\ndf[\"new_col\"] = df[\"a\"] + df[\"b\"]\n\n# âœ… Polars : expressions\ndf.with_columns(\n    (pl.col(\"a\") + pl.col(\"b\")).alias(\"new_col\")\n)\n\n\n5.2 Expressions de base\n\n\n\nExpression\nDescription\nExemple\n\n\n\n\npl.col(\"x\")\nSÃ©lectionner une colonne\npl.col(\"amount\")\n\n\npl.col(\"x\", \"y\")\nPlusieurs colonnes\npl.col(\"a\", \"b\", \"c\")\n\n\npl.all()\nToutes les colonnes\ndf.select(pl.all())\n\n\npl.exclude(\"x\")\nToutes sauf x\npl.exclude(\"id\")\n\n\npl.lit(42)\nValeur littÃ©rale\npl.lit(\"constant\")\n\n\npl.col(\"*\")\nToutes (autre syntaxe)\npl.col(\"*\")\n\n\n\n\n\nVoir le code\nimport polars as pl\n\ndf = pl.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\"],\n    \"age\": [25, 30, 35, 28],\n    \"salary\": [50000, 60000, 75000, 55000],\n    \"department\": [\"IT\", \"HR\", \"IT\", \"Finance\"]\n})\n\nprint(\"DataFrame original :\")\nprint(df)\n\n# SÃ©lectionner des colonnes avec expressions\nprint(\"\\nSÃ©lection avec expressions :\")\nprint(\n    df.select(\n        pl.col(\"name\"),\n        pl.col(\"salary\") / 12,  # Salaire mensuel\n    )\n)\n\n\n\n\nVoir le code\n# Expressions conditionnelles : when/then/otherwise\nprint(\"Expressions conditionnelles :\")\nprint(\n    df.with_columns(\n        pl.when(pl.col(\"age\") &gt;= 30)\n          .then(pl.lit(\"Senior\"))\n          .otherwise(pl.lit(\"Junior\"))\n          .alias(\"level\"),\n        \n        pl.when(pl.col(\"salary\") &gt; 60000)\n          .then(pl.lit(\"High\"))\n          .when(pl.col(\"salary\") &gt; 50000)\n          .then(pl.lit(\"Medium\"))\n          .otherwise(pl.lit(\"Low\"))\n          .alias(\"salary_band\")\n    )\n)\n\n\n\n\nVoir le code\n# ChaÃ®nage d'expressions\nprint(\"ChaÃ®nage d'expressions :\")\nprint(\n    df.with_columns(\n        # String operations\n        pl.col(\"name\").str.to_uppercase().alias(\"NAME_UPPER\"),\n        pl.col(\"name\").str.len_chars().alias(\"name_length\"),\n        \n        # Math operations\n        (pl.col(\"salary\") * 1.1).round(2).alias(\"salary_raised\"),\n    )\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#manipulations-de-donnÃ©es-essentielles",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#manipulations-de-donnÃ©es-essentielles",
    "title": "Polars pour Data Engineers",
    "section": "6. Manipulations de donnÃ©es essentielles",
    "text": "6. Manipulations de donnÃ©es essentielles\n\n6.1 SÃ©lection de colonnes\n\n\nVoir le code\ndf = pl.read_csv(\"data/benchmark.csv\")\n\n# SÃ©lection simple\nprint(\"SÃ©lection simple :\")\nprint(df.select(\"category\", \"amount\").head(3))\n\n# SÃ©lection avec transformation\nprint(\"\\nSÃ©lection avec transformation :\")\nprint(\n    df.select(\n        pl.col(\"category\"),\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    ).head(3)\n)\n\n# SÃ©lection par type\nprint(\"\\nColonnes numÃ©riques uniquement :\")\nprint(df.select(pl.col(pl.Float64, pl.Int64)).head(3))\n\n\n\n\n6.2 Filtrage\n\n\nVoir le code\n# Filtre simple\nprint(\"Filtre simple (amount &gt; 500) :\")\nprint(df.filter(pl.col(\"amount\") &gt; 500).head(3))\n\n# Filtres multiples (AND)\nprint(\"\\nFiltres multiples (AND) :\")\nprint(\n    df.filter(\n        (pl.col(\"amount\") &gt; 500) & \n        (pl.col(\"category\") == \"Electronics\")\n    ).head(3)\n)\n\n# Filtres multiples (OR)\nprint(\"\\nFiltres multiples (OR) :\")\nprint(\n    df.filter(\n        (pl.col(\"category\") == \"Electronics\") | \n        (pl.col(\"category\") == \"Books\")\n    ).head(3)\n)\n\n# Filtre avec is_in\nprint(\"\\nFiltre avec is_in :\")\nprint(\n    df.filter(\n        pl.col(\"category\").is_in([\"Electronics\", \"Books\"])\n    ).head(3)\n)\n\n\n\n\n6.3 Ajout / modification de colonnes\n\n\nVoir le code\nprint(\"Ajout de colonnes :\")\nresult = df.with_columns(\n    # Calcul\n    (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n    \n    # Valeur constante\n    pl.lit(\"USD\").alias(\"currency\"),\n    \n    # Transformation de colonne existante\n    pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\"),\n    \n    # Conditionnel\n    pl.when(pl.col(\"amount\") &gt; 500)\n      .then(pl.lit(\"High\"))\n      .otherwise(pl.lit(\"Low\"))\n      .alias(\"amount_level\")\n)\n\nprint(result.head(5))\n\n\n\n\n6.4 GroupBy & Aggregations\n\n\nVoir le code\nprint(\"GroupBy simple :\")\nprint(\n    df.group_by(\"category\").agg(\n        pl.col(\"amount\").sum().alias(\"total_amount\"),\n        pl.col(\"amount\").mean().alias(\"avg_amount\"),\n        pl.col(\"amount\").max().alias(\"max_amount\"),\n        pl.len().alias(\"count\")\n    ).sort(\"total_amount\", descending=True)\n)\n\n\n\n\nVoir le code\n# Aggregations avancÃ©es\nprint(\"Aggregations avancÃ©es :\")\nprint(\n    df.group_by(\"category\").agg(\n        # Statistiques\n        pl.col(\"amount\").mean().alias(\"avg\"),\n        pl.col(\"amount\").std().alias(\"std\"),\n        pl.col(\"amount\").quantile(0.5).alias(\"median\"),\n        \n        # Comptages conditionnels\n        (pl.col(\"amount\") &gt; 500).sum().alias(\"high_amount_count\"),\n        \n        # Premier/Dernier\n        pl.col(\"amount\").first().alias(\"first_amount\"),\n    )\n)\n\n\n\n\n6.5 Tri, renommage, suppression\n\n\nVoir le code\n# Tri\nprint(\"Tri dÃ©croissant :\")\nprint(df.sort(\"amount\", descending=True).head(3))\n\n# Tri multiple\nprint(\"\\nTri multiple :\")\nprint(df.sort([\"category\", \"amount\"], descending=[True, False]).head(5))\n\n# Renommer\nprint(\"\\nRenommer :\")\nprint(df.rename({\"amount\": \"montant\", \"quantity\": \"quantite\"}).head(2))\n\n# Supprimer des colonnes\nprint(\"\\nSupprimer colonnes :\")\nprint(df.drop(\"id\").head(2))\n\n\n\n\n6.6 Joins\n\n\nVoir le code\n# CrÃ©er des DataFrames pour les joins\norders = pl.DataFrame({\n    \"order_id\": [1, 2, 3, 4],\n    \"customer_id\": [101, 102, 101, 103],\n    \"amount\": [100, 200, 150, 300]\n})\n\ncustomers = pl.DataFrame({\n    \"customer_id\": [101, 102, 104],\n    \"name\": [\"Alice\", \"Bob\", \"Diana\"]\n})\n\nprint(\"Orders:\", orders)\nprint(\"\\nCustomers:\", customers)\n\n# Inner join\nprint(\"\\nInner Join :\")\nprint(orders.join(customers, on=\"customer_id\", how=\"inner\"))\n\n# Left join\nprint(\"\\nLeft Join :\")\nprint(orders.join(customers, on=\"customer_id\", how=\"left\"))\n\n\n\n\n6.7 Dates et timestamps\n\n\nVoir le code\nfrom datetime import datetime, date\n\ndf_dates = pl.DataFrame({\n    \"event\": [\"A\", \"B\", \"C\", \"D\"],\n    \"timestamp\": [\n        datetime(2024, 1, 15, 10, 30),\n        datetime(2024, 3, 20, 14, 45),\n        datetime(2024, 6, 5, 9, 0),\n        datetime(2024, 12, 25, 18, 30)\n    ]\n})\n\nprint(\"DataFrame avec dates :\")\nprint(df_dates)\n\nprint(\"\\nExtractions de dates :\")\nprint(\n    df_dates.with_columns(\n        pl.col(\"timestamp\").dt.year().alias(\"year\"),\n        pl.col(\"timestamp\").dt.month().alias(\"month\"),\n        pl.col(\"timestamp\").dt.day().alias(\"day\"),\n        pl.col(\"timestamp\").dt.hour().alias(\"hour\"),\n        pl.col(\"timestamp\").dt.weekday().alias(\"weekday\"),\n        pl.col(\"timestamp\").dt.strftime(\"%Y-%m-%d\").alias(\"date_str\"),\n    )\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#lazy-execution-le-game-changer",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#lazy-execution-le-game-changer",
    "title": "Polars pour Data Engineers",
    "section": "7. Lazy Execution â€” Le Game Changer",
    "text": "7. Lazy Execution â€” Le Game Changer\n\nğŸ¯ Câ€™est ce qui rend Polars adaptÃ© Ã  la production et aux gros volumes.\n\n\n7.1 CrÃ©er un LazyFrame\n\n\nVoir le code\n# Depuis un fichier (recommandÃ©)\nlf = pl.scan_csv(\"data/benchmark.csv\")\nprint(\"Type:\", type(lf))\nprint(\"\\nLazyFrame (pas encore exÃ©cutÃ©) :\")\nprint(lf)\n\n\n\n\nVoir le code\n# Depuis un DataFrame existant\ndf = pl.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\nlf = df.lazy()\nprint(\"Converti en LazyFrame:\", type(lf))\n\n\n\n\n7.2 Construire le pipeline\n\n\nVoir le code\n# Pipeline complet en Lazy\npipeline = (\n    pl.scan_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\"),\n        pl.col(\"category\").str.to_uppercase().alias(\"CATEGORY\")\n    )\n    .group_by(\"CATEGORY\")\n    .agg(\n        pl.col(\"total\").sum().alias(\"total_revenue\"),\n        pl.len().alias(\"transaction_count\")\n    )\n    .sort(\"total_revenue\", descending=True)\n)\n\nprint(\"Pipeline dÃ©fini (pas encore exÃ©cutÃ©) :\")\nprint(pipeline)\nprint(\"\\nâš ï¸ Rien n'a Ã©tÃ© lu ou calculÃ© !\")\n\n\n\n\n7.3 ExÃ©cuter avec .collect()\n\n\nVoir le code\nimport time\n\nstart = time.time()\nresult = pipeline.collect()  # MAINTENANT Ã§a s'exÃ©cute\nprint(f\"â±ï¸ Temps d'exÃ©cution : {time.time() - start:.3f}s\")\nprint(\"\\nRÃ©sultat :\")\nprint(result)\n\n\n\n\n7.4 Voir le plan dâ€™exÃ©cution\n\n\nVoir le code\n# Plan logique (ce que tu as Ã©crit)\nprint(\"=== PLAN LOGIQUE ===\")\nprint(pipeline.explain())\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Plan optimisÃ© (ce que Polars exÃ©cute rÃ©ellement)\nprint(\"=== PLAN OPTIMISÃ‰ ===\")\nprint(pipeline.explain(optimized=True))\n\n\n\n\nğŸ–¼ï¸ SchÃ©ma : Pipeline Lazy\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  scan_csv() â”‚â”€â”€â”€â”€â–¶â”‚  filter()   â”‚â”€â”€â”€â”€â–¶â”‚with_columns()â”‚â”€â”€â”€â–¶â”‚  group_by() â”‚\nâ”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚     â”‚  (plan)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                                   â–¼\n                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                            â”‚  collect()  â”‚\n                                                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                    â”‚    Query Optimizer          â”‚\n                                                    â”‚  â€¢ Predicate pushdown       â”‚\n                                                    â”‚  â€¢ Column pruning           â”‚\n                                                    â”‚  â€¢ Parallel execution       â”‚\n                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                                   â”‚\n                                                                   â–¼\n                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                                            â”‚  DataFrame  â”‚\n                                                            â”‚  (rÃ©sultat) â”‚\n                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#migration-pandas-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#migration-pandas-polars",
    "title": "Polars pour Data Engineers",
    "section": "8. Migration Pandas â†’ Polars",
    "text": "8. Migration Pandas â†’ Polars\n\n8.1 Tableau de correspondance\n\n\n\n\n\n\n\n\nOpÃ©ration\nPandas\nPolars\n\n\n\n\nLire CSV\npd.read_csv()\npl.read_csv() / pl.scan_csv()\n\n\nLire Parquet\npd.read_parquet()\npl.read_parquet() / pl.scan_parquet()\n\n\nSÃ©lection colonne\ndf[\"col\"]\ndf.select(\"col\")\n\n\nPlusieurs colonnes\ndf[[\"a\", \"b\"]]\ndf.select(\"a\", \"b\")\n\n\nFiltre\ndf[df[\"x\"] &gt; 5]\ndf.filter(pl.col(\"x\") &gt; 5)\n\n\nNouvelle colonne\ndf[\"new\"] = df[\"a\"] + 1\ndf.with_columns((pl.col(\"a\") + 1).alias(\"new\"))\n\n\nGroupBy\ndf.groupby(\"x\").agg({\"y\": \"sum\"})\ndf.group_by(\"x\").agg(pl.col(\"y\").sum())\n\n\nTri\ndf.sort_values(\"x\")\ndf.sort(\"x\")\n\n\nRenommer\ndf.rename(columns={\"a\": \"b\"})\ndf.rename({\"a\": \"b\"})\n\n\nDrop\ndf.drop(columns=[\"x\"])\ndf.drop(\"x\")\n\n\nReset index\ndf.reset_index()\nN/A (pas dâ€™index)\n\n\nApply\ndf.apply(func)\ndf.map_rows(func) âš ï¸ Ã©viter\n\n\n\n\n\n8.2 InteropÃ©rabilitÃ©\n\n\nVoir le code\nimport pandas as pd\nimport polars as pl\n\n# CrÃ©er un DataFrame Pandas\npandas_df = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\"],\n    \"age\": [25, 30]\n})\n\n# Pandas â†’ Polars\npolars_df = pl.from_pandas(pandas_df)\nprint(\"Pandas â†’ Polars :\")\nprint(polars_df)\n\n# Polars â†’ Pandas\nback_to_pandas = polars_df.to_pandas()\nprint(\"\\nPolars â†’ Pandas :\")\nprint(back_to_pandas)\n\n\n\n\n8.3 DiffÃ©rences clÃ©s Ã  retenir\n\n\n\n\n\n\n\n\nAspect\nPandas\nPolars\n\n\n\n\nIndex\nâœ… Index par dÃ©faut\nâŒ Pas dâ€™index\n\n\nModification in-place\nâœ… inplace=True\nâŒ Toujours immutable\n\n\nTypage\nFlexible\nStrict\n\n\nNaN vs null\nNaN (float)\nnull (natif)\n\n\nChaÃ®nage\nLimitÃ©\nNaturel et optimisÃ©\n\n\n\n\n\nVoir le code\n# Exemple de migration complÃ¨te\n\n# ============ VERSION PANDAS ============\n# df = pd.read_csv(\"data.csv\")\n# df = df[df[\"amount\"] &gt; 100]\n# df[\"total\"] = df[\"amount\"] * df[\"quantity\"]\n# result = df.groupby(\"category\").agg({\"total\": \"sum\"}).reset_index()\n\n# ============ VERSION POLARS (Eager) ============\nresult_eager = (\n    pl.read_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    )\n    .group_by(\"category\")\n    .agg(pl.col(\"total\").sum())\n)\n\n# ============ VERSION POLARS (Lazy - recommandÃ©) ============\nresult_lazy = (\n    pl.scan_csv(\"data/benchmark.csv\")\n    .filter(pl.col(\"amount\") &gt; 100)\n    .with_columns(\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total\")\n    )\n    .group_by(\"category\")\n    .agg(pl.col(\"total\").sum())\n    .collect()\n)\n\nprint(\"RÃ©sultat :\")\nprint(result_lazy)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "title": "Polars pour Data Engineers",
    "section": "9. Bonnes pratiques & Erreurs frÃ©quentes",
    "text": "9. Bonnes pratiques & Erreurs frÃ©quentes\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nProblÃ¨me\nSolution\n\n\n\n\n.apply() sur chaque ligne\nExtrÃªmement lent\nUtiliser expressions natives\n\n\ndf[\"col\"] style Pandas\nNe fonctionne pas\ndf.select(\"col\") ou pl.col()\n\n\nread_csv() sur 100 fichiers\nLent, beaucoup de RAM\nscan_csv(\"*.csv\") + glob\n\n\nOublier .collect()\nPas dâ€™exÃ©cution\nToujours .collect() Ã  la fin\n\n\nMÃ©langer eager/lazy\nErreurs de type\nRester cohÃ©rent dans le pipeline\n\n\nPas dâ€™alias sur les expressions\nNoms de colonnes illisibles\nToujours .alias(\"nom\")\n\n\n\n\n\nVoir le code\n# âŒ MAUVAIS : apply() ligne par ligne\n# df.map_rows(lambda row: row[0] * 2)  # TRÃˆS LENT\n\n# âœ… BON : expression native\ndf = pl.DataFrame({\"x\": [1, 2, 3]})\nresult = df.with_columns((pl.col(\"x\") * 2).alias(\"x_doubled\"))\nprint(\"âœ… Expression native :\")\nprint(result)\n\n\n\n\nVoir le code\n# âŒ MAUVAIS : read_csv sur plusieurs fichiers sÃ©parÃ©ment\n# dfs = [pl.read_csv(f) for f in files]  # Pas optimisÃ©\n\n# âœ… BON : scan_csv avec glob\nlf = pl.scan_csv(\"data/multi/*.csv\")\nprint(\"âœ… Scan avec glob :\")\nprint(lf.collect())\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nUtiliser Lazy en production\nOptimisation automatique\n\n\nPrÃ©fÃ©rer Parquet\n10x plus rapide que CSV, compression\n\n\nChaÃ®ner les expressions\nPlus lisible, plus optimisÃ©\n\n\nÃ‰viter .apply()\nUtiliser expressions natives\n\n\nProfiler avec .explain()\nComprendre lâ€™exÃ©cution\n\n\nToujours .alias()\nNoms de colonnes explicites\n\n\nscan_* pour gros fichiers\nLazy = optimisations\n\n\nStreaming pour &gt; RAM\ncollect(streaming=True)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#quiz-de-fin-de-module",
    "title": "Polars pour Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quel est le principal avantage de lâ€™architecture columnar de Polars ?\n\nPlus facile Ã  lire pour les humains\n\nOpÃ©rations vectorisÃ©es plus rapides et meilleure utilisation du cache CPU\n\nCompatible avec Excel\n\nUtilise moins de colonnes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le stockage columnar permet des opÃ©rations vectorisÃ©es (SIMD) et une meilleure utilisation du cache CPU car les donnÃ©es dâ€™une colonne sont contiguÃ«s en mÃ©moire.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre pl.read_csv() et pl.scan_csv() ?\n\nread_csv est plus rapide\n\nscan_csv crÃ©e un LazyFrame et permet lâ€™optimisation\n\nscan_csv ne supporte pas les gros fichiers\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” scan_csv crÃ©e un LazyFrame (plan dâ€™exÃ©cution) qui sera optimisÃ© avant exÃ©cution, tandis que read_csv charge immÃ©diatement tout en mÃ©moire.\n\n\n\n\nâ“ Q3. Comment ajouter une nouvelle colonne en Polars ?\n\ndf[\"new\"] = df[\"old\"] * 2\n\ndf.with_columns((pl.col(\"old\") * 2).alias(\"new\"))\n\ndf.add_column(\"new\", df[\"old\"] * 2)\n\ndf.insert(\"new\", df[\"old\"] * 2)\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En Polars, on utilise with_columns() avec des expressions. La syntaxe df[\"col\"] style Pandas ne fonctionne pas.\n\n\n\n\nâ“ Q4. Que fait le Query Optimizer avec â€œpredicate pushdownâ€ ?\n\nSupprime les colonnes inutiles\n\nApplique les filtres le plus tÃ´t possible dans le pipeline\n\nParallÃ©lise les calculs\n\nCompresse les donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le predicate pushdown dÃ©place les filtres le plus tÃ´t possible, rÃ©duisant ainsi la quantitÃ© de donnÃ©es Ã  traiter dans les Ã©tapes suivantes.\n\n\n\n\nâ“ Q5. Quand utiliser .collect() ?\n\nAprÃ¨s chaque opÃ©ration\n\nÃ€ la fin du pipeline Lazy pour dÃ©clencher lâ€™exÃ©cution\n\nPour convertir en Pandas\n\nPour Ã©crire un fichier\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .collect() dÃ©clenche lâ€™exÃ©cution dâ€™un LazyFrame et retourne un DataFrame. Sans .collect(), rien nâ€™est calculÃ©.\n\n\n\n\nâ“ Q6. Pourquoi Ã©viter .apply() en Polars ?\n\nCe nâ€™est pas supportÃ©\n\nCâ€™est lent car Ã§a passe par Python pour chaque ligne\n\nÃ‡a modifie les donnÃ©es en place\n\nÃ‡a consomme trop de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .apply() (ou map_rows) passe par Python pour chaque ligne, perdant tous les avantages du moteur Rust vectorisÃ©. PrÃ©fÃ©rer les expressions natives.\n\n\n\n\nâ“ Q7. Quel format de fichier est recommandÃ© en production avec Polars ?\n\nCSV\n\nJSON\n\nParquet\n\nExcel\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Parquet est columnar (comme Polars), compressÃ©, et supporte les types. Il est 10x+ plus rapide que CSV.\n\n\n\n\nâ“ Q8. Comment voir le plan dâ€™exÃ©cution optimisÃ© dâ€™un LazyFrame ?\n\nlf.show_plan()\n\nlf.explain(optimized=True)\n\nlf.describe()\n\nprint(lf)\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” .explain(optimized=True) affiche le plan dâ€™exÃ©cution aprÃ¨s les optimisations du Query Optimizer.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#mini-projet-pipeline-etl-polars",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#mini-projet-pipeline-etl-polars",
    "title": "Polars pour Data Engineers",
    "section": "Mini-projet : Pipeline ETL Polars",
    "text": "Mini-projet : Pipeline ETL Polars\n\nObjectif\nConstruire un pipeline ETL complet en mode Lazy qui : - Lit plusieurs fichiers CSV - Nettoie et transforme les donnÃ©es - AgrÃ¨ge par catÃ©gorie et pÃ©riode - Exporte en Parquet\n\n\nArchitecture\ndata/raw/*.csv\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   scan_csv()    â”‚  Lazy read (glob pattern)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    filter()     â”‚  Nettoyage (nulls, invalides)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ with_columns()  â”‚  Enrichissement\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   group_by()    â”‚  AgrÃ©gation\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    collect()    â”‚  ExÃ©cution optimisÃ©e\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\ndata/processed/output.parquet\n\n\nStructure projet\npolars-etl-project/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”‚   â”œâ”€â”€ transactions_01.csv\nâ”‚   â”‚   â”œâ”€â”€ transactions_02.csv\nâ”‚   â”‚   â””â”€â”€ transactions_03.csv\nâ”‚   â””â”€â”€ processed/\nâ”‚       â””â”€â”€ output.parquet\nâ”œâ”€â”€ src/\nâ”‚   â””â”€â”€ etl_pipeline.py\nâ””â”€â”€ requirements.txt\n\n\nVoir le code\n# Setup : crÃ©er les donnÃ©es de test\nimport polars as pl\nimport random\nfrom datetime import datetime, timedelta\nimport os\n\nos.makedirs(\"data/raw\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\n\ncategories = [\"Electronics\", \"Clothing\", \"Food\", \"Books\", \"Sports\"]\nbase_date = datetime(2024, 1, 1)\n\n# GÃ©nÃ©rer 3 fichiers CSV\nfor file_num in range(1, 4):\n    n_rows = 10000\n    data = {\n        \"transaction_id\": range(file_num * 10000, file_num * 10000 + n_rows),\n        \"timestamp\": [base_date + timedelta(days=random.randint(0, 365)) for _ in range(n_rows)],\n        \"category\": [random.choice(categories) for _ in range(n_rows)],\n        \"amount\": [round(random.uniform(-50, 1000), 2) for _ in range(n_rows)],  # Certains nÃ©gatifs !\n        \"quantity\": [random.randint(0, 100) for _ in range(n_rows)],  # Certains Ã  0 !\n        \"customer_id\": [random.randint(1000, 9999) for _ in range(n_rows)]\n    }\n    df = pl.DataFrame(data)\n    df.write_csv(f\"data/raw/transactions_{file_num:02d}.csv\")\n\nprint(\"âœ… DonnÃ©es de test crÃ©Ã©es (3 fichiers x 10,000 lignes)\")\n\n\n\n\nVoir le code\nimport polars as pl\nimport time\n\nprint(\"ğŸš€ DÃ©marrage du pipeline ETL Polars...\\n\")\nstart = time.time()\n\n# ============ PIPELINE LAZY ============\nresult = (\n    # 1. EXTRACT : Lire tous les CSV avec glob pattern\n    pl.scan_csv(\"data/raw/*.csv\")\n    \n    # 2. CLEAN : Filtrer les donnÃ©es invalides\n    .filter(\n        (pl.col(\"amount\") &gt; 0) &           # Montants positifs\n        (pl.col(\"quantity\") &gt; 0) &         # QuantitÃ©s positives\n        (pl.col(\"customer_id\").is_not_null())  # Pas de null\n    )\n    \n    # 3. TRANSFORM : Enrichir les donnÃ©es\n    .with_columns(\n        # Calculer le total\n        (pl.col(\"amount\") * pl.col(\"quantity\")).alias(\"total_revenue\"),\n        \n        # Extraire annÃ©e et mois\n        pl.col(\"timestamp\").str.to_datetime().dt.year().alias(\"year\"),\n        pl.col(\"timestamp\").str.to_datetime().dt.month().alias(\"month\"),\n        \n        # CatÃ©goriser les montants\n        pl.when(pl.col(\"amount\") &gt; 500)\n          .then(pl.lit(\"High\"))\n          .when(pl.col(\"amount\") &gt; 100)\n          .then(pl.lit(\"Medium\"))\n          .otherwise(pl.lit(\"Low\"))\n          .alias(\"amount_tier\"),\n        \n        # Uppercase category\n        pl.col(\"category\").str.to_uppercase().alias(\"category_upper\")\n    )\n    \n    # 4. AGGREGATE : Par catÃ©gorie et mois\n    .group_by([\"year\", \"month\", \"category_upper\"])\n    .agg(\n        pl.col(\"total_revenue\").sum().alias(\"total_revenue\"),\n        pl.col(\"total_revenue\").mean().alias(\"avg_revenue\"),\n        pl.len().alias(\"transaction_count\"),\n        pl.col(\"customer_id\").n_unique().alias(\"unique_customers\"),\n        (pl.col(\"amount_tier\") == \"High\").sum().alias(\"high_value_count\")\n    )\n    \n    # 5. SORT\n    .sort([\"year\", \"month\", \"total_revenue\"], descending=[False, False, True])\n    \n    # 6. EXECUTE\n    .collect()\n)\n\nexecution_time = time.time() - start\nprint(f\"Pipeline exÃ©cutÃ© en {execution_time:.3f} secondes\")\nprint(f\"RÃ©sultat : {result.height} lignes, {result.width} colonnes\\n\")\n\n# Afficher un aperÃ§u\nprint(\"AperÃ§u des rÃ©sultats :\")\nprint(result.head(10))\n\n# 7. EXPORT en Parquet\nresult.write_parquet(\"data/processed/monthly_summary.parquet\")\nprint(\"\\n RÃ©sultat exportÃ© : data/processed/monthly_summary.parquet\")\n\n\n\n\nVoir le code\n# VÃ©rifier le fichier Parquet\nprint(\"Lecture du fichier Parquet exportÃ© :\")\ndf_check = pl.read_parquet(\"data/processed/monthly_summary.parquet\")\nprint(df_check.describe())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#ressources-pour-aller-plus-loin",
    "title": "Polars pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nPolars User Guide â€” Documentation complÃ¨te\nPolars API Reference â€” RÃ©fÃ©rence API\nPolars GitHub â€” Code source\n\n\n\nğŸ“– Tutoriels & Articles\n\nPolars vs Pandas Benchmark â€” Benchmarks officiels\nModern Polars â€” Guide approfondi\n\n\n\nğŸ”§ Outils complÃ©mentaires\n\nDuckDB â€” SQL analytique ultra-rapide (compatible Polars)\nPyArrow â€” Format Arrow sous-jacent",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/17_polars_for_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/17_polars_for_data_engineering.html#prochaine-Ã©tape",
    "title": "Polars pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Polars, dÃ©couvrons dâ€™autres outils haute performance pour Python !\nğŸ‘‰ Module suivant : 18_high_performance_python â€” Python Haute Performance\nTu vas apprendre : - Dask : parallÃ©lisation de Pandas/NumPy - Vaex : traitement out-of-core - multiprocessing : parallÃ©lisme CPU - concurrent.futures : ThreadPool et ProcessPool - async/await : I/O asynchrone\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Polars pour Data Engineers.\n\n\nVoir le code\n# Nettoyage des fichiers temporaires (optionnel)\nimport shutil\nimport os\n\n# DÃ©commenter pour nettoyer\n# if os.path.exists(\"data\"):\n#     shutil.rmtree(\"data\")\n#     print(\"ğŸ§¹ Dossier data/ supprimÃ©\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "17 Â· Polars pour Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  orchestrer des containers Ã  grande Ã©chelle avec Kubernetes. Tu dÃ©couvriras comment dÃ©ployer, gÃ©rer et monitorer des applications data dans un cluster K8s â€” des compÃ©tences essentielles pour industrialiser tes pipelines !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#prÃ©requis",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#prÃ©requis",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 14_docker_for_data_engineers\n\n\nâœ… Requis\nMaÃ®triser les bases de Docker (images, containers, volumes)\n\n\nâœ… Requis\nConnaissances de base en YAML\n\n\nğŸ’¡ RecommandÃ©\nDocker Desktop ou Minikube installÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#objectifs-du-module",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#objectifs-du-module",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture et les concepts clÃ©s de Kubernetes\nInstaller et configurer un cluster Kubernetes local\nDÃ©ployer et gÃ©rer des applications avec kubectl\nUtiliser les Jobs et CronJobs pour des tÃ¢ches Data Engineering\nConfigurer le stockage persistant (PVC)\nDÃ©bugger des pods et diagnostiquer les erreurs courantes\nDÃ©ployer et orchestrer un pipeline ETL complet dans Kubernetes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#cest-quoi-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#cest-quoi-kubernetes",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "Câ€™est quoi Kubernetes ?",
    "text": "Câ€™est quoi Kubernetes ?\n\nâ˜¸ï¸ Kubernetes (K8s) est une plateforme open-source dâ€™orchestration de containers qui automatise le dÃ©ploiement, la mise Ã  lâ€™Ã©chelle et la gestion dâ€™applications containerisÃ©es.\n\nEn pratique, Kubernetes te permet de : - DÃ©ployer des containers sur plusieurs machines (un cluster) - Scaler automatiquement selon la charge - RedÃ©marrer les containers qui plantent - GÃ©rer la configuration et les secrets - Exposer tes applications au rÃ©seau\n\nKubernetes â‰  Docker (mais complÃ©mentaires)\n\n\n\n\n\n\n\n\nAspect\nDocker\nKubernetes\n\n\n\n\nRÃ´le\nCrÃ©er et exÃ©cuter des containers\nOrchestrer des containers\n\n\nÃ‰chelle\n1 machine\nCluster de machines\n\n\nFocus\nBuild & Run\nDeploy, Scale, Manage\n\n\nAnalogie\nLe musicien\nLe chef dâ€™orchestre\n\n\n\n\nğŸ’¡ Docker crÃ©e les containers, Kubernetes les orchestre Ã  grande Ã©chelle.\n\n\n\nAnalogies pour bien comprendre\n\n\n\n\n\n\n\nAnalogie\nExplication\n\n\n\n\nChef dâ€™orchestre\nK8s coordonne tous les containers (musiciens) pour quâ€™ils jouent en harmonie\n\n\nAgence immobiliÃ¨re\nK8s place tes containers (locataires) dans les nodes (appartements) disponibles\n\n\nPilote automatique\nTu dÃ©finis la destination (Ã©tat souhaitÃ©), K8s sâ€™occupe dâ€™y arriver et dâ€™y rester\n\n\nğŸ­ Usine automatisÃ©e\nTu donnes les plans, K8s fabrique, surveille et remplace les piÃ¨ces dÃ©fectueuses\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nLe nom Kubernetes vient du grec ÎºÏ…Î²ÎµÏÎ½Î®Ï„Î·Ï‚ (kuberná¸—tÄ“s) qui signifie â€œpiloteâ€ ou â€œgouverneurâ€ â€” celui qui tient la barre dâ€™un navire.\nK8s est nÃ© chez Google, inspirÃ© de leur systÃ¨me interne Borg qui gÃ¨re des milliards de containers depuis 2003. Google a ouvert le projet en 2014 et lâ€™a donnÃ© Ã  la Cloud Native Computing Foundation (CNCF).\nLe â€œ8â€ dans K8s reprÃ©sente les 8 lettres entre le K et le S de Kubernetes !\nğŸ“– Histoire de Kubernetes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#pourquoi-kubernetes-pour-un-data-engineer",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#pourquoi-kubernetes-pour-un-data-engineer",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "1. Pourquoi Kubernetes pour un Data Engineer ?",
    "text": "1. Pourquoi Kubernetes pour un Data Engineer ?\nEn Data Engineering moderne, tu dois souvent :\n\nExÃ©cuter des jobs ETL de maniÃ¨re fiable et planifiÃ©e\nDÃ©ployer des bases de donnÃ©es et des brokers (Kafka)\nFaire tourner des jobs Spark distribuÃ©s\nOrchestrer avec Airflow ou Prefect\nScaler selon le volume de donnÃ©es\n\n\nâŒ Sans Kubernetes\n\n\n\nProblÃ¨me\nConsÃ©quence\n\n\n\n\nDÃ©ploiement manuel sur chaque serveur\nLent et source dâ€™erreurs\n\n\nPas de redÃ©marrage automatique\nJobs perdus en cas de crash\n\n\nScaling manuel\nSous/sur-utilisation des ressources\n\n\nConfiguration dispersÃ©e\nDifficile Ã  maintenir\n\n\n\n\n\nâœ… Avec Kubernetes\n\n\n\nAvantage\nExemple concret\n\n\n\n\nDÃ©ploiement dÃ©claratif\nkubectl apply -f etl-job.yaml\n\n\nAuto-healing\nPod crashÃ© = recrÃ©Ã© automatiquement\n\n\nCronJobs natifs\nETL planifiÃ© sans cron externe\n\n\nSecrets management\nCredentials gÃ©rÃ©s proprement\n\n\nPortabilitÃ©\nMÃªme config en dev, staging, prod\n\n\n\n\n\nVue dâ€™ensemble : Cluster K8s\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   CLUSTER K8s                       â”‚\nâ”‚                                                     â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”‚   NODE 1    â”‚   â”‚   NODE 2    â”‚                â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                â”‚\nâ”‚   â”‚  â”‚ Pod A â”‚  â”‚   â”‚  â”‚ Pod C â”‚  â”‚                â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                â”‚\nâ”‚   â”‚  â”‚ Pod B â”‚  â”‚   â”‚  â”‚ Pod D â”‚  â”‚                â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#architecture-kubernetes-simplifiÃ©e",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#architecture-kubernetes-simplifiÃ©e",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "2. Architecture Kubernetes (simplifiÃ©e)",
    "text": "2. Architecture Kubernetes (simplifiÃ©e)\nUn cluster Kubernetes est composÃ© de deux types de machines :\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nAnalogie\n\n\n\n\nControl Plane (Master)\nCerveau du cluster, prend les dÃ©cisions\nLe management\n\n\nWorker Nodes\nExÃ©cutent les containers\nLes ouvriers\n\n\n\n\nControl Plane (les composants essentiels)\n\n\n\nComposant\nRÃ´le\n\n\n\n\nAPI Server\nPoint dâ€™entrÃ©e unique (kubectl â†’ API)\n\n\nScheduler\nDÃ©cide sur quel node placer les pods\n\n\n\n\nğŸ’¡ Pour ce module â€œFundamentalsâ€, on se concentre sur ces 2 composants. Les autres (etcd, Controller Manager) seront vus en niveau avancÃ©.\n\n\n\nWorker Nodes\n\n\n\nComposant\nRÃ´le\n\n\n\n\nKubelet\nAgent qui gÃ¨re les pods sur le node\n\n\nContainer Runtime\nDocker/containerd qui exÃ©cute les containers\n\n\n\n\n\nSchÃ©ma Architecture\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚           CONTROL PLANE                 â”‚\n                         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n       kubectl â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚   API Server   â”‚  â”‚   Scheduler   â”‚ â”‚\n                         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                    â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚                     â”‚                     â”‚\n              â–¼                     â–¼                     â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     WORKER NODE 1   â”‚ â”‚     WORKER NODE 2   â”‚ â”‚     WORKER NODE 3   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚    Kubelet    â”‚  â”‚ â”‚  â”‚    Kubelet    â”‚  â”‚ â”‚  â”‚    Kubelet    â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”   â”‚ â”‚  â”Œâ”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚ Pod â”‚  â”‚ Pod â”‚   â”‚ â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚   â”‚ â”‚  â”‚ Pod â”‚           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚ â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜   â”‚ â”‚  â””â”€â”€â”€â”€â”€â”˜           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#concepts-clÃ©s-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#concepts-clÃ©s-kubernetes",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "3. Concepts clÃ©s Kubernetes",
    "text": "3. Concepts clÃ©s Kubernetes\nVoici les concepts fondamentaux Ã  maÃ®triser :\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nPod\nPlus petite unitÃ© dÃ©ployable (1+ containers)\nUn appartement\n\n\nDeployment\nGÃ¨re les replicas de pods, rolling updates\nUn syndic dâ€™immeuble\n\n\nService\nExpose les pods sur le rÃ©seau\nLâ€™adresse postale\n\n\nNamespace\nIsolation logique des ressources\nUn quartier de la ville\n\n\nConfigMap\nConfiguration externe (non sensible)\nFichier .env public\n\n\nSecret\nDonnÃ©es sensibles (base64)\nUn coffre-fort\n\n\nPersistentVolumeClaim\nDemande de stockage persistant\nLocation dâ€™un disque dur\n\n\nJob\nTÃ¢che one-shot (sâ€™exÃ©cute puis se termine)\nUne mission ponctuelle\n\n\nCronJob\nJob planifiÃ© (comme cron Linux)\nUn rÃ©veil programmÃ©\n\n\n\n\nPod\nLe Pod est lâ€™unitÃ© de base dans Kubernetes : - Contient 1 ou plusieurs containers qui partagent le rÃ©seau et le stockage - A sa propre adresse IP dans le cluster - Est Ã©phÃ©mÃ¨re : peut Ãªtre dÃ©truit et recrÃ©Ã© Ã  tout moment\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mon-pod\nspec:\n  containers:\n  - name: mon-container\n    image: python:3.11-slim\n\n\nDeployment\nLe Deployment gÃ¨re un ensemble de pods identiques : - Garantit le nombre de replicas souhaitÃ© - GÃ¨re les rolling updates (mise Ã  jour sans downtime) - RecrÃ©e les pods qui crashent\n\n\nService\nLe Service expose les pods sur le rÃ©seau :\n\n\n\n\n\n\n\n\nType\nDescription\nUsage\n\n\n\n\nClusterIP\nIP interne au cluster\nCommunication entre pods\n\n\nNodePort\nPort ouvert sur chaque node\nTests, dÃ©veloppement\n\n\nLoadBalancer\nLoad balancer cloud\nProduction (AWS, GCP, Azure)\n\n\n\n\n\nSecret vs ConfigMap\n\n\n\nAspect\nConfigMap\nSecret\n\n\n\n\nUsage\nConfig non sensible\nPasswords, tokens, clÃ©s\n\n\nEncodage\nTexte clair\nBase64\n\n\nExemple\nURLs, feature flags\nDB_PASSWORD, API_KEY\n\n\n\nâš ï¸ Attention : Base64 nâ€™est PAS du chiffrement !\n\nBase64 est juste un encodage, pas une protection. Nâ€™importe qui avec accÃ¨s au cluster peut dÃ©coder les secrets. En production, utilise Sealed Secrets, HashiCorp Vault, ou les secrets managers cloud (AWS Secrets Manager, GCP Secret Manager).\n\n\n\nSchÃ©ma : Relations entre concepts\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       User â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Service    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                           â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Deployment  â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â–¼            â–¼            â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Pod 1 â”‚    â”‚ Pod 2 â”‚    â”‚ Pod 3 â”‚\n         â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n             â”‚            â”‚            â”‚\n             â–¼            â–¼            â–¼\n        Container    Container    Container",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#installation-de-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#installation-de-kubernetes",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "4. Installation de Kubernetes",
    "text": "4. Installation de Kubernetes\nPour ce module, tu as besoin dâ€™un cluster Kubernetes local.\n\n\n\nOption\nDifficultÃ©\nRecommandÃ© pour\n\n\n\n\nDocker Desktop\nâ­ Facile\nMac/Windows, dÃ©butants\n\n\nMinikube\nâ­â­ Moyen\nTous OS, plus de contrÃ´le\n\n\nk3d/k3s\nâ­â­ Moyen\nLÃ©ger, CI/CD\n\n\n\n\nOption 1 : Docker Desktop (recommandÃ©)\n\nOuvrir Docker Desktop\nAller dans Settings â†’ Kubernetes\nCocher Enable Kubernetes\nCliquer Apply & Restart\nAttendre que le status passe au vert\n\n\n\nOption 2 : Minikube\n# Installation (macOS avec Homebrew)\nbrew install minikube\n\n# Installation (Linux)\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n\n# DÃ©marrer le cluster\nminikube start\n\n# VÃ©rifier\nminikube status\n\n\nInstaller kubectl\nkubectl est lâ€™outil CLI pour interagir avec Kubernetes.\n# macOS\nbrew install kubectl\n\n# Linux\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install kubectl /usr/local/bin/kubectl\n\n# Windows (avec Docker Desktop, kubectl est inclus)\n\n\nVÃ©rification\n# Version de kubectl\nkubectl version --client\n\n# Liste des nodes du cluster\nkubectl get nodes\n\n# VÃ©rifier le contexte actif\nkubectl config get-contexts\n\n\nVoir le code\n%%bash\n# VÃ©rifier l'installation de kubectl\necho \"=== Version kubectl ===\"\nkubectl version --client\n\necho \"\"\necho \"=== Nodes du cluster ===\"\nkubectl get nodes\n\necho \"\"\necho \"=== Contexte actif ===\"\nkubectl config current-context",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#maÃ®triser-yaml-pour-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#maÃ®triser-yaml-pour-kubernetes",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "5. MaÃ®triser YAML pour Kubernetes",
    "text": "5. MaÃ®triser YAML pour Kubernetes\nMaintenant que Kubernetes est installÃ©, apprenons Ã  Ã©crire les fichiers de configuration YAML.\n\n5.1 Câ€™est quoi YAML et pourquoi Kubernetes lâ€™utilise ?\n\nYAML = â€œYAML Ainâ€™t Markup Languageâ€\n\nYAML est un format de fichier pour Ã©crire de la configuration de maniÃ¨re lisible par un humain.\n\nPourquoi YAML plutÃ´t que JSON ou XML ?\n\n\n\nFormat\nLisibilitÃ©\nExemple\n\n\n\n\nXML\nâŒ Verbeux\n&lt;name&gt;postgres&lt;/name&gt;\n\n\nJSON\nâš ï¸ Moyen\n{\"name\": \"postgres\"}\n\n\nYAML\nâœ… Simple\nname: postgres\n\n\n\nKubernetes a choisi YAML parce que : - Facile Ã  lire : pas de {} ni de \"\" partout - Facile Ã  Ã©crire : structure claire avec lâ€™indentation - Commentaires possibles : # ceci est un commentaire - Multi-documents : plusieurs ressources dans un seul fichier avec ---\n\n\nLe principe fondamental de Kubernetes\nKubernetes fonctionne sur le principe dÃ©claratif :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   TOI (humain)          â†’        KUBERNETES (robot)         â”‚\nâ”‚                                                             â”‚\nâ”‚   \"Je VEUX 3 pods       â†’    \"OK, je CRÃ‰E 3 pods et        â”‚\nâ”‚    avec nginx\"                 je m'assure qu'il y en       â”‚\nâ”‚                                a TOUJOURS 3\"                â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nTu dÃ©cris lâ€™Ã©tat souhaitÃ© dans un fichier YAML, Kubernetes sâ€™occupe dâ€™y arriver.\n\n\n\n\n5.2 Les rÃ¨gles dâ€™or du YAML\n\n5 rÃ¨gles Ã  ne JAMAIS oublier\n\n\n\n\n\n\n\n\n\n#\nRÃ¨gle\nâœ… Correct\nâŒ Incorrect\n\n\n\n\n1\nIndentation = 2 espaces\nname: test\nâ‡¥name: test (tab)\n\n\n2\nEspace aprÃ¨s les :\nname: postgres\nname:postgres\n\n\n3\nSensible Ã  la casse\napiVersion\nApiVersion\n\n\n4\nTiret - pour les listes\n- item\nitem\n\n\n5\nPas de tabs, jamais !\nespaces uniquement\ntabs = erreur\n\n\n\n\n\nLes types de donnÃ©es\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# STRINGS (texte)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nnom: postgres                    # Sans guillemets (recommandÃ©)\nnom: \"postgres\"                  # Avec guillemets (si caractÃ¨res spÃ©ciaux)\nmessage: \"Bonjour: monde\"        # Guillemets obligatoires si : dans la valeur\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# NOMBRES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nport: 5432                       # Entier\nprix: 19.99                      # DÃ©cimal\nreplicas: 3                      # Entier\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# BOOLÃ‰ENS (vrai/faux)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nactif: true                      # ou false\ndebug: yes                       # ou no (Ã©quivalent)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# NULL (vide)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nvaleur: null                     # ou ~\n\n\nLes structures de donnÃ©es\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DICTIONNAIRE = ensemble de clÃ©: valeur\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\npersonne:                        # &lt;- clÃ© parente\n  nom: Alice                     # &lt;- 2 espaces = enfant de \"personne\"\n  age: 30                        # &lt;- mÃªme niveau que \"nom\"\n  ville: Paris                   # &lt;- mÃªme niveau\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# LISTE = ensemble d'Ã©lÃ©ments (avec tiret -)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nfruits:                          # &lt;- clÃ© parente\n  - pomme                        # &lt;- premier Ã©lÃ©ment (tiret + espace)\n  - banane                       # &lt;- deuxiÃ¨me Ã©lÃ©ment\n  - orange                       # &lt;- troisiÃ¨me Ã©lÃ©ment\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# LISTE DE DICTIONNAIRES (trÃ¨s courant en K8s !)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nemployes:\n  - nom: Alice                   # &lt;- premier employÃ©\n    poste: Dev                   #    propriÃ©tÃ© du premier employÃ©\n    age: 30                      #    propriÃ©tÃ© du premier employÃ©\n  - nom: Bob                     # &lt;- deuxiÃ¨me employÃ© (nouveau tiret)\n    poste: DevOps                #    propriÃ©tÃ© du deuxiÃ¨me employÃ©\n    age: 25\n\nğŸ’¡ Astuce K8s : containers: est TOUJOURS une liste (mÃªme avec 1 seul container), donc toujours avec -\n\n\n\n\n\n5.3 Pourquoi plusieurs types de ressources ?\nAvant dâ€™Ã©crire du YAML, il faut comprendre POURQUOI Kubernetes a diffÃ©rentes ressources.\n\nLe problÃ¨me Ã  rÃ©soudre\nImaginons que tu veux dÃ©ployer une API Python :\n\n\n\nBesoin\nQuestion\nRessource K8s\n\n\n\n\nIsoler mes ressources\nâ€œComment sÃ©parer dev/prod ?â€\nNamespace\n\n\nExÃ©cuter mon code\nâ€œOÃ¹ tourne mon container ?â€\nPod\n\n\nAvoir plusieurs instances\nâ€œEt si un pod crashe ?â€\nDeployment\n\n\nAccÃ©der depuis le rÃ©seau\nâ€œComment appeler mon API ?â€\nService\n\n\nStocker la config\nâ€œOÃ¹ mettre mes variables ?â€\nConfigMap\n\n\nStocker les secrets\nâ€œOÃ¹ mettre mes passwords ?â€\nSecret\n\n\nPlanifier un job\nâ€œComment lancer mon ETL Ã  2h ?â€\nCronJob\n\n\n\n\n\nSchÃ©ma : Qui fait quoi ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        POURQUOI CHAQUE RESSOURCE ?                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚   ğŸ‘¤ Utilisateur                                                        â”‚\nâ”‚        â”‚                                                                â”‚\nâ”‚        â”‚ \"Je veux accÃ©der Ã  mon-api sur le port 80\"                    â”‚\nâ”‚        â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚    SERVICE      â”‚  â† Donne une adresse stable (DNS + IP)           â”‚\nâ”‚   â”‚   \"mon-api\"     â”‚  â† Load balance entre les pods                   â”‚\nâ”‚   â”‚    port: 80     â”‚                                                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚            â”‚                                                            â”‚\nâ”‚            â”‚ \"Envoie le trafic aux pods avec label app=mon-api\"        â”‚\nâ”‚            â–¼                                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚   DEPLOYMENT    â”‚  â† Garantit 3 replicas                           â”‚\nâ”‚   â”‚   replicas: 3   â”‚  â† RecrÃ©e les pods qui crashent                  â”‚\nâ”‚   â”‚                 â”‚  â† GÃ¨re les mises Ã  jour (rolling update)        â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚            â”‚                                                            â”‚\nâ”‚            â”‚ \"CrÃ©e et maintient 3 pods identiques\"                     â”‚\nâ”‚            â–¼                                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚\nâ”‚   â”‚  POD 1  â”‚ â”‚  POD 2  â”‚ â”‚  POD 3  â”‚  â† Chaque pod a son IP          â”‚\nâ”‚   â”‚ (nginx) â”‚ â”‚ (nginx) â”‚ â”‚ (nginx) â”‚  â† Chaque pod peut mourir       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPourquoi un Namespace ?\nUn Namespace est un espace isolÃ© dans le cluster, comme des dossiers sur ton ordinateur.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            CLUSTER K8S                                  â”‚\nâ”‚                                                                         â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\nâ”‚   â”‚  NAMESPACE: dev     â”‚    â”‚  NAMESPACE: prod    â”‚                   â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚                   â”‚\nâ”‚   â”‚  â”‚mon-apiâ”‚ â”‚  db   â”‚â”‚    â”‚  â”‚mon-apiâ”‚ â”‚  db   â”‚â”‚                   â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\nâ”‚                                                                         â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚\nâ”‚   â”‚ NAMESPACE: staging  â”‚    â”‚ NAMESPACE: data-eng â”‚                   â”‚\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚                   â”‚\nâ”‚   â”‚  â”‚mon-apiâ”‚          â”‚    â”‚  â”‚airflowâ”‚ â”‚ spark â”‚â”‚                   â”‚\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nPourquoi utiliser des Namespaces ?\n\n\n\n\n\n\n\nAvantage\nExemple\n\n\n\n\nIsolation\nLes ressources de dev ne voient pas celles de prod\n\n\nOrganisation\nSÃ©parer par Ã©quipe, par environnement, par projet\n\n\nQuotas\nLimiter les ressources CPU/RAM par namespace\n\n\nPermissions\nDonner accÃ¨s Ã  dev mais pas Ã  prod\n\n\n\nNamespaces par dÃ©faut :\n\n\n\nNamespace\nUsage\n\n\n\n\ndefault\nOÃ¹ vont les ressources si tu ne prÃ©cises pas\n\n\nkube-system\nComposants internes de K8s (ne pas toucher !)\n\n\nkube-public\nRessources accessibles Ã  tous\n\n\n\n# Lister les namespaces\nkubectl get namespaces\n\n# CrÃ©er un namespace\nkubectl create namespace mon-projet\n\n# Travailler dans un namespace spÃ©cifique\nkubectl get pods -n mon-projet\nkubectl apply -f manifest.yaml -n mon-projet\n\n# Changer le namespace par dÃ©faut\nkubectl config set-context --current --namespace=mon-projet\n\n\nPourquoi un Service EST indispensable ?\nSans Service :\nPod 1 : IP = 10.1.0.15  â† Cette IP change Ã  chaque recrÃ©ation du pod !\nPod 2 : IP = 10.1.0.23  â† Comment savoir quelle IP appeler ?\nPod 3 : IP = 10.1.0.41  â† Impossible de hardcoder ces IPs\nAvec Service :\nService \"mon-api\" : IP stable = 10.0.0.100 (ne change jamais)\n                    DNS = mon-api.default.svc.cluster.local\n                    â†’ Redirige automatiquement vers les pods vivants\n\n\nPourquoi un Deployment et pas juste des Pods ?\nSans Deployment (pods manuels) :\n- Pod crashe â†’ Personne ne le recrÃ©e â†’ Service DOWN \n- Mise Ã  jour â†’ Supprimer/recrÃ©er manuellement â†’ Downtime \n- Scaling â†’ CrÃ©er chaque pod Ã  la main â†’ Lent \nAvec Deployment :\n- Pod crashe â†’ Deployment le recrÃ©e automatiquement â†’ âœ…\n- Mise Ã  jour â†’ Rolling update sans downtime â†’ âœ…\n- Scaling â†’ kubectl scale deployment mon-api --replicas=10 â†’ âœ…\n\n\n\n\n5.4 Structure dâ€™un manifest YAML\n\nLa structure de base (4 champs obligatoires)\nTout manifest Kubernetes a cette structure :\napiVersion: ???     # 1. Quelle version de l'API ?\nkind: ???           # 2. Quel type de ressource ?\nmetadata:           # 3. Comment s'appelle-t-elle ?\n  name: ???\nspec:               # 4. Qu'est-ce que tu veux ?\n  ???\n\n\nTrouver apiVersion et kind\n\n\n\nJe veux crÃ©erâ€¦\nkind\napiVersion\n\n\n\n\nUn espace isolÃ©\nNamespace\nv1\n\n\nUn container simple\nPod\nv1\n\n\nPlusieurs replicas dâ€™un pod\nDeployment\napps/v1\n\n\nUn point dâ€™accÃ¨s rÃ©seau\nService\nv1\n\n\nDe la configuration\nConfigMap\nv1\n\n\nDes donnÃ©es secrÃ¨tes\nSecret\nv1\n\n\nUn job one-shot\nJob\nbatch/v1\n\n\nUn job planifiÃ©\nCronJob\nbatch/v1\n\n\n\n\nğŸ’¡ Astuce : kubectl api-resources te donne la liste complÃ¨te !\n\n\n\n\n\n5.5 Ã‰crire chaque ressource (ligne par ligne)\n\nNamespace (crÃ©er un espace isolÃ©)\n# namespace.yaml\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\napiVersion: v1                    # API de base\nkind: Namespace                   # Type = Namespace\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmetadata:\n  name: data-pipeline             # Nom du namespace\n  labels:                         # Labels optionnels\n    team: data-engineering\n    env: production\nkubectl apply -f namespace.yaml\nkubectl get namespaces\n\n\nPod (le plus simple)\n# pod.yaml\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\napiVersion: v1                    # Pods utilisent l'API v1\nkind: Pod                         # Je veux crÃ©er un Pod\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmetadata:\n  name: mon-premier-pod           # Nom du pod (unique dans le namespace)\n  namespace: data-pipeline        # Dans quel namespace ?\n  labels:                         # Labels = tags pour organiser\n    app: demo                     # Label arbitraire (clÃ©: valeur)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nspec:\n  containers:                     # Liste des containers (avec tiret !)\n  - name: nginx                   # Nom du container\n    image: nginx:alpine           # Image Docker Ã  utiliser\n    ports:\n    - containerPort: 80           # Port exposÃ© par le container\n\n\nDeployment (le standard en production)\n# deployment.yaml\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\napiVersion: apps/v1               # Deployments utilisent apps/v1\nkind: Deployment                  # Type = Deployment\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmetadata:\n  name: mon-api                   # Nom du deployment\n  namespace: data-pipeline        # Dans quel namespace ?\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nspec:\n  replicas: 3                     # Je veux 3 pods identiques\n  \n  selector:                       # Comment identifier MES pods ?\n    matchLabels:\n      app: mon-api                # Cherche les pods avec ce label\n  \n  template:                       # ModÃ¨le pour crÃ©er les pods\n    metadata:\n      labels:\n        app: mon-api              # âš ï¸ DOIT correspondre au selector !\n    spec:\n      containers:\n      - name: api\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n        resources:                # Limites de ressources\n          requests:               # Minimum garanti\n            memory: \"64Mi\"\n            cpu: \"50m\"\n          limits:                 # Maximum autorisÃ©\n            memory: \"128Mi\"\n            cpu: \"100m\"\nâš ï¸ La rÃ¨gle CRITIQUE du selector :\nspec.selector.matchLabels.app  â•â•â•â•â•â•â•â•—\n                                      â•‘ DOIVENT ÃŠTRE IDENTIQUES !\nspec.template.metadata.labels.app â•â•â•â•â•\n\nSi diffÃ©rents â†’ Erreur : \"selector does not match template labels\"\n\n\nService (exposer sur le rÃ©seau)\n# service.yaml\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\napiVersion: v1                    # Services utilisent l'API v1\nkind: Service                     # Type = Service\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmetadata:\n  name: mon-api-svc               # Nom = nom DNS dans le cluster\n  namespace: data-pipeline\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nspec:\n  selector:\n    app: mon-api                  # Cible les pods avec ce label\n  \n  type: ClusterIP                 # ClusterIP | NodePort | LoadBalancer\n  \n  ports:\n  - port: 80                      # Port du SERVICE (ce qu'on appelle)\n    targetPort: 80                # Port du CONTAINER (oÃ¹ Ã§a arrive)\nMapping des ports :\nClient â†’ Service:80 â†’ Pod:80 â†’ Container\n\ncurl http://mon-api-svc:80  â†’  redirigÃ© vers  â†’  container:80\n\n\nConfigMap (configuration non sensible)\n# configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: data-pipeline\ndata:                            # ClÃ©: valeur (toujours strings)\n  LOG_LEVEL: \"info\"\n  DB_HOST: \"postgres.data-pipeline.svc.cluster.local\"\n  DB_PORT: \"5432\"\n\n\nSecret (donnÃ©es sensibles)\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: db-credentials\n  namespace: data-pipeline\ntype: Opaque\nstringData:                      # K8s encode automatiquement en base64\n  DB_PASSWORD: \"SuperSecret123\"\n  API_KEY: \"sk-xxxx-yyyy-zzzz\"\n\nâš ï¸ Rappel : Base64 nâ€™est PAS du chiffrement ! Câ€™est juste un encodage.\n\n\n\n\n\n5.6 Exemple complet : DÃ©ployer une app de A Ã  Z\nVoici comment dÃ©ployer une application complÃ¨te :\n# 1. CrÃ©er le namespace\nkubectl create namespace demo-app\n\n# 2. Appliquer tous les manifests\nkubectl apply -f namespace.yaml\nkubectl apply -f configmap.yaml\nkubectl apply -f secret.yaml\nkubectl apply -f deployment.yaml\nkubectl apply -f service.yaml\n\n# OU tout d'un coup (si dans un dossier)\nkubectl apply -f ./manifests/\n\n# 3. VÃ©rifier\nkubectl get all -n demo-app\n\n# 4. Tester l'accÃ¨s\nkubectl port-forward svc/mon-api-svc 8080:80 -n demo-app\n# Ouvrir http://localhost:8080\n\n\n\n5.7 Outils pour tâ€™aider\n\nGÃ©nÃ©rer un template automatiquement\n# Ne jamais Ã©crire from scratch ! GÃ©nÃ¨re un template :\nkubectl create namespace mon-ns --dry-run=client -o yaml &gt; namespace.yaml\nkubectl create deployment nginx --image=nginx --dry-run=client -o yaml &gt; deployment.yaml\nkubectl create service clusterip nginx --tcp=80:80 --dry-run=client -o yaml &gt; service.yaml\n\n\nValider avant dâ€™appliquer\n# VÃ©rifier la syntaxe sans crÃ©er\nkubectl apply -f manifest.yaml --dry-run=client\n\n# Valider cÃ´tÃ© serveur (plus strict)\nkubectl apply -f manifest.yaml --dry-run=server\n\n\nExplorer avec kubectl explain\nkubectl explain pod\nkubectl explain pod.spec.containers\nkubectl explain deployment.spec.template.spec\n\n\n\n\n5.8 Checklist avant kubectl apply\nCHECKLIST YAML KUBERNETES\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n[ ] Indentation = 2 ESPACES (pas de tabs !)\n[ ] Espace aprÃ¨s chaque \":\"\n[ ] apiVersion correct pour le kind\n[ ] metadata.name prÃ©sent et unique\n[ ] namespace spÃ©cifiÃ© (si pas default)\n[ ] Pour Deployment : selector.matchLabels = template.labels\n[ ] containers: avec un tiret (c'est une liste !)\n[ ] ValidÃ© avec --dry-run=client\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#commandes-kubectl-essentielles",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#commandes-kubectl-essentielles",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "6. Commandes kubectl essentielles",
    "text": "6. Commandes kubectl essentielles\nVoici les commandes que tu utiliseras au quotidien :\n\nCRUD sur les ressources\n\n\n\nAction\nCommande\n\n\n\n\nCrÃ©er/Mettre Ã  jour\nkubectl apply -f manifest.yaml\n\n\nLire\nkubectl get pods\n\n\nDÃ©tails\nkubectl describe pod &lt;name&gt;\n\n\nSupprimer\nkubectl delete -f manifest.yaml\n\n\n\n\n\nLister les ressources\nkubectl get pods                    # Lister les pods\nkubectl get pods -o wide            # Avec plus de dÃ©tails (node, IP)\nkubectl get deployments             # Lister les deployments\nkubectl get services                # Lister les services\nkubectl get all                     # Tout lister\nkubectl get all -n &lt;namespace&gt;      # Dans un namespace spÃ©cifique\n\n\nDebug\nkubectl logs &lt;pod&gt;                  # Voir les logs\nkubectl logs -f &lt;pod&gt;               # Suivre les logs en temps rÃ©el\nkubectl logs -p &lt;pod&gt;               # Logs du container prÃ©cÃ©dent (aprÃ¨s crash)\nkubectl describe pod &lt;pod&gt;          # DÃ©tails + Events\nkubectl exec -it &lt;pod&gt; -- bash      # Shell dans le pod\nkubectl get events                  # Tous les events du namespace\n\n\nRÃ©seau\nkubectl port-forward &lt;pod&gt; 8080:80           # Forward un port\nkubectl port-forward svc/&lt;service&gt; 8080:80   # Forward depuis un service\n\n\nNamespaces\nkubectl get namespaces              # Lister les namespaces\nkubectl create namespace dev        # CrÃ©er un namespace\nkubectl config set-context --current --namespace=dev  # Changer de namespace par dÃ©faut\n\n\nCheat Sheet visuel\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    KUBECTL CHEAT SHEET                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  CREATE/UPDATE â”‚  kubectl apply -f manifest.yaml               â”‚\nâ”‚  READ          â”‚  kubectl get pods/deploy/svc/all              â”‚\nâ”‚  DESCRIBE      â”‚  kubectl describe &lt;resource&gt; &lt;name&gt;           â”‚\nâ”‚  DELETE        â”‚  kubectl delete -f manifest.yaml              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  LOGS          â”‚  kubectl logs -f &lt;pod&gt;                        â”‚\nâ”‚  SHELL         â”‚  kubectl exec -it &lt;pod&gt; -- bash               â”‚\nâ”‚  PORT-FORWARD  â”‚  kubectl port-forward &lt;pod&gt; 8080:80           â”‚\nâ”‚  EVENTS        â”‚  kubectl get events --sort-by='.lastTimestamp'â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n%%bash\n# Exemples de commandes kubectl\n\necho \"=== Namespaces ===\"\nkubectl get namespaces\n\necho \"\"\necho \"=== Pods dans tous les namespaces ===\"\nkubectl get pods --all-namespaces\n\necho \"\"\necho \"=== Services ===\"\nkubectl get services",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#premier-dÃ©ploiement-hello-world",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#premier-dÃ©ploiement-hello-world",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "7. Premier dÃ©ploiement : Hello World",
    "text": "7. Premier dÃ©ploiement : Hello World\nDÃ©ployons une application simple pour comprendre le cycle de dÃ©ploiement.\n\nManifest YAML : Deployment + Service\n# hello-world.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-world\n  labels:\n    app: hello-world\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello-world\n  template:\n    metadata:\n      labels:\n        app: hello-world\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hello-world-svc\nspec:\n  selector:\n    app: hello-world\n  ports:\n  - port: 80\n    targetPort: 80\n  type: NodePort\n\n\nDÃ©ployer\n# Appliquer le manifest\nkubectl apply -f hello-world.yaml\n\n# VÃ©rifier le dÃ©ploiement\nkubectl get deployments\nkubectl get pods\nkubectl get services\n\n\nAccÃ©der Ã  lâ€™application\nMÃ©thode 1 : Port-forward (recommandÃ©)\nkubectl port-forward svc/hello-world-svc 8080:80\n# Ouvrir http://localhost:8080\nMÃ©thode 2 : NodePort\n# Trouver le NodePort\nkubectl get svc hello-world-svc\n# AccÃ©der via http://localhost:&lt;NodePort&gt;\n\nğŸ’¡ Astuce entreprise : Le NodePort nâ€™est pas toujours autorisÃ© en entreprise (firewall). port-forward fonctionne partout !\n\n\n\nCycle de dÃ©ploiement\nkubectl apply     Deployment crÃ©Ã©     ReplicaSet crÃ©Ã©     Pods crÃ©Ã©s\n     â”‚                  â”‚                   â”‚                  â”‚\n     â–¼                  â–¼                   â–¼                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  YAML   â”‚ â”€â”€â”€â–¶  â”‚Deployment â”‚ â”€â”€â”€â–¶  â”‚ReplicaSet â”‚ â”€â”€â”€â–¶ â”‚  Pods   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                               â”‚\n                    Service expose les pods â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nNettoyage\nkubectl delete -f hello-world.yaml",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#kubernetes-pour-data-engineering",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#kubernetes-pour-data-engineering",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "8. Kubernetes pour Data Engineering",
    "text": "8. Kubernetes pour Data Engineering\nVoici les ressources Kubernetes particuliÃ¨rement utiles pour le Data Engineering.\n\n7.1 Job : tÃ¢che one-shot\nUn Job exÃ©cute un ou plusieurs pods jusquâ€™Ã  complÃ©tion : - ETL ponctuel - Migration de donnÃ©es - Backfill\n# etl-job.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-job\nspec:\n  template:\n    spec:\n      containers:\n      - name: etl\n        image: python:3.11-slim\n        command: [\"python\", \"-c\", \"print('ETL terminÃ© !')\"]\n      restartPolicy: Never\n  backoffLimit: 3  # Nombre de retries en cas d'Ã©chec\n# Lancer le job\nkubectl apply -f etl-job.yaml\n\n# Voir le status\nkubectl get jobs\n\n# Voir les logs\nkubectl logs job/etl-job\n\n\n\n7.2 CronJob : tÃ¢che planifiÃ©e\nUn CronJob lance des Jobs selon un schedule (comme cron Linux) : - ETL quotidien - Nettoyage de donnÃ©es - Rapports automatisÃ©s\n# etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h du matin\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: etl\n            image: my-etl-image:1.0\n            env:\n            - name: DB_HOST\n              value: \"postgres\"\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\nSchedule cron :\n\n\n\nExpression\nSignification\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h00\n\n\n*/15 * * * *\nToutes les 15 minutes\n\n\n0 0 * * 0\nTous les dimanches Ã  minuit\n\n\n0 8 1 * *\nLe 1er de chaque mois Ã  8h\n\n\n\n# Lister les cronjobs\nkubectl get cronjobs\n\n# DÃ©clencher manuellement\nkubectl create job --from=cronjob/etl-daily test-etl\n\n\n\n7.3 DÃ©ployer PostgreSQL dans K8s\n# postgres-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: \"de_db\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n\nâš ï¸ Note production : Les bases de donnÃ©es dans K8s nÃ©cessitent toujours un stockage persistant (PVC) (A voir plus en bas). En production, on prÃ©fÃ¨re souvent des services managÃ©s (Cloud SQL, RDS, Azure Database) pour la haute disponibilitÃ© et les backups automatiques.\n\n\n\nSchÃ©ma Pipeline Data Engineering sur K8s\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   CronJob    â”‚â”€â”€â”€â”€â–¶â”‚     Job      â”‚â”€â”€â”€â”€â–¶â”‚     Pod      â”‚\nâ”‚  (0 2 * * *) â”‚     â”‚              â”‚     â”‚  (etl.py)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                  â”‚\n                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n                          â”‚                       â”‚\n                          â–¼                       â–¼\n                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                   â”‚     PVC      â”‚       â”‚  PostgreSQL  â”‚\n                   â”‚ (input data) â”‚       â”‚   (output)   â”‚\n                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#stockage-volumes-pvc",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#stockage-volumes-pvc",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "9. Stockage : Volumes & PVC",
    "text": "9. Stockage : Volumes & PVC\nLes donnÃ©es ne doivent jamais vivre uniquement dans un Pod (Ã©phÃ©mÃ¨re).\n\nConcepts clÃ©s\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nPersistentVolume (PV)\nRessource de stockage provisionnÃ©e par lâ€™admin\n\n\nPersistentVolumeClaim (PVC)\nDemande de stockage par lâ€™utilisateur\n\n\nStorageClass\nType de stockage (SSD, HDD, cloudâ€¦)\n\n\n\n\n\nSchÃ©ma : Comment Ã§a marche\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Pod   â”‚â”€â”€â”€â”€â–¶â”‚   PVC   â”‚â”€â”€â”€â”€â–¶â”‚     PV      â”‚â”€â”€â”€â”€â–¶â”‚  Stockage rÃ©el   â”‚\nâ”‚         â”‚     â”‚ (claim) â”‚     â”‚ (provision) â”‚     â”‚ (disk, NFS, EBS) â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nExemple : PVC pour PostgreSQL\n# postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nAccess Modes :\n\n\n\nMode\nDescription\n\n\n\n\nReadWriteOnce\nLecture/Ã©criture par un seul node\n\n\nReadOnlyMany\nLecture seule par plusieurs nodes\n\n\nReadWriteMany\nLecture/Ã©criture par plusieurs nodes\n\n\n\n\n\nUtilisation dans un Deployment\nspec:\n  containers:\n  - name: postgres\n    volumeMounts:\n    - name: postgres-storage\n      mountPath: /var/lib/postgresql/data\n  volumes:\n  - name: postgres-storage\n    persistentVolumeClaim:\n      claimName: postgres-pvc\n# VÃ©rifier les PVC\nkubectl get pvc\n\n# VÃ©rifier les PV\nkubectl get pv",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#configuration-configmaps-secrets",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#configuration-configmaps-secrets",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "9. Configuration : ConfigMaps & Secrets",
    "text": "9. Configuration : ConfigMaps & Secrets\n\nConfigMap : configuration non sensible\n# config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\ndata:\n  DB_HOST: \"postgres\"\n  DB_PORT: \"5432\"\n  LOG_LEVEL: \"INFO\"\n\n\nSecret : donnÃ©es sensibles\n# secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secret\ntype: Opaque\ndata:\n  password: ZGVfcGFzc3dvcmQ=  # base64 de \"de_password\"\nCrÃ©er un secret en ligne de commande :\n# CrÃ©er un secret\nkubectl create secret generic postgres-secret \\\n  --from-literal=password=de_password\n\n# Encoder en base64\necho -n \"de_password\" | base64\n\n# DÃ©coder\necho \"ZGVfcGFzc3dvcmQ=\" | base64 -d\n\n\nUtilisation dans un Pod\nspec:\n  containers:\n  - name: etl\n    env:\n    # Depuis ConfigMap\n    - name: DB_HOST\n      valueFrom:\n        configMapKeyRef:\n          name: etl-config\n          key: DB_HOST\n    # Depuis Secret\n    - name: DB_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: postgres-secret\n          key: password",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#debug-monitoring",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#debug-monitoring",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "10. Debug & Monitoring",
    "text": "10. Debug & Monitoring\n\nCommandes de debug essentielles\n# Logs\nkubectl logs &lt;pod&gt;                  # Logs actuels\nkubectl logs -f &lt;pod&gt;               # Suivre en temps rÃ©el\nkubectl logs -p &lt;pod&gt;               # Logs du container prÃ©cÃ©dent (aprÃ¨s crash)\nkubectl logs &lt;pod&gt; -c &lt;container&gt;   # Logs d'un container spÃ©cifique\n\n# DÃ©tails et Events\nkubectl describe pod &lt;pod&gt;          # DÃ©tails complets + Events\nkubectl get events --sort-by='.lastTimestamp'  # Events triÃ©s par date\n\n# Shell dans le pod\nkubectl exec -it &lt;pod&gt; -- bash      # Ouvrir un shell\nkubectl exec -it &lt;pod&gt; -- sh        # Pour images Alpine\n\n# Ressources\nkubectl top pods                    # CPU/RAM des pods (si metrics-server)\nkubectl top nodes                   # CPU/RAM des nodes\n\n\nErreurs classiques et solutions\n\n\n\n\n\n\n\n\nErreur\nCause probable\nSolution\n\n\n\n\nCrashLoopBackOff\nLâ€™app plante au dÃ©marrage\nkubectl logs &lt;pod&gt; pour voir lâ€™erreur\n\n\nImagePullBackOff\nImage introuvable ou accÃ¨s refusÃ©\nVÃ©rifier nom/tag, credentials registry\n\n\nPending\nPas de ressources disponibles\nkubectl describe pod â†’ voir Events\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter resources.limits.memory\n\n\nCreateContainerConfigError\nConfigMap/Secret manquant\nVÃ©rifier que les refs existent\n\n\nErrImageNeverPull\nImage locale introuvable\nimagePullPolicy: IfNotPresent\n\n\n\n\n\nProcessus de debug\n1. kubectl get pods              â†’ Voir le status\n           â”‚\n           â–¼\n2. kubectl describe pod &lt;pod&gt;    â†’ Voir les Events\n           â”‚\n           â–¼\n3. kubectl logs &lt;pod&gt;            â†’ Voir les logs de l'app\n           â”‚\n           â–¼\n4. kubectl exec -it &lt;pod&gt; -- sh  â†’ Investiguer dans le container",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "11. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "11. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes des dÃ©butants\n\n\n\nErreur\nConsÃ©quence\n\n\n\n\nPas de resources.requests/limits\nPods non schedulÃ©s ou OOMKilled\n\n\nSecrets dans ConfigMap\nFuite de credentials\n\n\nBase de donnÃ©es sans PVC\nPerte de donnÃ©es au restart\n\n\nTout dans namespace default\nDifficile Ã  gÃ©rer, pas dâ€™isolation\n\n\nImages en :latest\nComportement non reproductible\n\n\nPas de labels\nImpossible de filtrer/organiser\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nToujours dÃ©finir requests/limits\nScheduling prÃ©visible, protection contre OOM\n\n\nUn namespace par projet/env\nIsolation, quotas, RBAC\n\n\nTags dâ€™image versionnÃ©s\nimage:1.0.0 au lieu de :latest\n\n\nLabels systÃ©matiques\nOrganisation et filtrage\n\n\nHealthchecks (probes)\nK8s sait si lâ€™app est prÃªte/vivante\n\n\nPVC pour toute donnÃ©e importante\nPersistance garantie\n\n\n\n\n\nExemple de labels recommandÃ©s\nmetadata:\n  name: etl-job\n  labels:\n    app: etl-pipeline\n    component: etl\n    env: dev\n    team: data-engineering\n    version: \"1.0.0\"\n\n\nExemple de resources requests/limits\nspec:\n  containers:\n  - name: etl\n    resources:\n      requests:\n        memory: \"256Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"512Mi\"\n        cpu: \"500m\"\n\n\n\nRessource\nUnitÃ©\nExemple\n\n\n\n\nCPU\nmillicores\n500m = 0.5 CPU\n\n\nMemory\nMi/Gi\n256Mi, 1Gi",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#quiz-de-fin-de-module",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre un Pod et un Deployment ?\n\nUn Pod contient plusieurs Deployments\n\nUn Deployment gÃ¨re des replicas de Pods et leur cycle de vie\n\nCâ€™est la mÃªme chose\n\nUn Pod est pour la production, un Deployment pour le dev\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Un Pod est lâ€™unitÃ© de base (Ã©phÃ©mÃ¨re), un Deployment gÃ¨re plusieurs replicas de Pods, les rolling updates, et recrÃ©e les Pods qui crashent.\n\n\n\n\nâ“ Q2. Pourquoi utiliser un PersistentVolumeClaim (PVC) ?\n\nPour augmenter la vitesse du CPU\n\nPour persister les donnÃ©es mÃªme si le Pod est supprimÃ©\n\nPour exposer un Pod sur internet\n\nPour chiffrer les secrets\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Les Pods sont Ã©phÃ©mÃ¨res. Sans PVC, les donnÃ©es sont perdues quand le Pod est supprimÃ© ou recrÃ©Ã©.\n\n\n\n\nâ“ Q3. Quelle commande permet de voir les logs dâ€™un Pod qui a crashÃ© ?\n\nkubectl logs &lt;pod&gt;\n\nkubectl logs -p &lt;pod&gt;\n\nkubectl describe pod &lt;pod&gt;\n\nkubectl get logs &lt;pod&gt;\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Lâ€™option -p (previous) affiche les logs du container prÃ©cÃ©dent, utile aprÃ¨s un crash.\n\n\n\n\nâ“ Q4. Lâ€™encodage Base64 des Secrets Kubernetes est-il sÃ©curisÃ© ?\n\nOui, câ€™est du chiffrement fort\n\nNon, Base64 est juste un encodage, pas du chiffrement\n\nOui, si on utilise une clÃ© de 256 bits\n\nÃ‡a dÃ©pend de la version de Kubernetes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Base64 est un simple encodage rÃ©versible. Nâ€™importe qui avec accÃ¨s au cluster peut dÃ©coder les secrets. En production, utilisez Sealed Secrets ou un secrets manager.\n\n\n\n\nâ“ Q5. Quelle ressource utiliser pour exÃ©cuter un ETL tous les jours Ã  2h du matin ?\n\nDeployment\n\nJob\n\nCronJob\n\nDaemonSet\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Un CronJob permet de planifier des Jobs selon une expression cron (0 2 * * *).\n\n\n\n\nâ“ Q6. Que signifie lâ€™erreur CrashLoopBackOff ?\n\nLâ€™image Docker nâ€™existe pas\n\nLe Pod manque de mÃ©moire\n\nLâ€™application dans le container plante au dÃ©marrage de maniÃ¨re rÃ©pÃ©tÃ©e\n\nLe rÃ©seau est inaccessible\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” K8s tente de redÃ©marrer le container mais il plante Ã  chaque fois. Utilisez kubectl logs &lt;pod&gt; pour voir lâ€™erreur.\n\n\n\n\nâ“ Q7. Quelle est la meilleure pratique pour les tags dâ€™images en production ?\n\nToujours utiliser :latest\n\nUtiliser des tags versionnÃ©s comme :1.0.0\n\nNe pas mettre de tag\n\nUtiliser la date comme tag\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Les tags versionnÃ©s garantissent la reproductibilitÃ©. :latest peut changer et causer des comportements inattendus.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#mini-projet-pipeline-etl-sur-kubernetes",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#mini-projet-pipeline-etl-sur-kubernetes",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "Mini-projet : Pipeline ETL sur Kubernetes",
    "text": "Mini-projet : Pipeline ETL sur Kubernetes\n\nObjectif\nDÃ©ployer un pipeline ETL complet sur Kubernetes : - PostgreSQL avec stockage persistant - CronJob Python qui charge des donnÃ©es CSV dans PostgreSQL\n\n\nContexte\nTu dois crÃ©er une stack Data Engineering minimale mais rÃ©aliste, entiÃ¨rement orchestrÃ©e par Kubernetes.\n\n\nStructure du projet\nk8s-etl-project/\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ postgres-secret.yaml\nâ”‚   â”œâ”€â”€ postgres-pvc.yaml\nâ”‚   â”œâ”€â”€ postgres-deployment.yaml\nâ”‚   â”œâ”€â”€ postgres-service.yaml\nâ”‚   â”œâ”€â”€ etl-configmap.yaml\nâ”‚   â””â”€â”€ etl-cronjob.yaml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sales.csv\nâ””â”€â”€ README.md\n\n\nÃ‰tapes\n\nCrÃ©er le namespace data-pipeline\nDÃ©ployer PostgreSQL (Secret + PVC + Deployment + Service)\nBuild & push lâ€™image Docker de lâ€™ETL\nDÃ©ployer le CronJob ETL\nTester manuellement : kubectl create job --from=cronjob/etl-daily test-etl\nVÃ©rifier les donnÃ©es dans PostgreSQL\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. manifests/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: data-pipeline\n  labels:\n    team: data-engineering\n2. manifests/postgres-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secret\n  namespace: data-pipeline\ntype: Opaque\ndata:\n  password: ZGVfcGFzc3dvcmQ=  # de_password\n3. manifests/postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\n  namespace: data-pipeline\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n4. manifests/postgres-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: data-pipeline\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: \"de_db\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n5. manifests/postgres-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: data-pipeline\nspec:\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\n  type: ClusterIP\n6. manifests/etl-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\n  namespace: data-pipeline\ndata:\n  DB_HOST: \"postgres\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"de_db\"\n  DB_USER: \"de_user\"\n7. manifests/etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\n  namespace: data-pipeline\n  labels:\n    app: etl-pipeline\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: etl-job\n        spec:\n          containers:\n          - name: etl\n            image: my-etl-image:1.0  # Ã€ remplacer par ton image\n            envFrom:\n            - configMapRef:\n                name: etl-config\n            env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: postgres-secret\n                  key: password\n            resources:\n              requests:\n                memory: \"128Mi\"\n                cpu: \"100m\"\n              limits:\n                memory: \"256Mi\"\n                cpu: \"200m\"\n          restartPolicy: OnFailure\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n8. etl/etl.py\nimport os\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef main():\n    # Config depuis variables d'environnement\n    db_host = os.environ['DB_HOST']\n    db_port = os.environ['DB_PORT']\n    db_name = os.environ['DB_NAME']\n    db_user = os.environ['DB_USER']\n    db_pass = os.environ['DB_PASSWORD']\n    \n    print(\"ğŸš€ DÃ©marrage ETL...\")\n    \n    # Simuler des donnÃ©es (en vrai: lire depuis PVC ou S3)\n    df = pd.DataFrame({\n        'date': ['2024-01-01', '2024-01-02'],\n        'product': ['Laptop', 'Mouse'],\n        'quantity': [5, 20],\n        'price': [999.99, 29.99]\n    })\n    df['total'] = df['quantity'] * df['price']\n    \n    # Charger dans PostgreSQL\n    engine = create_engine(\n        f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n    )\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© !\")\n\nif __name__ == \"__main__\":\n    main()\n9. Commandes de dÃ©ploiement\n# CrÃ©er le namespace\nkubectl apply -f manifests/namespace.yaml\n\n# DÃ©ployer PostgreSQL\nkubectl apply -f manifests/postgres-secret.yaml\nkubectl apply -f manifests/postgres-pvc.yaml\nkubectl apply -f manifests/postgres-deployment.yaml\nkubectl apply -f manifests/postgres-service.yaml\n\n# VÃ©rifier que PostgreSQL est prÃªt\nkubectl get pods -n data-pipeline\n\n# DÃ©ployer le CronJob\nkubectl apply -f manifests/etl-configmap.yaml\nkubectl apply -f manifests/etl-cronjob.yaml\n\n# Tester manuellement\nkubectl create job --from=cronjob/etl-daily test-etl -n data-pipeline\n\n# Voir les logs\nkubectl logs -f job/test-etl -n data-pipeline\n\n# VÃ©rifier dans PostgreSQL\nkubectl exec -it $(kubectl get pod -l app=postgres -n data-pipeline -o name) \\\n  -n data-pipeline -- psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#ressources-pour-aller-plus-loin",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nKubernetes Docs â€” Documentation officielle\nkubectl Cheat Sheet â€” Toutes les commandes\nKubernetes API Reference â€” RÃ©fÃ©rence API\n\n\n\nğŸ® Pratique\n\nKatacoda K8s â€” Tutoriels interactifs (gratuit)\nPlay with Kubernetes â€” Cluster K8s gratuit en ligne\nKiller.sh â€” Simulateur dâ€™examen CKA/CKAD\n\n\n\nğŸ“– Livres & Cours\n\nKubernetes Up & Running â€” Kelsey Hightower\nThe Kubernetes Book â€” Nigel Poulton\n\n\n\nğŸ”§ Outils utiles\n\nk9s â€” Terminal UI pour Kubernetes\nLens â€” IDE Kubernetes\nkubectx/kubens â€” Changer de contexte/namespace facilement",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/15_kubernetes_fundamentals.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/15_kubernetes_fundamentals.html#prochaine-Ã©tape",
    "title": "Kubernetes Fundamentals pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les fondamentaux de Kubernetes, passons aux workloads Data Engineering avancÃ©s !\nğŸ‘‰ Module suivant : 16_k8s_for_data_workloads â€” Kubernetes pour les workloads Data\nTu vas apprendre : - Spark on Kubernetes - Airflow on Kubernetes - Helm charts pour packager tes dÃ©ploiements - Horizontal Pod Autoscaler\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Kubernetes Fundamentals pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "15 Â· Kubernetes Fondamentaux"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html",
    "href": "notebooks/intermediate/19_pyspark_advanced.html",
    "title": "PySpark Advanced",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas apprendre Ã  optimiser Spark comme un expert. Tu dÃ©couvriras lâ€™architecture interne, les techniques dâ€™optimisation, et comment diagnostiquer et rÃ©soudre les problÃ¨mes de performance.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#prÃ©requis",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#prÃ©requis",
    "title": "PySpark Advanced",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 11 : PySpark for Data Engineering (bases Spark)\n\n\nâœ… Requis\nModule 18 : High Performance Python\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec des datasets &gt; 1 Go",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#objectifs-du-module",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#objectifs-du-module",
    "title": "PySpark Advanced",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture interne de Spark (Catalyst, Tungsten)\nExÃ©cuter Spark en production avec spark-submit\nOptimiser les partitions, shuffles et joins\nDiagnostiquer un job lent avec Spark UI\nRÃ©duire le temps dâ€™exÃ©cution de 80-90%\n\n\nObjectif concret\n\nTransformer un pipeline de 20 minutes en 2 minutes.\n\nCâ€™est ce qui distingue un Data Engineer junior dâ€™un senior sur Spark.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#rappels-spark-essentiels",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#rappels-spark-essentiels",
    "title": "PySpark Advanced",
    "section": "1. Rappels Spark Essentiels",
    "text": "1. Rappels Spark Essentiels\n\nğŸ’¡ Si tu as suivi le module 11 (PySpark for Data Engineering), cette section est un rappel rapide.\nSinon, commence par ce module avant de continuer â€” les concepts de base sont indispensables.\n\n\n1.1 SparkSession\n\n\nVoir le code\nfrom pyspark.sql import SparkSession\n\n# CrÃ©er une SparkSession (point d'entrÃ©e unique)\nspark = SparkSession.builder \\\n    .appName(\"PySpark Advanced\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\nprint(f\"Spark version: {spark.version}\")\nprint(f\"App name: {spark.sparkContext.appName}\")\n\n\n\n\n1.2 DataFrame vs RDD\n\n\n\nAspect\nRDD\nDataFrame\n\n\n\n\nAPI\nBas niveau\nHaut niveau\n\n\nOptimisation\nManuelle\nCatalyst (automatique)\n\n\nPerformance\nBaseline\n10-100x plus rapide\n\n\nUsage\nLegacy, cas spÃ©ciaux\nStandard\n\n\n\nğŸ‘‰ RÃ¨gle : Toujours utiliser DataFrame/Dataset, jamais RDD (sauf cas trÃ¨s spÃ©cifiques).\n\n\n1.3 Transformations vs Actions\n\n\n\n\n\n\n\n\nType\nExemples\nExÃ©cution\n\n\n\n\nTransformation\nfilter, select, join, groupBy\nLazy (diffÃ©rÃ©e)\n\n\nAction\ncount, collect, write, show\nImmÃ©diate\n\n\n\n\n\n1.4 Lazy Evaluation\ndf.filter(...)     # Rien ne s'exÃ©cute\n  .select(...)     # Rien ne s'exÃ©cute\n  .groupBy(...)    # Rien ne s'exÃ©cute\n  .count()         # MAINTENANT tout s'exÃ©cute !\nAvantage : Spark peut optimiser lâ€™ensemble du pipeline avant exÃ©cution.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#architecture-interne-ce-que-les-dÃ©butants-ne-savent-pas",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#architecture-interne-ce-que-les-dÃ©butants-ne-savent-pas",
    "title": "PySpark Advanced",
    "section": "2. Architecture Interne â€” Ce que les dÃ©butants ne savent pas",
    "text": "2. Architecture Interne â€” Ce que les dÃ©butants ne savent pas\n\nLa vraie maÃ®trise de Spark commence ici. Comprendre lâ€™architecture interne te permet de prÃ©dire et rÃ©soudre les problÃ¨mes de performance.\n\n\n2.1 Le cycle de vie dâ€™un Job Spark\nCode Python/SQL\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    Logical Plan     â”‚  Arbre d'opÃ©rations (ce que tu veux faire)\nâ”‚    (non optimisÃ©)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Catalyst       â”‚  ğŸ§  Optimiseur de requÃªtes\nâ”‚      Optimizer      â”‚  - Predicate pushdown\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Projection pruning\n           â”‚             - Join reordering\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Physical Plan     â”‚  Comment exÃ©cuter (stratÃ©gie)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚        DAG          â”‚  Directed Acyclic Graph\nâ”‚   (Stages + Tasks)  â”‚  - Stages = unitÃ©s de travail\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Tasks = exÃ©cution par partition\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Tungsten       â”‚  âš¡ ExÃ©cution optimisÃ©e\nâ”‚       Engine        â”‚  - Code generation\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Off-heap memory\n\n\n2.2 Catalyst Optimizer â€” Lâ€™arme secrÃ¨te\n\n\n\n\n\n\n\n\nOptimisation\nDescription\nGain\n\n\n\n\nPredicate pushdown\nFiltre appliquÃ© Ã  la source (Parquet, DB)\nI/O rÃ©duit drastiquement\n\n\nProjection pruning\nColonnes inutiles non lues\nI/O rÃ©duit\n\n\nConstant folding\nCalculs constants prÃ©-calculÃ©s\nCPU rÃ©duit\n\n\nJoin reordering\nOrdre optimal des joins\nShuffle rÃ©duit\n\n\n\n\n\n2.3 Tungsten Engine\n\nOff-heap memory : stockage hors JVM â†’ Ã©vite le Garbage Collector\nWhole-stage code generation : gÃ©nÃ¨re du bytecode optimisÃ© Ã  la volÃ©e\nVectorized execution : traitement par batch (comme Polars !)\n\n\n\nVoir le code\nfrom pyspark.sql.functions import col, sum as spark_sum\n\n# CrÃ©er des donnÃ©es de test\ndata = [(i, f\"cat_{i % 5}\", float(i * 10)) for i in range(1000)]\ndf = spark.createDataFrame(data, [\"id\", \"category\", \"amount\"])\n\n# Pipeline avec transformations\nresult = (\n    df\n    .filter(col(\"amount\") &gt; 100)\n    .select(\"category\", \"amount\")\n    .groupBy(\"category\")\n    .agg(spark_sum(\"amount\").alias(\"total\"))\n)\n\n# Voir le plan d'exÃ©cution\nprint(\"=\" * 50)\nprint(\"PLAN D'EXÃ‰CUTION (explain)\")\nprint(\"=\" * 50)\nresult.explain(\"formatted\")\n\n\n\n\nVoir le code\n# Plan complet avec toutes les Ã©tapes\nprint(\"PLAN COMPLET (Parsed â†’ Analyzed â†’ Optimized â†’ Physical)\")\nprint(\"=\" * 60)\nresult.explain(True)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#spark-submit-exÃ©cuter-spark-comme-un-pro",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#spark-submit-exÃ©cuter-spark-comme-un-pro",
    "title": "PySpark Advanced",
    "section": "3. spark-submit â€” ExÃ©cuter Spark comme un pro",
    "text": "3. spark-submit â€” ExÃ©cuter Spark comme un pro\n\nCompÃ©tence indispensable en entreprise. TrÃ¨s peu de formations lâ€™enseignent correctement.\n\n\n3.1 Pourquoi spark-submit ?\n\n\n\n\n\n\n\n\nContexte\nOutil\nUsage\n\n\n\n\nExploration, dÃ©veloppement\nNotebooks (Jupyter, Databricks)\nDev, prototypage\n\n\nProduction, CI/CD, scheduling\nspark-submit\nDÃ©ploiement rÃ©el\n\n\n\n\nğŸ’¡ En entreprise, spark-submit est rarement lancÃ© manuellement. Il est appelÃ© par : - Airflow (orchestration) â†’ Module 25 - CI/CD pipelines (GitLab CI, GitHub Actions) - Schedulers (cron, Kubernetes CronJobs)\n\n\n\n3.2 Deploy Modes\nCLIENT MODE                          CLUSTER MODE\nâ•â•â•â•â•â•â•â•â•â•â•                          â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Client    â”‚                      â”‚   Client    â”‚\nâ”‚   Machine   â”‚                      â”‚   Machine   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚             â”‚\nâ”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici       â”‚  (submit)   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n       â”‚                                    â”‚\n       â–¼                                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Cluster   â”‚                      â”‚   Cluster   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚Exec 1 â”‚  â”‚                      â”‚  â”‚Driver â”‚  â”‚ â—„â”€â”€ Driver ici\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\nâ”‚  â”‚Exec 2 â”‚  â”‚                      â”‚  â”‚Exec 1 â”‚  â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚                      â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”¤  â”‚\nâ”‚  â”‚Exec 3 â”‚  â”‚                      â”‚  â”‚Exec 2 â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nUsage: Debug, interactif            Usage: Production\n\n\n\nMode\nDriver tourne sur\nUsage\n\n\n\n\nclient\nMachine qui soumet\nDebug, logs visibles\n\n\ncluster\nWorker du cluster\nProduction\n\n\n\n\n\n3.3 Resource Managers (Masters)\n\n\n\nMaster\nCommande\nUsage\n\n\n\n\nLocal\n--master local[*]\nDev/test (tous les cores)\n\n\nLocal (N cores)\n--master local[4]\nDev/test (4 cores)\n\n\nStandalone\n--master spark://host:7077\nCluster Spark simple\n\n\nYARN\n--master yarn\nClusters Hadoop\n\n\nKubernetes\n--master k8s://https://...\nCloud native\n\n\n\n\n\n3.4 Syntaxe complÃ¨te spark-submit\nspark-submit \\\n  # === Resource Manager ===\n  --master local[4] \\\n  --deploy-mode client \\\n  \n  # === Ressources ===\n  --driver-memory 4g \\\n  --executor-memory 8g \\\n  --executor-cores 4 \\\n  --num-executors 10 \\\n  \n  # === Configuration Spark ===\n  --conf spark.sql.shuffle.partitions=200 \\\n  --conf spark.sql.adaptive.enabled=true \\\n  --conf spark.executor.memoryOverhead=2g \\\n  \n  # === DÃ©pendances ===\n  --packages io.delta:delta-spark_2.12:3.2.0 \\\n  --jars /path/to/postgres-42.7.jar \\\n  --py-files utils.zip \\\n  \n  # === Application ===\n  main.py \\\n  \n  # === Arguments application ===\n  --date 2024-01-01 \\\n  --env prod\n\n\n3.5 Structure projet production\nspark_project/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e\nâ”‚   â”œâ”€â”€ etl/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ extract.py\nâ”‚   â”‚   â”œâ”€â”€ transform.py\nâ”‚   â”‚   â””â”€â”€ load.py\nâ”‚   â””â”€â”€ utils/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â”œâ”€â”€ config.py\nâ”‚       â””â”€â”€ logger.py\nâ”œâ”€â”€ config/\nâ”‚   â”œâ”€â”€ dev.yaml\nâ”‚   â””â”€â”€ prod.yaml\nâ”œâ”€â”€ jars/\nâ”‚   â””â”€â”€ postgres-42.7.jar\nâ”œâ”€â”€ scripts/\nâ”‚   â”œâ”€â”€ run_local.sh\nâ”‚   â””â”€â”€ run_cluster.sh\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ setup.py                 # Pour crÃ©er .whl\nâ””â”€â”€ README.md\n\n\n3.6 Packaging pour production\n# âŒ MAUVAIS : liste de fichiers (difficile Ã  maintenir)\n--py-files utils.py,config.py,helpers.py\n\n# âœ… MIEUX : Package .zip\ncd src/ && zip -r ../app.zip . && cd ..\nspark-submit --py-files app.zip main.py\n\n# âœ… MEILLEUR : Wheel (.whl) - le plus propre\npip wheel . -w dist/\nspark-submit --py-files dist/myproject-1.0.0-py3-none-any.whl main.py\n\nğŸ’¡ En production, prÃ©fÃ¨re .zip ou .whl pour un dÃ©ploiement propre et versionnÃ©.\n\n\n\nVoir le code\n# Pattern : arguments en ligne de commande (main.py)\n\n# === Exemple de main.py pour spark-submit ===\nexample_main = '''\n#!/usr/bin/env python3\n\"\"\"Point d'entrÃ©e du job Spark.\"\"\"\n\nimport argparse\nfrom pyspark.sql import SparkSession\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"ETL Pipeline\")\n    parser.add_argument(\"--date\", required=True, help=\"Date de traitement (YYYY-MM-DD)\")\n    parser.add_argument(\"--env\", default=\"dev\", choices=[\"dev\", \"prod\"])\n    parser.add_argument(\"--input\", required=True, help=\"Chemin input\")\n    parser.add_argument(\"--output\", required=True, help=\"Chemin output\")\n    return parser.parse_args()\n\ndef main():\n    args = parse_args()\n    \n    spark = SparkSession.builder \\\n        .appName(f\"ETL-{args.env}-{args.date}\") \\\n        .getOrCreate()\n    \n    # Charger les donnÃ©es\n    df = spark.read.parquet(f\"{args.input}/date={args.date}\")\n    \n    # Transformations...\n    result = df.filter(df.amount &gt; 0)\n    \n    # Ã‰crire\n    result.write.mode(\"overwrite\").parquet(f\"{args.output}/date={args.date}\")\n    \n    spark.stop()\n\nif __name__ == \"__main__\":\n    main()\n'''\n\nprint(example_main)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#partitionnement-shuffle-la-source-de-lenteur",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#partitionnement-shuffle-la-source-de-lenteur",
    "title": "PySpark Advanced",
    "section": "4. Partitionnement & Shuffle â€” LA source de lenteur",
    "text": "4. Partitionnement & Shuffle â€” LA source de lenteur\n\n80% des problÃ¨mes de performance Spark viennent du shuffle et du partitionnement.\n\n\n4.1 Quâ€™est-ce quâ€™un Shuffle ?\nLe shuffle est la redistribution des donnÃ©es entre partitions. Il est nÃ©cessaire pour :\n\ngroupBy, reduceByKey\njoin (sauf broadcast)\ndistinct, repartition\norderBy (tri global)\n\nSHUFFLE = GOULOT D'Ã‰TRANGLEMENT\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nStage 1                              Stage 2\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚                          â”‚Partitionâ”‚\nâ”‚    1    â”‚â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¶â”‚   1'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚          â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\nâ”‚    2    â”‚       â”‚  SHUFFLE â”‚       â”‚   2'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ (rÃ©seau) â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚          â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Partitionâ”‚â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â–¶â”‚Partitionâ”‚\nâ”‚    3    â”‚                          â”‚   3'    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nCoÃ»t du Shuffle :\n1. SÃ©rialisation des donnÃ©es\n2. Ã‰criture sur disque (shuffle write)\n3. Transfert rÃ©seau\n4. Lecture depuis disque (shuffle read)\n5. DÃ©sÃ©rialisation\n\n\n4.2 repartition vs coalesce\n\n\n\n\n\n\n\n\nMÃ©thode\nShuffle\nUsage\n\n\n\n\nrepartition(n)\nâœ… Toujours\nAugmenter partitions, rÃ©Ã©quilibrer\n\n\ncoalesce(n)\nâŒ Non (si rÃ©duction)\nRÃ©duire partitions avant write\n\n\n\n\n\nVoir le code\nfrom pyspark.sql.functions import spark_partition_id\n\n# CrÃ©er un DataFrame\ndf = spark.range(0, 1000000)\nprint(f\"Partitions initiales : {df.rdd.getNumPartitions()}\")\n\n# repartition = SHUFFLE (redistribue les donnÃ©es)\ndf_repart = df.repartition(10)\nprint(f\"AprÃ¨s repartition(10) : {df_repart.rdd.getNumPartitions()}\")\n\n# coalesce = PAS DE SHUFFLE (combine les partitions existantes)\ndf_coal = df_repart.coalesce(3)\nprint(f\"AprÃ¨s coalesce(3) : {df_coal.rdd.getNumPartitions()}\")\n\n# Voir la distribution des donnÃ©es par partition\nprint(\"\\nDistribution aprÃ¨s repartition(10):\")\ndf_repart.groupBy(spark_partition_id().alias(\"partition\")).count().orderBy(\"partition\").show()\n\n\n\n\n4.3 Taille optimale des partitions\n\n\n\nTaille partition\nProblÃ¨me\n\n\n\n\nTrop petit (&lt; 10 MB)\nOverhead de scheduling, trop de tÃ¢ches\n\n\nOptimal (128-256 MB)\nâœ… Sweet spot\n\n\nTrop grand (&gt; 1 GB)\nOOM, mauvaise parallÃ©lisation\n\n\n\nFormule :\nnum_partitions = data_size_mb / target_partition_size_mb\n\nExemple : 50 GB de donnÃ©es\nnum_partitions = 50000 MB / 200 MB = 250 partitions\n\n\n4.4 Data Skew â€” Le tueur de performance\nÃ‰QUILIBRÃ‰ (bon)                     SKEWED (problÃ¨me !)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nâ”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚â”‚ 1M â”‚            â”‚100Kâ”‚â”‚100Kâ”‚â”‚100Kâ”‚â”‚        10M        â”‚\nâ””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nTemps: â–ˆâ–ˆâ–ˆâ–ˆ                         Temps: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n       4 tÃ¢ches parallÃ¨les                 â†‘\n       = temps minimal                     Un seul executor bloquÃ© !\n                                           Les autres attendent...\nTechniques anti-skew :\n\nBroadcast join : si une table est petite (&lt; 100 MB)\nSalting : ajouter une clÃ© alÃ©atoire pour distribuer\nAQE (Adaptive Query Execution) : Spark 3.0+ gÃ¨re automatiquement\n\n\n\nVoir le code\n# Activer AQE (recommandÃ© Spark 3.0+)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# VÃ©rifier la configuration\nprint(\"AQE enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\nprint(\"Skew Join enabled:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))\n\n\n\n\n4.5 Caching & Persistence\nQuand utiliser le cache ?\n\nDataFrame rÃ©utilisÃ© dans plusieurs actions\nCalcul coÃ»teux (join, aggregation) rÃ©utilisÃ©\nItÃ©rations (ML training)\n\ncache() vs persist()\n# cache() = persist(StorageLevel.MEMORY_AND_DISK)\ndf.cache()\n\n# persist() = contrÃ´le fin du niveau de stockage\nfrom pyspark import StorageLevel\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n\n\nNiveau\nRAM\nDisque\nSÃ©rialisÃ©\nUsage\n\n\n\n\nMEMORY_ONLY\nâœ…\nâŒ\nâŒ\nPetit DF, RAM suffisante\n\n\nMEMORY_AND_DISK\nâœ…\nâœ…\nâŒ\nDÃ©faut (cache())\n\n\nMEMORY_ONLY_SER\nâœ…\nâŒ\nâœ…\nÃ‰conomie RAM\n\n\nDISK_ONLY\nâŒ\nâœ…\nâœ…\nTrÃ¨s gros DF\n\n\nOFF_HEAP\nâœ…\nâŒ\nâœ…\nÃ‰viter GC Java\n\n\n\n\n\nVoir le code\nfrom pyspark import StorageLevel\nimport time\n\n# CrÃ©er un DataFrame avec calculs\ndf = spark.range(0, 5000000).withColumn(\"squared\", col(\"id\") ** 2)\n\n# Sans cache : chaque action recalcule tout\nstart = time.time()\ncount1 = df.filter(col(\"squared\") &gt; 1000000).count()\ncount2 = df.filter(col(\"squared\") &lt; 100).count()\nprint(f\"Sans cache : {time.time() - start:.2f}s\")\n\n# Avec cache : calcul une seule fois\ndf_cached = df.cache()\n\nstart = time.time()\ncount1 = df_cached.filter(col(\"squared\") &gt; 1000000).count()\ncount2 = df_cached.filter(col(\"squared\") &lt; 100).count()\nprint(f\"Avec cache : {time.time() - start:.2f}s\")\n\n# âš ï¸ IMPORTANT : libÃ©rer la mÃ©moire !\ndf_cached.unpersist()\nprint(\"\\nâœ… Cache libÃ©rÃ© avec unpersist()\")\n\n\nâš ï¸ Quand NE PAS cacher :\n\n\n\nSituation\nPourquoi\n\n\n\n\nDF utilisÃ© une seule fois\nGaspillage mÃ©moire\n\n\nDF trÃ¨s volumineux (&gt; RAM)\nDÃ©bordement disque lent\n\n\nAvant un shuffle\nInutile, donnÃ©es redistribuÃ©es\n\n\nPipeline simple et rapide\nOverhead du cache &gt; gain",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#joins-avancÃ©s-la-compÃ©tence-qui-change-tout",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#joins-avancÃ©s-la-compÃ©tence-qui-change-tout",
    "title": "PySpark Advanced",
    "section": "5. Joins AvancÃ©s â€” La compÃ©tence qui change tout",
    "text": "5. Joins AvancÃ©s â€” La compÃ©tence qui change tout\n\nUn excellent Data Engineer sait optimiser ses joins. Câ€™est souvent lÃ  que se gagne (ou se perd) le plus de temps.\n\n\n5.1 Types de Joins internes Spark\n\n\n\n\n\n\n\n\nType\nQuand Spark lâ€™utilise\nPerformance\n\n\n\n\nBroadcast Hash Join\nPetite table (&lt; seuil)\nâ­â­â­ Meilleur\n\n\nSort Merge Join\nGrandes tables\nâ­â­ Standard\n\n\nShuffle Hash Join\nTables moyennes\nâ­ Ã‰viter si possible\n\n\n\n\n\n5.2 Broadcast Join â€” Ton meilleur ami\nSORT MERGE JOIN (shuffle)           BROADCAST JOIN (pas de shuffle)\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•           â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n   Big Table      Small Table          Big Table      Small Table\n   â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”\n   â”‚  P1  â”‚       â”‚  P1  â”‚             â”‚  P1  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n   â”‚  P2  â”‚       â”‚  P2  â”‚             â”‚  P2  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ COPY â”‚\n   â”‚  P3  â”‚       â”‚  P3  â”‚             â”‚  P3  â”‚â—„â”€â”€â”€â”€â”€â”€â”‚      â”‚\n   â””â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”¬â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”˜\n      â”‚   SHUFFLE    â”‚                    â”‚\n      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               Broadcast to\n             â”‚                       all executors\n         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”                   (pas de shuffle !)\n         â”‚ JOIN  â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\nfrom pyspark.sql.functions import broadcast\n\n# Grande table (10M lignes simulÃ©es)\nbig_df = spark.range(0, 1000000).withColumn(\"category_id\", (col(\"id\") % 100).cast(\"int\"))\n\n# Petite table (100 lignes)\nsmall_df = spark.createDataFrame(\n    [(i, f\"Category {i}\") for i in range(100)],\n    [\"category_id\", \"category_name\"]\n)\n\n# âŒ SANS broadcast hint (Spark peut ou non broadcaster)\nresult_no_hint = big_df.join(small_df, \"category_id\")\nprint(\"Sans broadcast hint:\")\nresult_no_hint.explain()\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# âœ… AVEC broadcast hint (force le broadcast)\nresult_broadcast = big_df.join(broadcast(small_df), \"category_id\")\nprint(\"Avec broadcast():\")\nresult_broadcast.explain()\n\n\n\n\n5.3 Configurer le seuil de broadcast\n# DÃ©faut : 10 MB\n# Si une table &lt; 10 MB â†’ broadcast automatique\n\n# Augmenter pour broadcaster des tables plus grandes\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024)  # 100 MB\n\n# DÃ©sactiver le broadcast automatique (forcer shuffle)\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n\n\n5.4 Join Hints (Spark 3.0+)\n# DataFrame API\nbig_df.join(small_df.hint(\"broadcast\"), \"key\")\n\n# SQL\nspark.sql(\"\"\"\n    SELECT /*+ BROADCAST(small_table) */ *\n    FROM big_table\n    JOIN small_table ON big_table.key = small_table.key\n\"\"\")\n\n\n5.5 Anti-pattern : Join sur clÃ© skewed\n\n\nVoir le code\n# Configuration pour gÃ©rer le skew automatiquement (AQE)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\nprint(\"AQE Skew Join activÃ©\")\nprint(\"Spark va automatiquement dÃ©tecter et gÃ©rer les partitions skewed\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#lectureÃ©criture-optimisÃ©es",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#lectureÃ©criture-optimisÃ©es",
    "title": "PySpark Advanced",
    "section": "6. Lecture/Ã‰criture OptimisÃ©es",
    "text": "6. Lecture/Ã‰criture OptimisÃ©es\n\n6.1 Formats : Parquet toujours !\n\n\n\nFormat\nLecture\nÃ‰criture\nCompression\nPredicate Pushdown\n\n\n\n\nCSV\nğŸ¢ Lent\nğŸ¢ Lent\nâŒ\nâŒ\n\n\nJSON\nğŸ¢ Lent\nğŸ¢ Lent\nâŒ\nâŒ\n\n\nParquet\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ…\n\n\nORC\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ…\n\n\nDelta\nğŸš€ Rapide\nğŸš€ Rapide\nâœ…\nâœ… + ACID\n\n\n\n\nğŸ”­ Les formats modernes (Delta, Iceberg, Hudi) seront approfondis dans le module 21 (Lakehouse) :\n\n\n\nTransactions ACID\nTime Travel\nVacuum, Compaction\nZ-Ordering\n\n\n\n\n6.2 Predicate Pushdown\n\n\nVoir le code\nimport os\nimport shutil\n\n# CrÃ©er des donnÃ©es de test\ntest_data = spark.range(0, 100000).withColumn(\"category\", (col(\"id\") % 10).cast(\"string\"))\n\n# Sauvegarder en Parquet\noutput_path = \"/tmp/test_parquet\"\nif os.path.exists(output_path):\n    shutil.rmtree(output_path)\ntest_data.write.parquet(output_path)\n\n# Lecture avec filtre â†’ predicate pushdown\ndf_filtered = spark.read.parquet(output_path).filter(col(\"id\") &lt; 100)\n\nprint(\"Plan d'exÃ©cution avec Predicate Pushdown:\")\ndf_filtered.explain()\n# Observe \"PushedFilters\" dans le plan !\n\n\n\n\n6.3 Partitionnement sur disque\n\n\nVoir le code\nfrom pyspark.sql.functions import year, month, dayofmonth, lit\nfrom datetime import datetime, timedelta\nimport random\n\n# CrÃ©er des donnÃ©es avec dates\ndates = [(datetime(2024, 1, 1) + timedelta(days=random.randint(0, 90))).strftime(\"%Y-%m-%d\") \n         for _ in range(10000)]\ndata = [(i, dates[i % len(dates)], float(random.randint(10, 1000))) \n        for i in range(10000)]\n\ndf = spark.createDataFrame(data, [\"id\", \"date\", \"amount\"])\ndf = df.withColumn(\"date\", col(\"date\").cast(\"date\"))\ndf = df.withColumn(\"year\", year(\"date\")).withColumn(\"month\", month(\"date\"))\n\n# Ã‰criture partitionnÃ©e\noutput_partitioned = \"/tmp/partitioned_data\"\nif os.path.exists(output_partitioned):\n    shutil.rmtree(output_partitioned)\n\ndf.write.partitionBy(\"year\", \"month\").parquet(output_partitioned)\n\nprint(\"Structure crÃ©Ã©e:\")\nfor root, dirs, files in os.walk(output_partitioned):\n    level = root.replace(output_partitioned, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n    if level &lt; 2:  # Limiter la profondeur\n        for d in sorted(dirs)[:3]:\n            print(f\"{indent}  {d}/\")\n\n\n\n\n6.4 SchÃ©mas explicites\nPourquoi dÃ©finir un schÃ©ma ?\n\nÃ‰vite lâ€™infÃ©rence (coÃ»teuse sur gros fichiers)\nGarantit les types attendus\nDÃ©tecte les erreurs tÃ´t\n\n\n\nVoir le code\nfrom pyspark.sql.types import StructType, StructField, LongType, StringType, DecimalType, DateType\n\n# DÃ©finir un schÃ©ma explicite\nschema = StructType([\n    StructField(\"id\", LongType(), nullable=False),\n    StructField(\"customer_name\", StringType(), nullable=True),\n    StructField(\"amount\", DecimalType(10, 2), nullable=True),  # PrÃ©cision pour montants !\n    StructField(\"transaction_date\", DateType(), nullable=True)\n])\n\nprint(\"SchÃ©ma dÃ©fini:\")\nprint(schema.simpleString())\n\n# Utilisation\n# df = spark.read.schema(schema).parquet(\"data/transactions/\")\n\nprint(\"\\nğŸ’¡ Conseil : Utiliser DecimalType pour les montants financiers\")\nprint(\"   Ã‰vite les erreurs d'arrondi des float/double\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#udfs-le-piÃ¨ge-de-performance",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#udfs-le-piÃ¨ge-de-performance",
    "title": "PySpark Advanced",
    "section": "7. UDFs : Le piÃ¨ge de performance",
    "text": "7. UDFs : Le piÃ¨ge de performance\n\n7.1 Pourquoi les Python UDFs sont toxiques\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                 â”‚   SÃ©rialisation    â”‚                 â”‚\nâ”‚       JVM       â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚     Python      â”‚\nâ”‚     (Spark)     â”‚                    â”‚   (UDF lente)   â”‚\nâ”‚                 â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   DÃ©sÃ©rialisation  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â”‚\n                    TRÃˆS COÃ›TEUX !\n                    (par ligne)\n\n\n7.2 HiÃ©rarchie de performance\n\n\n\n\n\n\n\n\nType\nPerformance\nQuand lâ€™utiliser\n\n\n\n\nExpressions Spark natives\nâ­â­â­â­â­\nToujours si possible\n\n\nPandas UDF (vectorized)\nâ­â­â­\nSi besoin Python\n\n\nPython UDF\nâ­\nDernier recours\n\n\n\n\n\n7.3 Remplacer UDF par expressions natives\n\n\nVoir le code\nfrom pyspark.sql.functions import udf, when, col\nfrom pyspark.sql.types import StringType\nimport time\n\n# CrÃ©er des donnÃ©es de test\ndf = spark.range(0, 500000).withColumn(\"amount\", (col(\"id\") % 2000).cast(\"double\"))\n\n# âŒ MAUVAIS : Python UDF\n@udf(StringType())\ndef categorize_udf(amount):\n    if amount &gt; 1000:\n        return \"high\"\n    elif amount &gt; 100:\n        return \"medium\"\n    return \"low\"\n\nstart = time.time()\nresult_udf = df.withColumn(\"category\", categorize_udf(col(\"amount\")))\nresult_udf.write.mode(\"overwrite\").format(\"noop\").save()  # Force l'exÃ©cution\nudf_time = time.time() - start\nprint(f\"âŒ Python UDF : {udf_time:.2f}s\")\n\n# âœ… BON : Expression Spark native\nstart = time.time()\nresult_native = df.withColumn(\"category\",\n    when(col(\"amount\") &gt; 1000, \"high\")\n    .when(col(\"amount\") &gt; 100, \"medium\")\n    .otherwise(\"low\")\n)\nresult_native.write.mode(\"overwrite\").format(\"noop\").save()\nnative_time = time.time() - start\nprint(f\"âœ… Expression native : {native_time:.2f}s\")\n\nprint(f\"\\nğŸ“Š Speedup : {udf_time/native_time:.1f}x plus rapide avec expression native !\")\n\n\n\n\n7.4 Pandas UDF (si Python nÃ©cessaire)\n\n\nVoir le code\nfrom pyspark.sql.functions import pandas_udf\nimport pandas as pd\nimport numpy as np\n\n# Pandas UDF = vectorisÃ© (traite des Series, pas des scalaires)\n@pandas_udf(\"double\")\ndef log_transform(s: pd.Series) -&gt; pd.Series:\n    return np.log1p(s)\n\n# Test\ndf = spark.range(0, 100000).withColumn(\"value\", col(\"id\").cast(\"double\"))\n\nstart = time.time()\nresult = df.withColumn(\"log_value\", log_transform(col(\"value\")))\nresult.write.mode(\"overwrite\").format(\"noop\").save()\nprint(f\"Pandas UDF : {time.time() - start:.2f}s\")\n\nresult.show(5)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#configuration-tuning",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#configuration-tuning",
    "title": "PySpark Advanced",
    "section": "8. Configuration & Tuning",
    "text": "8. Configuration & Tuning\n\n8.1 ParamÃ¨tres essentiels\n\n\n\n\n\n\n\n\nParamÃ¨tre\nDÃ©faut\nRecommandation\n\n\n\n\nspark.sql.shuffle.partitions\n200\nAdapter Ã  la taille des donnÃ©es\n\n\nspark.default.parallelism\nSelon cluster\n2-3x num_cores\n\n\nspark.sql.autoBroadcastJoinThreshold\n10MB\n50-100MB\n\n\nspark.executor.memory\n1g\n4-16g selon cluster\n\n\nspark.driver.memory\n1g\n2-8g\n\n\nspark.executor.memoryOverhead\n10%\n15-20% pour PySpark\n\n\n\n\n\n8.2 Dimensionnement : Executors vs Cores\nâŒ MAUVAIS : 2 executors Ã— 10 cores chacun\n   - GC Java doit gÃ©rer Ã©norme heap (~50 GB)\n   - Si 1 executor crash â†’ 50% de perte\n   - ParallÃ©lisme moins granulaire\n\nâœ… BON : 10 executors Ã— 4 cores chacun  \n   - GC plus efficace (heap ~10 GB)\n   - Meilleure isolation des erreurs\n   - ParallÃ©lisme plus granulaire\nRÃ¨gles de dimensionnement :\nexecutor_cores = 4-5 max (sweet spot pour GC)\nexecutor_memory = 4-16g (selon donnÃ©es)\nnum_executors = (total_cores / executor_cores) - 1\n\n# RÃ©server pour le Driver et l'OS\ndriver_memory = 2-8g\nmemoryOverhead = 15-20% pour PySpark (sÃ©rialisation Python)\nExemple concret (cluster 100 cores, 400 GB RAM) :\nspark-submit \\\n  --executor-cores 4 \\\n  --executor-memory 12g \\\n  --num-executors 20 \\\n  --driver-memory 4g \\\n  --conf spark.executor.memoryOverhead=2g \\\n  main.py\n\n\n8.3 Adaptive Query Execution (AQE)\nLâ€™AQE (Spark 3.0+) rend certaines optimisations manuelles obsolÃ¨tes :\n\n\n\nAvant AQE (manuel)\nAvec AQE (automatique)\n\n\n\n\ncoalesce(n) aprÃ¨s filter\nAuto-coalesce des partitions\n\n\nCalculer shuffle.partitions\nAuto-optimize partitions\n\n\nDÃ©tecter skew manuellement\nAuto-skew handling\n\n\nBroadcast threshold fixe\nRuntime broadcast decisions\n\n\n\n\n\nVoir le code\n# Configuration AQE complÃ¨te (recommandÃ©e)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n\n# Avec AQE, tu peux laisser shuffle.partitions Ã©levÃ©\n# Spark optimisera automatiquement\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1000\")\n\nprint(\"âœ… Configuration AQE optimale :\")\nprint(f\"   adaptive.enabled = {spark.conf.get('spark.sql.adaptive.enabled')}\")\nprint(f\"   coalescePartitions = {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")\nprint(f\"   skewJoin = {spark.conf.get('spark.sql.adaptive.skewJoin.enabled')}\")\nprint(f\"   shuffle.partitions = {spark.conf.get('spark.sql.shuffle.partitions')}\")\nprint(\"\\nğŸ’¡ Avec AQE, Spark ajuste automatiquement le nombre de partitions !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#spark-ui-diagnostic",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#spark-ui-diagnostic",
    "title": "PySpark Advanced",
    "section": "9. Spark UI & Diagnostic",
    "text": "9. Spark UI & Diagnostic\n\nSavoir lire Spark UI = savoir debugger. Câ€™est la compÃ©tence qui fait la diffÃ©rence.\n\n\n9.1 AccÃ©der Ã  Spark UI\n\nLocal : http://localhost:4040\nCluster : via le Resource Manager (YARN, K8s dashboard)\nDatabricks : intÃ©grÃ© dans lâ€™interface\n\n\n\n9.2 Les onglets importants\n\n\n\nOnglet\nInformation\n\n\n\n\nJobs\nVue dâ€™ensemble des jobs, durÃ©e\n\n\nStages\nDÃ©tail des stages, shuffle read/write\n\n\nStorage\nDataFrames en cache\n\n\nEnvironment\nConfiguration Spark\n\n\nExecutors\nRessources, GC, mÃ©moire\n\n\nSQL\nPlans dâ€™exÃ©cution des requÃªtes\n\n\n\n\n\n9.3 MÃ©triques Ã  surveiller\n\n\n\nMÃ©trique\nSignification\nğŸš¨ ProblÃ¨me siâ€¦\n\n\n\n\nShuffle Read/Write\nDonnÃ©es Ã©changÃ©es\nTrÃ¨s Ã©levÃ© (&gt; 10 GB)\n\n\nSpill (Memory/Disk)\nDÃ©bordement mÃ©moire\n&gt; 0\n\n\nTask Duration\nTemps par tÃ¢che\nTrÃ¨s variable (skew !)\n\n\nGC Time\nGarbage Collection\n&gt; 10% du temps total\n\n\nInput/Output\nDonnÃ©es lues/Ã©crites\nBeaucoup plus que prÃ©vu\n\n\n\n\n\n9.4 Patterns de problÃ¨mes\n\n\n\n\n\n\n\n\nSymptÃ´me dans Spark UI\nDiagnostic\nSolution\n\n\n\n\n1 tÃ¢che 10x plus longue\nData skew\nSalting, broadcast, AQE\n\n\nShuffle &gt; 50 GB\nJoin non optimisÃ©\nBroadcast join\n\n\nSpill to disk\nMÃ©moire insuffisante\nPlus de RAM, moins de partitions\n\n\nGC Time &gt; 20%\nTrop dâ€™objets Java\nTungsten, plus de memoryOverhead\n\n\nBeaucoup de petites tÃ¢ches\nTrop de partitions\nCoalesce, AQE\n\n\n\n\n\nVoir le code\n# URL de Spark UI pour cette session\nprint(f\"ğŸ” Spark UI disponible sur : {spark.sparkContext.uiWebUrl}\")\nprint(\"\\nğŸ“Š Pour voir les mÃ©triques d'un job, lance une action puis consulte l'UI\")\n\n# Exemple : dÃ©clencher un job pour voir dans l'UI\ndf = spark.range(0, 1000000)\nresult = df.groupBy((col(\"id\") % 100).alias(\"group\")).count()\nresult.collect()  # Action qui dÃ©clenche le job\n\nprint(\"\\nâœ… Job exÃ©cutÃ© - consulte Spark UI pour voir les stages et mÃ©triques\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#bonnes-pratiques-anti-patterns",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#bonnes-pratiques-anti-patterns",
    "title": "PySpark Advanced",
    "section": "10. Bonnes pratiques & Anti-patterns",
    "text": "10. Bonnes pratiques & Anti-patterns\n\nâŒ Anti-patterns (Ã  Ã©viter absolument)\n\n\n\n\n\n\n\n\nAnti-pattern\nPourquoi câ€™est mal\nSolution\n\n\n\n\ncollect() sur 100M lignes\nOOM Driver garanti\nwrite() vers fichier\n\n\nCSV en production\nLent, pas de schema\nParquet/Delta\n\n\nUDF Python partout\n10-100x plus lent\nExpressions natives\n\n\nshuffle.partitions=200 toujours\nPas adaptÃ© aux donnÃ©es\nAjuster ou AQE\n\n\nPas de cache() sur DF rÃ©utilisÃ©\nRecalcul inutile\ncache() + unpersist()\n\n\nIgnorer Spark UI\nDebug Ã  lâ€™aveugle\nToujours vÃ©rifier\n\n\nJoin sans broadcast\nShuffle Ã©norme\nbroadcast() sur petites tables\n\n\nrepartition() avant write()\nShuffle inutile\ncoalesce() pour rÃ©duire\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nBÃ©nÃ©fice\n\n\n\n\nToujours Parquet\nI/O optimisÃ©, predicate pushdown\n\n\nBroadcast petites tables\nÃ‰vite shuffle\n\n\nAQE activÃ©\nOptimisation runtime\n\n\nPartitionner par date\nPartition pruning\n\n\nÃ‰viter UDFs\nPerformance native\n\n\nMonitorer Spark UI\nDebug efficace\n\n\nCache + unpersist\nÃ‰vite recalculs\n\n\nSchema explicite\nÃ‰vite infÃ©rence",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#mini-projet-optimisation-dun-pipeline",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#mini-projet-optimisation-dun-pipeline",
    "title": "PySpark Advanced",
    "section": "Mini-Projet : Optimisation dâ€™un pipeline",
    "text": "Mini-Projet : Optimisation dâ€™un pipeline\n\nObjectif\nRÃ©duire un pipeline de 20 minutes Ã  &lt; 3 minutes en appliquant les techniques apprises.\n\n\nScÃ©nario : E-commerce Analytics\n\n\n\nTable\nLignes\nFormat initial\n\n\n\n\nTransactions\n5M\nParquet\n\n\nProduits\n10K\nCSV\n\n\nClients\n500K\nParquet\n\n\n\n\n\nArchitecture cible\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Transactions  â”‚    â”‚    Produits    â”‚    â”‚    Clients     â”‚\nâ”‚    (5M rows)   â”‚    â”‚   (10K rows)   â”‚    â”‚   (500K rows)  â”‚\nâ”‚   [Parquet]    â”‚    â”‚ [CSVâ†’Parquet]  â”‚    â”‚   [Parquet]    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                     â”‚ broadcast           â”‚\n        â”‚                     â–¼                     â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\n                              â–¼\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚      Join + Aggregation       â”‚\n                 â”‚   - Broadcast products        â”‚\n                 â”‚   - AQE enabled               â”‚\n                 â”‚   - Native expressions        â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                 â”‚\n                                 â–¼\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚      Partitioned Output       â”‚\n                 â”‚   partitionBy(\"year\",\"month\") â”‚\n                 â”‚         [Parquet]             â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\nimport os\nimport shutil\nfrom pyspark.sql.functions import *\nfrom datetime import datetime, timedelta\nimport random\nimport time\n\n# Setup : crÃ©er les donnÃ©es de test\nprint(\"ğŸ“¦ CrÃ©ation des donnÃ©es de test...\")\n\n# Configuration optimale\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n# Nettoyer les rÃ©pertoires\nfor path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \"/tmp/output\"]:\n    if os.path.exists(path):\n        shutil.rmtree(path)\n\n# 1. Transactions (5M lignes)\ntransactions = spark.range(0, 500000).select(\n    col(\"id\").alias(\"transaction_id\"),\n    (col(\"id\") % 10000).alias(\"product_id\"),\n    (col(\"id\") % 50000).alias(\"customer_id\"),\n    (rand() * 1000).alias(\"amount\"),\n    date_add(lit(\"2024-01-01\"), (rand() * 90).cast(\"int\")).alias(\"date\")\n)\ntransactions.write.parquet(\"/tmp/transactions\")\nprint(f\"Transactions : {transactions.count()} lignes\")\n\n# 2. Produits (10K lignes) - CSV intentionnellement\nproducts_data = [(i, f\"Product_{i}\", f\"Category_{i % 50}\", float(10 + i % 100)) \n                 for i in range(10000)]\nproducts = spark.createDataFrame(products_data, \n    [\"product_id\", \"product_name\", \"category\", \"base_price\"])\nproducts.write.mode(\"overwrite\").option(\"header\", True).csv(\"/tmp/products\")\nprint(f\"Produits : {products.count()} lignes (CSV)\")\n\n# 3. Clients (500K lignes)\ncustomers = spark.range(0, 50000).select(\n    col(\"id\").alias(\"customer_id\"),\n    concat(lit(\"Customer_\"), col(\"id\")).alias(\"customer_name\"),\n    (col(\"id\") % 5).alias(\"segment\")\n)\ncustomers.write.parquet(\"/tmp/customers\")\nprint(f\"Clients : {customers.count()} lignes\")\n\nprint(\"\\n DonnÃ©es crÃ©Ã©es avec succÃ¨s !\")\n\n\n\n\nVoir le code\n# âŒ VERSION NON OPTIMISÃ‰E (baseline)\nprint(\"=\"*50)\nprint(\"âŒ PIPELINE NON OPTIMISÃ‰\")\nprint(\"=\"*50)\n\n# DÃ©sactiver AQE pour le baseline\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n\nstart = time.time()\n\n# Lecture\ntransactions_df = spark.read.parquet(\"/tmp/transactions\")\nproducts_df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/products\")  # âŒ InfÃ©rence\ncustomers_df = spark.read.parquet(\"/tmp/customers\")\n\n# UDF non optimisÃ©\n@udf(StringType())\ndef get_amount_category(amount):\n    if amount &gt; 500: return \"high\"\n    elif amount &gt; 100: return \"medium\"\n    return \"low\"\n\n# Joins sans broadcast\nresult = transactions_df \\\n    .join(products_df, \"product_id\") \\\n    .join(customers_df, \"customer_id\") \\\n    .withColumn(\"amount_category\", get_amount_category(col(\"amount\"))) \\\n    .groupBy(\"category\", \"segment\", \"amount_category\") \\\n    .agg(\n        count(\"*\").alias(\"num_transactions\"),\n        sum(\"amount\").alias(\"total_amount\")\n    )\n\n# Ã‰criture non partitionnÃ©e\nif os.path.exists(\"/tmp/output\"):\n    shutil.rmtree(\"/tmp/output\")\nresult.write.parquet(\"/tmp/output\")\n\nbaseline_time = time.time() - start\nprint(f\"\\nâ±ï¸ Temps baseline : {baseline_time:.2f}s\")\n\n\n\n\nVoir le code\n# âœ… VERSION OPTIMISÃ‰E\nprint(\"=\"*50)\nprint(\"âœ… PIPELINE OPTIMISÃ‰\")\nprint(\"=\"*50)\n\n# Activer AQE\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n\nstart = time.time()\n\n# 1. Lecture avec schema explicite pour CSV\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n\nproducts_schema = StructType([\n    StructField(\"product_id\", IntegerType()),\n    StructField(\"product_name\", StringType()),\n    StructField(\"category\", StringType()),\n    StructField(\"base_price\", DoubleType())\n])\n\ntransactions_df = spark.read.parquet(\"/tmp/transactions\")\nproducts_df = spark.read.option(\"header\", True).schema(products_schema).csv(\"/tmp/products\")  # âœ… Schema explicite\ncustomers_df = spark.read.parquet(\"/tmp/customers\")\n\n# 2. Broadcast les petites tables\nproducts_df = broadcast(products_df)\n\n# 3. Expression native au lieu de UDF\namount_category_expr = when(col(\"amount\") &gt; 500, \"high\") \\\n    .when(col(\"amount\") &gt; 100, \"medium\") \\\n    .otherwise(\"low\")\n\n# 4. Pipeline optimisÃ©\nresult_optimized = transactions_df \\\n    .join(products_df, \"product_id\") \\\n    .join(customers_df, \"customer_id\") \\\n    .withColumn(\"amount_category\", amount_category_expr) \\\n    .withColumn(\"year\", year(\"date\")) \\\n    .withColumn(\"month\", month(\"date\")) \\\n    .groupBy(\"category\", \"segment\", \"amount_category\", \"year\", \"month\") \\\n    .agg(\n        count(\"*\").alias(\"num_transactions\"),\n        sum(\"amount\").alias(\"total_amount\")\n    )\n\n# 5. Ã‰criture partitionnÃ©e\nif os.path.exists(\"/tmp/output_optimized\"):\n    shutil.rmtree(\"/tmp/output_optimized\")\nresult_optimized.write.partitionBy(\"year\", \"month\").parquet(\"/tmp/output_optimized\")\n\noptimized_time = time.time() - start\nprint(f\"\\nâ±ï¸ Temps optimisÃ© : {optimized_time:.2f}s\")\n\n\n\n\nVoir le code\n# RÃ©sumÃ© des optimisations\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š RÃ‰SUMÃ‰ DES OPTIMISATIONS\")\nprint(\"=\"*60)\n\nspeedup = baseline_time / optimized_time if optimized_time &gt; 0 else 0\nreduction = (1 - optimized_time / baseline_time) * 100 if baseline_time &gt; 0 else 0\n\nprint(f\"\"\"\nâ±ï¸ Temps baseline    : {baseline_time:.2f}s\nâ±ï¸ Temps optimisÃ©    : {optimized_time:.2f}s\nğŸš€ Speedup           : {speedup:.1f}x\nğŸ“‰ RÃ©duction         : {reduction:.0f}%\n\nOptimisations appliquÃ©es :\n  âœ… AQE activÃ© (Adaptive Query Execution)\n  âœ… Schema explicite pour CSV (pas d'infÃ©rence)\n  âœ… Broadcast join pour products (10K lignes)\n  âœ… Expression native au lieu de UDF Python\n  âœ… Ã‰criture partitionnÃ©e par year/month\n\"\"\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#quiz-de-fin-de-module",
    "title": "PySpark Advanced",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\n\n\nâ“ Q1. Quel composant de Spark optimise automatiquement le plan de requÃªte ?\n\nTungsten\n\nCatalyst\n\nDriver\n\nExecutor\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Catalyst est lâ€™optimiseur de requÃªtes qui applique predicate pushdown, projection pruning, et join reordering.\n\n\n\n\nâ“ Q2. Quelle opÃ©ration cause un shuffle ?\n\nfilter()\n\nselect()\n\ngroupBy()\n\nwithColumn()\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” groupBy() nÃ©cessite un shuffle pour regrouper les donnÃ©es par clÃ©. Les autres sont des transformations narrow.\n\n\n\n\nâ“ Q3. Quelle mÃ©thode utiliser pour rÃ©duire le nombre de partitions SANS shuffle ?\n\nrepartition()\n\ncoalesce()\n\npartitionBy()\n\nbucketBy()\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” coalesce() combine les partitions existantes sans shuffle (si rÃ©duction). repartition() cause toujours un shuffle.\n\n\n\n\nâ“ Q4. Quelle est la taille optimale dâ€™une partition Spark ?\n\n1-10 MB\n\n128-256 MB\n\n1-2 GB\n\n10-50 GB\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” 128-256 MB est le sweet spot. Trop petit = overhead, trop grand = OOM et mauvaise parallÃ©lisation.\n\n\n\n\nâ“ Q5. Pour joindre une table de 10 GB avec une table de 50 MB, quelle stratÃ©gie utiliser ?\n\nSort Merge Join\n\nShuffle Hash Join\n\nBroadcast Join\n\nNested Loop Join\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Broadcast Join envoie la petite table (50 MB) Ã  tous les executors, Ã©vitant le shuffle de la grande table.\n\n\n\n\nâ“ Q6. Pourquoi les Python UDFs sont-ils lents ?\n\nPython est un langage interprÃ©tÃ©\n\nSÃ©rialisation JVM â†”ï¸ Python pour chaque ligne\n\nLe GIL de Python\n\nManque de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Chaque ligne nÃ©cessite une sÃ©rialisation JVMâ†’Python et dÃ©sÃ©rialisation Pythonâ†’JVM, ce qui est trÃ¨s coÃ»teux.\n\n\n\n\nâ“ Q7. Que signifie â€œSpill to diskâ€ dans Spark UI ?\n\nLes donnÃ©es sont Ã©crites en Parquet\n\nLa mÃ©moire est insuffisante, donnÃ©es Ã©crites sur disque\n\nLe cache est activÃ©\n\nLe shuffle est terminÃ©\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Spill indique que la mÃ©moire est insuffisante et les donnÃ©es dÃ©bordent sur le disque, ce qui ralentit lâ€™exÃ©cution.\n\n\n\n\nâ“ Q8. Quel deploy mode utiliser en production ?\n\nclient\n\ncluster\n\nlocal\n\nstandalone\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En mode cluster, le Driver tourne sur un worker du cluster, ce qui est plus robuste pour la production.\n\n\n\n\nâ“ Q9. Que fait lâ€™AQE (Adaptive Query Execution) ?\n\nCompile le code Python\n\nOptimise le plan dâ€™exÃ©cution au runtime\n\nCompresse les donnÃ©es\n\nGÃ¨re lâ€™authentification\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” AQE optimise dynamiquement le plan dâ€™exÃ©cution pendant lâ€™exÃ©cution (coalesce, skew handling, broadcast).\n\n\n\n\nâ“ Q10. Combien de cores par executor est recommandÃ© ?\n\n1 core\n\n4-5 cores\n\n10-15 cores\n\nTous les cores disponibles\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” 4-5 cores est le sweet spot. Plus de cores = heap plus grand = GC moins efficace.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#ressources-pour-aller-plus-loin",
    "title": "PySpark Advanced",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark SQL Guide\nSpark Configuration\nTuning Spark\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Spark Performance Tuning\nUnderstanding Spark Shuffle\n\n\n\nğŸ”§ Outils\n\nSpark UI â€” Diagnostic local\nspark-submit â€” Guide officiel",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/19_pyspark_advanced.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/19_pyspark_advanced.html#prochaine-Ã©tape",
    "title": "PySpark Advanced",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises lâ€™optimisation Spark, passons aux fonctionnalitÃ©s SQL avancÃ©es !\nğŸ‘‰ Module suivant : 20_spark_sql_deep_dive â€” Spark SQL Deep Dive\nTu vas apprendre :\n\nWindow functions avancÃ©es\nCTEs et subqueries\nOptimisation SQL\nSpark SQL vs DataFrame API\n\n\n\nâš ï¸ Note : Spark Streaming sera couvert dans le module 24 Kafka.\n\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nArchitecture\nCatalyst, Tungsten, DAG\n\n\nspark-submit\nDeploy modes, packaging, structure projet\n\n\nPartitionnement\nShuffle, repartition vs coalesce, skew\n\n\nCaching\ncache() vs persist(), storage levels\n\n\nJoins\nBroadcast, Sort Merge, hints\n\n\nI/O\nParquet, partitionnement disque, schemas\n\n\nUDFs\nÃ‰viter Python UDF, expressions natives\n\n\nTuning\nAQE, executors/cores, configuration\n\n\nDiagnostic\nSpark UI, mÃ©triques\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module PySpark Advanced.\n\n\nVoir le code\n# Nettoyage\nspark.stop()\nprint(\"âœ… SparkSession arrÃªtÃ©e\")\n\n# Nettoyage des fichiers temporaires (optionnel)\n# import shutil\n# for path in [\"/tmp/transactions\", \"/tmp/products\", \"/tmp/customers\", \n#              \"/tmp/output\", \"/tmp/output_optimized\", \"/tmp/test_parquet\", \"/tmp/partitioned_data\"]:\n#     if os.path.exists(path):\n#         shutil.rmtree(path)\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "19 Â· PySpark AvancÃ©"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html",
    "href": "notebooks/intermediate/24_kafka_streaming.html",
    "title": "Kafka, Python & Structured Streaming",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le streaming de donnÃ©es â€” la capacitÃ© Ã  traiter des flux continus dâ€™Ã©vÃ©nements en temps rÃ©el plutÃ´t que des batches pÃ©riodiques.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#le-temps-rÃ©el-pour-le-data-engineering",
    "href": "notebooks/intermediate/24_kafka_streaming.html#le-temps-rÃ©el-pour-le-data-engineering",
    "title": "Kafka, Python & Structured Streaming",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le streaming de donnÃ©es â€” la capacitÃ© Ã  traiter des flux continus dâ€™Ã©vÃ©nements en temps rÃ©el plutÃ´t que des batches pÃ©riodiques.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#prÃ©requis",
    "href": "notebooks/intermediate/24_kafka_streaming.html#prÃ©requis",
    "title": "Kafka, Python & Structured Streaming",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nModule\nCompÃ©tence\nPourquoi ?\n\n\n\n\nâœ… 19\nPySpark Advanced\nDataFrame API\n\n\nâœ… 21\nSpark on K8s\nDÃ©ploiement\n\n\nâœ… 23\nTable Formats\nDelta Lake comme Sink",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#objectifs",
    "href": "notebooks/intermediate/24_kafka_streaming.html#objectifs",
    "title": "Kafka, Python & Structured Streaming",
    "section": "Objectifs",
    "text": "Objectifs\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les architectures de streaming (Lambda vs Kappa)\nDÃ©ployer Apache Kafka et crÃ©er des topics\nÃ‰crire des producteurs/consommateurs Python natifs\nConstruire des pipelines Spark Structured Streaming\nMaÃ®triser Watermarks et Windowing pour le temps dâ€™Ã©vÃ©nement\nUtiliser foreachBatch + MERGE INTO pour des sinks transactionnels",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#introduction-pourquoi-le-temps-rÃ©el",
    "href": "notebooks/intermediate/24_kafka_streaming.html#introduction-pourquoi-le-temps-rÃ©el",
    "title": "Kafka, Python & Structured Streaming",
    "section": "1. Introduction â€” Pourquoi le Temps RÃ©el ?",
    "text": "1. Introduction â€” Pourquoi le Temps RÃ©el ?\n\n1.1 Lâ€™Ã©volution des architectures de donnÃ©es\nHistoriquement, le traitement de donnÃ©es Ã©tait batch : on collecte les donnÃ©es pendant X heures, puis on les traite. Mais les besoins modernes exigent une latence plus faible.\nARCHITECTURE LAMBDA (2010s)           ARCHITECTURE KAPPA (2020s)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                             â”‚      â”‚                             â”‚\nâ”‚  Source â”€â”€â”¬â”€â”€ Batch Layer   â”‚      â”‚  Source â”€â”€â”€ Stream Layer    â”‚\nâ”‚           â”‚   (Spark Batch) â”‚      â”‚             (Kafka + SSS)   â”‚\nâ”‚           â”‚        â”‚        â”‚      â”‚                  â”‚          â”‚\nâ”‚           â””â”€â”€ Speed Layer   â”‚      â”‚                  â”‚          â”‚\nâ”‚               (Storm)       â”‚      â”‚                  â”‚          â”‚\nâ”‚                  â”‚          â”‚      â”‚                  â”‚          â”‚\nâ”‚           Serving Layer     â”‚      â”‚           Data Lake         â”‚\nâ”‚                             â”‚      â”‚           (Delta)           â”‚\nâ”‚  âš ï¸ 2 codebases Ã  maintenir â”‚      â”‚  âœ… 1 seul pipeline         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Cas dâ€™usage du temps rÃ©el\n\n\n\nDomaine\nExemple\nLatence requise\n\n\n\n\nFraude\nDÃ©tecter transaction suspecte\n&lt; 1 seconde\n\n\nIoT\nAlerter si capteur anormal\n&lt; 5 secondes\n\n\nE-commerce\nRecommandations live\n&lt; 100 ms\n\n\nMonitoring\nAlerter si service down\n&lt; 30 secondes\n\n\nFinance\nTrading algorithmique\n&lt; 10 ms\n\n\n\n\n\n1.3 Batch vs Streaming : Les diffÃ©rences fondamentales\n\n\n\nAspect\nBatch\nStreaming\n\n\n\n\nDonnÃ©es\nBornÃ©es (bounded)\nNon-bornÃ©es (unbounded)\n\n\nTraitement\nPÃ©riodique (horaire, quotidien)\nContinu\n\n\nLatence\nMinutes Ã  heures\nSecondes Ã  millisecondes\n\n\nÃ‰tat\nRecalculÃ© Ã  chaque run\nMaintenu entre Ã©vÃ©nements\n\n\nComplexitÃ©\nPlus simple\nPlus complexe (temps, Ã©tat)\n\n\n\n\n\n1.4 Micro-batch vs Continuous\nIl existe deux modÃ¨les de traitement streaming :\nMICRO-BATCH (Spark SSS dÃ©faut)        CONTINUOUS (Flink, Kafka Streams)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                             â”‚      â”‚                             â”‚\nâ”‚  â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”  â”‚      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚\nâ”‚  â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚  â”‚      â”‚  Traitement Ã©vÃ©nement par   â”‚\nâ”‚  â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜  â”‚      â”‚  Ã©vÃ©nement                  â”‚\nâ”‚  Batches de 100ms-1s       â”‚      â”‚                             â”‚\nâ”‚                             â”‚      â”‚                             â”‚\nâ”‚  Latence: ~1s              â”‚      â”‚  Latence: ~10ms             â”‚\nâ”‚  Throughput: TrÃ¨s Ã©levÃ©    â”‚      â”‚  Throughput: Ã‰levÃ©          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nSpark Structured Streaming utilise le micro-batch par dÃ©faut (suffisant pour 90% des cas).\n\n\n1.5 Garanties de livraison\nUn concept crucial en streaming : que se passe-t-il si le systÃ¨me plante ?\n\n\n\n\n\n\n\n\nGarantie\nDescription\nQuand lâ€™utiliser\n\n\n\n\nAt-most-once\nMessage traitÃ© 0 ou 1 fois\nLogs non critiques\n\n\nAt-least-once\nMessage traitÃ© 1+ fois (doublons possibles)\nCompteurs, mÃ©triques\n\n\nExactly-once\nMessage traitÃ© exactement 1 fois\nTransactions financiÃ¨res\n\n\n\nAT-MOST-ONCE              AT-LEAST-ONCE           EXACTLY-ONCE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Fire & Forgetâ”‚          â”‚ Retry until â”‚         â”‚ Transactionalâ”‚\nâ”‚             â”‚          â”‚ ACK         â”‚         â”‚ + Idempotent â”‚\nâ”‚ Peut perdre â”‚          â”‚ Peut dupliquerâ”‚        â”‚ Parfait     â”‚\nâ”‚ des messagesâ”‚          â”‚ des messages â”‚         â”‚             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     ğŸ˜¢                       ğŸ˜                      ğŸ¯\nExactly-once est le Saint Graal, mais plus complexe Ã  implÃ©menter. Spark SSS + Kafka + Delta Lake peuvent lâ€™atteindre !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#apache-kafka-le-bus-de-messagerie",
    "href": "notebooks/intermediate/24_kafka_streaming.html#apache-kafka-le-bus-de-messagerie",
    "title": "Kafka, Python & Structured Streaming",
    "section": "2. Apache Kafka â€” Le Bus de Messagerie",
    "text": "2. Apache Kafka â€” Le Bus de Messagerie\n\n2.1 Quâ€™est-ce que Kafka ?\nApache Kafka est une plateforme de streaming distribuÃ©e crÃ©Ã©e par LinkedIn en 2011. Câ€™est devenu le standard de facto pour le streaming de donnÃ©es.\nKafka nâ€™est PAS une base de donnÃ©es, mais un log distribuÃ© oÃ¹ les messages sont :\n\nÃ‰crits de maniÃ¨re append-only (jamais modifiÃ©s)\nPersistÃ©s sur disque (pas juste en mÃ©moire)\nRÃ©pliquÃ©s pour la tolÃ©rance aux pannes\n\n\n\n2.2 Architecture de Kafka\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      KAFKA CLUSTER                              â”‚\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\nâ”‚  â”‚  Broker 1   â”‚  â”‚  Broker 2   â”‚  â”‚  Broker 3   â”‚            â”‚\nâ”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚            â”‚\nâ”‚  â”‚ Topic A P0  â”‚  â”‚ Topic A P1  â”‚  â”‚ Topic A P2  â”‚  â† Partitionsâ”‚\nâ”‚  â”‚ Topic B P1  â”‚  â”‚ Topic B P0  â”‚  â”‚ Topic B P2  â”‚    rÃ©parties â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\nâ”‚         â”‚                â”‚                â”‚                    â”‚\nâ”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\nâ”‚                          â”‚                                     â”‚\nâ”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚\nâ”‚                  â”‚  ZooKeeper/   â”‚  â† Coordination             â”‚\nâ”‚                  â”‚  KRaft        â”‚    (metadata, leaders)      â”‚\nâ”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â–²                                          â”‚\n         â”‚                                          â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ Producers â”‚                            â”‚ Consumers â”‚\n   â”‚ (Python)  â”‚                            â”‚ (Spark)   â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.3 Concepts clÃ©s\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nBroker\nServeur Kafka qui stocke les messages\n\n\nTopic\nCatÃ©gorie/flux de messages (comme une table)\n\n\nPartition\nSous-division dâ€™un topic pour le parallÃ©lisme\n\n\nOffset\nPosition dâ€™un message dans une partition\n\n\nProducer\nApplication qui envoie des messages\n\n\nConsumer\nApplication qui lit des messages\n\n\nConsumer Group\nGroupe de consumers qui se partagent les partitions\n\n\n\n\n\n2.4 Topics et Partitions\nTopic: \"orders\" avec 3 partitions\n\nPartition 0:  [msg0] [msg3] [msg6] [msg9]  ...  â†’ Offset croissant\nPartition 1:  [msg1] [msg4] [msg7] [msg10] ...\nPartition 2:  [msg2] [msg5] [msg8] [msg11] ...\n\nChaque partition :\nâ€¢ Est ordonnÃ©e (FIFO dans la partition)\nâ€¢ Peut Ãªtre lue par UN consumer du groupe\nâ€¢ Est rÃ©pliquÃ©e sur plusieurs brokers\nClÃ© de message : DÃ©termine la partition. Messages avec la mÃªme clÃ© â†’ mÃªme partition â†’ ordre garanti.\n\n\n2.5 Installation Kafka avec Docker\nNous allons dÃ©ployer Kafka en local avec Docker Compose. Deux options :\n\nZooKeeper : Mode classique (stable)\nKRaft : Nouveau mode sans ZooKeeper (Kafka 3.3+)\n\n\n\nVoir le code\n# Docker Compose pour Kafka avec ZooKeeper\n\ndocker_compose_kafka = '''\nversion: \"3.8\"\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    container_name: zookeeper\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n      ZOOKEEPER_TICK_TIME: 2000\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    container_name: kafka\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"      # Pour les clients externes\n      - \"29092:29092\"    # Pour les clients Docker internes\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n\n  # Schema Registry (optionnel mais recommandÃ©)\n  schema-registry:\n    image: confluentinc/cp-schema-registry:7.5.0\n    container_name: schema-registry\n    depends_on:\n      - kafka\n    ports:\n      - \"8081:8081\"\n    environment:\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092\n'''\n\nprint(docker_compose_kafka)\nprint(\"\\n# DÃ©marrer avec : docker-compose up -d\")\nprint(\"# VÃ©rifier : docker-compose ps\")\n\n\n\n\nVoir le code\n# Commandes CLI Kafka essentielles\n\nkafka_cli = '''\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Gestion des Topics\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# CrÃ©er un topic\ndocker exec kafka kafka-topics --create \\\n  --bootstrap-server localhost:9092 \\\n  --topic orders \\\n  --partitions 3 \\\n  --replication-factor 1\n\n# Lister les topics\ndocker exec kafka kafka-topics --list \\\n  --bootstrap-server localhost:9092\n\n# DÃ©crire un topic\ndocker exec kafka kafka-topics --describe \\\n  --bootstrap-server localhost:9092 \\\n  --topic orders\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Produire et Consommer (test rapide)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Produire des messages (interactif)\ndocker exec -it kafka kafka-console-producer \\\n  --bootstrap-server localhost:9092 \\\n  --topic orders\n\n# Consommer des messages (depuis le dÃ©but)\ndocker exec kafka kafka-console-consumer \\\n  --bootstrap-server localhost:9092 \\\n  --topic orders \\\n  --from-beginning\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Consumer Groups\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Lister les consumer groups\ndocker exec kafka kafka-consumer-groups --list \\\n  --bootstrap-server localhost:9092\n\n# Voir le lag d'un group\ndocker exec kafka kafka-consumer-groups --describe \\\n  --bootstrap-server localhost:9092 \\\n  --group my-consumer-group\n'''\n\nprint(kafka_cli)\n\n\n\n\nExercice 1 : DÃ©ployer Kafka et crÃ©er un topic\nObjectif : Mettre en place lâ€™infrastructure Kafka.\n# 1. CrÃ©er docker-compose.yml avec le contenu ci-dessus\n\n# 2. DÃ©marrer Kafka\ndocker-compose up -d\n\n# 3. CrÃ©er un topic \"events\" avec 3 partitions\n# TODO\n\n# 4. VÃ©rifier que le topic existe\n# TODO\n\n# 5. Tester avec console-producer et console-consumer\n# TODO\n\n\nğŸ’¡ Solution\n\n# 3. CrÃ©er topic\ndocker exec kafka kafka-topics --create \\\n  --bootstrap-server localhost:9092 \\\n  --topic events --partitions 3 --replication-factor 1\n\n# 4. VÃ©rifier\ndocker exec kafka kafka-topics --describe \\\n  --bootstrap-server localhost:9092 --topic events\n\n# 5. Test\n# Terminal 1: docker exec -it kafka kafka-console-producer --bootstrap-server localhost:9092 --topic events\n# Terminal 2: docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic events --from-beginning",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#schema-registry-sÃ©rialisation",
    "href": "notebooks/intermediate/24_kafka_streaming.html#schema-registry-sÃ©rialisation",
    "title": "Kafka, Python & Structured Streaming",
    "section": "3. Schema Registry & SÃ©rialisation",
    "text": "3. Schema Registry & SÃ©rialisation\n\n3.1 Le problÃ¨me de la sÃ©rialisation\nKafka transporte des bytes. Il faut donc sÃ©rialiser/dÃ©sÃ©rialiser les donnÃ©es. Trois formats populaires :\n\n\n\nFormat\nAvantages\nInconvÃ©nients\n\n\n\n\nJSON\nLisible, flexible\nVerbeux, pas de schÃ©ma\n\n\nAvro\nCompact, schÃ©ma, Ã©volution\nMoins lisible\n\n\nProtobuf\nTrÃ¨s compact, typage fort\nPlus complexe\n\n\n\n\n\n3.2 Pourquoi un Schema Registry ?\nSans Schema Registry, chaque producteur/consommateur doit connaÃ®tre le schÃ©ma. ProblÃ¨mes :\n\nComment Ã©voluer le schÃ©ma sans casser les consumers ?\nComment valider que les messages sont conformes ?\n\nSANS SCHEMA REGISTRY              AVEC SCHEMA REGISTRY\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Producer            â”‚          â”‚ Producer            â”‚\nâ”‚ {schÃ©ma hardcodÃ©}   â”‚          â”‚ â†’ Enregistre schÃ©ma â”‚\nâ”‚         â”‚           â”‚          â”‚ â†’ Envoie schema_id  â”‚\nâ”‚         â–¼           â”‚          â”‚         â”‚           â”‚\nâ”‚    [message]        â”‚          â”‚    [id + message]   â”‚\nâ”‚         â”‚           â”‚          â”‚         â”‚           â”‚\nâ”‚         â–¼           â”‚          â”‚         â–¼           â”‚\nâ”‚ Consumer            â”‚          â”‚ Consumer            â”‚\nâ”‚ {schÃ©ma hardcodÃ©}   â”‚          â”‚ â†’ RÃ©cupÃ¨re schÃ©ma   â”‚\nâ”‚                     â”‚          â”‚   par id            â”‚\nâ”‚ ğŸ˜° SchÃ©ma dÃ©sync!   â”‚          â”‚ âœ… Toujours Ã  jour  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.3 CompatibilitÃ© de schÃ©ma\nLe Schema Registry vÃ©rifie la compatibilitÃ© lors de lâ€™Ã©volution :\n\n\n\n\n\n\n\n\nMode\nDescription\nExemple autorisÃ©\n\n\n\n\nBACKWARD\nNouveau schÃ©ma peut lire ancien\nAjouter champ optionnel\n\n\nFORWARD\nAncien schÃ©ma peut lire nouveau\nSupprimer champ optionnel\n\n\nFULL\nLes deux\nAjouter/supprimer champ optionnel\n\n\nNONE\nPas de vÃ©rification\nTout (dangereux)\n\n\n\n\n\nVoir le code\n# Exemple de schÃ©ma Avro\n\navro_schema = '''\n{\n  \"type\": \"record\",\n  \"name\": \"Order\",\n  \"namespace\": \"com.example\",\n  \"fields\": [\n    {\"name\": \"order_id\", \"type\": \"string\"},\n    {\"name\": \"customer_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"},\n    {\"name\": \"timestamp\", \"type\": \"long\", \"logicalType\": \"timestamp-millis\"},\n    {\"name\": \"status\", \"type\": {\"type\": \"enum\", \"name\": \"Status\", \"symbols\": [\"PENDING\", \"COMPLETED\", \"CANCELLED\"]}},\n    {\"name\": \"notes\", \"type\": [\"null\", \"string\"], \"default\": null}  // Champ optionnel\n  ]\n}\n'''\n\nprint(\"SchÃ©ma Avro pour les commandes :\")\nprint(avro_schema)\nprint(\"\\nğŸ’¡ Points clÃ©s :\")\nprint(\"â€¢ 'type': 'record' â†’ structure comme une classe\")\nprint(\"â€¢ 'logicalType' â†’ types avancÃ©s (timestamp, date, decimal)\")\nprint(\"â€¢ ['null', 'string'] â†’ champ optionnel (union type)\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#python-natif-pour-kafka",
    "href": "notebooks/intermediate/24_kafka_streaming.html#python-natif-pour-kafka",
    "title": "Kafka, Python & Structured Streaming",
    "section": "4. Python Natif pour Kafka",
    "text": "4. Python Natif pour Kafka\nAvant dâ€™utiliser Spark, apprenons Ã  interagir avec Kafka en Python pur. Deux librairies principales :\n\n\n\nLibrairie\nAvantages\nQuand lâ€™utiliser\n\n\n\n\nkafka-python\nSimple, pur Python\nScripts simples, prototypage\n\n\nconfluent-kafka\nPerformant, Schema Registry\nProduction, Avro\n\n\n\n\n4.1 Installation\n\n\nVoir le code\n# Installation des librairies Kafka Python\n# !pip install kafka-python confluent-kafka fastavro\n\nprint(\"Librairies Ã  installer :\")\nprint(\"pip install kafka-python       # Client simple\")\nprint(\"pip install confluent-kafka    # Client performant + Schema Registry\")\nprint(\"pip install fastavro           # SÃ©rialisation Avro\")\n\n\n\n\n4.2 Producteur Python (kafka-python)\n\n\nVoir le code\n# Producteur Kafka simple avec kafka-python\n\nproducer_simple = '''\nfrom kafka import KafkaProducer\nimport json\nimport time\nfrom datetime import datetime\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Configuration du producteur\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    \n    # SÃ©rialisation JSON\n    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n    key_serializer=lambda k: k.encode('utf-8') if k else None,\n    \n    # Configuration de fiabilitÃ©\n    acks='all',              # Attendre confirmation de tous les replicas\n    retries=3,               # RÃ©essayer en cas d'erreur\n    retry_backoff_ms=100,    # DÃ©lai entre retries\n)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Envoyer des messages\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef send_order(order_id, customer_id, amount):\n    \"\"\"Envoyer une commande au topic 'orders'\"\"\"\n    message = {\n        \"order_id\": order_id,\n        \"customer_id\": customer_id,\n        \"amount\": amount,\n        \"timestamp\": datetime.now().isoformat(),\n        \"status\": \"PENDING\"\n    }\n    \n    # Envoyer avec une clÃ© (mÃªme customer â†’ mÃªme partition â†’ ordre garanti)\n    future = producer.send(\n        topic='orders',\n        key=customer_id,    # ClÃ© pour le partitionnement\n        value=message\n    )\n    \n    # Attendre confirmation (synchrone)\n    try:\n        record_metadata = future.get(timeout=10)\n        print(f\"âœ… Message envoyÃ©: partition={record_metadata.partition}, offset={record_metadata.offset}\")\n    except Exception as e:\n        print(f\"âŒ Erreur: {e}\")\n\n# Simuler un flux de commandes\nfor i in range(10):\n    send_order(\n        order_id=f\"ORD-{i:04d}\",\n        customer_id=f\"CUST-{i % 3:03d}\",  # 3 customers\n        amount=round(100 + i * 10.5, 2)\n    )\n    time.sleep(0.5)\n\n# Important : flush avant de quitter\nproducer.flush()\nproducer.close()\n'''\n\nprint(producer_simple)\n\n\n\n\n4.3 Consommateur Python (kafka-python)\n\n\nVoir le code\n# Consommateur Kafka simple avec kafka-python\n\nconsumer_simple = '''\nfrom kafka import KafkaConsumer\nimport json\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Configuration du consommateur\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nconsumer = KafkaConsumer(\n    'orders',                             # Topic(s) Ã  consommer\n    bootstrap_servers=['localhost:9092'],\n    \n    # DÃ©sÃ©rialisation JSON\n    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n    key_deserializer=lambda k: k.decode('utf-8') if k else None,\n    \n    # Consumer Group (pour le parallÃ©lisme)\n    group_id='order-processor-group',\n    \n    # OÃ¹ commencer Ã  lire\n    auto_offset_reset='earliest',  # 'earliest' = depuis le dÃ©but, 'latest' = nouveaux messages seulement\n    \n    # Commit des offsets\n    enable_auto_commit=True,       # Commit automatique\n    auto_commit_interval_ms=5000,  # Toutes les 5 secondes\n)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Boucle de consommation\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"ğŸ§ En attente de messages...\")\n\ntry:\n    for message in consumer:\n        # MÃ©tadonnÃ©es du message\n        print(f\"\\nğŸ“© Message reÃ§u:\")\n        print(f\"   Topic: {message.topic}\")\n        print(f\"   Partition: {message.partition}\")\n        print(f\"   Offset: {message.offset}\")\n        print(f\"   Key: {message.key}\")\n        print(f\"   Timestamp: {message.timestamp}\")\n        \n        # Contenu du message\n        order = message.value\n        print(f\"   Order: {order}\")\n        \n        # Logique mÃ©tier : alerter si montant Ã©levÃ©\n        if order.get('amount', 0) &gt; 500:\n            print(f\"   âš ï¸ ALERTE: Commande de {order['amount']}â‚¬ !\")\n\nexcept KeyboardInterrupt:\n    print(\"\\nğŸ›‘ ArrÃªt du consommateur\")\nfinally:\n    consumer.close()\n'''\n\nprint(consumer_simple)\n\n\n\n\n4.4 Producteur avec Avro et Schema Registry (confluent-kafka)\n\n\nVoir le code\n# Producteur avec Avro et Schema Registry\n\nproducer_avro = '''\nfrom confluent_kafka import Producer\nfrom confluent_kafka.schema_registry import SchemaRegistryClient\nfrom confluent_kafka.schema_registry.avro import AvroSerializer\nfrom confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Configuration Schema Registry\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nschema_registry_conf = {'url': 'http://localhost:8081'}\nschema_registry_client = SchemaRegistryClient(schema_registry_conf)\n\n# SchÃ©ma Avro\norder_schema = \"\"\"\n{\n  \"type\": \"record\",\n  \"name\": \"Order\",\n  \"fields\": [\n    {\"name\": \"order_id\", \"type\": \"string\"},\n    {\"name\": \"customer_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"},\n    {\"name\": \"timestamp\", \"type\": \"long\"}\n  ]\n}\n\"\"\"\n\n# SÃ©rialiseurs\navro_serializer = AvroSerializer(\n    schema_registry_client,\n    order_schema,\n    lambda obj, ctx: obj  # Conversion dict â†’ Avro\n)\nstring_serializer = StringSerializer('utf-8')\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Configuration producteur\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nproducer_conf = {\n    'bootstrap.servers': 'localhost:9092',\n    'acks': 'all'\n}\nproducer = Producer(producer_conf)\n\n# Callback de confirmation\ndef delivery_report(err, msg):\n    if err:\n        print(f\"âŒ Erreur: {err}\")\n    else:\n        print(f\"âœ… Message livrÃ©: {msg.topic()}[{msg.partition()}] @ {msg.offset()}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Envoyer des messages\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport time\n\nfor i in range(5):\n    order = {\n        \"order_id\": f\"ORD-{i:04d}\",\n        \"customer_id\": f\"CUST-{i % 3:03d}\",\n        \"amount\": 100.0 + i * 25.5,\n        \"timestamp\": int(time.time() * 1000)\n    }\n    \n    producer.produce(\n        topic='orders-avro',\n        key=string_serializer(order[\"customer_id\"]),\n        value=avro_serializer(order, SerializationContext('orders-avro', MessageField.VALUE)),\n        callback=delivery_report\n    )\n    producer.poll(0)  # DÃ©clencher les callbacks\n\nproducer.flush()  # Attendre que tous les messages soient envoyÃ©s\n'''\n\nprint(producer_avro)\n\n\n\n\nExercice 2 : Producteur et Consommateur Python\nObjectif : CrÃ©er un systÃ¨me dâ€™alertes en temps rÃ©el.\n# 1. CrÃ©er un producteur qui envoie des logs au format:\n# {\"level\": \"INFO|WARN|ERROR|FATAL\", \"message\": \"...\", \"timestamp\": \"...\"}\n\n# 2. CrÃ©er un consommateur qui:\n#    - Affiche tous les messages\n#    - Alerte (print spÃ©cial) si level == \"FATAL\"\n\n# TODO: ImplÃ©menter\n\n\nğŸ’¡ Solution\n\n# Producer\nimport random\nlevels = [\"INFO\", \"INFO\", \"INFO\", \"WARN\", \"ERROR\", \"FATAL\"]\nfor i in range(20):\n    log = {\n        \"level\": random.choice(levels),\n        \"message\": f\"Event {i}\",\n        \"timestamp\": datetime.now().isoformat()\n    }\n    producer.send('logs', value=log)\n\n# Consumer\nfor msg in consumer:\n    log = msg.value\n    if log['level'] == 'FATAL':\n        print(f\"ğŸš¨ FATAL ALERT: {log['message']}\")\n    else:\n        print(f\"[{log['level']}] {log['message']}\")\n\n\n\n4.5 AperÃ§u de Faust (Stream Processing Python)\nFaust est un framework Python pour le traitement de streams, inspirÃ© de Kafka Streams (Java). IdÃ©al pour du traitement lÃ©ger sans Spark.\n\n\nVoir le code\n# Exemple Faust (aperÃ§u)\n\nfaust_example = '''\nimport faust\n\n# CrÃ©er l'application Faust\napp = faust.App(\n    'order-processor',\n    broker='kafka://localhost:9092',\n    value_serializer='json'\n)\n\n# DÃ©finir le schÃ©ma du message\nclass Order(faust.Record):\n    order_id: str\n    customer_id: str\n    amount: float\n\n# Topic source\norders_topic = app.topic('orders', value_type=Order)\n\n# Agent de traitement (comme un consumer intelligent)\n@app.agent(orders_topic)\nasync def process_orders(orders):\n    async for order in orders:\n        print(f\"Processing: {order.order_id}\")\n        \n        # Logique mÃ©tier\n        if order.amount &gt; 1000:\n            print(f\"âš ï¸ High-value order: {order.amount}\")\n            # Envoyer vers un autre topic\n            await high_value_topic.send(value=order)\n\n# Table pour agrÃ©gation (state)\norder_counts = app.Table('order-counts', default=int)\n\n@app.agent(orders_topic)\nasync def count_by_customer(orders):\n    async for order in orders:\n        order_counts[order.customer_id] += 1\n        print(f\"{order.customer_id}: {order_counts[order.customer_id]} orders\")\n\n# Lancer avec: faust -A myapp worker -l info\n'''\n\nprint(faust_example)\nprint(\"\\nğŸ’¡ Quand utiliser Faust vs Spark SSS :\")\nprint(\"â€¢ Faust : Traitement lÃ©ger, alertes, routing, &lt; 100K events/s\")\nprint(\"â€¢ Spark SSS : AgrÃ©gations complexes, ML, joins, gros volumes\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#spark-structured-streaming-sss",
    "href": "notebooks/intermediate/24_kafka_streaming.html#spark-structured-streaming-sss",
    "title": "Kafka, Python & Structured Streaming",
    "section": "5. Spark Structured Streaming (SSS)",
    "text": "5. Spark Structured Streaming (SSS)\n\n5.1 Le modÃ¨le de programmation\nLâ€™idÃ©e gÃ©niale de Spark Structured Streaming : traiter un stream comme un DataFrame qui grandit Ã  lâ€™infini.\n                     Temps â†’\n                     \nStream d'Ã©vÃ©nements: [e1] [e2] [e3] [e4] [e5] [e6] ...\n                      â”‚    â”‚    â”‚    â”‚    â”‚    â”‚\n                      â–¼    â–¼    â–¼    â–¼    â–¼    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              DataFrame \"illimitÃ©\"                   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚ e1  â”‚ e2  â”‚ e3  â”‚ e4  â”‚ e5  â”‚ e6  â”‚ ... â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                     â”‚\nâ”‚  Tu Ã©cris le mÃªme code que pour un batch !          â”‚\nâ”‚  df.filter().groupBy().agg()                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nAvantage : Tu utilises la mÃªme API DataFrame que tu connais dÃ©jÃ  !\n\n\nVoir le code\n# Configuration Spark pour le streaming avec Kafka\n\nspark_streaming_config = '''\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Kafka Streaming Demo\") \\\n    .config(\"spark.jars.packages\", \n            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n            \"io.delta:delta-spark_2.12:3.1.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# RÃ©duire les logs pour plus de clartÃ©\nspark.sparkContext.setLogLevel(\"WARN\")\n'''\n\nprint(spark_streaming_config)\n\n\n\n\n5.2 Lire depuis Kafka avec readStream\n\n\nVoir le code\n# Lire un stream Kafka\n\nread_kafka = '''\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Lecture du stream Kafka\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nkafka_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"orders\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n\n# Le DataFrame brut contient ces colonnes :\n# key (binary), value (binary), topic, partition, offset, timestamp, timestampType\n\nprint(\"SchÃ©ma Kafka brut:\")\nkafka_df.printSchema()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DÃ©sÃ©rialisation JSON\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# DÃ©finir le schÃ©ma du message JSON\norder_schema = StructType([\n    StructField(\"order_id\", StringType(), True),\n    StructField(\"customer_id\", StringType(), True),\n    StructField(\"amount\", DoubleType(), True),\n    StructField(\"timestamp\", StringType(), True),\n    StructField(\"status\", StringType(), True)\n])\n\n# Parser le JSON\norders_df = kafka_df \\\n    .selectExpr(\"CAST(key AS STRING) as customer_key\", \"CAST(value AS STRING) as json_value\") \\\n    .select(\n        col(\"customer_key\"),\n        from_json(col(\"json_value\"), order_schema).alias(\"data\")\n    ) \\\n    .select(\"customer_key\", \"data.*\")\n\nprint(\"SchÃ©ma aprÃ¨s parsing:\")\norders_df.printSchema()\n'''\n\nprint(read_kafka)\n\n\n\n\n5.3 Output Modes et Sinks\n\n\n\n\n\n\n\n\nOutput Mode\nDescription\nQuand lâ€™utiliser\n\n\n\n\nAppend\nSeulement les nouvelles lignes\nSans agrÃ©gation\n\n\nComplete\nToute la table rÃ©sultat\nAvec agrÃ©gation, petits rÃ©sultats\n\n\nUpdate\nSeulement les lignes modifiÃ©es\nAvec agrÃ©gation, grands rÃ©sultats\n\n\n\n\n\nVoir le code\n# Ã‰crire le stream (diffÃ©rents sinks)\n\nwrite_stream = '''\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Sink Console (pour debug)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nquery_console = orders_df.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"append\") \\\n    .option(\"truncate\", False) \\\n    .start()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Sink Parquet/Delta avec Checkpointing\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nquery_delta = orders_df.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoints/orders\") \\\n    .option(\"path\", \"s3a://silver/orders_streaming/\") \\\n    .start()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Sink Kafka (pour pipeline)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nquery_kafka = orders_df \\\n    .selectExpr(\"customer_id AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"topic\", \"orders-processed\") \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoints/orders-kafka\") \\\n    .start()\n\n# Attendre la terminaison\nquery_console.awaitTermination()\n'''\n\nprint(write_stream)\nprint(\"\\nğŸ’¡ Le checkpointLocation est CRUCIAL pour :\")\nprint(\"â€¢ Reprendre aprÃ¨s un crash (exactly-once)\")\nprint(\"â€¢ Stocker l'Ã©tat des agrÃ©gations\")\nprint(\"â€¢ Suivre les offsets Kafka\")\n\n\n\n\nExercice 3 : Pipeline Kafka â†’ Spark SSS â†’ Console\n# 1. Lire le topic 'events' crÃ©Ã© Ã  l'exercice 1\n# 2. Parser le JSON\n# 3. Filtrer les events avec level = 'ERROR' ou 'FATAL'\n# 4. Afficher dans la console\n\n\nğŸ’¡ Solution\n\nschema = StructType([\n    StructField(\"level\", StringType()),\n    StructField(\"message\", StringType()),\n    StructField(\"timestamp\", StringType())\n])\n\nevents_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"events\") \\\n    .load() \\\n    .selectExpr(\"CAST(value AS STRING)\") \\\n    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .filter(col(\"level\").isin(\"ERROR\", \"FATAL\"))\n\nquery = events_df.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"append\") \\\n    .start()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#gestion-du-temps-et-de-lÃ©tat",
    "href": "notebooks/intermediate/24_kafka_streaming.html#gestion-du-temps-et-de-lÃ©tat",
    "title": "Kafka, Python & Structured Streaming",
    "section": "6. Gestion du Temps et de lâ€™Ã‰tat",
    "text": "6. Gestion du Temps et de lâ€™Ã‰tat\n\n6.1 Le problÃ¨me du temps dâ€™Ã©vÃ©nement\nEn streaming, il y a deux temps diffÃ©rents :\n\n\n\n\n\n\n\n\nTemps\nDescription\nExemple\n\n\n\n\nEvent Time\nQuand lâ€™Ã©vÃ©nement sâ€™est produit\nTimestamp dans le message\n\n\nProcessing Time\nQuand Spark traite lâ€™Ã©vÃ©nement\nHeure du serveur\n\n\n\nProblÃ¨me des messages dÃ©sordonnÃ©s :\n\nEvent Time:    10:00   10:01   10:02   10:03   10:04\n                 â”‚       â”‚       â”‚       â”‚       â”‚\n                 â–¼       â–¼       â–¼       â–¼       â–¼\nArrivÃ©e:       [e1]    [e3]    [e2]    [e5]    [e4]   â† DÃ©sordonnÃ©s !\n                 â”‚       â”‚       â”‚       â”‚       â”‚\nProcessing:   10:05   10:05   10:06   10:06   10:07\n\nSi tu agrÃ¨ges par Processing Time â†’ rÃ©sultat faux\nSi tu agrÃ¨ges par Event Time â†’ rÃ©sultat correct\n\n\n6.2 Windowing (FenÃªtrage temporel)\nPour agrÃ©ger sur le temps, on utilise des fenÃªtres :\nTUMBLING WINDOW (non-chevauchantes)     SLIDING WINDOW (chevauchantes)\n\n   [â”€â”€â”€â”€5minâ”€â”€â”€â”€][â”€â”€â”€â”€5minâ”€â”€â”€â”€]         [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n                                             [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n   10:00      10:05      10:10                  [â”€â”€â”€â”€5minâ”€â”€â”€â”€]\n                                        \n   Chaque event appartient Ã             Chaque event peut appartenir\n   UNE SEULE fenÃªtre                    Ã  PLUSIEURS fenÃªtres\n\n\nVoir le code\n# Windowing avec Spark SSS\n\nwindowing_example = '''\nfrom pyspark.sql.functions import window, col, count, sum as spark_sum, to_timestamp\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PrÃ©parer le timestamp d'Ã©vÃ©nement\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\norders_with_ts = orders_df \\\n    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Tumbling Window : AgrÃ©gation par fenÃªtre de 5 minutes\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntumbling_agg = orders_with_ts \\\n    .groupBy(\n        window(col(\"event_time\"), \"5 minutes\"),  # FenÃªtre de 5 min\n        col(\"customer_id\")\n    ) \\\n    .agg(\n        count(\"*\").alias(\"order_count\"),\n        spark_sum(\"amount\").alias(\"total_amount\")\n    )\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Sliding Window : FenÃªtre de 10 min, glissant toutes les 5 min\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsliding_agg = orders_with_ts \\\n    .groupBy(\n        window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"),  # 10 min, slide 5 min\n        col(\"customer_id\")\n    ) \\\n    .agg(\n        count(\"*\").alias(\"order_count\")\n    )\n\n# Le rÃ©sultat contient une colonne \"window\" avec start/end\n'''\n\nprint(windowing_example)\n\n\n\n\n6.3 Watermarks : GÃ©rer les donnÃ©es en retard\nProblÃ¨me : Combien de temps attendre les messages en retard avant de fermer une fenÃªtre ?\nSans Watermark :                    Avec Watermark (10 min) :\n                                    \nÃ‰tat infini ! ğŸ˜±                    Ã‰tat limitÃ© âœ…\n                                    \nFenÃªtre 10:00-10:05                 FenÃªtre 10:00-10:05\n  â””â”€ Garde l'Ã©tat POUR TOUJOURS       â””â”€ Ferme Ã  10:15 (event time)\n     en attendant les retards            Late data aprÃ¨s â†’ ignorÃ©\n                                    \nMÃ©moire : EXPLOSE ğŸ’¥               MÃ©moire : Stable âœ…\n\n\nVoir le code\n# Watermarks avec Spark SSS\n\nwatermark_example = '''\nfrom pyspark.sql.functions import window, col, count, to_timestamp\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# AgrÃ©gation avec Watermark\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Le watermark dit : \"Je tolÃ¨re jusqu'Ã  10 minutes de retard\"\n# AprÃ¨s 10 min, les donnÃ©es en retard sont ignorÃ©es et l'Ã©tat nettoyÃ©\n\nwindowed_counts = orders_with_ts \\\n    .withWatermark(\"event_time\", \"10 minutes\") \\  # â† Crucial !\n    .groupBy(\n        window(col(\"event_time\"), \"5 minutes\"),\n        col(\"customer_id\")\n    ) \\\n    .count()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Comment Ã§a marche ?\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# 1. Spark suit le \"max event time\" vu jusqu'ici\n# 2. Watermark = max_event_time - 10 minutes\n# 3. Les fenÃªtres dont window.end &lt; watermark sont finalisÃ©es\n# 4. Les donnÃ©es avec event_time &lt; watermark sont ignorÃ©es\n#\n# Exemple :\n# - Max event time vu : 10:30\n# - Watermark : 10:20\n# - FenÃªtre 10:00-10:05 : FINALISÃ‰E (end 10:05 &lt; 10:20)\n# - Message avec event_time 10:18 : ACCEPTÃ‰\n# - Message avec event_time 10:15 : IGNORÃ‰ (&lt; watermark)\n\n# Ã‰crire en mode Update (pour les agrÃ©gations)\nquery = windowed_counts.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"update\") \\\n    .option(\"truncate\", False) \\\n    .start()\n'''\n\nprint(watermark_example)\nprint(\"\\nğŸ’¡ Choisir le bon watermark :\")\nprint(\"â€¢ Trop court (1 min) â†’ Perd des donnÃ©es en retard\")\nprint(\"â€¢ Trop long (1 heure) â†’ Trop de mÃ©moire utilisÃ©e\")\nprint(\"â€¢ RÃ¨gle : Analyser le retard typique de tes donnÃ©es\")\n\n\n\n\nExercice 4 : AgrÃ©gation avec Watermark\n# Calculer le nombre d'erreurs par fenÃªtre de 5 minutes\n# avec un watermark de 10 minutes\n\n# TODO: Lire depuis 'events', filtrer ERROR/FATAL, agrÃ©ger par window\n\n\nğŸ’¡ Solution\n\nerror_counts = events_df \\\n    .filter(col(\"level\").isin(\"ERROR\", \"FATAL\")) \\\n    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n    .withWatermark(\"event_time\", \"10 minutes\") \\\n    .groupBy(window(col(\"event_time\"), \"5 minutes\")) \\\n    .count()\n\nquery = error_counts.writeStream \\\n    .format(\"console\") \\\n    .outputMode(\"update\") \\\n    .start()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#opÃ©rations-avancÃ©es-et-sinks-transactionnelles",
    "href": "notebooks/intermediate/24_kafka_streaming.html#opÃ©rations-avancÃ©es-et-sinks-transactionnelles",
    "title": "Kafka, Python & Structured Streaming",
    "section": "7. OpÃ©rations AvancÃ©es et Sinks Transactionnelles",
    "text": "7. OpÃ©rations AvancÃ©es et Sinks Transactionnelles\n\n7.1 Streaming Joins\nSpark SSS supporte plusieurs types de joins :\n\n\n\n\n\n\n\n\nType\nDescription\nExemple\n\n\n\n\nStream-Static\nStream JOIN table batch\nEnrichir orders avec customers\n\n\nStream-Stream\nDeux streams\nJOIN clicks avec impressions\n\n\n\n\n\nVoir le code\n# Streaming Joins\n\njoin_examples = '''\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Stream-Static Join : Enrichir avec une dimension\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Table statique (dimension)\ncustomers_df = spark.read.parquet(\"s3a://dims/customers/\")\n\n# Stream\norders_stream = spark.readStream.format(\"kafka\")...\n\n# Join\nenriched_orders = orders_stream.join(\n    customers_df,\n    orders_stream.customer_id == customers_df.id,\n    \"left\"  # LEFT JOIN pour garder toutes les commandes\n)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Stream-Stream Join : Deux flux\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Deux streams\nclicks_stream = spark.readStream.format(\"kafka\").option(\"subscribe\", \"clicks\")...\nimpressions_stream = spark.readStream.format(\"kafka\").option(\"subscribe\", \"impressions\")...\n\n# Join avec watermark (obligatoire pour stream-stream)\nclicks_with_wm = clicks_stream \\\n    .withWatermark(\"click_time\", \"10 minutes\")\n\nimpressions_with_wm = impressions_stream \\\n    .withWatermark(\"impression_time\", \"10 minutes\")\n\n# Join avec condition de temps\nmatched = clicks_with_wm.join(\n    impressions_with_wm,\n    expr(\"\"\"\n        clicks.ad_id = impressions.ad_id AND\n        click_time &gt;= impression_time AND\n        click_time &lt;= impression_time + interval 1 hour\n    \"\"\"),\n    \"inner\"\n)\n'''\n\nprint(join_examples)\n\n\n\n\n7.2 foreachBatch : Le pont entre streaming et batch\nforeachBatch permet dâ€™appliquer nâ€™importe quelle logique batch sur chaque micro-batch. Câ€™est crucial pour les upserts avec MERGE INTO.\n\n\nVoir le code\n# foreachBatch avec MERGE INTO Delta\n\nforeach_batch_example = '''\nfrom delta.tables import DeltaTable\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Fonction de traitement par micro-batch\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef upsert_to_delta(micro_batch_df, batch_id):\n    \"\"\"\n    AppelÃ©e pour chaque micro-batch.\n    micro_batch_df : DataFrame Spark normal (pas streaming)\n    batch_id : Identifiant unique du batch\n    \"\"\"\n    print(f\"Processing batch {batch_id} with {micro_batch_df.count()} records\")\n    \n    # VÃ©rifier si la table existe\n    if DeltaTable.isDeltaTable(spark, \"s3a://silver/customers/\"):\n        # Table existe â†’ MERGE (upsert)\n        target = DeltaTable.forPath(spark, \"s3a://silver/customers/\")\n        \n        target.alias(\"target\").merge(\n            micro_batch_df.alias(\"source\"),\n            \"target.customer_id = source.customer_id\"\n        ).whenMatchedUpdate(\n            set={\n                \"name\": \"source.name\",\n                \"email\": \"source.email\",\n                \"updated_at\": \"current_timestamp()\"\n            }\n        ).whenNotMatchedInsert(\n            values={\n                \"customer_id\": \"source.customer_id\",\n                \"name\": \"source.name\",\n                \"email\": \"source.email\",\n                \"created_at\": \"current_timestamp()\",\n                \"updated_at\": \"current_timestamp()\"\n            }\n        ).execute()\n    else:\n        # Table n'existe pas â†’ CREATE\n        micro_batch_df.write \\\n            .format(\"delta\") \\\n            .mode(\"overwrite\") \\\n            .save(\"s3a://silver/customers/\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Utiliser foreachBatch\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nquery = customers_stream.writeStream \\\n    .foreachBatch(upsert_to_delta) \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoints/customers-upsert\") \\\n    .outputMode(\"update\") \\\n    .start()\n'''\n\nprint(foreach_batch_example)\nprint(\"\\nğŸ’¡ Avantages de foreachBatch :\")\nprint(\"â€¢ Utiliser MERGE INTO (upserts)\")\nprint(\"â€¢ Ã‰crire vers plusieurs sinks\")\nprint(\"â€¢ Appliquer n'importe quelle logique batch\")\nprint(\"â€¢ Exactly-once avec checkpointing\")\n\n\n\n\nExercice 5 : Upsert streaming avec foreachBatch\n# CrÃ©er un pipeline qui :\n# 1. Lit des updates de statut de commande depuis Kafka\n# 2. Fait un MERGE INTO vers une table Delta 'order_status'\n# 3. Met Ã  jour le statut si la commande existe, sinon insÃ¨re\n\n\nğŸ’¡ Solution\n\ndef upsert_order_status(df, batch_id):\n    if DeltaTable.isDeltaTable(spark, \"/tmp/order_status\"):\n        target = DeltaTable.forPath(spark, \"/tmp/order_status\")\n        target.alias(\"t\").merge(\n            df.alias(\"s\"), \"t.order_id = s.order_id\"\n        ).whenMatchedUpdate(\n            set={\"status\": \"s.status\", \"updated_at\": \"current_timestamp()\"}\n        ).whenNotMatchedInsertAll().execute()\n    else:\n        df.write.format(\"delta\").save(\"/tmp/order_status\")\n\nstatus_stream.writeStream \\\n    .foreachBatch(upsert_order_status) \\\n    .option(\"checkpointLocation\", \"/tmp/cp/status\") \\\n    .start()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#kafka-connect-debezium-aperÃ§u",
    "href": "notebooks/intermediate/24_kafka_streaming.html#kafka-connect-debezium-aperÃ§u",
    "title": "Kafka, Python & Structured Streaming",
    "section": "8. Kafka Connect & Debezium (AperÃ§u)",
    "text": "8. Kafka Connect & Debezium (AperÃ§u)\n\n8.1 Quâ€™est-ce que Kafka Connect ?\nKafka Connect est un framework pour connecter Kafka Ã  dâ€™autres systÃ¨mes sans code :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     KAFKA CONNECT                               â”‚\nâ”‚                                                                 â”‚\nâ”‚   SOURCES                      SINKS                           â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”‚ PostgreSQL   â”‚             â”‚ Elasticsearchâ”‚                â”‚\nâ”‚   â”‚ MySQL        â”‚ â”€â”€â–¶ KAFKA â”€â”€â–¶â”‚ S3           â”‚                â”‚\nâ”‚   â”‚ MongoDB      â”‚             â”‚ Snowflake    â”‚                â”‚\nâ”‚   â”‚ Files (CSV)  â”‚             â”‚ BigQuery     â”‚                â”‚\nâ”‚   â”‚ APIs         â”‚             â”‚ Redis        â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                                 â”‚\nâ”‚   Configuration JSON, pas de code !                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n8.2 Debezium : CDC temps rÃ©el\nDebezium est un connecteur Kafka Connect pour le Change Data Capture :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   PostgreSQL   â”‚         â”‚   Debezium   â”‚         â”‚   Kafka    â”‚\nâ”‚                â”‚         â”‚              â”‚         â”‚            â”‚\nâ”‚   WAL logs â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Connector   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Topic     â”‚\nâ”‚   (changes)    â”‚         â”‚              â”‚         â”‚  per table â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nChaque INSERT/UPDATE/DELETE â†’ Message Kafka automatique !\n\n\nVoir le code\n# Exemple de configuration Debezium\n\ndebezium_config = '''\n{\n  \"name\": \"postgres-connector\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n    \"database.hostname\": \"postgres\",\n    \"database.port\": \"5432\",\n    \"database.user\": \"debezium\",\n    \"database.password\": \"secret\",\n    \"database.dbname\": \"inventory\",\n    \"database.server.name\": \"dbserver1\",\n    \"table.include.list\": \"public.customers,public.orders\",\n    \"plugin.name\": \"pgoutput\",\n    \n    \"transforms\": \"unwrap\",\n    \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n    \"transforms.unwrap.drop.tombstones\": \"false\"\n  }\n}\n'''\n\nprint(\"Configuration Debezium pour PostgreSQL :\")\nprint(debezium_config)\nprint(\"\\nğŸ’¡ Quand utiliser Debezium vs code custom :\")\nprint(\"â€¢ Debezium : CDC standard depuis DB, pas de code Ã  maintenir\")\nprint(\"â€¢ Code custom : Logique complexe, sources non supportÃ©es\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#dÃ©ploiement-observabilitÃ©",
    "href": "notebooks/intermediate/24_kafka_streaming.html#dÃ©ploiement-observabilitÃ©",
    "title": "Kafka, Python & Structured Streaming",
    "section": "9. DÃ©ploiement & ObservabilitÃ©",
    "text": "9. DÃ©ploiement & ObservabilitÃ©\n\n9.1 DÃ©ployer SSS sur Kubernetes\nPour la production, on utilise le Spark Operator (voir Module 21) :\n\n\nVoir le code\n# SparkApplication pour un job streaming\n\nspark_streaming_k8s = '''\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: kafka-to-delta-streaming\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: my-registry/spark-streaming:latest\n  mainApplicationFile: s3a://code/streaming_job.py\n  sparkVersion: \"3.5.0\"\n  \n  # Important pour le streaming !\n  restartPolicy:\n    type: Always  # RedÃ©marrer automatiquement si crash\n    onFailureRetries: 3\n    onFailureRetryInterval: 60\n  \n  driver:\n    cores: 1\n    memory: \"2g\"\n  \n  executor:\n    cores: 2\n    instances: 3\n    memory: \"4g\"\n  \n  deps:\n    packages:\n      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\n      - io.delta:delta-spark_2.12:3.1.0\n'''\n\nprint(spark_streaming_k8s)\n\n\n\n\n9.2 MÃ©triques Ã  surveiller\n\n\n\nMÃ©trique\nDescription\nSeuil dâ€™alerte\n\n\n\n\nInput Rate\nMessages/sec entrants\n-\n\n\nProcessing Rate\nMessages/sec traitÃ©s\n&lt; Input Rate\n\n\nBatch Duration\nTemps de traitement\n&gt; Trigger interval\n\n\nBacklog\nMessages en attente\nCroissant\n\n\nWatermark Lag\nRetard du watermark\n&gt; Seuil attendu\n\n\n\n\n\nVoir le code\n# Monitoring du streaming\n\nmonitoring = '''\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# AccÃ©der aux mÃ©triques dans le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nquery = df.writeStream...\n\n# Progression du dernier batch\nprint(query.lastProgress)\n\n# Status actuel\nprint(query.status)\n\n# Exemple de lastProgress :\n# {\n#   \"id\": \"abc123\",\n#   \"runId\": \"def456\",\n#   \"batchId\": 42,\n#   \"numInputRows\": 1000,\n#   \"inputRowsPerSecond\": 500.0,\n#   \"processedRowsPerSecond\": 450.0,\n#   \"durationMs\": {\n#     \"triggerExecution\": 2000,\n#     \"getBatch\": 100,\n#     \"queryPlanning\": 50\n#   },\n#   \"eventTime\": {\n#     \"watermark\": \"2024-01-15T10:20:00.000Z\"\n#   }\n# }\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Listener pour mÃ©triques custom\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclass MetricsListener(StreamingQueryListener):\n    def onQueryStarted(self, event):\n        print(f\"Query started: {event.id}\")\n    \n    def onQueryProgress(self, event):\n        # Envoyer vers Prometheus/Grafana\n        metrics.gauge(\"streaming_input_rate\", event.progress.inputRowsPerSecond)\n        metrics.gauge(\"streaming_batch_duration\", event.progress.durationMs[\"triggerExecution\"])\n    \n    def onQueryTerminated(self, event):\n        print(f\"Query terminated: {event.id}\")\n\nspark.streams.addListener(MetricsListener())\n'''\n\nprint(monitoring)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#mini-projet-pipeline-temps-rÃ©el-complet",
    "href": "notebooks/intermediate/24_kafka_streaming.html#mini-projet-pipeline-temps-rÃ©el-complet",
    "title": "Kafka, Python & Structured Streaming",
    "section": "10. Mini-Projet : Pipeline Temps RÃ©el Complet",
    "text": "10. Mini-Projet : Pipeline Temps RÃ©el Complet\n\nObjectif\nConstruire un pipeline dâ€™ingestion transactionnel : Python Producer â†’ Kafka â†’ Spark SSS â†’ Delta Lake\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Producteur   â”‚   â”‚     Kafka     â”‚   â”‚ Spark Structured â”‚   â”‚    Delta     â”‚\nâ”‚   (Python)     â”‚â”€â”€â–¶â”‚   (Docker)    â”‚â”€â”€â–¶â”‚    Streaming     â”‚â”€â”€â–¶â”‚    Lake      â”‚\nâ”‚                â”‚   â”‚               â”‚   â”‚                  â”‚   â”‚              â”‚\nâ”‚ Simule des     â”‚   â”‚ Topic:        â”‚   â”‚ â€¢ Watermark      â”‚   â”‚ â€¢ MERGE INTO â”‚\nâ”‚ transactions   â”‚   â”‚ transactions  â”‚   â”‚ â€¢ Window 5 min   â”‚   â”‚ â€¢ Silver     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Ã‰TAPE 1 : Producteur Python\n\nproducer_code = '''\nfrom kafka import KafkaProducer\nimport json\nimport time\nimport random\nfrom datetime import datetime\n\nproducer = KafkaProducer(\n    bootstrap_servers=[\"localhost:9092\"],\n    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n)\n\ncustomers = [\"CUST-001\", \"CUST-002\", \"CUST-003\", \"CUST-004\", \"CUST-005\"]\nproducts = [\"Laptop\", \"Phone\", \"Tablet\", \"Watch\", \"Headphones\"]\n\nprint(\"ğŸš€ Sending transactions...\")\n\nfor i in range(100):\n    transaction = {\n        \"transaction_id\": f\"TXN-{i:06d}\",\n        \"customer_id\": random.choice(customers),\n        \"product\": random.choice(products),\n        \"amount\": round(random.uniform(10, 500), 2),\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    producer.send(\"transactions\", value=transaction)\n    print(f\"Sent: {transaction[\\'transaction_id\\']}\")\n    time.sleep(0.5)  # Simuler un flux\n\nproducer.flush()\nprint(\"âœ… Done!\")\n'''\n\nprint(\"# producer.py\")\nprint(producer_code)\n\n\n\n\nVoir le code\n# Ã‰TAPE 2 : Job Spark Streaming\n\nstreaming_job = '''\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col, window, sum as spark_sum, count, to_timestamp\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\nfrom delta.tables import DeltaTable\n\n# Spark Session\nspark = SparkSession.builder \\\n    .appName(\"Realtime Transactions\") \\\n    .config(\"spark.jars.packages\", \n            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n            \"io.delta:delta-spark_2.12:3.1.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .getOrCreate()\n\n# SchÃ©ma\nschema = StructType([\n    StructField(\"transaction_id\", StringType()),\n    StructField(\"customer_id\", StringType()),\n    StructField(\"product\", StringType()),\n    StructField(\"amount\", DoubleType()),\n    StructField(\"timestamp\", StringType())\n])\n\n# Lire Kafka\nraw_stream = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"transactions\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n\n# Parser JSON\ntransactions = raw_stream \\\n    .selectExpr(\"CAST(value AS STRING)\") \\\n    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n\n# AgrÃ©gation par fenÃªtre de 5 min avec watermark 10 min\nwindowed_stats = transactions \\\n    .withWatermark(\"event_time\", \"10 minutes\") \\\n    .groupBy(\n        window(col(\"event_time\"), \"5 minutes\"),\n        col(\"customer_id\")\n    ) \\\n    .agg(\n        count(\"*\").alias(\"transaction_count\"),\n        spark_sum(\"amount\").alias(\"total_amount\")\n    )\n\n# Fonction pour upsert vers Delta\ndef upsert_to_delta(batch_df, batch_id):\n    # Flatten window column\n    flat_df = batch_df.selectExpr(\n        \"window.start as window_start\",\n        \"window.end as window_end\",\n        \"customer_id\",\n        \"transaction_count\",\n        \"total_amount\"\n    )\n    \n    if flat_df.count() == 0:\n        return\n    \n    target_path = \"/tmp/delta/customer_stats\"\n    \n    if DeltaTable.isDeltaTable(spark, target_path):\n        target = DeltaTable.forPath(spark, target_path)\n        target.alias(\"t\").merge(\n            flat_df.alias(\"s\"),\n            \"t.window_start = s.window_start AND t.customer_id = s.customer_id\"\n        ).whenMatchedUpdate(set={\n            \"transaction_count\": \"s.transaction_count\",\n            \"total_amount\": \"s.total_amount\"\n        }).whenNotMatchedInsertAll().execute()\n    else:\n        flat_df.write.format(\"delta\").save(target_path)\n    \n    print(f\"Batch {batch_id}: Processed {flat_df.count()} records\")\n\n# Lancer le stream\nquery = windowed_stats.writeStream \\\n    .foreachBatch(upsert_to_delta) \\\n    .option(\"checkpointLocation\", \"/tmp/checkpoints/customer_stats\") \\\n    .outputMode(\"update\") \\\n    .trigger(processingTime=\"30 seconds\") \\\n    .start()\n\nquery.awaitTermination()\n'''\n\nprint(\"# streaming_job.py\")\nprint(streaming_job)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#quiz",
    "href": "notebooks/intermediate/24_kafka_streaming.html#quiz",
    "title": "Kafka, Python & Structured Streaming",
    "section": "Quiz",
    "text": "Quiz\nQ1. DiffÃ©rence entre Event Time et Processing Time ?\n\n\nR\n\nEvent Time = quand lâ€™Ã©vÃ©nement sâ€™est produit. Processing Time = quand Spark le traite.\n\nQ2. RÃ´le du Watermark ?\n\n\nR\n\nDÃ©finir la tolÃ©rance au retard et permettre le nettoyage de lâ€™Ã©tat.\n\nQ3. DiffÃ©rence At-least-once vs Exactly-once ?\n\n\nR\n\nAt-least-once peut dupliquer. Exactly-once garantit un seul traitement.\n\nQ4. Pourquoi utiliser foreachBatch ?\n\n\nR\n\nPour appliquer une logique batch (MERGE INTO) sur chaque micro-batch.\n\nQ5. Quâ€™est-ce quâ€™un Consumer Group ?\n\n\nR\n\nGroupe de consumers qui se partagent les partitions pour parallÃ©liser.\n\nQ6. Tumbling vs Sliding Window ?\n\n\nR\n\nTumbling = non-chevauchantes. Sliding = chevauchantes.\n\nQ7. RÃ´le du checkpointLocation ?\n\n\nR\n\nStocker les offsets et lâ€™Ã©tat pour recovery et exactly-once.\n\nQ8. Quand utiliser Debezium ?\n\n\nR\n\nPour du CDC temps rÃ©el depuis une base de donnÃ©es vers Kafka.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#ressources",
    "href": "notebooks/intermediate/24_kafka_streaming.html#ressources",
    "title": "Kafka, Python & Structured Streaming",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nKafka Documentation\nSpark Structured Streaming Guide\nDebezium Documentation\nFaust Documentation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/24_kafka_streaming.html#prochaine-Ã©tape",
    "title": "Kafka, Python & Structured Streaming",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module 25 : 25_dbt_data_quality â€” dbt + Data Quality",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/24_kafka_streaming.html#rÃ©capitulatif",
    "href": "notebooks/intermediate/24_kafka_streaming.html#rÃ©capitulatif",
    "title": "Kafka, Python & Structured Streaming",
    "section": "ğŸ“ RÃ©capitulatif",
    "text": "ğŸ“ RÃ©capitulatif\n\n\n\nConcept\nAppris\n\n\n\n\nKafka\nTopics, Partitions, Offsets, Consumer Groups\n\n\nPython Kafka\nkafka-python, confluent-kafka, Faust\n\n\nSpark SSS\nreadStream, writeStream, Output Modes\n\n\nTemps\nEvent Time, Watermarks, Windowing\n\n\nAvancÃ©\nforeachBatch, MERGE INTO, Stream Joins\n\n\nOps\nCheckpointing, Monitoring, K8s\n\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant le streaming de donnÃ©es.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "24 Â· Kafka & Streaming"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#contexte",
    "href": "notebooks/intermediate/26_projet_integrateur.html#contexte",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Contexte",
    "text": "Contexte\nTu viens de rejoindre lâ€™Ã©quipe Data Engineering dâ€™Olist, la plus grande plateforme e-commerce du BrÃ©sil. Olist connecte des petits commerÃ§ants aux grandes marketplaces comme Amazon, Mercado Libre, etc.\nActuellement, les donnÃ©es sont stockÃ©es dans des fichiers CSV et analysÃ©es manuellement par lâ€™Ã©quipe BI. Ton manager te confie une mission critique :\n\nâ€œNous avons besoin dâ€™une architecture Lakehouse moderne. Tu dois construire un pipeline complet qui ingÃ¨re nos donnÃ©es en temps rÃ©el, les transforme, et les met Ã  disposition pour les dashboards analytiques.â€",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#ta-mission",
    "href": "notebooks/intermediate/26_projet_integrateur.html#ta-mission",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Ta Mission",
    "text": "Ta Mission\nConstruire un Data Pipeline complet de bout en bout :\nCSV (Kaggle) â†’ Kafka â†’ Spark SSS â†’ Delta Lake (Bronze/Silver) â†’ dbt (Gold) â†’ Dashboard-ready",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#dataset",
    "href": "notebooks/intermediate/26_projet_integrateur.html#dataset",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Dataset",
    "text": "Dataset\nBrazilian E-Commerce Public Dataset by Olist\nğŸ”— Kaggle : https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\nCe dataset contient ~100 000 commandes rÃ©elles (anonymisÃ©es) passÃ©es sur Olist entre 2016 et 2018.\n\nTables disponibles\n\n\n\n\n\n\n\n\nFichier\nDescription\nLignes\n\n\n\n\nolist_orders_dataset.csv\nCommandes\n~100K\n\n\nolist_order_items_dataset.csv\nLignes de commande\n~113K\n\n\nolist_customers_dataset.csv\nClients\n~100K\n\n\nolist_products_dataset.csv\nProduits\n~33K\n\n\nolist_sellers_dataset.csv\nVendeurs\n~3K\n\n\nolist_order_payments_dataset.csv\nPaiements\n~104K\n\n\nolist_order_reviews_dataset.csv\nAvis clients\n~100K\n\n\nolist_geolocation_dataset.csv\nGÃ©olocalisation\n~1M\n\n\nproduct_category_name_translation.csv\nTraduction catÃ©gories\n~71\n\n\n\n\n\nSchÃ©ma relationnel\n                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                              â”‚    customers     â”‚\n                              â”‚  customer_id (PK)â”‚\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                       â”‚\n                                       â”‚ 1:N\n                                       â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1:N     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     N:1     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   sellers    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     orders       â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   payments   â”‚\nâ”‚ seller_id(PK)â”‚            â”‚  order_id (PK)   â”‚             â”‚              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  customer_id(FK) â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       â–²                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚                             â”‚\n       â”‚ N:1                         â”‚ 1:N\n       â”‚                             â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    N:1     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     1:N     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   products   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   order_items    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   reviews    â”‚\nâ”‚product_id(PK)â”‚            â”‚ order_id (FK)    â”‚             â”‚              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ product_id (FK)  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ seller_id (FK)   â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#architecture-cible",
    "href": "notebooks/intermediate/26_projet_integrateur.html#architecture-cible",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Architecture Cible",
    "text": "Architecture Cible\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         PIPELINE E-COMMERCE OLIST                               â”‚\nâ”‚                                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚   CSV Files  â”‚    â”‚    KAFKA    â”‚    â”‚   BRONZE    â”‚    â”‚   SILVER    â”‚    â”‚\nâ”‚  â”‚   (Kaggle)   â”‚â”€â”€â”€â–¶â”‚   Topics    â”‚â”€â”€â”€â–¶â”‚  (Delta)    â”‚â”€â”€â”€â–¶â”‚  (Delta)    â”‚    â”‚\nâ”‚  â”‚              â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚\nâ”‚  â”‚ â€¢ orders     â”‚    â”‚ â€¢ raw_ordersâ”‚    â”‚  Append     â”‚    â”‚ MERGE INTO  â”‚    â”‚\nâ”‚  â”‚ â€¢ items      â”‚    â”‚ â€¢ raw_items â”‚    â”‚  Raw data   â”‚    â”‚ Deduplicatedâ”‚    â”‚\nâ”‚  â”‚ â€¢ customers  â”‚    â”‚ â€¢ raw_custs â”‚    â”‚  Partitionedâ”‚    â”‚ Enriched    â”‚    â”‚\nâ”‚  â”‚ â€¢ products   â”‚    â”‚ â€¢ raw_prods â”‚    â”‚             â”‚    â”‚ Validated   â”‚    â”‚\nâ”‚  â”‚ â€¢ sellers    â”‚    â”‚             â”‚    â”‚             â”‚    â”‚             â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚         â”‚                   â”‚                  â”‚                  â”‚           â”‚\nâ”‚         â”‚            Spark SSS          Spark SSS          Spark SSS          â”‚\nâ”‚         â”‚                                                  + foreachBatch     â”‚\nâ”‚         â”‚                                                                     â”‚\nâ”‚         â–¼                                                        â”‚            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                               â–¼            â”‚\nâ”‚  â”‚  Producers   â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚  â”‚  (Python)    â”‚                                    â”‚      GOLD       â”‚     â”‚\nâ”‚  â”‚              â”‚                                    â”‚     (dbt)       â”‚     â”‚\nâ”‚  â”‚ Simulate     â”‚                                    â”‚                 â”‚     â”‚\nâ”‚  â”‚ streaming    â”‚                                    â”‚ â€¢ daily_sales   â”‚     â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚ â€¢ seller_perf   â”‚     â”‚\nâ”‚                                                      â”‚ â€¢ customer_rfm  â”‚     â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â€¢ product_stats â”‚     â”‚\nâ”‚  â”‚           ORCHESTRATION (Airflow)           â”‚    â”‚ â€¢ delivery_perf â”‚     â”‚\nâ”‚  â”‚                                             â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚  â”‚  [Producers] â†’ [Bronze] â†’ [Silver] â†’ [dbt] â†’ [GE Validation]            â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\nâ”‚                                                                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚\nâ”‚  â”‚        DATA QUALITY (Great Expectations)    â”‚                            â”‚\nâ”‚  â”‚  â€¢ Bronze: schema, completeness             â”‚                            â”‚\nâ”‚  â”‚  â€¢ Silver: business rules, uniqueness       â”‚                            â”‚\nâ”‚  â”‚  â€¢ Gold: consistency, freshness             â”‚                            â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#livrables-attendus",
    "href": "notebooks/intermediate/26_projet_integrateur.html#livrables-attendus",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Livrables Attendus",
    "text": "Livrables Attendus\nTu dois produire les livrables suivants :\nolist_pipeline/\nâ”‚\nâ”œâ”€â”€ docker-compose.yml           # 1. Infrastructure complÃ¨te\nâ”‚\nâ”œâ”€â”€ producers/                   # 2. Producteurs Kafka\nâ”‚   â”œâ”€â”€ orders_producer.py\nâ”‚   â”œâ”€â”€ items_producer.py\nâ”‚   â”œâ”€â”€ customers_producer.py\nâ”‚   â”œâ”€â”€ products_producer.py\nâ”‚   â””â”€â”€ sellers_producer.py\nâ”‚\nâ”œâ”€â”€ spark_jobs/                  # 3 & 4. Jobs Spark\nâ”‚   â”œâ”€â”€ bronze/\nâ”‚   â”‚   â”œâ”€â”€ ingest_orders.py\nâ”‚   â”‚   â”œâ”€â”€ ingest_items.py\nâ”‚   â”‚   â””â”€â”€ ingest_customers.py\nâ”‚   â””â”€â”€ silver/\nâ”‚       â”œâ”€â”€ silver_orders.py     # Avec MERGE INTO\nâ”‚       â”œâ”€â”€ silver_customers.py\nâ”‚       â””â”€â”€ silver_order_items.py\nâ”‚\nâ”œâ”€â”€ dbt_olist/                   # 5. Projet dbt\nâ”‚   â”œâ”€â”€ dbt_project.yml\nâ”‚   â”œâ”€â”€ models/\nâ”‚   â”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ intermediate/\nâ”‚   â”‚   â””â”€â”€ gold/\nâ”‚   â””â”€â”€ tests/\nâ”‚\nâ”œâ”€â”€ great_expectations/          # 6. Data Quality\nâ”‚   â””â”€â”€ expectations/\nâ”‚\nâ”œâ”€â”€ dags/                        # 7. Airflow DAGs\nâ”‚   â””â”€â”€ olist_pipeline_dag.py\nâ”‚\nâ””â”€â”€ README.md                    # 8. Documentation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#tables-gold-attendues",
    "href": "notebooks/intermediate/26_projet_integrateur.html#tables-gold-attendues",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Tables Gold Attendues",
    "text": "Tables Gold Attendues\nTu dois crÃ©er 5 models Gold dans dbt :\n\n1. gold_daily_sales\nChiffre dâ€™affaires quotidien.\n\n\n\nColonne\nType\nDescription\n\n\n\n\norder_date\nDATE\nDate de commande\n\n\ntotal_orders\nINT\nNombre de commandes\n\n\ntotal_revenue\nDECIMAL\nCA total\n\n\navg_order_value\nDECIMAL\nPanier moyen\n\n\ntotal_items\nINT\nNombre dâ€™articles vendus\n\n\n\n\n\n2. gold_seller_performance\nMÃ©triques par vendeur.\n\n\n\nColonne\nType\nDescription\n\n\n\n\nseller_id\nSTRING\nID vendeur\n\n\nseller_city\nSTRING\nVille\n\n\ntotal_orders\nINT\nCommandes traitÃ©es\n\n\ntotal_revenue\nDECIMAL\nCA gÃ©nÃ©rÃ©\n\n\navg_review_score\nDECIMAL\nNote moyenne\n\n\navg_delivery_days\nDECIMAL\nDÃ©lai moyen livraison\n\n\n\n\n\n3. gold_customer_rfm\nSegmentation RFM (Recency, Frequency, Monetary).\n\n\n\nColonne\nType\nDescription\n\n\n\n\ncustomer_unique_id\nSTRING\nID client unique\n\n\nrecency_days\nINT\nJours depuis derniÃ¨re commande\n\n\nfrequency\nINT\nNombre de commandes\n\n\nmonetary\nDECIMAL\nTotal dÃ©pensÃ©\n\n\nrfm_segment\nSTRING\nSegment (Champions, At Risk, etc.)\n\n\n\n\n\n4. gold_product_analytics\nPerformance des produits.\n\n\n\nColonne\nType\nDescription\n\n\n\n\nproduct_category\nSTRING\nCatÃ©gorie (EN)\n\n\ntotal_sold\nINT\nQuantitÃ© vendue\n\n\ntotal_revenue\nDECIMAL\nCA\n\n\navg_price\nDECIMAL\nPrix moyen\n\n\navg_review_score\nDECIMAL\nNote moyenne\n\n\n\n\n\n5. gold_delivery_performance\nPerformance des livraisons.\n\n\n\nColonne\nType\nDescription\n\n\n\n\nseller_state\nSTRING\nÃ‰tat du vendeur\n\n\ncustomer_state\nSTRING\nÃ‰tat du client\n\n\ntotal_deliveries\nINT\nNombre de livraisons\n\n\navg_delivery_days\nDECIMAL\nDÃ©lai moyen\n\n\non_time_rate\nDECIMAL\n% livrÃ© Ã  temps\n\n\nlate_rate\nDECIMAL\n% en retard",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#spÃ©cifications-techniques",
    "href": "notebooks/intermediate/26_projet_integrateur.html#spÃ©cifications-techniques",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "SpÃ©cifications Techniques",
    "text": "SpÃ©cifications Techniques\n\n1. Infrastructure (Docker Compose)\nServices requis :\n\n\n\n\n\n\n\n\n\nService\nImage\nPorts\nRÃ´le\n\n\n\n\nzookeeper\nconfluentinc/cp-zookeeper:7.5.0\n2181\nCoordination Kafka\n\n\nkafka\nconfluentinc/cp-kafka:7.5.0\n9092, 29092\nMessage broker\n\n\nschema-registry\nconfluentinc/cp-schema-registry:7.5.0\n8081\nGestion schÃ©mas\n\n\nminio\nminio/minio\n9000, 9001\nS3 local (Delta Lake)\n\n\nspark-master\nbitnami/spark:3.5\n8080, 7077\nSpark Master\n\n\nspark-worker\nbitnami/spark:3.5\n8081\nSpark Worker\n\n\npostgres\npostgres:15\n5432\nMÃ©tadonnÃ©es Airflow\n\n\nairflow-webserver\napache/airflow:2.8.0\n8082\nUI Airflow\n\n\nairflow-scheduler\napache/airflow:2.8.0\n-\nScheduler\n\n\n\n\n\n2. Producteurs Kafka\nChaque producteur doit :\n\nLire le fichier CSV correspondant\nEnvoyer les lignes une par une vers Kafka (simuler du streaming)\nAjouter un dÃ©lai alÃ©atoire (50-200ms) entre les messages\nSimuler du late data : 5% des messages avec un timestamp dÃ©calÃ© de 1-5 minutes\nSimuler des doublons : 2% des messages envoyÃ©s 2 fois\n\nTopics Kafka : - raw_orders - raw_order_items - raw_customers - raw_products - raw_sellers\nFormat des messages : JSON\n{\n  \"order_id\": \"e481f51cbdc54678b7cc49136f2d6af7\",\n  \"customer_id\": \"9ef432eb6251297304e76186b10a928d\",\n  \"order_status\": \"delivered\",\n  \"order_purchase_timestamp\": \"2017-10-02 10:56:33\",\n  \"_ingestion_timestamp\": \"2024-01-15T10:30:00Z\"\n}\n\n\n3. Couche Bronze (Spark SSS â†’ Delta)\nMode : Append (donnÃ©es brutes, pas de transformation)\nChaque job Bronze doit :\n\nLire depuis le topic Kafka correspondant\nParser le JSON\nAjouter une colonne _bronze_ingested_at (timestamp dâ€™ingestion)\nÃ‰crire en append dans Delta Lake\nPartitionner par date dâ€™ingestion (_ingestion_date)\nConfigurer le checkpointing\n\nChemins Delta :\ns3a://lakehouse/bronze/orders/\ns3a://lakehouse/bronze/order_items/\ns3a://lakehouse/bronze/customers/\ns3a://lakehouse/bronze/products/\ns3a://lakehouse/bronze/sellers/\n\n\n4. Couche Silver (Spark SSS + MERGE INTO) â­\nMode : foreachBatch + MERGE INTO (upserts, dÃ©duplication)\nCâ€™est lâ€™Ã©tape clÃ© du projet. Chaque job Silver doit :\n\nLire depuis Bronze (streaming ou batch)\nDÃ©dupliquer sur la clÃ© primaire (garder le plus rÃ©cent)\nValider les donnÃ©es (filtrer les invalides)\nEnrichir si nÃ©cessaire (jointures)\nMERGE INTO Delta Lake Silver\n\nPattern Ã  utiliser :\ndef upsert_to_silver(batch_df, batch_id):\n    # 1. DÃ©duplication\n    deduped = batch_df.dropDuplicates([\"order_id\"])\n    \n    # 2. MERGE INTO\n    delta_table = DeltaTable.forPath(spark, \"s3a://lakehouse/silver/orders\")\n    \n    delta_table.alias(\"target\").merge(\n        deduped.alias(\"source\"),\n        \"target.order_id = source.order_id\"\n    ).whenMatchedUpdate(\n        condition=\"source._bronze_ingested_at &gt; target._bronze_ingested_at\",\n        set={...}\n    ).whenNotMatchedInsertAll().execute()\n\nbronze_stream.writeStream \\\n    .foreachBatch(upsert_to_silver) \\\n    .option(\"checkpointLocation\", \"/checkpoints/silver_orders\") \\\n    .start()\nTables Silver :\n\n\n\n\n\n\n\n\nTable\nClÃ© primaire\nEnrichissement\n\n\n\n\nsilver_orders\norder_id\n+ customer info\n\n\nsilver_order_items\norder_id + product_id + seller_id\n+ product info\n\n\nsilver_customers\ncustomer_id\nDÃ©dup sur customer_unique_id\n\n\nsilver_products\nproduct_id\n+ category translation\n\n\nsilver_sellers\nseller_id\n-\n\n\n\n\n\n5. Couche Gold (dbt)\nStructure du projet dbt :\ndbt_olist/\nâ”œâ”€â”€ dbt_project.yml\nâ”œâ”€â”€ packages.yml                 # dbt-utils, dbt-expectations\nâ”‚\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ staging/                 # Vues sur Silver\nâ”‚   â”‚   â”œâ”€â”€ _sources.yml\nâ”‚   â”‚   â”œâ”€â”€ stg_orders.sql\nâ”‚   â”‚   â”œâ”€â”€ stg_order_items.sql\nâ”‚   â”‚   â”œâ”€â”€ stg_customers.sql\nâ”‚   â”‚   â”œâ”€â”€ stg_products.sql\nâ”‚   â”‚   â””â”€â”€ stg_sellers.sql\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ intermediate/            # Transformations mÃ©tier\nâ”‚   â”‚   â”œâ”€â”€ int_orders_enriched.sql\nâ”‚   â”‚   â””â”€â”€ int_order_items_enriched.sql\nâ”‚   â”‚\nâ”‚   â””â”€â”€ gold/                    # Tables analytiques\nâ”‚       â”œâ”€â”€ _gold__models.yml    # Tests + docs\nâ”‚       â”œâ”€â”€ gold_daily_sales.sql\nâ”‚       â”œâ”€â”€ gold_seller_performance.sql\nâ”‚       â”œâ”€â”€ gold_customer_rfm.sql\nâ”‚       â”œâ”€â”€ gold_product_analytics.sql\nâ”‚       â””â”€â”€ gold_delivery_performance.sql\nâ”‚\nâ”œâ”€â”€ macros/\nâ”‚   â””â”€â”€ rfm_segment.sql          # Macro pour segmentation RFM\nâ”‚\nâ””â”€â”€ tests/\n    â””â”€â”€ assert_positive_revenue.sql\nMatÃ©rialisations : - staging/ : view - intermediate/ : ephemeral ou view - gold/ : incremental (avec unique_key)\n\n\n6. Data Quality (Great Expectations)\nCrÃ©er des suites dâ€™expectations pour chaque couche :\nSuite Bronze : - Schema validation (colonnes prÃ©sentes) - expect_column_values_to_not_be_null sur les IDs\nSuite Silver : - expect_column_values_to_be_unique sur les clÃ©s primaires - expect_column_values_to_be_between sur les montants (0 - 100000) - expect_column_values_to_be_in_set sur les statuts\nSuite Gold : - expect_column_values_to_be_between sur les mÃ©triques - expect_table_row_count_to_be_between (freshness check) - Tests de cohÃ©rence (total Gold = total Silver)\n\n\n7. Orchestration (Airflow)\nDAG principal : olist_pipeline\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ check_freshness â”‚  VÃ©rifier que les donnÃ©es arrivent\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ bronze_to_silverâ”‚  Spark job (MERGE INTO)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚    dbt_run      â”‚  dbt run --select gold\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   dbt_test      â”‚  dbt test\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  ge_validate    â”‚  Great Expectations checkpoint\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n    â–¼         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”\nâ”‚notify â”‚ â”‚notify â”‚\nâ”‚successâ”‚ â”‚failureâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜\nConfiguration : - Schedule : 0 6 * * * (tous les jours Ã  6h) - Retries : 2 - Alertes : Email ou Slack on_failure",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#hints-ressources",
    "href": "notebooks/intermediate/26_projet_integrateur.html#hints-ressources",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "ğŸ’¡ Hints & Ressources",
    "text": "ğŸ’¡ Hints & Ressources\n\nRappels des patterns clÃ©s\n\n\n\nModule\nPattern\nOÃ¹ lâ€™utiliser\n\n\n\n\n24\nreadStream.format(\"kafka\")\nBronze ingestion\n\n\n24\nforeachBatch\nSilver MERGE\n\n\n23\nDeltaTable.forPath().merge()\nSilver MERGE\n\n\n23\nwhenMatchedUpdate / whenNotMatchedInsert\nSilver MERGE\n\n\n25\n{ ref('...') }\ndbt models\n\n\n25\n{ config(materialized='incremental') }\nGold models\n\n\n25\nis_incremental()\nGold models\n\n\n22\nBashOperator\nAirflow tasks\n\n\n\n\n\nVoir le code\n# Hint 1 : Structure du producteur Kafka\n\nproducer_template = '''\nfrom kafka import KafkaProducer\nimport pandas as pd\nimport json\nimport time\nimport random\nfrom datetime import datetime, timedelta\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\ndf = pd.read_csv('data/olist_orders_dataset.csv')\n\nfor idx, row in df.iterrows():\n    message = row.to_dict()\n    \n    # Ajouter timestamp d'ingestion\n    ingestion_ts = datetime.now()\n    \n    # Simuler late data (5%)\n    if random.random() &lt; 0.05:\n        ingestion_ts -= timedelta(minutes=random.randint(1, 5))\n    \n    message['_ingestion_timestamp'] = ingestion_ts.isoformat()\n    \n    producer.send('raw_orders', value=message)\n    \n    # Simuler doublons (2%)\n    if random.random() &lt; 0.02:\n        producer.send('raw_orders', value=message)\n    \n    # DÃ©lai alÃ©atoire\n    time.sleep(random.uniform(0.05, 0.2))\n\nproducer.flush()\n'''\n\nprint(producer_template)\n\n\n\n\nVoir le code\n# Hint 2 : Pattern MERGE INTO pour Silver\n\nmerge_pattern = '''\nfrom delta.tables import DeltaTable\nfrom pyspark.sql.functions import col, current_timestamp\n\ndef upsert_orders_to_silver(batch_df, batch_id):\n    \"\"\"Upsert orders vers Silver avec dÃ©duplication.\"\"\"\n    \n    if batch_df.count() == 0:\n        return\n    \n    # 1. DÃ©duplication (garder le plus rÃ©cent)\n    from pyspark.sql.window import Window\n    from pyspark.sql.functions import row_number\n    \n    window = Window.partitionBy(\"order_id\").orderBy(col(\"_ingestion_timestamp\").desc())\n    deduped = batch_df.withColumn(\"_row_num\", row_number().over(window)) \\\n                      .filter(col(\"_row_num\") == 1) \\\n                      .drop(\"_row_num\")\n    \n    # 2. Ajouter timestamp Silver\n    enriched = deduped.withColumn(\"_silver_updated_at\", current_timestamp())\n    \n    # 3. MERGE INTO\n    silver_path = \"s3a://lakehouse/silver/orders\"\n    \n    if DeltaTable.isDeltaTable(spark, silver_path):\n        delta_table = DeltaTable.forPath(spark, silver_path)\n        \n        delta_table.alias(\"target\").merge(\n            enriched.alias(\"source\"),\n            \"target.order_id = source.order_id\"\n        ).whenMatchedUpdate(\n            condition=\"source._ingestion_timestamp &gt; target._ingestion_timestamp\",\n            set={\n                \"order_status\": \"source.order_status\",\n                \"order_delivered_customer_date\": \"source.order_delivered_customer_date\",\n                \"_ingestion_timestamp\": \"source._ingestion_timestamp\",\n                \"_silver_updated_at\": \"source._silver_updated_at\"\n            }\n        ).whenNotMatchedInsertAll().execute()\n    else:\n        # PremiÃ¨re exÃ©cution : crÃ©er la table\n        enriched.write.format(\"delta\").mode(\"overwrite\").save(silver_path)\n    \n    print(f\"Batch {batch_id}: {enriched.count()} records merged to Silver\")\n'''\n\nprint(merge_pattern)\n\n\n\n\nVoir le code\n# Hint 3 : Model dbt incremental\n\ndbt_incremental = '''\n-- models/gold/gold_daily_sales.sql\n\n{{ config(\n    materialized='incremental',\n    unique_key='order_date',\n    incremental_strategy='merge'\n) }}\n\nWITH orders AS (\n    SELECT * FROM {{ ref('int_orders_enriched') }}\n    {% if is_incremental() %}\n    WHERE DATE(order_purchase_timestamp) &gt;= (\n        SELECT MAX(order_date) - INTERVAL 2 DAY FROM {{ this }}\n    )\n    {% endif %}\n),\n\ndaily_agg AS (\n    SELECT\n        DATE(order_purchase_timestamp) AS order_date,\n        COUNT(DISTINCT order_id) AS total_orders,\n        SUM(total_amount) AS total_revenue,\n        AVG(total_amount) AS avg_order_value,\n        SUM(total_items) AS total_items\n    FROM orders\n    WHERE order_status = 'delivered'\n    GROUP BY DATE(order_purchase_timestamp)\n)\n\nSELECT * FROM daily_agg\n'''\n\nprint(dbt_incremental)\n\n\n\n\nVoir le code\n# Hint 4 : Macro RFM pour dbt\n\nrfm_macro = '''\n-- macros/rfm_segment.sql\n\n{% macro rfm_segment(recency, frequency, monetary) %}\n    CASE\n        -- Champions : RÃ©cent, FrÃ©quent, Gros dÃ©pensier\n        WHEN {{ recency }} &lt;= 30 AND {{ frequency }} &gt;= 3 AND {{ monetary }} &gt;= 500 THEN 'Champions'\n        \n        -- Loyal Customers : FrÃ©quent\n        WHEN {{ frequency }} &gt;= 3 THEN 'Loyal Customers'\n        \n        -- Potential Loyalists : RÃ©cent, pas encore frÃ©quent\n        WHEN {{ recency }} &lt;= 30 AND {{ frequency }} &lt; 3 THEN 'Potential Loyalists'\n        \n        -- At Risk : Pas rÃ©cent mais Ã©tait frÃ©quent\n        WHEN {{ recency }} &gt; 90 AND {{ frequency }} &gt;= 2 THEN 'At Risk'\n        \n        -- Hibernating : Pas rÃ©cent, peu frÃ©quent\n        WHEN {{ recency }} &gt; 90 THEN 'Hibernating'\n        \n        -- Others\n        ELSE 'Others'\n    END\n{% endmacro %}\n\n-- Utilisation dans gold_customer_rfm.sql :\n-- SELECT\n--     customer_unique_id,\n--     recency_days,\n--     frequency,\n--     monetary,\n--     {{ rfm_segment('recency_days', 'frequency', 'monetary') }} AS rfm_segment\n-- FROM rfm_base\n'''\n\nprint(rfm_macro)\n\n\n\n\nVoir le code\n# Hint 5 : DAG Airflow\n\nairflow_dag = '''\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\nfrom airflow.utils.dates import days_ago\nfrom datetime import timedelta\n\ndefault_args = {\n    'owner': 'data-engineering',\n    'depends_on_past': False,\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5),\n    'email_on_failure': True,\n    'email': ['data-team@olist.com'],\n}\n\nwith DAG(\n    dag_id='olist_pipeline',\n    default_args=default_args,\n    description='Pipeline E-commerce Olist',\n    schedule_interval='0 6 * * *',\n    start_date=days_ago(1),\n    catchup=False,\n    tags=['olist', 'lakehouse'],\n) as dag:\n    \n    # 1. VÃ©rifier la fraÃ®cheur des sources\n    check_freshness = BashOperator(\n        task_id='check_freshness',\n        bash_command='cd /opt/dbt && dbt source freshness',\n    )\n    \n    # 2. Spark : Bronze â†’ Silver\n    bronze_to_silver = BashOperator(\n        task_id='bronze_to_silver',\n        bash_command='spark-submit /opt/spark_jobs/silver/run_all_silver.py',\n    )\n    \n    # 3. dbt run\n    dbt_run = BashOperator(\n        task_id='dbt_run',\n        bash_command='cd /opt/dbt && dbt run --select gold',\n    )\n    \n    # 4. dbt test\n    dbt_test = BashOperator(\n        task_id='dbt_test',\n        bash_command='cd /opt/dbt && dbt test --select gold',\n    )\n    \n    # 5. Great Expectations\n    ge_validate = BashOperator(\n        task_id='ge_validate',\n        bash_command='great_expectations checkpoint run gold_checkpoint',\n    )\n    \n    # 6. Notification succÃ¨s\n    notify_success = BashOperator(\n        task_id='notify_success',\n        bash_command='echo \"Pipeline completed successfully!\"',\n        trigger_rule='all_success',\n    )\n    \n    # DÃ©pendances\n    check_freshness &gt;&gt; bronze_to_silver &gt;&gt; dbt_run &gt;&gt; dbt_test &gt;&gt; ge_validate &gt;&gt; notify_success\n'''\n\nprint(airflow_dag)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#critÃ¨res-dÃ©valuation",
    "href": "notebooks/intermediate/26_projet_integrateur.html#critÃ¨res-dÃ©valuation",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "CritÃ¨res dâ€™Ã‰valuation",
    "text": "CritÃ¨res dâ€™Ã‰valuation\n\n\n\n\n\n\n\n\nCritÃ¨re\nPoints\nDÃ©tail\n\n\n\n\nInfrastructure\n/10\nDocker Compose fonctionne, tous les services up\n\n\nProducteurs Kafka\n/10\n5 producteurs, late data + doublons simulÃ©s\n\n\nBronze (Append)\n/10\nDonnÃ©es ingÃ©rÃ©es, partitionnÃ©es, checkpointing\n\n\nSilver (MERGE INTO)\n/20\nâ­ Pattern foreachBatch + MERGE correct, dÃ©dup\n\n\nGold (dbt)\n/20\n5 models, incremental, ref() correct\n\n\nTests dbt\n/10\nTests passent, couverture suffisante\n\n\nGreat Expectations\n/10\nSuites crÃ©Ã©es, checkpoint fonctionne\n\n\nAirflow DAG\n/5\nDAG fonctionne, dÃ©pendances correctes\n\n\nDocumentation\n/5\nREADME clair, schÃ©mas, instructions\n\n\nTOTAL\n/100",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#compÃ©tences-validÃ©es",
    "href": "notebooks/intermediate/26_projet_integrateur.html#compÃ©tences-validÃ©es",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "CompÃ©tences ValidÃ©es",
    "text": "CompÃ©tences ValidÃ©es\nEn complÃ©tant ce projet, tu valides les compÃ©tences suivantes :\n\n\n\nModule\nCompÃ©tence\nAppliquÃ©e dans\nâœ…\n\n\n\n\n14-16\nPython, environnements\nProducteurs Kafka\nâ˜\n\n\n17\nSQL\nTransformations dbt\nâ˜\n\n\n18-20\nPySpark DataFrame\nJobs Spark\nâ˜\n\n\n22\nAirflow\nOrchestration DAG\nâ˜\n\n\n23\nDelta Lake, MERGE INTO\nBronze â†’ Silver\nâ˜\n\n\n24\nKafka, Spark SSS, foreachBatch\nIngestion streaming\nâ˜\n\n\n25\ndbt, Great Expectations\nGold + QualitÃ©\nâ˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#extensions-possibles-bonus",
    "href": "notebooks/intermediate/26_projet_integrateur.html#extensions-possibles-bonus",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "Extensions Possibles (Bonus)",
    "text": "Extensions Possibles (Bonus)\nSi tu as terminÃ© le projet principal, voici des extensions pour aller plus loin :\n\n\n\n\n\n\n\n\nExtension\nDescription\nDifficultÃ©\n\n\n\n\nMonitoring\nAjouter Prometheus + Grafana pour monitorer le pipeline\nâ­â­\n\n\nKubernetes\nDÃ©ployer sur K8s avec Spark Operator\nâ­â­â­\n\n\nML Pipeline\nAjouter un modÃ¨le de prÃ©diction (churn, LTV)\nâ­â­\n\n\nCDC\nUtiliser Debezium pour capturer les changes\nâ­â­\n\n\nData Catalog\nIntÃ©grer DataHub ou Amundsen\nâ­â­â­\n\n\nStreamlit\nCrÃ©er un dashboard interactif\nâ­",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#solution-complÃ¨te",
    "href": "notebooks/intermediate/26_projet_integrateur.html#solution-complÃ¨te",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "ğŸ“ Solution ComplÃ¨te",
    "text": "ğŸ“ Solution ComplÃ¨te\n\n\nğŸ“‚ Cliquer pour voir la solution complÃ¨te\n\n\ndocker-compose.yml\nversion: '3.8'\n\nservices:\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # KAFKA\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    container_name: zookeeper\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    container_name: kafka\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n      - \"29092:29092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n\n  schema-registry:\n    image: confluentinc/cp-schema-registry:7.5.0\n    container_name: schema-registry\n    depends_on:\n      - kafka\n    ports:\n      - \"8081:8081\"\n    environment:\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092\n\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # STORAGE (MinIO = S3 local)\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  minio:\n    image: minio/minio\n    container_name: minio\n    ports:\n      - \"9000:9000\"\n      - \"9001:9001\"\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio_data:/data\n\n  # CrÃ©er le bucket au dÃ©marrage\n  minio-setup:\n    image: minio/mc\n    depends_on:\n      - minio\n    entrypoint: &gt;\n      /bin/sh -c \"\n      sleep 5;\n      mc alias set myminio http://minio:9000 minioadmin minioadmin;\n      mc mb myminio/lakehouse --ignore-existing;\n      exit 0;\n      \"\n\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # SPARK\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  spark-master:\n    image: bitnami/spark:3.5\n    container_name: spark-master\n    environment:\n      - SPARK_MODE=master\n      - SPARK_RPC_AUTHENTICATION_ENABLED=no\n      - SPARK_RPC_ENCRYPTION_ENABLED=no\n    ports:\n      - \"8080:8080\"\n      - \"7077:7077\"\n    volumes:\n      - ./spark_jobs:/opt/spark_jobs\n      - ./data:/opt/data\n\n  spark-worker:\n    image: bitnami/spark:3.5\n    container_name: spark-worker\n    depends_on:\n      - spark-master\n    environment:\n      - SPARK_MODE=worker\n      - SPARK_MASTER_URL=spark://spark-master:7077\n      - SPARK_WORKER_MEMORY=2G\n      - SPARK_WORKER_CORES=2\n    volumes:\n      - ./spark_jobs:/opt/spark_jobs\n      - ./data:/opt/data\n\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # AIRFLOW\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  postgres:\n    image: postgres:15\n    container_name: postgres\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: airflow\n      POSTGRES_DB: airflow\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  airflow-webserver:\n    image: apache/airflow:2.8.0\n    container_name: airflow-webserver\n    depends_on:\n      - postgres\n    environment:\n      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n      AIRFLOW__CORE__FERNET_KEY: ''\n      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'\n    ports:\n      - \"8082:8080\"\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./dbt_olist:/opt/dbt\n    command: bash -c \"airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && airflow webserver\"\n\n  airflow-scheduler:\n    image: apache/airflow:2.8.0\n    container_name: airflow-scheduler\n    depends_on:\n      - airflow-webserver\n    environment:\n      AIRFLOW__CORE__EXECUTOR: LocalExecutor\n      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n    volumes:\n      - ./dags:/opt/airflow/dags\n      - ./dbt_olist:/opt/dbt\n    command: airflow scheduler\n\nvolumes:\n  minio_data:\n  postgres_data:\n\n\n\nproducers/orders_producer.py\nfrom kafka import KafkaProducer\nimport pandas as pd\nimport json\nimport time\nimport random\nfrom datetime import datetime, timedelta\n\n# Configuration\nKAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']\nTOPIC = 'raw_orders'\nCSV_PATH = 'data/olist_orders_dataset.csv'\n\n# Producteur Kafka\nproducer = KafkaProducer(\n    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n    value_serializer=lambda v: json.dumps(v, default=str).encode('utf-8'),\n    key_serializer=lambda k: k.encode('utf-8') if k else None\n)\n\n# Lire le CSV\ndf = pd.read_csv(CSV_PATH)\nprint(f\"Loaded {len(df)} orders\")\n\n# Envoyer les messages\nfor idx, row in df.iterrows():\n    message = row.to_dict()\n    \n    # Timestamp d'ingestion\n    ingestion_ts = datetime.now()\n    \n    # Simuler late data (5%)\n    if random.random() &lt; 0.05:\n        ingestion_ts -= timedelta(minutes=random.randint(1, 5))\n    \n    message['_ingestion_timestamp'] = ingestion_ts.isoformat()\n    \n    # Envoyer\n    producer.send(\n        topic=TOPIC,\n        key=message['order_id'],\n        value=message\n    )\n    \n    # Simuler doublons (2%)\n    if random.random() &lt; 0.02:\n        producer.send(topic=TOPIC, key=message['order_id'], value=message)\n    \n    # Log progress\n    if idx % 1000 == 0:\n        print(f\"Sent {idx}/{len(df)} messages\")\n    \n    # DÃ©lai alÃ©atoire (simuler streaming)\n    time.sleep(random.uniform(0.05, 0.2))\n\nproducer.flush()\nprint(f\"Done! Sent {len(df)} messages to {TOPIC}\")\n\n\n\nspark_jobs/bronze/ingest_orders.py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_json, col, current_timestamp, to_date\nfrom pyspark.sql.types import StructType, StructField, StringType\n\n# Spark Session\nspark = SparkSession.builder \\\n    .appName(\"Bronze - Ingest Orders\") \\\n    .config(\"spark.jars.packages\", \n            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n            \"io.delta:delta-spark_2.12:3.1.0,\"\n            \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\n# Schema des orders\norder_schema = StructType([\n    StructField(\"order_id\", StringType()),\n    StructField(\"customer_id\", StringType()),\n    StructField(\"order_status\", StringType()),\n    StructField(\"order_purchase_timestamp\", StringType()),\n    StructField(\"order_approved_at\", StringType()),\n    StructField(\"order_delivered_carrier_date\", StringType()),\n    StructField(\"order_delivered_customer_date\", StringType()),\n    StructField(\"order_estimated_delivery_date\", StringType()),\n    StructField(\"_ingestion_timestamp\", StringType())\n])\n\n# Lire depuis Kafka\nkafka_df = spark.readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n    .option(\"subscribe\", \"raw_orders\") \\\n    .option(\"startingOffsets\", \"earliest\") \\\n    .load()\n\n# Parser le JSON\nparsed_df = kafka_df \\\n    .selectExpr(\"CAST(value AS STRING) as json_value\") \\\n    .select(from_json(col(\"json_value\"), order_schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .withColumn(\"_bronze_ingested_at\", current_timestamp()) \\\n    .withColumn(\"_ingestion_date\", to_date(col(\"_ingestion_timestamp\")))\n\n# Ã‰crire en Bronze (Append)\nquery = parsed_df.writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"s3a://lakehouse/checkpoints/bronze_orders\") \\\n    .option(\"path\", \"s3a://lakehouse/bronze/orders\") \\\n    .partitionBy(\"_ingestion_date\") \\\n    .start()\n\nquery.awaitTermination()\n\n\n\nspark_jobs/silver/silver_orders.py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, current_timestamp, row_number\nfrom pyspark.sql.window import Window\nfrom delta.tables import DeltaTable\n\n# Spark Session (mÃªme config que Bronze)\nspark = SparkSession.builder \\\n    .appName(\"Silver - Orders MERGE\") \\\n    .config(\"spark.jars.packages\", \n            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n            \"io.delta:delta-spark_2.12:3.1.0,\"\n            \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\nBRONZE_PATH = \"s3a://lakehouse/bronze/orders\"\nSILVER_PATH = \"s3a://lakehouse/silver/orders\"\nCHECKPOINT_PATH = \"s3a://lakehouse/checkpoints/silver_orders\"\n\ndef upsert_to_silver(batch_df, batch_id):\n    \"\"\"Upsert vers Silver avec dÃ©duplication.\"\"\"\n    \n    if batch_df.count() == 0:\n        print(f\"Batch {batch_id}: No data\")\n        return\n    \n    # 1. DÃ©duplication (garder le plus rÃ©cent par order_id)\n    window = Window.partitionBy(\"order_id\").orderBy(col(\"_bronze_ingested_at\").desc())\n    deduped = batch_df \\\n        .withColumn(\"_row_num\", row_number().over(window)) \\\n        .filter(col(\"_row_num\") == 1) \\\n        .drop(\"_row_num\")\n    \n    # 2. Ajouter timestamp Silver\n    enriched = deduped.withColumn(\"_silver_updated_at\", current_timestamp())\n    \n    # 3. MERGE INTO\n    if DeltaTable.isDeltaTable(spark, SILVER_PATH):\n        delta_table = DeltaTable.forPath(spark, SILVER_PATH)\n        \n        delta_table.alias(\"target\").merge(\n            enriched.alias(\"source\"),\n            \"target.order_id = source.order_id\"\n        ).whenMatchedUpdate(\n            condition=\"source._bronze_ingested_at &gt; target._bronze_ingested_at\",\n            set={\n                \"order_status\": \"source.order_status\",\n                \"order_delivered_carrier_date\": \"source.order_delivered_carrier_date\",\n                \"order_delivered_customer_date\": \"source.order_delivered_customer_date\",\n                \"_bronze_ingested_at\": \"source._bronze_ingested_at\",\n                \"_silver_updated_at\": \"source._silver_updated_at\"\n            }\n        ).whenNotMatchedInsertAll().execute()\n        \n        print(f\"Batch {batch_id}: MERGED {enriched.count()} records\")\n    else:\n        # PremiÃ¨re exÃ©cution\n        enriched.write.format(\"delta\").mode(\"overwrite\").save(SILVER_PATH)\n        print(f\"Batch {batch_id}: CREATED table with {enriched.count()} records\")\n\n# Lire Bronze en streaming\nbronze_stream = spark.readStream \\\n    .format(\"delta\") \\\n    .load(BRONZE_PATH)\n\n# Ã‰crire avec foreachBatch\nquery = bronze_stream.writeStream \\\n    .foreachBatch(upsert_to_silver) \\\n    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n    .trigger(processingTime=\"30 seconds\") \\\n    .start()\n\nquery.awaitTermination()\n\n\n\ndbt_olist/models/gold/gold_daily_sales.sql\n{{ config(\n    materialized='incremental',\n    unique_key='order_date',\n    incremental_strategy='merge'\n) }}\n\nWITH orders AS (\n    SELECT\n        o.order_id,\n        o.order_status,\n        DATE(o.order_purchase_timestamp) AS order_date,\n        oi.price,\n        oi.freight_value\n    FROM {{ ref('stg_orders') }} o\n    JOIN {{ ref('stg_order_items') }} oi ON o.order_id = oi.order_id\n    WHERE o.order_status = 'delivered'\n    {% if is_incremental() %}\n    AND DATE(o.order_purchase_timestamp) &gt;= (\n        SELECT MAX(order_date) - INTERVAL 2 DAY FROM {{ this }}\n    )\n    {% endif %}\n)\n\nSELECT\n    order_date,\n    COUNT(DISTINCT order_id) AS total_orders,\n    SUM(price + freight_value) AS total_revenue,\n    AVG(price + freight_value) AS avg_order_value,\n    COUNT(*) AS total_items\nFROM orders\nGROUP BY order_date\nORDER BY order_date\n\n\n\ndbt_olist/models/gold/gold_customer_rfm.sql\n{{ config(materialized='table') }}\n\nWITH customer_orders AS (\n    SELECT\n        c.customer_unique_id,\n        o.order_id,\n        o.order_purchase_timestamp,\n        oi.price + oi.freight_value AS order_value\n    FROM {{ ref('stg_customers') }} c\n    JOIN {{ ref('stg_orders') }} o ON c.customer_id = o.customer_id\n    JOIN {{ ref('stg_order_items') }} oi ON o.order_id = oi.order_id\n    WHERE o.order_status = 'delivered'\n),\n\nrfm_base AS (\n    SELECT\n        customer_unique_id,\n        DATEDIFF(day, MAX(order_purchase_timestamp), CURRENT_DATE) AS recency_days,\n        COUNT(DISTINCT order_id) AS frequency,\n        SUM(order_value) AS monetary\n    FROM customer_orders\n    GROUP BY customer_unique_id\n)\n\nSELECT\n    customer_unique_id,\n    recency_days,\n    frequency,\n    ROUND(monetary, 2) AS monetary,\n    {{ rfm_segment('recency_days', 'frequency', 'monetary') }} AS rfm_segment\nFROM rfm_base\n\n\n\ndbt_olist/models/gold/_gold__models.yml\nversion: 2\n\nmodels:\n  - name: gold_daily_sales\n    description: \"Chiffre d'affaires quotidien\"\n    columns:\n      - name: order_date\n        tests:\n          - unique\n          - not_null\n      - name: total_revenue\n        tests:\n          - not_null\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n\n  - name: gold_customer_rfm\n    description: \"Segmentation RFM des clients\"\n    columns:\n      - name: customer_unique_id\n        tests:\n          - unique\n          - not_null\n      - name: rfm_segment\n        tests:\n          - accepted_values:\n              values: ['Champions', 'Loyal Customers', 'Potential Loyalists', 'At Risk', 'Hibernating', 'Others']\n\n  - name: gold_seller_performance\n    description: \"Performance des vendeurs\"\n    columns:\n      - name: seller_id\n        tests:\n          - unique\n          - not_null\n\n  - name: gold_product_analytics\n    description: \"Analyse des produits par catÃ©gorie\"\n    columns:\n      - name: product_category\n        tests:\n          - unique\n          - not_null\n\n  - name: gold_delivery_performance\n    description: \"Performance des livraisons\"\n    columns:\n      - name: on_time_rate\n        tests:\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 1",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#fÃ©licitations",
    "href": "notebooks/intermediate/26_projet_integrateur.html#fÃ©licitations",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "ğŸ‰ FÃ©licitations !",
    "text": "ğŸ‰ FÃ©licitations !\nEn complÃ©tant ce projet, tu as construit un pipeline de donnÃ©es complet utilisable en production.\nTu maÃ®trises maintenant : - âœ… Lâ€™ingestion temps rÃ©el avec Kafka - âœ… Le traitement streaming avec Spark Structured Streaming - âœ… Lâ€™architecture Lakehouse avec Delta Lake - âœ… Le pattern MERGE INTO pour les upserts - âœ… La modÃ©lisation analytique avec dbt - âœ… La validation de qualitÃ© avec Great Expectations - âœ… Lâ€™orchestration avec Airflow",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#ressources",
    "href": "notebooks/intermediate/26_projet_integrateur.html#ressources",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDataset Olist sur Kaggle\nDelta Lake Documentation\ndbt Documentation\nGreat Expectations Documentation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/26_projet_integrateur.html#et-maintenant",
    "href": "notebooks/intermediate/26_projet_integrateur.html#et-maintenant",
    "title": "Projet IntÃ©grateur : Pipeline E-commerce Olist",
    "section": "ğŸš€ Et Maintenant ?",
    "text": "ğŸš€ Et Maintenant ?\nCe projet constitue une base solide pour ton portfolio. Tu peux :\n\nLe dÃ©ployer sur le cloud (AWS, GCP, Azure)\nAjouter du monitoring (Prometheus + Grafana)\nIntÃ©grer du ML (prÃ©diction de churn, recommandations)\nLe prÃ©senter en entretien comme preuve de tes compÃ©tences\n\nBonne chance pour la suite de ton parcours Data Engineering ! ğŸ“",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ“¦ **Projet IntermÃ©diaire** (Olist)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html",
    "href": "notebooks/intermediate/25_dbt_data_quality.html",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  industrialiser tes transformations SQL et Ã  garantir la qualitÃ© de tes donnÃ©es avec les outils standards de lâ€™industrie.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#industrialiser-les-transformations-et-garantir-la-qualitÃ©",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#industrialiser-les-transformations-et-garantir-la-qualitÃ©",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  industrialiser tes transformations SQL et Ã  garantir la qualitÃ© de tes donnÃ©es avec les outils standards de lâ€™industrie.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#prÃ©requis",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#prÃ©requis",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nModule\nCompÃ©tence\nPourquoi ?\n\n\n\n\nâœ… 20\nSpark SQL\nSQL avancÃ©\n\n\nâœ… 23\nTable Formats\nDelta Lake comme cible\n\n\nâœ… 24\nStreaming\nDonnÃ©es incrÃ©mentales",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#objectifs",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#objectifs",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "Objectifs",
    "text": "Objectifs\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre le concept de Transformation-as-Code (TaC)\nMaÃ®triser dbt : Models, Sources, Tests, Documentation\nImplÃ©menter des tests de qualitÃ© avec dbt et Great Expectations\nCrÃ©er des modÃ¨les incrÃ©mentaux performants\nDÃ©ployer dbt dans un pipeline CI/CD",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#introduction-le-problÃ¨me-Ã -rÃ©soudre",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#introduction-le-problÃ¨me-Ã -rÃ©soudre",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "1. Introduction â€” Le ProblÃ¨me Ã  RÃ©soudre",
    "text": "1. Introduction â€” Le ProblÃ¨me Ã  RÃ©soudre\n\n1.1 Le chaos des transformations SQL\nImagine une Ã©quipe data typique sans dbt :\nSITUATION TYPIQUE SANS DBT :\n\nğŸ“ scripts/\nâ”œâ”€â”€ transform_users_v2_final_FINAL.sql      â† Quelle version utiliser ?\nâ”œâ”€â”€ transform_users_v2_final.sql\nâ”œâ”€â”€ transform_users_v2.sql\nâ”œâ”€â”€ create_dashboard_table_john.sql         â† Qui a Ã©crit Ã§a ?\nâ”œâ”€â”€ fix_bug_urgent.sql                      â† Ã‡a fait quoi exactement ?\nâ””â”€â”€ old/\n    â””â”€â”€ ... 50 fichiers abandonnÃ©s\n\nPROBLÃˆMES :\nâ€¢ Aucune traÃ§abilitÃ© (qui a modifiÃ© quoi ?)\nâ€¢ Pas de tests (les donnÃ©es sont-elles correctes ?)\nâ€¢ Pas de documentation (c'est quoi cette colonne ?)\nâ€¢ Ordre d'exÃ©cution manuel (quelle requÃªte lancer en premier ?)\nâ€¢ Pas de review de code (erreurs passent en production)\n\n\n1.2 La solution : Transformation-as-Code (TaC)\nTransformation-as-Code applique les principes du dÃ©veloppement logiciel aux transformations de donnÃ©es :\n\n\n\nPrincipe Dev\nApplication Data\n\n\n\n\nVersioning\nTransformations dans Git\n\n\nTests unitaires\nTests sur les donnÃ©es\n\n\nDocumentation\nAuto-gÃ©nÃ©rÃ©e depuis le code\n\n\nCode Review\nPR avant mise en production\n\n\nCI/CD\nDÃ©ploiement automatisÃ©\n\n\n\n\n\n1.3 Quâ€™est-ce que dbt ?\ndbt (Data Build Tool) est un outil open-source qui permet de transformer les donnÃ©es dans ton Data Warehouse/Lakehouse en utilisant uniquement du SQL.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        dbt - Data Build Tool                    â”‚\nâ”‚                                                                 â”‚\nâ”‚   ENTRÃ‰E              TRAITEMENT              SORTIE            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚   â”‚  Fichiersâ”‚       â”‚              â”‚       â”‚  Tables  â”‚       â”‚\nâ”‚   â”‚  SQL +   â”‚  â”€â”€â”€â–¶ â”‚  dbt run     â”‚  â”€â”€â”€â–¶ â”‚  ou Vues â”‚       â”‚\nâ”‚   â”‚  Jinja   â”‚       â”‚              â”‚       â”‚  testÃ©es â”‚       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\nâ”‚                                                                 â”‚\nâ”‚   + Tests automatiques                                          â”‚\nâ”‚   + Documentation gÃ©nÃ©rÃ©e                                       â”‚\nâ”‚   + Lineage (traÃ§abilitÃ©)                                       â”‚\nâ”‚   + Gestion des dÃ©pendances                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.4 Ce que dbt fait et ne fait PAS\n\n\n\ndbt FAIT âœ…\ndbt NE FAIT PAS âŒ\n\n\n\n\nTransformer (T de ELT)\nExtraire les donnÃ©es (E)\n\n\nTester les donnÃ©es\nCharger les donnÃ©es (L)\n\n\nDocumenter\nOrchestrer (scheduling)\n\n\nGÃ©rer les dÃ©pendances\nCalcul distribuÃ©\n\n\nVersionner\nIngestion temps rÃ©el\n\n\n\ndbt sâ€™intÃ¨gre avec : Spark (pour le calcul), Airflow (pour lâ€™orchestration), Delta/Iceberg (pour le stockage).\n\n\n1.5 dbt Core vs dbt Cloud\n\n\n\nAspect\ndbt Core\ndbt Cloud\n\n\n\n\nPrix\nGratuit (open-source)\nPayant (SaaS)\n\n\nInstallation\nCLI local\nIDE web\n\n\nScheduling\nManuel (Airflow, cron)\nIntÃ©grÃ©\n\n\nCI/CD\nÃ€ configurer\nIntÃ©grÃ©\n\n\nIDE\nVS Code, vim, etc.\nIDE web dÃ©diÃ©\n\n\nCollaboration\nVia Git\nVia plateforme\n\n\n\nDans ce module, nous utilisons dbt Core (CLI) car câ€™est ce que tu utiliseras dans un environnement Spark/Kubernetes.\n\n\n1.6 dbt dans lâ€™architecture Data\nOÃ¹ se place dbt dans ton architecture ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         ARCHITECTURE MODERNE                            â”‚\nâ”‚                                                                         â”‚\nâ”‚   INGESTION          STOCKAGE           TRANSFORMATION      SERVING     â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚ Kafka   â”‚       â”‚ Bronze  â”‚        â”‚             â”‚    â”‚ BI Tool â”‚  â”‚\nâ”‚   â”‚ Spark   â”‚  â”€â”€â”€â–¶ â”‚ (Raw)   â”‚  â”€â”€â”€â–¶  â”‚    dbt      â”‚â”€â”€â”€â–¶â”‚ ML      â”‚  â”‚\nâ”‚   â”‚ Airbyte â”‚       â”‚ Silver  â”‚        â”‚             â”‚    â”‚ API     â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ Gold    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚                        â”‚\nâ”‚                     Delta Lake            Tests &                      â”‚\nâ”‚                     Iceberg               Documentation                â”‚\nâ”‚                                                                         â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    ORCHESTRATION (Airflow / K8s)                â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#installation-et-structure-dun-projet-dbt",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#installation-et-structure-dun-projet-dbt",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "2. Installation et Structure dâ€™un Projet dbt",
    "text": "2. Installation et Structure dâ€™un Projet dbt\n\n2.1 Installation\ndbt sâ€™installe via pip avec un adapter spÃ©cifique Ã  ta base de donnÃ©es :\n\n\nVoir le code\n# Installation de dbt\n\ninstallation = '''\n# Adapter selon ton Data Warehouse/Lakehouse :\n\npip install dbt-core              # Core (obligatoire)\n\n# + UN adapter selon ta cible :\npip install dbt-postgres          # PostgreSQL\npip install dbt-snowflake         # Snowflake\npip install dbt-bigquery          # BigQuery\npip install dbt-spark             # Spark (Databricks, EMR, local)\npip install dbt-duckdb            # DuckDB (local, lÃ©ger)\npip install dbt-trino             # Trino/Presto\n\n# VÃ©rifier l'installation\ndbt --version\n'''\n\nprint(installation)\nprint(\"\\nğŸ’¡ Pour ce module, on utilise dbt-duckdb (lÃ©ger, local) ou dbt-spark.\")\n\n\n\n\n2.2 Initialisation dâ€™un projet\nLa commande dbt init crÃ©e la structure de base :\n\n\nVoir le code\n# CrÃ©er un nouveau projet dbt\n\ninit_project = '''\n# CrÃ©er un nouveau projet\ndbt init my_dbt_project\n\n# Structure crÃ©Ã©e :\nmy_dbt_project/\nâ”‚\nâ”œâ”€â”€ dbt_project.yml          # Configuration principale du projet\nâ”œâ”€â”€ profiles.yml             # Connexions aux bases (souvent dans ~/.dbt/)\nâ”‚\nâ”œâ”€â”€ models/                  # â­ TES TRANSFORMATIONS SQL\nâ”‚   â””â”€â”€ example/\nâ”‚       â”œâ”€â”€ my_first_model.sql\nâ”‚       â””â”€â”€ schema.yml       # Tests et documentation\nâ”‚\nâ”œâ”€â”€ seeds/                   # DonnÃ©es statiques (CSV)\nâ”‚\nâ”œâ”€â”€ snapshots/               # Historisation (SCD Type 2)\nâ”‚\nâ”œâ”€â”€ macros/                  # Fonctions Jinja rÃ©utilisables\nâ”‚\nâ”œâ”€â”€ tests/                   # Tests SQL custom\nâ”‚\nâ”œâ”€â”€ analyses/                # RequÃªtes ad-hoc (non matÃ©rialisÃ©es)\nâ”‚\nâ””â”€â”€ target/                  # Fichiers compilÃ©s (gÃ©nÃ©rÃ©)\n'''\n\nprint(init_project)\n\n\n\n\n2.3 Le fichier dbt_project.yml\nCâ€™est le fichier de configuration principal de ton projet. Il dÃ©finit le nom, la version, et les paramÃ¨tres par dÃ©faut.\n\n\nVoir le code\n# dbt_project.yml - Configuration principale\n\ndbt_project_yml = '''\n# dbt_project.yml\n\nname: 'ecommerce_analytics'      # Nom du projet (sans espaces, sans tirets)\nversion: '1.0.0'\nconfig-version: 2\n\n# Profil de connexion (dÃ©fini dans profiles.yml)\nprofile: 'ecommerce_analytics'\n\n# Chemins des diffÃ©rents composants\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\ntarget-path: \"target\"           # OÃ¹ dbt Ã©crit les fichiers compilÃ©s\nclean-targets: [\"target\", \"dbt_packages\"]\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Configuration des modÃ¨les (TRÃˆS IMPORTANT)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nmodels:\n  ecommerce_analytics:           # Doit correspondre au \"name\" ci-dessus\n    \n    # Configuration par dÃ©faut pour TOUS les modÃ¨les\n    +materialized: view          # Par dÃ©faut : crÃ©er des vues\n    \n    # Configuration par dossier\n    staging:                     # Dossier models/staging/\n      +materialized: view\n      +schema: staging           # Ã‰crire dans le schÃ©ma \"staging\"\n    \n    silver:\n      +materialized: table       # Tables complÃ¨tes\n      +schema: silver\n    \n    gold:\n      +materialized: incremental # IncrÃ©mental pour la performance\n      +schema: gold\n'''\n\nprint(dbt_project_yml)\n\n\n\n\n2.4 Le fichier profiles.yml\nCe fichier contient les informations de connexion Ã  ta base de donnÃ©es. Il est gÃ©nÃ©ralement stockÃ© dans ~/.dbt/profiles.yml (pas dans le repo Git pour des raisons de sÃ©curitÃ©).\n\n\nVoir le code\n# profiles.yml - Connexions aux bases de donnÃ©es\n\nprofiles_yml = '''\n# ~/.dbt/profiles.yml\n\necommerce_analytics:             # Doit correspondre au \"profile\" dans dbt_project.yml\n  \n  target: dev                    # Environnement par dÃ©faut\n  \n  outputs:\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Environnement DEV (dÃ©veloppement local)\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    dev:\n      type: duckdb               # Adapter DuckDB (lÃ©ger, local)\n      path: /tmp/dev.duckdb\n      threads: 4\n    \n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Environnement PROD (Spark/Databricks)\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    prod:\n      type: spark\n      method: thrift             # ou \"odbc\", \"http\"\n      host: spark-thrift-server.company.com\n      port: 10001\n      schema: analytics\n      threads: 8\n    \n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Exemple Snowflake\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    snowflake_prod:\n      type: snowflake\n      account: xy12345.us-east-1\n      user: \"{{ env_var('SNOWFLAKE_USER') }}\"      # Variable d'environnement\n      password: \"{{ env_var('SNOWFLAKE_PASSWORD') }}\"\n      role: TRANSFORMER\n      database: ANALYTICS\n      warehouse: TRANSFORM_WH\n      schema: GOLD\n      threads: 8\n'''\n\nprint(profiles_yml)\nprint(\"\\nğŸ’¡ Utilise env_var() pour les secrets, jamais en dur !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#les-models-cÅ“ur-de-dbt",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#les-models-cÅ“ur-de-dbt",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "3. Les Models â€” CÅ“ur de dbt",
    "text": "3. Les Models â€” CÅ“ur de dbt\n\n3.1 Quâ€™est-ce quâ€™un Model ?\nUn model dbt est simplement un fichier SQL qui contient une requÃªte SELECT. dbt se charge de crÃ©er la table ou vue correspondante.\nFICHIER SQL                           CE QUE DBT FAIT\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ -- models/users.sql    â”‚          â”‚ CREATE TABLE users AS   â”‚\nâ”‚                        â”‚   â”€â”€â”€â–¶   â”‚ SELECT                  â”‚\nâ”‚ SELECT                 â”‚          â”‚   id,                   â”‚\nâ”‚   id,                  â”‚          â”‚   name,                 â”‚\nâ”‚   name,                â”‚          â”‚   email                 â”‚\nâ”‚   email                â”‚          â”‚ FROM raw.users          â”‚\nâ”‚ FROM raw.users         â”‚          â”‚                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nTu Ã©cris le SELECT, dbt gÃ¨re le CREATE/INSERT !\n\n\nVoir le code\n# Exemple de model simple\n\nmodel_simple = '''\n-- models/staging/stg_orders.sql\n\n-- Configuration du model (optionnel, peut Ãªtre dans dbt_project.yml)\n{{ config(\n    materialized='view',\n    schema='staging'\n) }}\n\n-- Le SELECT qui dÃ©finit le model\nSELECT\n    order_id,\n    customer_id,\n    order_date,\n    CAST(amount AS DECIMAL(10, 2)) AS amount,\n    status,\n    created_at\nFROM {{ source('raw', 'orders') }}  -- RÃ©fÃ©rence Ã  une source externe\nWHERE order_date &gt;= '2024-01-01'    -- Filtrer les donnÃ©es rÃ©centes\n'''\n\nprint(model_simple)\nprint(\"\\nğŸ’¡ Points clÃ©s :\")\nprint(\"â€¢ {{ config(...) }} : Configuration spÃ©cifique au model\")\nprint(\"â€¢ {{ source(...) }} : RÃ©fÃ©rence Ã  une table externe\")\nprint(\"â€¢ Pas de CREATE TABLE : dbt le gÃ©nÃ¨re automatiquement\")\n\n\n\n\n3.2 La fonction ref() â€” La clÃ© de dbt\nref() est LA fonction la plus importante de dbt. Elle permet de rÃ©fÃ©rencer un autre model, et dbt : 1. Comprend la dÃ©pendance entre les models 2. ExÃ©cute les models dans le bon ordre 3. GÃ©nÃ¨re le lineage (traÃ§abilitÃ©)\nSANS ref() (MAUVAIS)                 AVEC ref() (BON)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ SELECT *                â”‚         â”‚ SELECT *                â”‚\nâ”‚ FROM analytics.users    â”‚         â”‚ FROM {{ ref('users') }} â”‚\nâ”‚                         â”‚         â”‚                         â”‚\nâ”‚ âŒ SchÃ©ma hardcodÃ©      â”‚         â”‚ âœ… dbt rÃ©sout le schÃ©ma â”‚\nâ”‚ âŒ Pas de dÃ©pendance    â”‚         â”‚ âœ… DÃ©pendance trackÃ©e   â”‚\nâ”‚ âŒ Pas de lineage       â”‚         â”‚ âœ… Lineage automatique  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Exemple avec ref()\n\nmodel_with_ref = '''\n-- models/silver/silver_orders.sql\n\n{{ config(materialized='table') }}\n\nSELECT\n    o.order_id,\n    o.customer_id,\n    c.customer_name,\n    c.customer_email,\n    o.order_date,\n    o.amount,\n    o.status\nFROM {{ ref('stg_orders') }} o           -- RÃ©fÃ©rence au model stg_orders\nLEFT JOIN {{ ref('stg_customers') }} c   -- RÃ©fÃ©rence au model stg_customers\n    ON o.customer_id = c.customer_id\nWHERE o.status != 'cancelled'\n'''\n\nprint(model_with_ref)\nprint(\"\\nğŸ“Š Ce que dbt comprend :\")\nprint(\"\")\nprint(\"   stg_orders â”€â”€â”€â”€â”€â”€â”\")\nprint(\"                    â”œâ”€â”€â–¶ silver_orders\")\nprint(\"   stg_customers â”€â”€â”€â”˜\")\nprint(\"\")\nprint(\"dbt exÃ©cutera stg_orders et stg_customers AVANT silver_orders.\")\n\n\n\n\n3.3 Les Sources â€” RÃ©fÃ©rencer les donnÃ©es externes\nLes sources reprÃ©sentent les donnÃ©es qui existent AVANT dbt (tables raw, donnÃ©es ingÃ©rÃ©es par Kafka/Spark, etc.).\nOn les dÃ©finit dans un fichier schema.yml :\n\n\nVoir le code\n# DÃ©finition des sources\n\nsources_yml = '''\n# models/staging/schema.yml\n\nversion: 2\n\nsources:\n  - name: raw                      # Nom logique de la source\n    description: \"DonnÃ©es brutes ingÃ©rÃ©es par Kafka/Spark\"\n    database: bronze               # Base de donnÃ©es (optionnel)\n    schema: public                 # SchÃ©ma\n    \n    tables:\n      - name: orders\n        description: \"Commandes brutes\"\n        columns:\n          - name: order_id\n            description: \"Identifiant unique de la commande\"\n          - name: amount\n            description: \"Montant en euros\"\n        \n        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n        # FRESHNESS : VÃ©rifier que les donnÃ©es arrivent\n        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n        loaded_at_field: created_at  # Colonne de timestamp\n        freshness:\n          warn_after: {count: 12, period: hour}   # Warning si &gt; 12h\n          error_after: {count: 24, period: hour}  # Erreur si &gt; 24h\n      \n      - name: customers\n        description: \"Clients\"\n        freshness:\n          warn_after: {count: 24, period: hour}\n          error_after: {count: 48, period: hour}\n'''\n\nprint(sources_yml)\nprint(\"\\nğŸ’¡ La freshness permet de dÃ©tecter les problÃ¨mes d'ingestion !\")\nprint(\"   Commande : dbt source freshness\")\n\n\n\n\n3.4 source() vs ref()\n\n\n\nFonction\nUtilisation\nExemple\n\n\n\n\nsource()\nDonnÃ©es externes Ã  dbt\nTables raw, Kafka, APIs\n\n\nref()\nDonnÃ©es crÃ©Ã©es par dbt\nAutres models\n\n\n\nFlux de donnÃ©es :\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   RAW TABLES    â”‚       â”‚   STAGING       â”‚       â”‚   SILVER/GOLD   â”‚\nâ”‚   (externes)    â”‚       â”‚   (dbt)         â”‚       â”‚   (dbt)         â”‚\nâ”‚                 â”‚       â”‚                 â”‚       â”‚                 â”‚\nâ”‚   raw.orders    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   stg_orders    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   silver_orders â”‚\nâ”‚                 â”‚       â”‚                 â”‚       â”‚                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                         â”‚                         â”‚\n   source()                   ref()                     ref()\n\n\n3.5 MatÃ©rialisations\nLa matÃ©rialisation dÃ©finit COMMENT dbt crÃ©e lâ€™objet dans la base de donnÃ©es :\n\n\n\n\n\n\n\n\nMatÃ©rialisation\nDescription\nQuand lâ€™utiliser\n\n\n\n\nview\nCrÃ©e une vue SQL\nStaging, peu de donnÃ©es\n\n\ntable\nCrÃ©e une table (full refresh)\nSilver, taille moyenne\n\n\nincremental\nAjoute seulement les nouvelles lignes\nGold, gros volumes\n\n\nephemeral\nCTE (pas de table crÃ©Ã©e)\nSous-requÃªtes rÃ©utilisables\n\n\n\n\n\nVoir le code\n# Comparaison des matÃ©rialisations\n\nmaterializations = '''\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# VIEW : Pas de stockage, requÃªte Ã  chaque lecture\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n{{ config(materialized='view') }}\nSELECT * FROM {{ source('raw', 'events') }}\n\n-- dbt gÃ©nÃ¨re : CREATE VIEW ... AS SELECT ...\n-- âœ… Toujours Ã  jour\n-- âŒ Lent si requÃªte complexe\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TABLE : Stockage physique, reconstruction complÃ¨te\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n{{ config(materialized='table') }}\nSELECT * FROM {{ ref('stg_events') }}\n\n-- dbt gÃ©nÃ¨re : CREATE TABLE ... AS SELECT ... (ou DROP + CREATE)\n-- âœ… Lecture rapide\n-- âŒ Rebuild complet Ã  chaque run\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# INCREMENTAL : Seulement les nouvelles donnÃ©es\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n{{ config(\n    materialized='incremental',\n    unique_key='event_id'           # Pour le MERGE/upsert\n) }}\n\nSELECT * FROM {{ ref('stg_events') }}\n{% if is_incremental() %}           -- Condition pour les runs incrÃ©mentaux\nWHERE event_time &gt; (SELECT MAX(event_time) FROM {{ this }})\n{% endif %}\n\n-- dbt gÃ©nÃ¨re : INSERT INTO ... SELECT ... WHERE ...\n-- âœ… TrÃ¨s rapide pour gros volumes\n-- âŒ Plus complexe Ã  maintenir\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# EPHEMERAL : CTE, pas de table\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n{{ config(materialized='ephemeral') }}\nSELECT DISTINCT customer_id FROM {{ ref('orders') }}\n\n-- dbt l'injecte comme CTE dans les models qui le ref()\n-- âœ… Pas de table supplÃ©mentaire\n-- âŒ RecalculÃ© Ã  chaque utilisation\n'''\n\nprint(materializations)\n\n\n\n\nExercice 1 : CrÃ©er un projet dbt avec 3 models\nObjectif : CrÃ©er une structure dbt avec staging â†’ silver.\nStructure Ã  crÃ©er :\n\necommerce_dbt/\nâ”œâ”€â”€ dbt_project.yml\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ schema.yml          # Sources + tests\nâ”‚   â”‚   â”œâ”€â”€ stg_orders.sql      # Nettoyage orders\nâ”‚   â”‚   â””â”€â”€ stg_customers.sql   # Nettoyage customers\nâ”‚   â””â”€â”€ silver/\nâ”‚       â””â”€â”€ silver_orders.sql   # Join orders + customers\n\n\nğŸ’¡ Solution\n\n-- models/staging/stg_orders.sql\n{{ config(materialized='view') }}\nSELECT\n    order_id,\n    customer_id,\n    CAST(amount AS DECIMAL(10,2)) AS amount,\n    order_date\nFROM {{ source('raw', 'orders') }}\n\n-- models/staging/stg_customers.sql\n{{ config(materialized='view') }}\nSELECT\n    customer_id,\n    TRIM(name) AS customer_name,\n    LOWER(email) AS email\nFROM {{ source('raw', 'customers') }}\n\n-- models/silver/silver_orders.sql\n{{ config(materialized='table') }}\nSELECT\n    o.order_id,\n    o.customer_id,\n    c.customer_name,\n    c.email,\n    o.amount,\n    o.order_date\nFROM {{ ref('stg_orders') }} o\nLEFT JOIN {{ ref('stg_customers') }} c USING (customer_id)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#tests-de-qualitÃ©-dans-dbt",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#tests-de-qualitÃ©-dans-dbt",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "4. Tests de QualitÃ© dans dbt",
    "text": "4. Tests de QualitÃ© dans dbt\n\n4.1 Pourquoi tester les donnÃ©es ?\nLes donnÃ©es, comme le code, peuvent avoir des bugs :\n\n\n\nProblÃ¨me\nConsÃ©quence\nTest dbt\n\n\n\n\nDoublons sur order_id\nRevenus gonflÃ©s\nunique\n\n\ncustomer_id manquant\nJoins Ã©chouent\nnot_null\n\n\nStatus invalide\nDashboard cassÃ©\naccepted_values\n\n\nFK inexistante\nDonnÃ©es orphelines\nrelationships\n\n\n\n\n\n4.2 Tests GÃ©nÃ©riques (built-in)\ndbt fournit 4 tests de base, dÃ©finis dans schema.yml :\n\n\nVoir le code\n# Tests gÃ©nÃ©riques dans schema.yml\n\ngeneric_tests = '''\n# models/silver/schema.yml\n\nversion: 2\n\nmodels:\n  - name: silver_orders\n    description: \"Table des commandes enrichies avec infos clients\"\n    \n    columns:\n      - name: order_id\n        description: \"Identifiant unique de la commande\"\n        tests:\n          - unique              # Pas de doublons\n          - not_null            # Jamais NULL\n      \n      - name: customer_id\n        description: \"RÃ©fÃ©rence au client\"\n        tests:\n          - not_null\n          - relationships:      # ClÃ© Ã©trangÃ¨re\n              to: ref('stg_customers')\n              field: customer_id\n      \n      - name: status\n        description: \"Statut de la commande\"\n        tests:\n          - accepted_values:    # Valeurs autorisÃ©es\n              values: ['pending', 'shipped', 'delivered', 'cancelled']\n      \n      - name: amount\n        description: \"Montant en euros\"\n        tests:\n          - not_null\n'''\n\nprint(generic_tests)\nprint(\"\\n# ExÃ©cuter les tests :\")\nprint(\"dbt test                    # Tous les tests\")\nprint(\"dbt test --select silver_orders  # Tests d'un model\")\n\n\n\n\n4.3 Tests Singuliers (custom)\nPour des validations complexes, tu peux Ã©crire des tests SQL custom. Un test rÃ©ussit sâ€™il retourne 0 lignes.\n\n\nVoir le code\n# Tests singuliers (custom SQL)\n\nsingular_tests = '''\n-- tests/assert_positive_amounts.sql\n\n-- Ce test Ã‰CHOUE si la requÃªte retourne des lignes\n-- (on cherche les cas problÃ©matiques)\n\nSELECT\n    order_id,\n    amount\nFROM {{ ref('silver_orders') }}\nWHERE amount &lt;= 0\n\n-- Si cette requÃªte retourne des lignes = montants nÃ©gatifs ou nuls = Ã‰CHEC\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- tests/assert_orders_have_recent_dates.sql\n\nSELECT\n    order_id,\n    order_date\nFROM {{ ref('silver_orders') }}\nWHERE order_date &lt; '2020-01-01'\n   OR order_date &gt; CURRENT_DATE\n\n-- DÃ©tecte les dates suspectes (trop anciennes ou dans le futur)\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- tests/assert_revenue_consistency.sql\n\n-- VÃ©rifier que le total des commandes = total du dashboard\nSELECT 1\nFROM (\n    SELECT SUM(amount) AS orders_total FROM {{ ref('silver_orders') }}\n) orders\nJOIN (\n    SELECT SUM(revenue) AS dashboard_total FROM {{ ref('gold_dashboard') }}\n) dashboard\nON 1=1\nWHERE ABS(orders_total - dashboard_total) &gt; 0.01  -- TolÃ©rance\n'''\n\nprint(singular_tests)\n\n\n\n\n4.4 Packages dbt : Tests avancÃ©s\nLe dbt Hub contient des packages avec des tests supplÃ©mentaires. Les plus utiles :\n\n\n\n\n\n\n\nPackage\nTests fournis\n\n\n\n\ndbt-utils\nsurrogate_key, not_null_proportion, at_least_one\n\n\ndbt-expectations\nTests style Great Expectations dans dbt\n\n\ndbt-audit-helper\ncompare_relations pour les migrations\n\n\n\n\n\nVoir le code\n# Installation et utilisation des packages dbt\n\npackages = '''\n# packages.yml (Ã  la racine du projet)\n\npackages:\n  - package: dbt-labs/dbt_utils\n    version: 1.1.1\n  \n  - package: calogica/dbt_expectations\n    version: 0.10.1\n  \n  - package: dbt-labs/audit_helper\n    version: 0.9.0\n\n# Installer les packages :\n# dbt deps\n'''\n\nusage_examples = '''\n# Utilisation dans schema.yml\n\nmodels:\n  - name: silver_orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n      \n      - name: amount\n        tests:\n          # Test dbt-expectations : valeur entre 0 et 10000\n          - dbt_expectations.expect_column_values_to_be_between:\n              min_value: 0\n              max_value: 10000\n          \n          # Test dbt-utils : au moins 95% non-null\n          - dbt_utils.not_null_proportion:\n              at_least: 0.95\n      \n      - name: email\n        tests:\n          # Test dbt-expectations : format email\n          - dbt_expectations.expect_column_values_to_match_regex:\n              regex: \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n'''\n\nprint(\"# packages.yml\")\nprint(packages)\nprint(\"\\n# Utilisation des tests avancÃ©s\")\nprint(usage_examples)\n\n\n\n\nExercice 2 : Ajouter des tests de qualitÃ©\nObjectif : Ajouter des tests gÃ©nÃ©riques et un test singulier.\n# 1. Dans schema.yml, ajouter pour silver_orders :\n#    - order_id : unique, not_null\n#    - amount : not_null\n#    - status : accepted_values\n\n# 2. CrÃ©er un test singulier qui vÃ©rifie :\n#    - Aucune commande avec amount &gt; 100000 (fraude potentielle)\n\n\nğŸ’¡ Solution\n\n# schema.yml\nmodels:\n  - name: silver_orders\n    columns:\n      - name: order_id\n        tests: [unique, not_null]\n      - name: amount\n        tests: [not_null]\n      - name: status\n        tests:\n          - accepted_values:\n              values: ['pending', 'shipped', 'delivered', 'cancelled']\n-- tests/assert_no_suspicious_amounts.sql\nSELECT order_id, amount\nFROM {{ ref('silver_orders') }}\nWHERE amount &gt; 100000",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#documentation-et-lineage",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#documentation-et-lineage",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "5. Documentation et Lineage",
    "text": "5. Documentation et Lineage\n\n5.1 Documentation auto-gÃ©nÃ©rÃ©e\ndbt gÃ©nÃ¨re automatiquement un site de documentation Ã  partir des fichiers schema.yml :\n\n\nVoir le code\n# Documentation complÃ¨te dans schema.yml\n\ndocumentation = '''\n# models/silver/schema.yml\n\nversion: 2\n\nmodels:\n  - name: silver_orders\n    description: |\n      Table des commandes enrichies avec les informations clients.\n      \n      **Source** : Kafka topic `orders` via Spark Streaming\n      **Refresh** : IncrÃ©mental toutes les heures\n      **Owner** : data-engineering@company.com\n      \n      ## RÃ¨gles mÃ©tier\n      - Les commandes annulÃ©es sont exclues\n      - Les montants sont en EUR\n    \n    columns:\n      - name: order_id\n        description: \"Identifiant unique de la commande (UUID)\"\n        tests: [unique, not_null]\n      \n      - name: customer_id\n        description: \"RÃ©fÃ©rence au client. Voir `stg_customers`.\"\n      \n      - name: amount\n        description: |\n          Montant total de la commande en EUR.\n          Inclut les taxes, exclut les frais de livraison.\n      \n      - name: order_date\n        description: \"Date de crÃ©ation de la commande (timezone UTC)\"\n'''\n\nprint(documentation)\nprint(\"\\n# GÃ©nÃ©rer et servir la documentation :\")\nprint(\"dbt docs generate    # GÃ©nÃ¨re le site dans target/\")\nprint(\"dbt docs serve       # Lance un serveur local (http://localhost:8080)\")\n\n\n\n\n5.2 Le Lineage Graph\nLe lineage (traÃ§abilitÃ©) montre dâ€™oÃ¹ viennent les donnÃ©es et oÃ¹ elles vont :\nLINEAGE GRAPH GÃ‰NÃ‰RÃ‰ PAR DBT :\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   SOURCE    â”‚     â”‚   STAGING   â”‚     â”‚     SILVER      â”‚     â”‚     GOLD     â”‚\nâ”‚             â”‚     â”‚             â”‚     â”‚                 â”‚     â”‚              â”‚\nâ”‚ raw.orders  â”‚â”€â”€â”€â”€â–¶â”‚ stg_orders  â”‚â”€â”€â”€â”€â–¶â”‚                 â”‚     â”‚              â”‚\nâ”‚             â”‚     â”‚             â”‚     â”‚  silver_orders  â”‚â”€â”€â”€â”€â–¶â”‚ gold_revenue â”‚\nâ”‚ raw.customersâ”€â”€â”€â”€â–¶â”‚stg_customersâ”‚â”€â”€â”€â”€â–¶â”‚                 â”‚     â”‚              â”‚\nâ”‚             â”‚     â”‚             â”‚     â”‚                 â”‚     â”‚              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nTu peux voir :\nâ€¢ D'oÃ¹ vient chaque colonne (upstream)\nâ€¢ Qui utilise cette table (downstream)\nâ€¢ L'impact d'un changement",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#great-expectations-tests-avancÃ©s",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#great-expectations-tests-avancÃ©s",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "6. Great Expectations â€” Tests AvancÃ©s",
    "text": "6. Great Expectations â€” Tests AvancÃ©s\n\n6.1 Quâ€™est-ce que Great Expectations ?\nGreat Expectations (GE) est un framework Python pour la validation et le profiling des donnÃ©es. Il complÃ¨te dbt :\n\n\n\n\n\n\n\n\nAspect\ndbt\nGreat Expectations\n\n\n\n\nFocus\nTransformation + Tests simples\nTests avancÃ©s + Profiling\n\n\nLangage\nSQL/YAML\nPython\n\n\nOutput\nPass/Fail\nRapport HTML dÃ©taillÃ©\n\n\nProfiling\nNon\nOui (auto-gÃ©nÃ©ration)\n\n\n\n\n\n6.2 Concepts clÃ©s de GE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    GREAT EXPECTATIONS                           â”‚\nâ”‚                                                                 â”‚\nâ”‚   DATASOURCE          EXPECTATION SUITE        CHECKPOINT       â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚   â”‚ Connexionâ”‚       â”‚ Liste de rÃ¨gles  â”‚     â”‚ ExÃ©cutionâ”‚     â”‚\nâ”‚   â”‚ aux data â”‚  â”€â”€â”€â–¶ â”‚ (expectations)   â”‚ â”€â”€â”€â–¶â”‚ + Rapportâ”‚     â”‚\nâ”‚   â”‚          â”‚       â”‚                  â”‚     â”‚          â”‚     â”‚\nâ”‚   â”‚ - Spark  â”‚       â”‚ - not_null       â”‚     â”‚ - Valide â”‚     â”‚\nâ”‚   â”‚ - Pandas â”‚       â”‚ - between        â”‚     â”‚ - HTML   â”‚     â”‚\nâ”‚   â”‚ - SQL    â”‚       â”‚ - regex          â”‚     â”‚ - Slack  â”‚     â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚                                                                 â”‚\nâ”‚   DATA DOCS : Site web avec tous les rapports de validation    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Installation et setup de Great Expectations\n\nge_setup = '''\n# Installation\npip install great-expectations\n\n# Initialiser un projet GE\ngreat_expectations init\n\n# Structure crÃ©Ã©e :\ngreat_expectations/\nâ”œâ”€â”€ great_expectations.yml      # Configuration principale\nâ”œâ”€â”€ expectations/               # Suites d'expectations\nâ”œâ”€â”€ checkpoints/                # DÃ©finitions des checkpoints\nâ”œâ”€â”€ plugins/                    # Extensions custom\nâ””â”€â”€ uncommitted/\n    â””â”€â”€ data_docs/              # Rapports HTML gÃ©nÃ©rÃ©s\n'''\n\nprint(ge_setup)\n\n\n\n\nVoir le code\n# Utilisation de Great Expectations avec Python\n\nge_example = '''\nimport great_expectations as gx\nfrom great_expectations.core.expectation_configuration import ExpectationConfiguration\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 1. CrÃ©er un contexte GE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncontext = gx.get_context()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 2. Connecter une datasource (Pandas, Spark, SQL)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Exemple avec Pandas\ndatasource = context.sources.add_pandas(name=\"my_pandas_ds\")\ndata_asset = datasource.add_dataframe_asset(name=\"orders_df\")\n\n# Charger les donnÃ©es\nimport pandas as pd\ndf = pd.read_parquet(\"s3://gold/orders/\")\nbatch_request = data_asset.build_batch_request(dataframe=df)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 3. CrÃ©er une Expectation Suite\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsuite = context.add_expectation_suite(expectation_suite_name=\"orders_suite\")\n\n# Ajouter des expectations\nsuite.add_expectation(\n    ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_not_be_null\",\n        kwargs={\"column\": \"order_id\"}\n    )\n)\n\nsuite.add_expectation(\n    ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_be_between\",\n        kwargs={\n            \"column\": \"amount\",\n            \"min_value\": 0,\n            \"max_value\": 50000,\n            \"mostly\": 0.99  # 99% des valeurs doivent Ãªtre dans cette plage\n        }\n    )\n)\n\nsuite.add_expectation(\n    ExpectationConfiguration(\n        expectation_type=\"expect_column_values_to_match_regex\",\n        kwargs={\n            \"column\": \"email\",\n            \"regex\": r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n        }\n    )\n)\n\ncontext.save_expectation_suite(suite)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 4. CrÃ©er et exÃ©cuter un Checkpoint\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncheckpoint = context.add_or_update_checkpoint(\n    name=\"orders_checkpoint\",\n    validations=[\n        {\n            \"batch_request\": batch_request,\n            \"expectation_suite_name\": \"orders_suite\"\n        }\n    ]\n)\n\n# ExÃ©cuter la validation\nresult = checkpoint.run()\n\nprint(f\"Validation rÃ©ussie : {result.success}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# 5. GÃ©nÃ©rer les Data Docs (rapport HTML)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncontext.build_data_docs()\ncontext.open_data_docs()  # Ouvre dans le navigateur\n'''\n\nprint(ge_example)\n\n\n\n\n6.3 Data Profiling automatique\nGE peut analyser tes donnÃ©es et gÃ©nÃ©rer automatiquement des expectations :\n\n\nVoir le code\n# Profiling automatique avec Great Expectations\n\nprofiling = '''\nfrom great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler\n\n# CrÃ©er un profiler\nprofiler = UserConfigurableProfiler(\n    profile_dataset=validator,  # Validator avec les donnÃ©es\n    excluded_expectations=None,\n    ignored_columns=[],\n    not_null_only=False,\n    primary_or_compound_key=[\"order_id\"],\n    semantic_types_dict=None,\n    table_expectations_only=False,\n    value_set_threshold=\"many\"  # \"many\", \"few\", ou un nombre\n)\n\n# GÃ©nÃ©rer la suite d'expectations basÃ©e sur les donnÃ©es actuelles\nsuite = profiler.build_suite()\n\n# Afficher les expectations gÃ©nÃ©rÃ©es\nprint(f\"Nombre d'expectations gÃ©nÃ©rÃ©es : {len(suite.expectations)}\")\nfor exp in suite.expectations[:5]:\n    print(f\"  - {exp.expectation_type}\")\n\n# Sauvegarder\ncontext.save_expectation_suite(suite, expectation_suite_name=\"orders_profiled\")\n'''\n\nprint(profiling)\nprint(\"\\nğŸ’¡ Le profiling auto-gÃ©nÃ¨re des expectations comme :\")\nprint(\"â€¢ expect_column_values_to_be_in_set (pour les catÃ©gories)\")\nprint(\"â€¢ expect_column_mean_to_be_between (pour les numÃ©riques)\")\nprint(\"â€¢ expect_column_proportion_of_unique_values_to_be_between\")\n\n\n\n\nExercice 3 : Profiler une table Gold avec Great Expectations\nObjectif : CrÃ©er une suite dâ€™expectations pour valider une table Gold.\n# 1. Charger la table gold_orders dans un DataFrame Pandas\n# 2. CrÃ©er une expectation suite avec :\n#    - order_id : not_null, unique\n#    - amount : between 0 and 100000, mostly 0.99\n#    - status : in_set ['pending', 'shipped', 'delivered']\n# 3. ExÃ©cuter la validation\n# 4. GÃ©nÃ©rer le rapport HTML\n\n\nğŸ’¡ Solution\n\nimport great_expectations as gx\n\ncontext = gx.get_context()\n\n# CrÃ©er suite\nsuite = context.add_expectation_suite(\"gold_orders_suite\")\n\nsuite.add_expectation(ExpectationConfiguration(\n    expectation_type=\"expect_column_values_to_not_be_null\",\n    kwargs={\"column\": \"order_id\"}\n))\nsuite.add_expectation(ExpectationConfiguration(\n    expectation_type=\"expect_column_values_to_be_unique\",\n    kwargs={\"column\": \"order_id\"}\n))\nsuite.add_expectation(ExpectationConfiguration(\n    expectation_type=\"expect_column_values_to_be_between\",\n    kwargs={\"column\": \"amount\", \"min_value\": 0, \"max_value\": 100000, \"mostly\": 0.99}\n))\nsuite.add_expectation(ExpectationConfiguration(\n    expectation_type=\"expect_column_values_to_be_in_set\",\n    kwargs={\"column\": \"status\", \"value_set\": [\"pending\", \"shipped\", \"delivered\"]}\n))\n\ncontext.save_expectation_suite(suite)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#dbt-avec-sparklakehouse",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#dbt-avec-sparklakehouse",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "7. dbt avec Spark/Lakehouse",
    "text": "7. dbt avec Spark/Lakehouse\n\n7.1 Configuration dbt-spark\nPour utiliser dbt avec Spark (Databricks, EMR, local) :\n\n\nVoir le code\n# Configuration dbt-spark\n\nspark_config = '''\n# ~/.dbt/profiles.yml pour Spark\n\necommerce_analytics:\n  target: dev\n  outputs:\n    \n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Databricks\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    databricks:\n      type: databricks\n      catalog: main              # Unity Catalog\n      schema: analytics\n      host: adb-xxx.azuredatabricks.net\n      http_path: /sql/1.0/warehouses/xxx\n      token: \"{{ env_var('DATABRICKS_TOKEN') }}\"\n      threads: 4\n    \n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Spark Thrift Server (EMR, local)\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    spark_thrift:\n      type: spark\n      method: thrift\n      host: spark-thrift.company.com\n      port: 10001\n      user: dbt_user\n      schema: analytics\n      threads: 4\n    \n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    # Spark local (pour dev)\n    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    spark_local:\n      type: spark\n      method: session\n      schema: default\n      threads: 1\n'''\n\nprint(spark_config)\n\n\n\n\n7.2 MatÃ©rialisation incrÃ©mentale avec Delta Lake\nLâ€™incrÃ©mental est crucial pour les gros volumes. Voici comment lâ€™implÃ©menter proprement :\n\n\nVoir le code\n# Model incrÃ©mental avec Delta Lake\n\nincremental_model = '''\n-- models/gold/gold_daily_revenue.sql\n\n{{ config(\n    materialized='incremental',\n    unique_key='date_customer_key',       -- Pour le MERGE\n    incremental_strategy='merge',         -- MERGE INTO (vs append, delete+insert)\n    file_format='delta',                  -- Format Delta Lake\n    partition_by=['order_date']           -- Partitionnement\n) }}\n\nWITH source_data AS (\n    SELECT\n        order_date,\n        customer_id,\n        SUM(amount) AS daily_revenue,\n        COUNT(*) AS order_count,\n        -- ClÃ© unique pour le MERGE\n        CONCAT(order_date, '-', customer_id) AS date_customer_key\n    FROM {{ ref('silver_orders') }}\n    \n    -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    -- FILTRE INCRÃ‰MENTAL : Seulement les nouvelles donnÃ©es\n    -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n    {% if is_incremental() %}\n    WHERE order_date &gt; (\n        SELECT MAX(order_date) - INTERVAL 1 DAY  -- Marge de sÃ©curitÃ©\n        FROM {{ this }}\n    )\n    {% endif %}\n    \n    GROUP BY order_date, customer_id\n)\n\nSELECT * FROM source_data\n'''\n\nprint(incremental_model)\nprint(\"\\nğŸ’¡ Explication :\")\nprint(\"â€¢ is_incremental() : True si la table existe et ce n'est pas un --full-refresh\")\nprint(\"â€¢ {{ this }} : RÃ©fÃ©rence Ã  la table actuelle\")\nprint(\"â€¢ INTERVAL 1 DAY : Marge pour les donnÃ©es en retard (late data)\")\nprint(\"\")\nprint(\"# Commandes :\")\nprint(\"dbt run --select gold_daily_revenue          # Run incrÃ©mental\")\nprint(\"dbt run --select gold_daily_revenue --full-refresh  # Rebuild complet\")\n\n\n\n\n7.3 Macros Jinja\nLes macros sont des fonctions rÃ©utilisables Ã©crites en Jinja. TrÃ¨s utiles pour Ã©viter la duplication de code.\n\n\nVoir le code\n# Macros Jinja rÃ©utilisables\n\nmacros = '''\n-- macros/deduplicate.sql\n\n{% macro deduplicate(relation, partition_by, order_by) %}\n    {#\n        DÃ©duplique une table en gardant la ligne la plus rÃ©cente.\n        \n        Args:\n            relation: La table source\n            partition_by: Colonne(s) pour identifier les doublons\n            order_by: Colonne pour dÃ©terminer quelle ligne garder\n    #}\n    \n    SELECT * FROM (\n        SELECT\n            *,\n            ROW_NUMBER() OVER (\n                PARTITION BY {{ partition_by }}\n                ORDER BY {{ order_by }} DESC\n            ) AS _row_num\n        FROM {{ relation }}\n    )\n    WHERE _row_num = 1\n{% endmacro %}\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- macros/generate_surrogate_key.sql\n\n{% macro generate_surrogate_key(columns) %}\n    {#\n        GÃ©nÃ¨re une clÃ© surrogate Ã  partir de plusieurs colonnes.\n    #}\n    \n    MD5(CONCAT(\n        {% for col in columns %}\n            COALESCE(CAST({{ col }} AS STRING), '_null_')\n            {% if not loop.last %}, '-', {% endif %}\n        {% endfor %}\n    ))\n{% endmacro %}\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- macros/cents_to_euros.sql\n\n{% macro cents_to_euros(column_name) %}\n    ROUND({{ column_name }} / 100.0, 2)\n{% endmacro %}\n'''\n\nusage = '''\n-- Utilisation dans un model :\n\n-- models/silver/silver_orders.sql\n\n{{ config(materialized='table') }}\n\nWITH deduplicated AS (\n    {{ deduplicate(\n        relation=ref('stg_orders'),\n        partition_by='order_id',\n        order_by='updated_at'\n    ) }}\n)\n\nSELECT\n    {{ generate_surrogate_key(['order_id', 'customer_id']) }} AS order_sk,\n    order_id,\n    customer_id,\n    {{ cents_to_euros('amount_cents') }} AS amount,\n    order_date\nFROM deduplicated\n'''\n\nprint(\"# Macros\")\nprint(macros)\nprint(\"\\n# Utilisation\")\nprint(usage)\n\n\n\n\nExercice 4 : CrÃ©er un model incrÃ©mental\nObjectif : CrÃ©er un model Gold incrÃ©mental pour les mÃ©triques journaliÃ¨res.\n-- CrÃ©er gold_daily_metrics qui :\n-- 1. AgrÃ¨ge par date : total_orders, total_revenue, avg_order_value\n-- 2. Est incrÃ©mental sur order_date\n-- 3. Utilise une marge de 2 jours pour les late data\n\n\nğŸ’¡ Solution\n\n{{ config(\n    materialized='incremental',\n    unique_key='order_date',\n    incremental_strategy='merge'\n) }}\n\nSELECT\n    order_date,\n    COUNT(*) AS total_orders,\n    SUM(amount) AS total_revenue,\n    AVG(amount) AS avg_order_value\nFROM {{ ref('silver_orders') }}\n{% if is_incremental() %}\nWHERE order_date &gt;= (SELECT MAX(order_date) - INTERVAL 2 DAY FROM {{ this }})\n{% endif %}\nGROUP BY order_date",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#cicd-et-orchestration",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#cicd-et-orchestration",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "8. CI/CD et Orchestration",
    "text": "8. CI/CD et Orchestration\n\n8.1 Workflow Git avec dbt\nWORKFLOW RECOMMANDÃ‰ :\n\n1. DEVELOP (feature branch)\n   â””â”€â–¶ Modifier les models\n   â””â”€â–¶ dbt run --select +modified_model+  (test local)\n   â””â”€â–¶ dbt test --select +modified_model+\n\n2. PULL REQUEST\n   â””â”€â–¶ CI automatique : dbt build (run + test)\n   â””â”€â–¶ Review par un collÃ¨gue\n   â””â”€â–¶ Merge si tout est vert âœ…\n\n3. DEPLOY (main branch)\n   â””â”€â–¶ CD automatique : dbt run --target prod\n   â””â”€â–¶ dbt test --target prod\n\n\nVoir le code\n# GitHub Actions pour dbt CI/CD\n\ngithub_actions = '''\n# .github/workflows/dbt-ci.yml\n\nname: dbt CI/CD\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # CI : Tests sur Pull Request\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  dbt-test:\n    runs-on: ubuntu-latest\n    if: github.event_name == 'pull_request'\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      \n      - name: Install dbt\n        run: pip install dbt-core dbt-snowflake\n      \n      - name: Setup dbt profile\n        run: |\n          mkdir -p ~/.dbt\n          echo \"${{ secrets.DBT_PROFILES }}\" &gt; ~/.dbt/profiles.yml\n      \n      - name: dbt deps\n        run: dbt deps\n      \n      - name: dbt build (run + test)\n        run: dbt build --target ci\n        env:\n          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  # CD : DÃ©ploiement sur main\n  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  dbt-deploy:\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main' && github.event_name == 'push'\n    \n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Setup Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: '3.11'\n      \n      - name: Install dbt\n        run: pip install dbt-core dbt-snowflake\n      \n      - name: Setup dbt profile\n        run: |\n          mkdir -p ~/.dbt\n          echo \"${{ secrets.DBT_PROFILES }}\" &gt; ~/.dbt/profiles.yml\n      \n      - name: dbt deps\n        run: dbt deps\n      \n      - name: dbt run (production)\n        run: dbt run --target prod\n      \n      - name: dbt test (production)\n        run: dbt test --target prod\n'''\n\nprint(github_actions)\n\n\n\n\n8.2 Orchestration avec Airflow\n\n\nVoir le code\n# DAG Airflow pour dbt\n\nairflow_dag = '''\n# dags/dbt_daily_run.py\n\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom airflow.utils.dates import days_ago\n\n# Configuration\nDBT_PROJECT_DIR = \"/opt/dbt/ecommerce_analytics\"\nDBT_PROFILES_DIR = \"/opt/dbt/profiles\"\n\ndefault_args = {\n    \"owner\": \"data-engineering\",\n    \"depends_on_past\": False,\n    \"retries\": 1,\n}\n\nwith DAG(\n    dag_id=\"dbt_daily_run\",\n    default_args=default_args,\n    description=\"ExÃ©cution quotidienne de dbt\",\n    schedule_interval=\"0 6 * * *\",  # Tous les jours Ã  6h\n    start_date=days_ago(1),\n    catchup=False,\n    tags=[\"dbt\", \"analytics\"],\n) as dag:\n    \n    # VÃ©rifier la fraÃ®cheur des sources\n    dbt_source_freshness = BashOperator(\n        task_id=\"dbt_source_freshness\",\n        bash_command=f\"cd {DBT_PROJECT_DIR} && dbt source freshness --profiles-dir {DBT_PROFILES_DIR}\"\n    )\n    \n    # ExÃ©cuter les models\n    dbt_run = BashOperator(\n        task_id=\"dbt_run\",\n        bash_command=f\"cd {DBT_PROJECT_DIR} && dbt run --profiles-dir {DBT_PROFILES_DIR}\"\n    )\n    \n    # ExÃ©cuter les tests\n    dbt_test = BashOperator(\n        task_id=\"dbt_test\",\n        bash_command=f\"cd {DBT_PROJECT_DIR} && dbt test --profiles-dir {DBT_PROFILES_DIR}\"\n    )\n    \n    # GÃ©nÃ©rer la documentation\n    dbt_docs = BashOperator(\n        task_id=\"dbt_docs_generate\",\n        bash_command=f\"cd {DBT_PROJECT_DIR} && dbt docs generate --profiles-dir {DBT_PROFILES_DIR}\"\n    )\n    \n    # Ordre d'exÃ©cution\n    dbt_source_freshness &gt;&gt; dbt_run &gt;&gt; dbt_test &gt;&gt; dbt_docs\n'''\n\nprint(airflow_dag)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#mini-projet-couche-gold-industrialisÃ©e",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#mini-projet-couche-gold-industrialisÃ©e",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "9. Mini-Projet : Couche Gold IndustrialisÃ©e",
    "text": "9. Mini-Projet : Couche Gold IndustrialisÃ©e\n\nObjectif\nCrÃ©er une couche Gold complÃ¨te avec dbt : models, tests, documentation, incrÃ©mental.\nARCHITECTURE DU PROJET :\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         dbt Project                              â”‚\nâ”‚                                                                  â”‚\nâ”‚  STAGING                 SILVER                    GOLD          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚stg_ordersâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚silver_   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚gold_daily_   â”‚ â”‚\nâ”‚  â”‚          â”‚           â”‚orders    â”‚           â”‚revenue       â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚(incremental) â”‚ â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚  â”‚stg_      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\nâ”‚  â”‚customers â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚gold_customer â”‚ â”‚\nâ”‚                                                â”‚_summary      â”‚ â”‚\nâ”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                                                  â”‚\nâ”‚  + Tests sur chaque model                                        â”‚\nâ”‚  + Documentation complÃ¨te                                        â”‚\nâ”‚  + Great Expectations pour validation finale                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Structure complÃ¨te du mini-projet\n\nproject_structure = '''\necommerce_dbt/\nâ”œâ”€â”€ dbt_project.yml\nâ”œâ”€â”€ packages.yml\nâ”‚\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ staging/\nâ”‚   â”‚   â”œâ”€â”€ _sources.yml         # Sources raw\nâ”‚   â”‚   â”œâ”€â”€ stg_orders.sql\nâ”‚   â”‚   â””â”€â”€ stg_customers.sql\nâ”‚   â”‚\nâ”‚   â”œâ”€â”€ silver/\nâ”‚   â”‚   â”œâ”€â”€ _silver__models.yml  # Tests + docs\nâ”‚   â”‚   â””â”€â”€ silver_orders.sql\nâ”‚   â”‚\nâ”‚   â””â”€â”€ gold/\nâ”‚       â”œâ”€â”€ _gold__models.yml    # Tests + docs\nâ”‚       â”œâ”€â”€ gold_daily_revenue.sql      # IncrÃ©mental\nâ”‚       â””â”€â”€ gold_customer_summary.sql   # Table\nâ”‚\nâ”œâ”€â”€ macros/\nâ”‚   â””â”€â”€ deduplicate.sql\nâ”‚\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ assert_positive_revenue.sql\nâ”‚   â””â”€â”€ assert_no_future_dates.sql\nâ”‚\nâ””â”€â”€ great_expectations/\n    â””â”€â”€ expectations/\n        â””â”€â”€ gold_orders_suite.json\n'''\n\nprint(project_structure)\n\n\n\n\nVoir le code\n# Models du mini-projet\n\nmodels = '''\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- models/gold/gold_daily_revenue.sql\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n{{ config(\n    materialized='incremental',\n    unique_key='revenue_date',\n    incremental_strategy='merge'\n) }}\n\nSELECT\n    order_date AS revenue_date,\n    COUNT(DISTINCT order_id) AS total_orders,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    SUM(amount) AS total_revenue,\n    AVG(amount) AS avg_order_value,\n    CURRENT_TIMESTAMP() AS updated_at\nFROM {{ ref('silver_orders') }}\n{% if is_incremental() %}\nWHERE order_date &gt;= (SELECT MAX(revenue_date) - INTERVAL 2 DAY FROM {{ this }})\n{% endif %}\nGROUP BY order_date\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- models/gold/gold_customer_summary.sql\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n{{ config(materialized='table') }}\n\nSELECT\n    customer_id,\n    customer_name,\n    email,\n    COUNT(DISTINCT order_id) AS lifetime_orders,\n    SUM(amount) AS lifetime_revenue,\n    AVG(amount) AS avg_order_value,\n    MIN(order_date) AS first_order_date,\n    MAX(order_date) AS last_order_date,\n    DATEDIFF(day, MIN(order_date), MAX(order_date)) AS customer_tenure_days\nFROM {{ ref('silver_orders') }}\nGROUP BY customer_id, customer_name, email\n'''\n\nprint(models)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#quiz",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#quiz",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "Quiz",
    "text": "Quiz\nQ1. Quâ€™est-ce que la Transformation-as-Code ?\n\n\nR\n\nAppliquer les principes du dÃ©veloppement logiciel (versioning, tests, CI/CD) aux transformations de donnÃ©es.\n\nQ2. Quel est le rÃ´le de ref() dans dbt ?\n\n\nR\n\nRÃ©fÃ©rencer un autre model dbt pour crÃ©er une dÃ©pendance et permettre le lineage automatique.\n\nQ3. DiffÃ©rence entre table et incremental ?\n\n\nR\n\ntable = rebuild complet Ã  chaque run. incremental = ajoute seulement les nouvelles lignes.\n\nQ4. Test gÃ©nÃ©rique vs test singulier ?\n\n\nR\n\nGÃ©nÃ©rique = built-in (unique, not_null). Singulier = SQL custom dans tests/.\n\nQ5. Comment dbt gÃ¨re lâ€™ordre dâ€™exÃ©cution ?\n\n\nR\n\nVia le DAG des dÃ©pendances crÃ©Ã© par les appels Ã  ref() et source().\n\nQ6. Comment gÃ©nÃ©rer la documentation dbt ?\n\n\nR\n\ndbt docs generate puis dbt docs serve.\n\nQ7. Quâ€™est-ce quâ€™un Checkpoint dans Great Expectations ?\n\n\nR\n\nExÃ©cution dâ€™une suite dâ€™expectations sur une datasource avec gÃ©nÃ©ration de rapport.\n\nQ8. Comment intÃ©grer dbt dans CI/CD ?\n\n\nR\n\nGitHub Actions : dbt build sur PR, dbt run --target prod sur merge.\n\nQ9. Avantage de lâ€™incrÃ©mental vs full-refresh ?\n\n\nR\n\nPerformance : traite seulement les nouvelles donnÃ©es, pas tout recalculer.\n\nQ10. RÃ´le de Jinja dans dbt ?\n\n\nR\n\nTemplating SQL : variables, conditions, boucles, macros rÃ©utilisables.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#ressources",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#ressources",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\ndbt Documentation\ndbt Learn (cours gratuit)\ndbt Hub (packages)\nGreat Expectations Documentation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#prochaine-Ã©tape",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module 26 : 26_projet_integrateur.ipynb â€” Projet Final\nSynthÃ¨se de toutes les compÃ©tences : Kafka â†’ Spark on K8s â†’ Delta Lake â†’ dbt",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/25_dbt_data_quality.html#rÃ©capitulatif",
    "href": "notebooks/intermediate/25_dbt_data_quality.html#rÃ©capitulatif",
    "title": "dbt (Data Build Tool) & Data Quality",
    "section": "ğŸ“ RÃ©capitulatif",
    "text": "ğŸ“ RÃ©capitulatif\n\n\n\nConcept\nAppris\n\n\n\n\ndbt\nModels, ref(), source(), matÃ©rialisations\n\n\nTests\nGÃ©nÃ©riques, singuliers, dbt-expectations\n\n\nDocumentation\nschema.yml, dbt docs, lineage\n\n\nGreat Expectations\nExpectations, Checkpoints, Data Docs\n\n\nIncrÃ©mental\nis_incremental(), unique_key, merge\n\n\nCI/CD\nGitHub Actions, Airflow\n\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant dbt et la Data Quality.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ”§ Industrialisation",
      "25 Â· dbt & Data Quality"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas transformer un Data Lake en Data Lakehouse â€” combinant flexibilitÃ© et ACID.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#du-data-lake-au-data-lakehouse",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#du-data-lake-au-data-lakehouse",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas transformer un Data Lake en Data Lakehouse â€” combinant flexibilitÃ© et ACID.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#prÃ©requis",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#prÃ©requis",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nModule\nCompÃ©tence\nPourquoi ?\n\n\n\n\nâœ… 19\nPySpark Advanced\nDataFrame API\n\n\nâœ… 20\nSpark SQL\nSQL, Catalyst\n\n\nâœ… 22\nCloud Storage\nMinIO, s3a://",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#objectifs",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#objectifs",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "Objectifs",
    "text": "Objectifs\n\nComprendre pourquoi Parquet seul ne suffit pas\nMaÃ®triser ACID pour les Data Lakes\nUtiliser Delta Lake : MERGE, Time Travel, Schema Evolution\nComprendre Iceberg : Hidden Partitioning\nOptimiser avec OPTIMIZE, Z-ORDER, VACUUM",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#introduction-pourquoi-les-table-formats",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#introduction-pourquoi-les-table-formats",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "1. Introduction â€” Pourquoi les Table Formats ?",
    "text": "1. Introduction â€” Pourquoi les Table Formats ?\n\n1.1 Lâ€™Ã©volution du stockage\n2000s: DATA WAREHOUSE     2010s: DATA LAKE        2020s: LAKEHOUSE\nâ€¢ Oracle, Teradata        â€¢ Hadoop, S3+Parquet    â€¢ Delta, Iceberg\nâ€¢ ACID âœ…                 â€¢ Flexible              â€¢ ACID âœ…\nâ€¢ CoÃ»teux ğŸ’°              â€¢ Pas ACID âŒ           â€¢ Flexible\nâ€¢ Rigide                  â€¢ Cheap ğŸ’µ              â€¢ Cheap ğŸ’µ\n\n\n1.2 Le problÃ¨me avec Parquet seul\n\n\n\nScÃ©nario\nProblÃ¨me\n\n\n\n\nDeux jobs Ã©crivent en mÃªme temps\nğŸ’¥ DonnÃ©es corrompues\n\n\nUPDATE 100 lignes sur 10M\nğŸ˜° RÃ©Ã©crire toute la partition\n\n\nJob Ã©choue Ã  mi-chemin\nğŸ—‘ï¸ Fichiers partiels\n\n\nVoir donnÃ©es dâ€™il y a 3 jours\nâŒ Impossible\n\n\nSchÃ©ma source change\nğŸ’” Erreurs de lecture\n\n\n\nLes Table Formats rÃ©solvent TOUS ces problÃ¨mes.\n\n\n1.3 Câ€™est quoi un Data Lakehouse ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  DATA LAKEHOUSE                     â”‚\nâ”‚                                                     â”‚\nâ”‚   DATA LAKE          +       DATA WAREHOUSE         â”‚\nâ”‚   â€¢ Stockage cheap           â€¢ ACID                 â”‚\nâ”‚   â€¢ Formats ouverts          â€¢ Schema enforce       â”‚\nâ”‚   â€¢ FlexibilitÃ©              â€¢ SQL performant       â”‚\nâ”‚                                                     â”‚\nâ”‚   = Le meilleur des deux mondes !                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nLes Table Formats ajoutent une couche de mÃ©tadonnÃ©es (Transaction Log) au-dessus des fichiers Parquet.\n\n\nExercice 1 : Identifier les limites de Parquet\n\n\n\nScÃ©nario\nProblÃ¨me ?\n\n\n\n\nJob A lit pendant que Job B Ã©crit\n?\n\n\nUPDATE 100 lignes sur 10M\n?\n\n\nSuppression accidentelle dâ€™une partition\n?\n\n\n\n\n\nğŸ’¡ RÃ©ponses\n\n\nDirty Read : donnÃ©es partielles/incohÃ©rentes\nRÃ©Ã©criture totale : toute la partition\nPas de rollback : donnÃ©es perdues",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#les-limites-du-data-lake-classique",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#les-limites-du-data-lake-classique",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "2. Les Limites du Data Lake Classique",
    "text": "2. Les Limites du Data Lake Classique\n\n2.1 Pas de transactions ACID\n\n\n\nPropriÃ©tÃ©\nSignification\nParquet seul ?\n\n\n\n\nAtomicity\nTout-ou-rien\nâŒ Fichiers partiels\n\n\nConsistency\nContraintes respectÃ©es\nâŒ Pas de validation\n\n\nIsolation\nOpÃ©rations isolÃ©es\nâŒ Dirty reads\n\n\nDurability\nDonnÃ©es persistantes\nâœ… Sur S3\n\n\n\n\n\n2.2 Mutations coÃ»teuses\nUPDATE 1 ligne = lire 10M lignes â†’ modifier â†’ rÃ©Ã©crire 10M lignes = minutes\n\n\n2.3 Small Files Problem\nStreaming 5min/jour = 288 fichiers/jour Ã— 365 = 100K+ fichiers/an\n\n\n2.4 SchÃ©ma rigide\nPas de schema enforcement natif. Changer partitionnement = rÃ©Ã©criture totale.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#introduction-aux-table-formats",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#introduction-aux-table-formats",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "3. Introduction aux Table Formats",
    "text": "3. Introduction aux Table Formats\n\n3.1 Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚           QUERY ENGINE (Spark, Trino)       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚         TABLE FORMAT LAYER                  â”‚\nâ”‚      (Delta Lake / Iceberg / Hudi)          â”‚\nâ”‚  â€¢ Transaction Log                          â”‚\nâ”‚  â€¢ ACID, Time Travel, Schema Evolution      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚     FILE FORMAT (Parquet) sur STORAGE (S3)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 Le Transaction Log\nVersion 0 : ADD file_001.parquet\nVersion 1 : ADD file_002.parquet\nVersion 2 : ADD file_003.parquet, REMOVE file_001.parquet\n\nÃ‰tat actuel = { file_002, file_003 }\nÃ‰tat v1     = { file_001, file_002 }  â† Time Travel !\n\n\n3.3 Comparaison Delta vs Iceberg vs Hudi\n\n\n\nCritÃ¨re\nDelta Lake\nIceberg\nHudi\n\n\n\n\nCrÃ©ateur\nDatabricks\nNetflix\nUber\n\n\nMoteurs\nSpark +++\nMulti-engine\nSpark, Flink\n\n\nACID\nâœ…\nâœ…\nâœ…\n\n\nTime Travel\nâœ…âœ…\nâœ…âœ…\nâœ…\n\n\nSchema Evolution\nâœ…\nâœ…âœ… (best)\nâœ…\n\n\nHidden Partitioning\nâŒ\nâœ… (unique!)\nâŒ\n\n\nZ-Ordering\nâœ…\nâœ…\nâŒ\n\n\nCatalog requis\nOptionnel\nObligatoire\nObligatoire\n\n\nCDC natif\nChange Data Feed\nâŒ\nâœ…âœ… (best)\n\n\n\n\n\n3.4 Lequel choisir ?\n\nğŸ¯ Delta Lake : Spark, Databricks, simplicitÃ©, dÃ©butants\nğŸ¯ Iceberg : Multi-engine, Hidden Partitioning, AWS Athena\nğŸ¯ Hudi : CDC massif, near-real-time, AWS EMR",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#delta-lake-deep-dive",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#delta-lake-deep-dive",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "4. Delta Lake â€” Deep Dive",
    "text": "4. Delta Lake â€” Deep Dive\n\n4.1 Architecture : _delta_log/\ns3://bucket/sales/\nâ”œâ”€â”€ _delta_log/\nâ”‚   â”œâ”€â”€ 00000000000000000000.json  â† Version 0\nâ”‚   â”œâ”€â”€ 00000000000000000001.json  â† Version 1\nâ”‚   â”œâ”€â”€ 00000000000000000010.checkpoint.parquet\nâ”‚   â””â”€â”€ _last_checkpoint\nâ”œâ”€â”€ part-00000-abc.parquet\nâ””â”€â”€ part-00001-def.parquet\nChaque JSON contient :\n\nadd : nouveau fichier ajoutÃ©\nremove : fichier retirÃ©\ncommitInfo : mÃ©tadonnÃ©es de lâ€™opÃ©ration\nstats : min/max pour Data Skipping\n\n\n\nVoir le code\n# Configuration Spark avec Delta Lake\n\nconfig = '''\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"Delta Lake Demo\") \\\n    .config(\"spark.jars.packages\", \"io.delta:delta-spark_2.12:3.1.0\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n# Pour MinIO\nspark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\nspark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\nspark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\nspark.conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n'''\nprint(config)\n\n\n\n\nVoir le code\n# CrÃ©er une table Delta\n\ncreate_examples = '''\n# MÃ‰THODE 1 : Depuis DataFrame\ndf = spark.createDataFrame([\n    (1, \"Alice\", 1200.0, \"2024-01-15\"),\n    (2, \"Bob\", 350.0, \"2024-01-15\"),\n], [\"id\", \"customer\", \"amount\", \"date\"])\n\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"s3a://silver/sales/\")\n\n# MÃ‰THODE 2 : SQL\nspark.sql(\"\"\"\n    CREATE TABLE sales (id INT, customer STRING, amount DOUBLE, date DATE)\n    USING DELTA\n    LOCATION 's3a://silver/sales/'\n\"\"\")\n\n# MÃ‰THODE 3 : Convertir Parquet existant (SANS copie !)\nspark.sql(\"CONVERT TO DELTA parquet.`s3a://bronze/old_table/`\")\n'''\nprint(create_examples)\n\n\n\n\nExercice 2 : CrÃ©er une table Delta\n# 1. CrÃ©er DataFrame\ndata = [(1, \"Laptop\", 1200.0), (2, \"Mouse\", 25.0)]\ndf = spark.createDataFrame(data, [\"id\", \"product\", \"price\"])\n\n# 2. Sauvegarder en Delta\n# TODO: df.write.format(\"delta\")...\n\n# 3. Lister _delta_log/\n\n\nğŸ’¡ Solution\n\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/products\")\n\n\n\n4.4 MERGE INTO : CDC et Upsert\nTARGET               SOURCE CDC             APRÃˆS MERGE\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚ name  â”‚      â”‚ id â”‚ name  â”‚ op â”‚    â”‚ id â”‚ name  â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice â”‚      â”‚ 1  â”‚ Alice â”‚ U  â”‚ â†’  â”‚ 1  â”‚ ALICE â”‚ Updated\nâ”‚ 2  â”‚ Bob   â”‚      â”‚ 3  â”‚ New   â”‚ I  â”‚ â†’  â”‚ 3  â”‚ New   â”‚ Inserted\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ 2  â”‚ Bob   â”‚ D  â”‚    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜ (Bob deleted)\n                    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# MERGE INTO - Exemple complet\n\nmerge_sql = '''\nMERGE INTO target_table AS target\nUSING source_cdc AS source\nON target.id = source.id\n\nWHEN MATCHED AND source.op = 'D' THEN DELETE\n\nWHEN MATCHED AND source.op = 'U' THEN\n    UPDATE SET target.name = source.name, target.amount = source.amount\n\nWHEN NOT MATCHED THEN\n    INSERT (id, name, amount) VALUES (source.id, source.name, source.amount)\n'''\n\nmerge_python = '''\nfrom delta.tables import DeltaTable\n\ntarget = DeltaTable.forPath(spark, \"s3a://silver/customers/\")\nsource = spark.read.parquet(\"s3a://bronze/cdc/\")\n\ntarget.alias(\"t\").merge(source.alias(\"s\"), \"t.id = s.id\") \\\n    .whenMatchedDelete(condition=\"s.op = 'D'\") \\\n    .whenMatchedUpdate(condition=\"s.op = 'U'\", set={\"name\": \"s.name\"}) \\\n    .whenNotMatchedInsert(values={\"id\": \"s.id\", \"name\": \"s.name\"}) \\\n    .execute()\n'''\n\nprint(\"SQL:\", merge_sql)\nprint(\"\\nPython:\", merge_python)\n\n\n\n\nExercice 3 : CDC avec MERGE INTO\n# Initial\ninitial = [(1, \"Alice\"), (2, \"Bob\")]\n\n# CDC\ncdc = [(1, \"ALICE\", \"U\"), (3, \"New\", \"I\"), (2, None, \"D\")]\n\n# TODO: CrÃ©er table, appliquer MERGE\n\n\nğŸ’¡ Solution\n\ntarget = DeltaTable.forPath(spark, \"/tmp/customers\")\ntarget.alias(\"t\").merge(df_cdc.alias(\"s\"), \"t.id = s.id\") \\\n    .whenMatchedDelete(condition=\"s.op = 'D'\") \\\n    .whenMatchedUpdate(condition=\"s.op = 'U'\", set={\"name\": \"s.name\"}) \\\n    .whenNotMatchedInsert(values={\"id\": \"s.id\", \"name\": \"s.name\"}) \\\n    .execute()\n\n\n\n4.5 Time Travel\nVersion 0     Version 1     Version 2     Version 3\n(Create)      (INSERT)      (UPDATE)      (DELETE)\n   â”‚              â”‚              â”‚              â”‚\n   â–¼              â–¼              â–¼              â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 1 row  â”‚ â†’ â”‚ 3 rows â”‚ â†’ â”‚ 3 rows â”‚ â†’ â”‚ 2 rows â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚modifiedâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nTu peux lire N'IMPORTE quelle version !\n\n\nVoir le code\n# Time Travel\n\ntime_travel = '''\n# Voir l'historique\nspark.sql(\"DESCRIBE HISTORY sales\").show()\n\n# Lire par version\ndf_v1 = spark.sql(\"SELECT * FROM sales VERSION AS OF 1\")\ndf_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"path/\")\n\n# Lire par timestamp\ndf = spark.sql(\"SELECT * FROM sales TIMESTAMP AS OF '2024-01-03 12:00:00'\")\n\n# RESTORE : Revenir Ã  une version\nspark.sql(\"RESTORE TABLE sales TO VERSION AS OF 5\")\n'''\n\nprint(time_travel)\nprint(\"\\nğŸ’¡ Use cases: Audit, rollback, debug, ML reproductibility\")\n\n\n\n\nExercice 4 : Restaurer aprÃ¨s erreur\n# OUPS ! DELETE sans WHERE\nspark.sql(\"DELETE FROM sales\")\n\n# TODO: Utiliser Time Travel pour restaurer\n\n\nğŸ’¡ Solution\n\nspark.sql(\"DESCRIBE HISTORY sales\").show()  # Trouver version avant DELETE\nspark.sql(\"RESTORE TABLE sales TO VERSION AS OF 1\")\n\n\n\n4.6 Schema Management\n\n\n\nMode\nComportement\nOption\n\n\n\n\nEnforcement\nRejette schÃ©mas diffÃ©rents\n(dÃ©faut)\n\n\nEvolution\nAjoute nouvelles colonnes\nmergeSchema=true\n\n\nOverwrite\nRemplace schÃ©ma entier\noverwriteSchema=true\n\n\n\n# Schema Evolution\ndf_new.write.format(\"delta\") \\\n    .mode(\"append\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .save(\"path/\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#apache-iceberg-deep-dive",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#apache-iceberg-deep-dive",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "5. Apache Iceberg â€” Deep Dive",
    "text": "5. Apache Iceberg â€” Deep Dive\n\n5.1 Architecture\ns3://bucket/sales/\nâ”œâ”€â”€ metadata/\nâ”‚   â”œâ”€â”€ v1.metadata.json     â† Metadata File\nâ”‚   â”œâ”€â”€ snap-xxx.avro        â† Manifest List\nâ”‚   â””â”€â”€ xxx-m0.avro          â† Manifest File\nâ””â”€â”€ data/\n    â””â”€â”€ part-00000.parquet\nStructure arborescente vs log sÃ©quentiel = plus rapide pour grandes tables.\n\n\n5.2 Hidden Partitioning (Killer Feature)\nCREATE TABLE events (\n    event_id LONG,\n    event_ts TIMESTAMP,\n    user_id LONG\n)\nUSING iceberg\nPARTITIONED BY (\n    days(event_ts),      -- Partition par jour (cachÃ© !)\n    bucket(16, user_id)  -- Bucket hash\n)\n\n-- L'utilisateur n'a PAS besoin de connaÃ®tre le partitionnement !\nSELECT * FROM events WHERE event_ts = '2024-01-15 10:00:00'\n-- Partition pruning automatique !\nTransforms : years(), months(), days(), hours(), bucket(N, col), truncate(L, col)\n\n\nExercice 5 : Table Iceberg avec Hidden Partitioning\n\n\nğŸ’¡ Solution\n\nCREATE TABLE my_catalog.db.page_views (\n    view_id LONG, view_timestamp TIMESTAMP, user_id LONG\n)\nUSING iceberg\nPARTITIONED BY (days(view_timestamp), bucket(8, user_id))\n\n\n\n5.3 Schema Evolution avancÃ©e\nALTER TABLE events ADD COLUMN source STRING\nALTER TABLE events RENAME COLUMN payload TO event_data  -- Sans rÃ©Ã©criture !\nALTER TABLE events DROP COLUMN deprecated              -- Sans rÃ©Ã©criture !\n\n\n5.4 Catalog (obligatoire)\nIceberg nÃ©cessite un catalog : Hive Metastore, AWS Glue, Nessie, Unity Catalog.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#maintenance-optimisation",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#maintenance-optimisation",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "7. Maintenance & Optimisation",
    "text": "7. Maintenance & Optimisation\n\n7.1 Compaction (OPTIMIZE)\n-- Delta\nOPTIMIZE sales\nOPTIMIZE sales WHERE date &gt;= '2024-01-01'\n\n-- Iceberg\nCALL catalog.system.rewrite_data_files(table =&gt; 'db.sales')\n\n\n7.2 Z-Ordering\nOrganise donnÃ©es par colonnes filtrÃ©es â†’ meilleur Data Skipping.\nOPTIMIZE sales ZORDER BY (customer_id, product_id)\nRÃ¨gle : Partition par faible cardinalitÃ©, Z-ORDER par haute cardinalitÃ©.\n\n\n7.3 VACUUM\n-- Supprimer fichiers &gt; 7 jours\nVACUUM sales\nVACUUM sales RETAIN 168 HOURS\n\n-- Iceberg\nCALL catalog.system.expire_snapshots(table =&gt; 'db.sales', retain_last =&gt; 5)\nâš ï¸ AprÃ¨s VACUUM, Time Travel sur versions supprimÃ©es impossible !\n\n\nExercice 7 : Compacter une table\n# VÃ©rifier fichiers avant\nspark.sql(\"DESCRIBE DETAIL delta.`path`\").select(\"numFiles\").show()\n\n# TODO: OPTIMIZE + ZORDER\n\n# VÃ©rifier aprÃ¨s\n\n\nğŸ’¡ Solution\n\nspark.sql(\"OPTIMIZE delta.`path` ZORDER BY (id)\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#migration-parquet-delta",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#migration-parquet-delta",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "8. Migration Parquet â†’ Delta",
    "text": "8. Migration Parquet â†’ Delta\n\n8.1 Conversion in-place (SANS copie !)\nCONVERT TO DELTA parquet.`s3a://bronze/old_table/`\nCONVERT TO DELTA parquet.`s3a://bronze/partitioned/`\n    PARTITIONED BY (date STRING)\nCrÃ©e _delta_log/ sans dÃ©placer les donnÃ©es. Temps : secondes.\n\n\n8.2 Migration avec CTAS\nCREATE TABLE new_delta USING DELTA AS\nSELECT * FROM parquet.`s3a://bronze/old/`",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#apache-hudi-aperÃ§u",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#apache-hudi-aperÃ§u",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "9. Apache Hudi â€” AperÃ§u",
    "text": "9. Apache Hudi â€” AperÃ§u\n\nCopy-on-Write vs Merge-on-Read\n\n\n\n\nCoW\nMoR\n\n\n\n\nÃ‰criture\nRÃ©Ã©crit fichier entier (ğŸ¢)\nÃ‰crit delta log (ğŸš€)\n\n\nLecture\nLit fichier (ğŸš€)\nMerge base + deltas (ğŸ¢)\n\n\nUse case\nLectures frÃ©quentes\nCDC intensif\n\n\n\nQuand utiliser Hudi : CDC massif, near-real-time, AWS EMR.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#mini-projet-lakehouse-avec-minio",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#mini-projet-lakehouse-avec-minio",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "10. Mini-Projet : Lakehouse avec MinIO",
    "text": "10. Mini-Projet : Lakehouse avec MinIO\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    MinIO                            â”‚\nâ”‚  bronze/          silver/           gold/           â”‚\nâ”‚  (CSV)     â†’     (Delta)      â†’    (Delta)          â”‚\nâ”‚  orders.csv       orders/           daily_sales/    â”‚\nâ”‚                   _delta_log/       _delta_log/     â”‚\nâ”‚      â†“               â†“                  â†“           â”‚\nâ”‚   Upload       MERGE INTO          OPTIMIZE         â”‚\nâ”‚               (Dedupe+CDC)         ZORDER BY        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nÃ‰tapes :\n\nDÃ©marrer MinIO\nConfigurer Spark + Delta\nBronze â†’ Silver (MERGE)\nTime Travel audit\nSilver â†’ Gold (agrÃ©gations)\nOPTIMIZE + ZORDER\nVACUUM",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#quiz",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#quiz",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "Quiz",
    "text": "Quiz\nQ1. Pourquoi Parquet seul ne garantit pas ACID ?\n\n\nR\n\nPas de Transaction Log pour atomicitÃ©/isolation.\n\nQ2. DiffÃ©rence Schema Enforcement vs Evolution ?\n\n\nR\n\nEnforcement rejette, Evolution adapte.\n\nQ3. Quel format supporte Hidden Partitioning ?\n\n\nR\n\nApache Iceberg uniquement.\n\nQ4. RÃ´le du Transaction Log Delta ?\n\n\nR\n\nEnregistre add/remove pour ACID et Time Travel.\n\nQ5. Objectif de ZORDER BY ?\n\n\nR\n\nOrganiser donnÃ©es pour meilleur Data Skipping.\n\nQ6. Quelle opÃ©ration supprime les vieux fichiers ?\n\n\nR\n\nVACUUM (Delta) ou expire_snapshots (Iceberg).",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#ressources",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#ressources",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDelta Lake Docs\nApache Iceberg Docs\nApache Hudi Docs",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#prochaine-Ã©tape",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module 24 : 24_kafka_streaming â€” Kafka & Streaming",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/intermediate/23_table_formats_delta_iceberg.html#rÃ©capitulatif",
    "href": "notebooks/intermediate/23_table_formats_delta_iceberg.html#rÃ©capitulatif",
    "title": "Table Formats : Delta Lake & Apache Iceberg",
    "section": "ğŸ“ RÃ©capitulatif",
    "text": "ğŸ“ RÃ©capitulatif\n\n\n\nConcept\nAppris\n\n\n\n\nData Lakehouse\nLake + Warehouse\n\n\nACID\nAtomicitÃ©, Consistance, Isolation, DurabilitÃ©\n\n\nDelta Lake\nTransaction Log, MERGE, Time Travel\n\n\nIceberg\nHidden Partitioning, Schema Evolution\n\n\nMaintenance\nOPTIMIZE, ZORDER, VACUUM\n\n\nMigration\nCONVERT TO DELTA\n\n\n\nğŸ‰ FÃ©licitations ! Module Table Formats terminÃ©.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "23 Â· Table Formats (Delta, Iceberg)"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html",
    "title": "PySpark for Data Engineering",
    "section": "",
    "text": "Ce module prÃ©sente PySpark, lâ€™API Python pour Apache Spark â€” le moteur de traitement distribuÃ© le plus utilisÃ© en Big Data.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prÃ©requis",
    "title": "PySpark for Data Engineering",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre MapReduce et ses limites\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nMaÃ®triser SQL (module 07)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#objectifs-du-module",
    "title": "PySpark for Data Engineering",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nComprendre lâ€™architecture Spark (Driver, Executors, Cluster Manager)\nCrÃ©er et manipuler des DataFrames distribuÃ©s\nÃ‰crire des transformations et actions\nUtiliser Spark SQL\nOptimiser les performances (partitioning, caching, broadcast)\nLire/Ã©crire des fichiers (CSV, JSON, Parquet)\nDÃ©couvrir le streaming temps rÃ©el",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pyspark-dans-lÃ©cosystÃ¨me-big-data",
    "title": "PySpark for Data Engineering",
    "section": "PySpark dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "PySpark dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 que Spark a remplacÃ© MapReduce comme moteur de traitement Big Data. Voici pourquoi :\n\nRappel : MapReduce vs Spark\nMapReduce :  DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                  â†‘           â†‘              â†‘              â†‘\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               LENT ! (I/O disque)\n\nSpark :      DISQUE â†’ Transformations â†’ MÃ‰MOIRE â†’ ... â†’ MÃ‰MOIRE â†’ Action\n                                          â†‘                â†‘\n                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           RAPIDE ! (in-memory)\n\n\nRappel : Les 5V et Spark\n\n\n\nV\nComment Spark rÃ©pond\n\n\n\n\nVolume\nTraitement distribuÃ© sur cluster (To â†’ Po)\n\n\nVelocity\nSpark Streaming pour le temps rÃ©el\n\n\nVariety\nLit CSV, JSON, Parquet, JDBC, Avroâ€¦\n\n\nVeracity\nTransformations pour nettoyer les donnÃ©es\n\n\nValue\nSpark SQL, MLlib pour extraire de la valeur\n\n\n\n\n\nPosition dans lâ€™Ã©cosystÃ¨me\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Ã‰COSYSTÃˆME BIG DATA                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources           Traitement              Stockage            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚                                                                 â”‚\nâ”‚   Kafka    â”€â”                         â”Œâ”€â–º  Data Lake (S3)       â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  âš¡ SPARK âš¡  â”€â”€â”€â”€â”€â”€â”¼â”€â–º  Data Warehouse          â”‚\nâ”‚   JDBC     â”€â”¤     (PySpark)          â”œâ”€â–º  NoSQL (MongoDB)       â”‚\nâ”‚   APIs     â”€â”˜                         â””â”€â–º  Elasticsearch        â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter toutes les cellules de code !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#installation-et-setup",
    "title": "PySpark for Data Engineering",
    "section": "Installation et Setup",
    "text": "Installation et Setup\nPySpark nÃ©cessite Java. VÃ©rifions dâ€™abord lâ€™installation.\n\n\nVoir le code\n# Installation de PySpark\n!pip install pyspark pandas numpy pyarrow\n\n\n\n\nVoir le code\n# VÃ©rifier Java\n!java -version\n\n\n\n\nVoir le code\n# Imports de base\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\nprint(\"âœ… Imports rÃ©ussis !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quest-ce-que-spark",
    "title": "PySpark for Data Engineering",
    "section": "Quâ€™est-ce que Spark ?",
    "text": "Quâ€™est-ce que Spark ?\nApache Spark est un moteur de traitement distribuÃ© ultra-rapide pour le Big Data.\n\nConcepts clÃ©s\n\nSparkSession : Point dâ€™entrÃ©e de toute application Spark\nDataFrame : Collection distribuÃ©e de donnÃ©es organisÃ©es en colonnes\nRDD : Resilient Distributed Dataset (bas niveau)\nTransformations : OpÃ©rations lazy (map, filter, select, etc.)\nActions : DÃ©clenchent lâ€™exÃ©cution (count, collect, show, etc.)\n\n\n\nAvantages de Spark\n\nVitesse : 100x plus rapide que MapReduce\nScalabilitÃ© : De quelques MB Ã  plusieurs PB\nSimplicitÃ© : API unifiÃ©e (Python, Scala, Java, R)\nVersatilitÃ© : Batch, Streaming, ML, Graph processing",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-une-sparksession",
    "title": "PySpark for Data Engineering",
    "section": "1.1 CrÃ©er une SparkSession",
    "text": "1.1 CrÃ©er une SparkSession\n\n\nVoir le code\n# CrÃ©er une SparkSession\nspark = SparkSession.builder \\\n    .appName(\"PySpark Data Engineering Tutorial\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n    .getOrCreate()\n\nprint(\" SparkSession crÃ©Ã©e\")\nprint(f\"Version Spark : {spark.version}\")\nprint(f\"Application : {spark.sparkContext.appName}\")\nprint(f\"Master : {spark.sparkContext.master}\")\n\n\n\n\nVoir le code\n# Configuration du logging\nspark.sparkContext.setLogLevel(\"ERROR\")\nprint(\" Logging configurÃ© sur ERROR\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#premiers-dataframes",
    "title": "PySpark for Data Engineering",
    "section": "1.2 Premiers DataFrames",
    "text": "1.2 Premiers DataFrames\n\n\nVoir le code\n# MÃ©thode 1 : Depuis une liste Python\ndata = [\n    (1, \"Alice\", 25, \"Paris\", 45000),\n    (2, \"Bob\", 30, \"Lyon\", 55000),\n    (3, \"Charlie\", 35, \"Paris\", 60000),\n    (4, \"David\", 28, \"Marseille\", 50000),\n    (5, \"Eve\", 32, \"Lyon\", 58000)\n]\n\ncolumns = [\"id\", \"nom\", \"age\", \"ville\", \"salaire\"]\n\ndf = spark.createDataFrame(data, columns)\n\nprint(\" Premier DataFrame crÃ©Ã© :\")\ndf.show()\n\n\n\n\nVoir le code\n# MÃ©thode 2 : Depuis un Pandas DataFrame\npandas_df = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'D'],\n    'prix': [10.5, 20.0, 15.75, 30.0],\n    'quantite': [100, 50, 75, 25]\n})\n\nspark_df = spark.createDataFrame(pandas_df)\n\nprint(\" DataFrame depuis Pandas :\")\nspark_df.show()\n\n\n\n\nVoir le code\n# MÃ©thode 3 : Avec un schÃ©ma explicite\nschema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"nom\", StringType(), False),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"ville\", StringType(), True),\n    StructField(\"salaire\", IntegerType(), True)\n])\n\ndf_with_schema = spark.createDataFrame(data, schema)\n\nprint(\" DataFrame avec schÃ©ma explicite :\")\ndf_with_schema.printSchema()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explorer-un-dataframe",
    "title": "PySpark for Data Engineering",
    "section": "1.3 Explorer un DataFrame",
    "text": "1.3 Explorer un DataFrame\n\n\nVoir le code\n# Afficher le schÃ©ma\nprint(\" SchÃ©ma du DataFrame :\")\ndf.printSchema()\n\n# Afficher les premiÃ¨res lignes\nprint(\"\\n PremiÃ¨res lignes :\")\ndf.show(3)\n\n# Compter les lignes\nprint(f\"\\n Nombre de lignes : {df.count()}\")\n\n# Colonnes\nprint(f\"\\n Colonnes : {df.columns}\")\n\n# Types de donnÃ©es\nprint(\"\\n Types de donnÃ©es :\")\nprint(df.dtypes)\n\n\n\n\nVoir le code\n# Statistiques descriptives\nprint(\" Statistiques descriptives :\")\ndf.describe().show()\n\n# Statistiques sur colonnes spÃ©cifiques\nprint(\"\\n Statistiques sur 'age' et 'salaire' :\")\ndf.select('age', 'salaire').describe().show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#sÃ©lection-de-colonnes",
    "title": "PySpark for Data Engineering",
    "section": "2.1 SÃ©lection de colonnes",
    "text": "2.1 SÃ©lection de colonnes\n\n\nVoir le code\n# SÃ©lectionner des colonnes\nprint(\" SÃ©lection de colonnes :\")\ndf.select(\"nom\", \"ville\").show()\n\n# Avec alias\nprint(\"\\n Avec alias :\")\ndf.select(\n    F.col(\"nom\").alias(\"employee_name\"),\n    F.col(\"salaire\").alias(\"salary\")\n).show()\n\n# SÃ©lectionner avec expressions\nprint(\"\\n Avec expressions :\")\ndf.select(\n    \"nom\",\n    (F.col(\"salaire\") * 12).alias(\"salaire_annuel\")\n).show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#filtrage",
    "title": "PySpark for Data Engineering",
    "section": "2.2 Filtrage",
    "text": "2.2 Filtrage\n\n\nVoir le code\n# Filtrer les lignes\nprint(\" EmployÃ©s de Paris :\")\ndf.filter(F.col(\"ville\") == \"Paris\").show()\n\n# Filtres multiples avec AND\nprint(\"\\n EmployÃ©s de Paris avec salaire &gt; 50000 :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") & \n    (F.col(\"salaire\") &gt; 50000)\n).show()\n\n# Filtres avec OR\nprint(\"\\n EmployÃ©s de Paris OU Lyon :\")\ndf.filter(\n    (F.col(\"ville\") == \"Paris\") | \n    (F.col(\"ville\") == \"Lyon\")\n).show()\n\n# Filtrer avec IN\nprint(\"\\n Villes avec IN :\")\ndf.filter(F.col(\"ville\").isin([\"Paris\", \"Lyon\"])).show()\n\n\n\n\nVoir le code\n# Filtres avancÃ©s\nprint(\" Noms commenÃ§ant par 'A' :\")\ndf.filter(F.col(\"nom\").startswith(\"A\")).show()\n\nprint(\"\\n Noms contenant 'li' :\")\ndf.filter(F.col(\"nom\").contains(\"li\")).show()\n\nprint(\"\\n Age entre 25 et 30 :\")\ndf.filter(F.col(\"age\").between(25, 30)).show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ajouter-et-modifier-des-colonnes",
    "title": "PySpark for Data Engineering",
    "section": "2.3 Ajouter et modifier des colonnes",
    "text": "2.3 Ajouter et modifier des colonnes\n\n\nVoir le code\n# Ajouter une nouvelle colonne\ndf_with_bonus = df.withColumn(\n    \"bonus\",\n    F.col(\"salaire\") * 0.1\n)\n\nprint(\" Ajout de la colonne 'bonus' :\")\ndf_with_bonus.show()\n\n# Modifier une colonne existante\ndf_modified = df.withColumn(\n    \"salaire\",\n    F.col(\"salaire\") * 1.05  # Augmentation de 5%\n)\n\nprint(\"\\n Salaire augmentÃ© de 5% :\")\ndf_modified.show()\n\n\n\n\nVoir le code\n# Ajouter plusieurs colonnes\ndf_enriched = df \\\n    .withColumn(\"salaire_mensuel\", F.col(\"salaire\")) \\\n    .withColumn(\"salaire_annuel\", F.col(\"salaire\") * 12) \\\n    .withColumn(\"bonus\", F.col(\"salaire\") * 0.1) \\\n    .withColumn(\"total_annuel\", F.col(\"salaire_annuel\") + F.col(\"bonus\"))\n\nprint(\" DataFrame enrichi :\")\ndf_enriched.select(\"nom\", \"salaire_mensuel\", \"salaire_annuel\", \"bonus\", \"total_annuel\").show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#renommer-et-supprimer-des-colonnes",
    "title": "PySpark for Data Engineering",
    "section": "2.4 Renommer et supprimer des colonnes",
    "text": "2.4 Renommer et supprimer des colonnes\n\n\nVoir le code\n# Renommer une colonne\ndf_renamed = df.withColumnRenamed(\"nom\", \"employee_name\")\nprint(\" Colonne renommÃ©e :\")\ndf_renamed.show(3)\n\n# Supprimer des colonnes\ndf_dropped = df.drop(\"age\", \"ville\")\nprint(\"\\n Colonnes supprimÃ©es :\")\ndf_dropped.show(3)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#tri",
    "title": "PySpark for Data Engineering",
    "section": "2.5 Tri",
    "text": "2.5 Tri\n\n\nVoir le code\n# Trier par salaire (ascendant)\nprint(\" Tri par salaire (croissant) :\")\ndf.orderBy(\"salaire\").show()\n\n# Trier par salaire (descendant)\nprint(\"\\n Tri par salaire (dÃ©croissant) :\")\ndf.orderBy(F.col(\"salaire\").desc()).show()\n\n# Tri multiple\nprint(\"\\n Tri par ville puis salaire :\")\ndf.orderBy(\"ville\", F.col(\"salaire\").desc()).show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#agrÃ©gations-simples",
    "title": "PySpark for Data Engineering",
    "section": "3.1 AgrÃ©gations simples",
    "text": "3.1 AgrÃ©gations simples\n\n\nVoir le code\n# Statistiques de base\nprint(\" Statistiques simples :\")\ndf.select(\n    F.count(\"*\").alias(\"total\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\"),\n    F.sum(\"salaire\").alias(\"salaire_total\")\n).show()\n\n\n\n\nVoir le code\n# AgrÃ©gations multiples\nfrom pyspark.sql.functions import stddev, variance\n\nprint(\" Statistiques avancÃ©es :\")\ndf.agg(\n    F.count(\"*\").alias(\"count\"),\n    F.avg(\"age\").alias(\"age_moyen\"),\n    F.stddev(\"salaire\").alias(\"salaire_stddev\"),\n    F.variance(\"salaire\").alias(\"salaire_variance\")\n).show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#groupby",
    "title": "PySpark for Data Engineering",
    "section": "3.2 GroupBy",
    "text": "3.2 GroupBy\n\n\nVoir le code\n# Grouper par ville\nprint(\" Statistiques par ville :\")\ndf.groupBy(\"ville\").agg(\n    F.count(\"*\").alias(\"nb_employes\"),\n    F.avg(\"salaire\").alias(\"salaire_moyen\"),\n    F.min(\"salaire\").alias(\"salaire_min\"),\n    F.max(\"salaire\").alias(\"salaire_max\")\n).orderBy(\"ville\").show()\n\n\n\n\nVoir le code\n# CrÃ©er un DataFrame plus complexe pour les exemples\ndata_ventes = [\n    (\"2024-01\", \"Paris\", \"Produit A\", 100, 1500),\n    (\"2024-01\", \"Paris\", \"Produit B\", 50, 2000),\n    (\"2024-01\", \"Lyon\", \"Produit A\", 75, 1200),\n    (\"2024-02\", \"Paris\", \"Produit A\", 120, 1800),\n    (\"2024-02\", \"Lyon\", \"Produit B\", 60, 2400),\n    (\"2024-02\", \"Marseille\", \"Produit A\", 90, 1350),\n]\n\ncolumns_ventes = [\"mois\", \"ville\", \"produit\", \"quantite\", \"montant\"]\ndf_ventes = spark.createDataFrame(data_ventes, columns_ventes)\n\nprint(\" DonnÃ©es de ventes :\")\ndf_ventes.show()\n\n\n\n\nVoir le code\n# GroupBy multiple\nprint(\" Ventes par mois et ville :\")\ndf_ventes.groupBy(\"mois\", \"ville\").agg(\n    F.sum(\"quantite\").alias(\"total_quantite\"),\n    F.sum(\"montant\").alias(\"total_montant\"),\n    F.count(\"*\").alias(\"nb_transactions\")\n).orderBy(\"mois\", \"ville\").show()\n\n\n\n\nVoir le code\n# AgrÃ©gations conditionnelles\nprint(\" AgrÃ©gations conditionnelles :\")\ndf_ventes.groupBy(\"ville\").agg(\n    F.sum(\"montant\").alias(\"total\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit A\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_a\"),\n    F.sum(F.when(F.col(\"produit\") == \"Produit B\", F.col(\"montant\")).otherwise(0)).alias(\"total_produit_b\")\n).show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#window-functions",
    "title": "PySpark for Data Engineering",
    "section": "3.3 Window Functions",
    "text": "3.3 Window Functions\n\n\nVoir le code\n# Ranking dans chaque ville\nfrom pyspark.sql.window import Window\n\nwindow_spec = Window.partitionBy(\"ville\").orderBy(F.col(\"salaire\").desc())\n\ndf_ranked = df.withColumn(\n    \"rank\",\n    F.row_number().over(window_spec)\n)\n\nprint(\"ğŸ† Ranking des salaires par ville :\")\ndf_ranked.orderBy(\"ville\", \"rank\").show()\n\n\n\n\nVoir le code\n# Calculs cumulatifs\nwindow_cumul = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\ndf_cumul = df.withColumn(\n    \"salaire_cumul\",\n    F.sum(\"salaire\").over(window_cumul)\n)\n\nprint(\" Salaire cumulÃ© par ville :\")\ndf_cumul.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_cumul\").orderBy(\"ville\", \"id\").show()\n\n\n\n\nVoir le code\n# Calcul de moyennes mobiles\nwindow_rolling = Window.partitionBy(\"ville\").orderBy(\"id\").rowsBetween(-1, 1)\n\ndf_rolling = df.withColumn(\n    \"salaire_avg_3\",\n    F.avg(\"salaire\").over(window_rolling)\n)\n\nprint(\" Moyenne mobile sur 3 lignes :\")\ndf_rolling.select(\"id\", \"nom\", \"ville\", \"salaire\", \"salaire_avg_3\").orderBy(\"ville\", \"id\").show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-dataframes-pour-les-exemples",
    "title": "PySpark for Data Engineering",
    "section": "4.1 CrÃ©er des DataFrames pour les exemples",
    "text": "4.1 CrÃ©er des DataFrames pour les exemples\n\n\nVoir le code\n# DataFrame employÃ©s\nemployes = spark.createDataFrame([\n    (1, \"Alice\", \"IT\"),\n    (2, \"Bob\", \"Finance\"),\n    (3, \"Charlie\", \"IT\"),\n    (4, \"David\", \"HR\")\n], [\"emp_id\", \"nom\", \"dept_id\"])\n\n# DataFrame dÃ©partements\ndepartements = spark.createDataFrame([\n    (\"IT\", \"Information Technology\", \"Paris\"),\n    (\"Finance\", \"Finance Department\", \"Lyon\"),\n    (\"HR\", \"Human Resources\", \"Marseille\"),\n    (\"Marketing\", \"Marketing Department\", \"Paris\")\n], [\"dept_id\", \"dept_name\", \"location\"])\n\nprint(\"ğŸ‘¥ EmployÃ©s :\")\nemployes.show()\n\nprint(\"\\nğŸ¢ DÃ©partements :\")\ndepartements.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#types-de-jointures",
    "title": "PySpark for Data Engineering",
    "section": "4.2 Types de jointures",
    "text": "4.2 Types de jointures\n\n\nVoir le code\n# INNER JOIN (par dÃ©faut)\nprint(\"ğŸ”— INNER JOIN :\")\nemployes.join(departements, \"dept_id\", \"inner\").show()\n\n# LEFT JOIN\nprint(\"\\nğŸ”— LEFT JOIN :\")\nemployes.join(departements, \"dept_id\", \"left\").show()\n\n# RIGHT JOIN\nprint(\"\\nğŸ”— RIGHT JOIN :\")\nemployes.join(departements, \"dept_id\", \"right\").show()\n\n# FULL OUTER JOIN\nprint(\"\\nğŸ”— FULL OUTER JOIN :\")\nemployes.join(departements, \"dept_id\", \"outer\").show()\n\n\n\n\nVoir le code\n# Jointure avec colonnes diffÃ©rentes\nemployes_alt = employes.withColumnRenamed(\"dept_id\", \"department\")\n\nprint(\" Jointure avec colonnes diffÃ©rentes :\")\nemployes_alt.join(\n    departements,\n    employes_alt.department == departements.dept_id,\n    \"inner\"\n).select(\n    employes_alt[\"*\"],\n    departements.dept_name,\n    departements.location\n).show()\n\n\n\n\nVoir le code\n# Jointures multiples\nsalaires = spark.createDataFrame([\n    (1, 45000),\n    (2, 55000),\n    (3, 50000),\n    (4, 48000)\n], [\"emp_id\", \"salaire\"])\n\nprint(\"ğŸ”— Jointures multiples :\")\nresult = employes \\\n    .join(departements, \"dept_id\", \"inner\") \\\n    .join(salaires, \"emp_id\", \"inner\")\n\nresult.select(\"nom\", \"dept_name\", \"location\", \"salaire\").show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#csv",
    "title": "PySpark for Data Engineering",
    "section": "5.1 CSV",
    "text": "5.1 CSV\n\n\nVoir le code\n# CrÃ©er des donnÃ©es de test\nimport os\nos.makedirs('data', exist_ok=True)\n\n# Ã‰crire en CSV\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\" CSV Ã©crit\")\n\n# Lire le CSV\ndf_from_csv = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(\"data/employes.csv\")\n\nprint(\"\\n CSV lu :\")\ndf_from_csv.show(3)\ndf_from_csv.printSchema()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#json",
    "title": "PySpark for Data Engineering",
    "section": "5.2 JSON",
    "text": "5.2 JSON\n\n\nVoir le code\n# Ã‰crire en JSON\ndf.write \\\n    .mode(\"overwrite\") \\\n    .json(\"data/employes.json\")\n\nprint(\" JSON Ã©crit\")\n\n# Lire le JSON\ndf_from_json = spark.read.json(\"data/employes.json\")\n\nprint(\"\\n JSON lu :\")\ndf_from_json.show(3)\ndf_from_json.printSchema()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#parquet-format-recommandÃ©",
    "title": "PySpark for Data Engineering",
    "section": "5.3 Parquet (Format recommandÃ©)",
    "text": "5.3 Parquet (Format recommandÃ©)\n\n\nVoir le code\n# Ã‰crire en Parquet\ndf.write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"data/employes.parquet\")\n\nprint(\" Parquet Ã©crit\")\n\n# Lire le Parquet\ndf_from_parquet = spark.read.parquet(\"data/employes.parquet\")\n\nprint(\"\\n Parquet lu :\")\ndf_from_parquet.show(3)\ndf_from_parquet.printSchema()\n\n\n\n\nVoir le code\n# Parquet avec partitionnement\ndf.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"ville\") \\\n    .parquet(\"data/employes_partitioned.parquet\")\n\nprint(\" Parquet partitionnÃ© Ã©crit\")\n\n# Lire avec filtre de partition (trÃ¨s performant)\ndf_paris = spark.read \\\n    .parquet(\"data/employes_partitioned.parquet\") \\\n    .filter(F.col(\"ville\") == \"Paris\")\n\nprint(\"\\n Parquet partitionnÃ© lu (ville=Paris) :\")\ndf_paris.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#options-dÃ©criture",
    "title": "PySpark for Data Engineering",
    "section": "5.4 Options dâ€™Ã©criture",
    "text": "5.4 Options dâ€™Ã©criture\n\n\nVoir le code\n# Mode d'Ã©criture\n# - \"overwrite\" : Ã‰crase les donnÃ©es existantes\n# - \"append\" : Ajoute aux donnÃ©es existantes\n# - \"ignore\" : Ne fait rien si le fichier existe\n# - \"error\" (default) : Erreur si le fichier existe\n\n# Compression\ndf.write \\\n    .mode(\"overwrite\") \\\n    .option(\"compression\", \"snappy\") \\\n    .parquet(\"data/employes_compressed.parquet\")\n\nprint(\" Parquet compressÃ© Ã©crit\")\n\n# ContrÃ´ler le nombre de fichiers\ndf.coalesce(1).write \\\n    .mode(\"overwrite\") \\\n    .csv(\"data/employes_single_file.csv\")\n\nprint(\" CSV en un seul fichier Ã©crit\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#crÃ©er-des-vues-temporaires",
    "title": "PySpark for Data Engineering",
    "section": "6.1 CrÃ©er des vues temporaires",
    "text": "6.1 CrÃ©er des vues temporaires\n\n\nVoir le code\n# CrÃ©er une vue temporaire\ndf.createOrReplaceTempView(\"employes\")\ndf_ventes.createOrReplaceTempView(\"ventes\")\n\nprint(\" Vues temporaires crÃ©Ã©es\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#requÃªtes-sql",
    "title": "PySpark for Data Engineering",
    "section": "6.2 RequÃªtes SQL",
    "text": "6.2 RequÃªtes SQL\n\n\nVoir le code\n# RequÃªte SQL simple\nresult = spark.sql(\"\"\"\n    SELECT nom, ville, salaire\n    FROM employes\n    WHERE salaire &gt; 50000\n    ORDER BY salaire DESC\n\"\"\")\n\nprint(\" EmployÃ©s avec salaire &gt; 50000 :\")\nresult.show()\n\n\n\n\nVoir le code\n# AgrÃ©gation avec SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        ville,\n        COUNT(*) as nb_employes,\n        AVG(salaire) as salaire_moyen,\n        MIN(salaire) as salaire_min,\n        MAX(salaire) as salaire_max\n    FROM employes\n    GROUP BY ville\n    ORDER BY salaire_moyen DESC\n\"\"\")\n\nprint(\" Statistiques par ville :\")\nresult.show()\n\n\n\n\nVoir le code\n# Window functions en SQL\nresult = spark.sql(\"\"\"\n    SELECT \n        nom,\n        ville,\n        salaire,\n        ROW_NUMBER() OVER (PARTITION BY ville ORDER BY salaire DESC) as rank_ville,\n        DENSE_RANK() OVER (ORDER BY salaire DESC) as rank_global\n    FROM employes\n    ORDER BY ville, rank_ville\n\"\"\")\n\nprint(\" Ranking avec SQL :\")\nresult.show()\n\n\n\n\nVoir le code\n# CTE (Common Table Expression)\nresult = spark.sql(\"\"\"\n    WITH stats_ville AS (\n        SELECT \n            ville,\n            AVG(salaire) as salaire_moyen\n        FROM employes\n        GROUP BY ville\n    )\n    SELECT \n        e.nom,\n        e.ville,\n        e.salaire,\n        s.salaire_moyen,\n        ROUND(e.salaire - s.salaire_moyen, 2) as diff_moyenne\n    FROM employes e\n    JOIN stats_ville s ON e.ville = s.ville\n    ORDER BY e.ville, e.salaire DESC\n\"\"\")\n\nprint(\" Comparaison Ã  la moyenne :\")\nresult.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#partitionnement",
    "title": "PySpark for Data Engineering",
    "section": "9.1 Partitionnement",
    "text": "9.1 Partitionnement\n\n\nVoir le code\n# VÃ©rifier le nombre de partitions\nprint(f\"Nombre de partitions : {df.rdd.getNumPartitions()}\")\n\n# Repartitionner (shuffle)\ndf_repartitioned = df.repartition(4)\nprint(f\"AprÃ¨s repartition : {df_repartitioned.rdd.getNumPartitions()}\")\n\n# Coalesce (pas de shuffle, moins coÃ»teux)\ndf_coalesced = df.coalesce(2)\nprint(f\"AprÃ¨s coalesce : {df_coalesced.rdd.getNumPartitions()}\")\n\n\n\n\nVoir le code\n# Repartitionner par colonne (utile avant les groupBy)\ndf_repartitioned_by_ville = df.repartition(\"ville\")\n\n# Maintenant les groupBy sur 'ville' seront plus efficaces\ndf_repartitioned_by_ville.groupBy(\"ville\").count().show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#caching",
    "title": "PySpark for Data Engineering",
    "section": "9.2 Caching",
    "text": "9.2 Caching\n\n\nVoir le code\n# Cache un DataFrame en mÃ©moire\ndf_cached = df.cache()\n\n# PremiÃ¨re action : calcul complet\nprint(\"PremiÃ¨re action (calcul complet) :\")\ndf_cached.count()\n\n# DeuxiÃ¨me action : utilise le cache (beaucoup plus rapide)\nprint(\"\\nDeuxiÃ¨me action (utilise le cache) :\")\ndf_cached.show()\n\n# LibÃ©rer le cache\ndf_cached.unpersist()\nprint(\"\\n Cache libÃ©rÃ©\")\n\n\n\n\nVoir le code\n# Persist avec diffÃ©rents niveaux de stockage\nfrom pyspark import StorageLevel\n\n# MEMORY_ONLY : En mÃ©moire uniquement\ndf.persist(StorageLevel.MEMORY_ONLY)\n\n# MEMORY_AND_DISK : MÃ©moire + disque si nÃ©cessaire\ndf.persist(StorageLevel.MEMORY_AND_DISK)\n\n# DISK_ONLY : Disque uniquement\ndf.persist(StorageLevel.DISK_ONLY)\n\nprint(\" DiffÃ©rents niveaux de persistance disponibles\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#broadcast-joins",
    "title": "PySpark for Data Engineering",
    "section": "9.3 Broadcast Joins",
    "text": "9.3 Broadcast Joins\n\n\nVoir le code\n# Pour les petits DataFrames (&lt; 10MB), utilisez broadcast\nfrom pyspark.sql.functions import broadcast\n\n# departements est petit, on le broadcast\nresult = employes.join(\n    broadcast(departements),\n    \"dept_id\",\n    \"inner\"\n)\n\nprint(\" Broadcast join :\")\nresult.show()\n\n# Ã‰vite le shuffle, beaucoup plus rapide !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#Ã©viter-les-udfs-quand-possible",
    "title": "PySpark for Data Engineering",
    "section": "9.4 Ã‰viter les UDFs quand possible",
    "text": "9.4 Ã‰viter les UDFs quand possible\n\n\nVoir le code\n# âŒ Avec UDF (lent)\ndef add_ten(x):\n    return x + 10\n\nadd_ten_udf = udf(add_ten, IntegerType())\ndf.withColumn(\"salaire_plus_10_udf\", add_ten_udf(F.col(\"salaire\")))\n\n# âœ… Avec fonction native (rapide)\ndf.withColumn(\"salaire_plus_10\", F.col(\"salaire\") + 10)\n\nprint(\" Les fonctions natives sont toujours plus rapides !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#explain-plans",
    "title": "PySpark for Data Engineering",
    "section": "9.5 Explain Plans",
    "text": "9.5 Explain Plans\n\n\nVoir le code\n# Voir le plan d'exÃ©cution\nprint(\" Plan d'exÃ©cution :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain()\n\n# Plan dÃ©taillÃ©\nprint(\"\\n Plan d'exÃ©cution dÃ©taillÃ© :\")\ndf.filter(F.col(\"salaire\") &gt; 50000).explain(extended=True)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#extract",
    "title": "PySpark for Data Engineering",
    "section": "11.1 Extract",
    "text": "11.1 Extract\n\n\nVoir le code\ndef extract_data(spark, path):\n    \"\"\"Extrait des donnÃ©es depuis plusieurs sources\"\"\"\n    print(f\"ğŸ“¥ Extraction depuis {path}\")\n    \n    # CrÃ©er des donnÃ©es de test\n    data = [\n        (1, \"2024-01-15\", \"Paris\", \"Produit A\", 100, 1500, \"online\"),\n        (2, \"2024-01-15\", \"Lyon\", \"Produit B\", 50, 2000, \"store\"),\n        (3, \"2024-01-16\", \"Paris\", \"Produit A\", 75, 1200, \"online\"),\n        (4, \"2024-01-16\", \"Marseille\", \"Produit C\", 120, 1800, \"online\"),\n        (5, \"2024-01-17\", \"Lyon\", \"Produit B\", 60, 2400, \"store\"),\n        (6, \"2024-01-17\", None, \"Produit A\", 90, None, \"online\"),  # DonnÃ©es sales\n    ]\n    \n    columns = [\"id\", \"date\", \"ville\", \"produit\", \"quantite\", \"montant\", \"canal\"]\n    df = spark.createDataFrame(data, columns)\n    \n    # Sauvegarder les donnÃ©es brutes\n    df.write.mode(\"overwrite\").parquet(f\"{path}/ventes_raw.parquet\")\n    \n    print(f\" {df.count()} lignes extraites\")\n    return df\n\n# Test\ndf_raw = extract_data(spark, \"spark_pipeline/raw\")\ndf_raw.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#transform",
    "title": "PySpark for Data Engineering",
    "section": "11.2 Transform",
    "text": "11.2 Transform\n\n\nVoir le code\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    print(\" Transformation des donnÃ©es\")\n    \n    # 1. Convertir la date\n    df = df.withColumn(\"date\", F.to_date(F.col(\"date\")))\n    \n    # 2. GÃ©rer les valeurs manquantes\n    df = df.fillna({\n        \"ville\": \"Inconnu\",\n        \"montant\": 0\n    })\n    \n    # 3. Filtrer les donnÃ©es invalides\n    df = df.filter(\n        (F.col(\"quantite\") &gt; 0) & \n        (F.col(\"montant\") &gt;= 0)\n    )\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    df = df.withColumn(\n        \"prix_unitaire\",\n        F.when(F.col(\"quantite\") &gt; 0, F.col(\"montant\") / F.col(\"quantite\")).otherwise(0)\n    )\n    \n    df = df.withColumn(\n        \"annee\",\n        F.year(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"mois\",\n        F.month(F.col(\"date\"))\n    )\n    \n    df = df.withColumn(\n        \"jour_semaine\",\n        F.dayofweek(F.col(\"date\"))\n    )\n    \n    # 5. CatÃ©goriser\n    df = df.withColumn(\n        \"categorie_montant\",\n        F.when(F.col(\"montant\") &lt; 1500, \"Faible\")\n         .when(F.col(\"montant\") &lt; 2000, \"Moyen\")\n         .otherwise(\"Ã‰levÃ©\")\n    )\n    \n    # 6. Ajouter metadata\n    df = df.withColumn(\"processed_at\", F.current_timestamp())\n    \n    print(f\" {df.count()} lignes transformÃ©es\")\n    return df\n\n# Test\ndf_transformed = transform_data(df_raw)\ndf_transformed.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#aggregate",
    "title": "PySpark for Data Engineering",
    "section": "11.3 Aggregate",
    "text": "11.3 Aggregate\n\n\nVoir le code\ndef aggregate_data(df):\n    \"\"\"CrÃ©e des agrÃ©gations mÃ©tier\"\"\"\n    print(\" AgrÃ©gation des donnÃ©es\")\n    \n    # AgrÃ©gation par ville et produit\n    agg_ville_produit = df.groupBy(\"ville\", \"produit\").agg(\n        F.sum(\"quantite\").alias(\"total_quantite\"),\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.avg(\"prix_unitaire\").alias(\"prix_moyen\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"ville\", \"produit\")\n    \n    print(\"\\n AgrÃ©gation par ville et produit :\")\n    agg_ville_produit.show()\n    \n    # AgrÃ©gation par canal\n    agg_canal = df.groupBy(\"canal\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\"),\n        F.avg(\"montant\").alias(\"montant_moyen\")\n    )\n    \n    print(\"\\n AgrÃ©gation par canal :\")\n    agg_canal.show()\n    \n    # AgrÃ©gation temporelle\n    agg_temporelle = df.groupBy(\"annee\", \"mois\").agg(\n        F.sum(\"montant\").alias(\"total_montant\"),\n        F.count(\"*\").alias(\"nb_transactions\")\n    ).orderBy(\"annee\", \"mois\")\n    \n    print(\"\\n AgrÃ©gation temporelle :\")\n    agg_temporelle.show()\n    \n    return {\n        \"ville_produit\": agg_ville_produit,\n        \"canal\": agg_canal,\n        \"temporelle\": agg_temporelle\n    }\n\n# Test\naggregations = aggregate_data(df_transformed)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#load",
    "title": "PySpark for Data Engineering",
    "section": "11.4 Load",
    "text": "11.4 Load\n\n\nVoir le code\ndef load_data(df, aggregations, output_path):\n    \"\"\"Charge les donnÃ©es dans le datalake\"\"\"\n    print(\" Chargement des donnÃ©es\")\n    \n    # 1. DonnÃ©es transformÃ©es (partitionnÃ©es par date)\n    df.write \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"annee\", \"mois\") \\\n        .parquet(f\"{output_path}/ventes_transformed\")\n    print(\" DonnÃ©es transformÃ©es sauvegardÃ©es\")\n    \n    # 2. AgrÃ©gations\n    for name, agg_df in aggregations.items():\n        agg_df.write \\\n            .mode(\"overwrite\") \\\n            .parquet(f\"{output_path}/agg_{name}\")\n        print(f\" AgrÃ©gation '{name}' sauvegardÃ©e\")\n    \n    # 3. Export CSV pour l'analyse\n    df.coalesce(1).write \\\n        .mode(\"overwrite\") \\\n        .option(\"header\", \"true\") \\\n        .csv(f\"{output_path}/ventes_export.csv\")\n    print(\" Export CSV crÃ©Ã©\")\n\n# Test\nload_data(df_transformed, aggregations, \"spark_pipeline/output\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#pipeline-complet-orchestrÃ©",
    "title": "PySpark for Data Engineering",
    "section": "11.5 Pipeline complet orchestrÃ©",
    "text": "11.5 Pipeline complet orchestrÃ©\n\n\nVoir le code\ndef run_pipeline(spark):\n    \"\"\"ExÃ©cute le pipeline ETL complet\"\"\"\n    import time\n    \n    start_time = time.time()\n    print(\"=\"*60)\n    print(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE PYSPARK\")\n    print(\"=\"*60)\n    \n    try:\n        # EXTRACT\n        print(\"\\n PHASE 1: EXTRACTION\")\n        df_raw = extract_data(spark, \"spark_pipeline/raw\")\n        \n        # TRANSFORM\n        print(\"\\n PHASE 2: TRANSFORMATION\")\n        df_transformed = transform_data(df_raw)\n        \n        # Cache pour les performances\n        df_transformed.cache()\n        \n        # AGGREGATE\n        print(\"\\n PHASE 3: AGRÃ‰GATION\")\n        aggregations = aggregate_data(df_transformed)\n        \n        # LOAD\n        print(\"\\n PHASE 4: CHARGEMENT\")\n        load_data(df_transformed, aggregations, \"spark_pipeline/output\")\n        \n        # STATISTICS\n        duration = time.time() - start_time\n        print(\"\\n\" + \"=\"*60)\n        print(\"STATISTIQUES DU PIPELINE\")\n        print(\"=\"*60)\n        print(f\"DurÃ©e totale: {duration:.2f}s\")\n        print(f\"Lignes traitÃ©es: {df_transformed.count()}\")\n        print(f\"Partitions: {df_transformed.rdd.getNumPartitions()}\")\n        print(\"=\"*60)\n        print(\"PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        print(\"=\"*60)\n        \n        # LibÃ©rer le cache\n        df_transformed.unpersist()\n        \n        return True\n        \n    except Exception as e:\n        print(f\"\\n ERREUR: {e}\")\n        import traceback\n        traceback.print_exc()\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline(spark)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ce-que-vous-avez-appris",
    "title": "PySpark for Data Engineering",
    "section": "Ce que vous avez appris",
    "text": "Ce que vous avez appris\n\nFondamentaux Spark : Architecture, concepts, SparkSession\nDataFrames : CrÃ©ation, transformations, actions\nTransformations : Select, filter, withColumn, orderBy\nAgrÃ©gations : GroupBy, agrÃ©gations complexes, window functions\nJointures : Inner, left, right, outer, broadcast\nI/O : CSV, JSON, Parquet avec partitionnement\nSpark SQL : RequÃªtes SQL, CTEs, window functions\nOptimisation : Partitionnement, caching, broadcast joins\nUDFs : Fonctions personnalisÃ©es\nStreaming : Traitement temps rÃ©el (introduction)\nPipeline ETL : Architecture complÃ¨te production-ready",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#diffÃ©rences-clÃ©s-pandas-vs-pyspark",
    "title": "PySpark for Data Engineering",
    "section": "DiffÃ©rences clÃ©s Pandas vs PySpark",
    "text": "DiffÃ©rences clÃ©s Pandas vs PySpark\n\n\n\n\n\n\n\n\nAspect\nPandas\nPySpark\n\n\n\n\nExÃ©cution\nEager (immÃ©diate)\nLazy (diffÃ©rÃ©e)\n\n\nDonnÃ©es\nEn mÃ©moire (single machine)\nDistribuÃ©es (cluster)\n\n\nScalabilitÃ©\nLimitÃ© Ã  la RAM\nQuasi illimitÃ©\n\n\nAPI\ndf[df['col'] &gt; 5]\ndf.filter(F.col('col') &gt; 5)\n\n\nPerformances\nRapide pour petites donnÃ©es\nRapide pour Big Data",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#quand-utiliser-pyspark",
    "title": "PySpark for Data Engineering",
    "section": "Quand utiliser PySpark ?",
    "text": "Quand utiliser PySpark ?\nâœ… Utilisez PySpark si : - DonnÃ©es &gt; 10 GB - Besoin de parallÃ©lisation - Traitement distribuÃ© nÃ©cessaire - Streaming en temps rÃ©el\nâŒ Utilisez Pandas si : - DonnÃ©es &lt; 10 GB - Prototypage rapide - Analyses exploratoires - Machine learning local",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaines-Ã©tapes",
    "title": "PySpark for Data Engineering",
    "section": "Prochaines Ã©tapes",
    "text": "Prochaines Ã©tapes\n\nPratiquer : CrÃ©er des pipelines avec vos propres donnÃ©es\nApprofondir :\n\nMLlib (Machine Learning)\nGraphX (Graph processing)\nDelta Lake (ACID transactions)\n\nProduction :\n\nDatabricks\nAWS EMR\nAzure Synapse\nGoogle Dataproc\n\nOrchestration :\n\nApache Airflow\nPrefect\nDagster",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources",
    "title": "PySpark for Data Engineering",
    "section": "Ressources ğŸ“š",
    "text": "Ressources ğŸ“š\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy\n\n\nFÃ©licitations ! ğŸ‰ Vous maÃ®trisez maintenant les fondamentaux de PySpark !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#ressources-1",
    "title": "PySpark for Data Engineering",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation PySpark\nSpark by Examples\nLearning Spark (Oâ€™Reilly)\nDatabricks Academy â€” Cours gratuits\n\n\nğŸ­ Plateformes Cloud\n\n\n\nPlateforme\nService Spark\n\n\n\n\nDatabricks\nDatabricks Lakehouse\n\n\nAWS\nEMR (Elastic MapReduce)\n\n\nAzure\nSynapse Analytics, HDInsight\n\n\nGCP\nDataproc",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/11_pyspark_for_data_engineering.html#prochaine-Ã©tape",
    "title": "PySpark for Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant le traitement Big Data avec PySpark !\nPour continuer ton parcours Data Engineering,\nğŸ‘‰ Module suivant : 12_orchestration_pipelines â€” Orchestration de pipelines\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module PySpark et le parcours sur les bases de donnÃ©es et le Big Data !\n\n\nVoir le code\n# Fermer la SparkSession\nspark.stop()\nprint(\"âœ… SparkSession fermÃ©e\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "11 Â· Introduction PySpark"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html",
    "title": "Elasticsearch for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente Elasticsearch, le moteur de recherche et dâ€™analytics distribuÃ©.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prÃ©requis",
    "title": "Elasticsearch for Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#objectifs-du-module",
    "title": "Elasticsearch for Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nComprendre lâ€™architecture Elasticsearch (index, shards, replicas)\nUtiliser Elasticvue pour interagir avec le cluster\nCrÃ©er des index avec mapping appropriÃ©\nCrÃ©er des Index Templates pour automatiser les mappings\nIndexer, rechercher, modifier et supprimer des documents\nÃ‰crire des requÃªtes de recherche (match, bool, fuzzy, range)\nRÃ©aliser des agrÃ©gations analytiques",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#version-elasticsearch",
    "title": "Elasticsearch for Data Engineers",
    "section": "Version Elasticsearch",
    "text": "Version Elasticsearch\nCe cours utilise Elasticsearch 8.x (version recommandÃ©e : 8.12+).\n\n\n\n\n\n\n\nVersion\nNotes\n\n\n\n\n8.x\nâœ… RecommandÃ©e â€” SÃ©curitÃ© par dÃ©faut, nouvelles fonctionnalitÃ©s\n\n\n7.x\nâš ï¸ Encore supportÃ©e mais migration conseillÃ©e\n\n\n6.x et avant\nâŒ ObsolÃ¨te\n\n\n\n\nğŸ’¡ Les exemples de ce cours fonctionnent avec ES 7.x et 8.x. Pour ES 8.x, la sÃ©curitÃ© est activÃ©e par dÃ©faut â€” nous la dÃ©sactivons pour les tests locaux.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#elasticsearch-dans-lÃ©cosystÃ¨me-big-data",
    "title": "Elasticsearch for Data Engineers",
    "section": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "ğŸ¯ Elasticsearch dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module 08 les diffÃ©rents types de bases NoSQL. Elasticsearch est un moteur de recherche (Search Engine), parfois classÃ© Ã  part des bases NoSQL traditionnelles.\n\nPosition dans les types NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Document  â”‚ ClÃ©-Valeurâ”‚  Colonnes â”‚  Graphe   â”‚ Search Engine  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚                â”‚\nâ”‚  MongoDB  â”‚   Redis   â”‚ Cassandra â”‚   Neo4j   â”‚ ELASTICSEARCH  â”‚\nâ”‚           â”‚           â”‚           â”‚           â”‚     â—„â”€â”€â”€       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment Elasticsearch rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs nÅ“uds)\n\n\nVelocity\nIndexation temps rÃ©el, near real-time search\n\n\nVariety\nDocuments JSON flexibles, analyse full-text\n\n\nVeracity\nScoring de pertinence, recherche approximative\n\n\nValue\nRecherche instantanÃ©e, dashboards Kibana\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nElasticsearch\n\n\n\n\nCAP\nAP (Availability + Partition tolerance) par dÃ©faut\n\n\nBASE\nEventually consistent (cohÃ©rence Ã©ventuelle)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#pourquoi-elasticsearch-en-data-engineering",
    "title": "Elasticsearch for Data Engineers",
    "section": "Pourquoi Elasticsearch en Data Engineering ?",
    "text": "Pourquoi Elasticsearch en Data Engineering ?\n\n\n\n\n\n\n\nCas dâ€™usage\nDescription\n\n\n\n\nLogs & Monitoring\nStack ELK (Elasticsearch, Logstash, Kibana)\n\n\nRecherche full-text\nMoteur de recherche pour sites web, e-commerce\n\n\nAnalytics temps rÃ©el\nDashboards et mÃ©triques en temps rÃ©el\n\n\nAlerting\nDÃ©tection dâ€™anomalies, alertes sur seuils\n\n\n\n\nğŸ’¡ ELK Stack = Elasticsearch + Logstash + Kibana â€” trÃ¨s utilisÃ© en entreprise.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#concepts-fondamentaux",
    "title": "Elasticsearch for Data Engineers",
    "section": "1. Concepts fondamentaux",
    "text": "1. Concepts fondamentaux\n\nVocabulaire\n\n\n\nSQL\nElasticsearch\nDescription\n\n\n\n\nDatabase\nCluster\nEnsemble de nÅ“uds\n\n\nTable\nIndex\nCollection de documents\n\n\nRow\nDocument\nUne entrÃ©e (JSON)\n\n\nColumn\nField\nUn champ du document\n\n\nSchema\nMapping\nStructure des champs\n\n\n\n\n\nShards & Replicas\nIndex \"clients\"\nâ”œâ”€â”€ Primary Shard 0  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 0\nâ”œâ”€â”€ Primary Shard 1  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 1\nâ””â”€â”€ Primary Shard 2  â”€â”€â”€â”€â”€â”€â–º  Replica Shard 2\n\n\n\nConcept\nRÃ´le\n\n\n\n\nShard\nPartition des donnÃ©es (scalabilitÃ©)\n\n\nReplica\nCopie dâ€™un shard (haute disponibilitÃ©)\n\n\n\n\n\nÃ‰tat du cluster\n\n\n\nÃ‰tat\nSignification\n\n\n\n\nğŸŸ¢ Green\nTous les shards OK\n\n\nğŸŸ¡ Yellow\nPrimaires OK, replicas non allouÃ©s (1 seul nÅ“ud)\n\n\nğŸ”´ Red\nDonnÃ©es inaccessibles",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#installation",
    "title": "Elasticsearch for Data Engineers",
    "section": "2. Installation",
    "text": "2. Installation\n\nÃ‰tape 1 : TÃ©lÃ©charger Elasticsearch\nğŸ‘‰ https://www.elastic.co/downloads/elasticsearch\n\nTÃ©lÃ©charger le ZIP pour ton OS\nDÃ©zipper dans un dossier (ex: C:\\elasticsearch)\nModifier config/elasticsearch.yml :\n\n# DÃ©sactiver la sÃ©curitÃ© pour les tests\nxpack.security.enabled: false\n\nLancer Elasticsearch :\n\n# Windows\n.\\bin\\elasticsearch.bat\n\n# macOS / Linux\n./bin/elasticsearch\n\nVÃ©rifier : ouvrir http://localhost:9200 dans le navigateur\n\n\n\n\nÃ‰tape 2 : Installer Elasticvue\nElasticvue est une interface graphique pour Elasticsearch â€” beaucoup plus simple que les commandes curl !\nOptions dâ€™installation :\n\n\n\nOption\nLien\n\n\n\n\nApp Web (recommandÃ©)\nhttps://app.elasticvue.com\n\n\nExtension Chrome\nChrome Web Store\n\n\nExtension Firefox\nFirefox Add-ons\n\n\nApp Desktop\nelasticvue.com",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#guide-elasticvue-prise-en-main",
    "title": "Elasticsearch for Data Engineers",
    "section": "3. Guide Elasticvue â€” Prise en main",
    "text": "3. Guide Elasticvue â€” Prise en main\n\n3.1 Connexion au cluster\n\nOuvrir Elasticvue\nCliquer sur â€œAdd Clusterâ€\nRemplir :\n\nName : Local (ou ce que tu veux)\nURI : http://localhost:9200\n\nCliquer â€œConnectâ€\n\nâœ… Tu devrais voir le statut du cluster (ğŸŸ¢ Green ou ğŸŸ¡ Yellow)\n\n\n\n3.2 Interface principale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Elasticvue                                    [Cluster: Local] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚             â”‚                                               â”‚\nâ”‚     Home    â”‚   Cluster Health: ğŸŸ¢ Green                    â”‚\nâ”‚             â”‚   Nodes: 1                                    â”‚\nâ”‚     Indices â”‚   Indices: 3                                  â”‚\nâ”‚             â”‚   Documents: 1,234                            â”‚\nâ”‚     Search  â”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚     REST    â”‚   â—„â”€â”€ C'est ici qu'on Ã©crit les requÃªtes !    â”‚\nâ”‚             â”‚                                               â”‚\nâ”‚     Settingsâ”‚                                               â”‚\nâ”‚             â”‚                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nOnglets importants :\n\n\n\nOnglet\nUsage\n\n\n\n\nIndices\nVoir/crÃ©er/supprimer des index\n\n\nSearch\nRechercher visuellement dans un index\n\n\nREST\nÃ‰crire des requÃªtes (comme Kibana Dev Tools)\n\n\n\n\n\n\n3.3 Utiliser la console REST\nLâ€™onglet REST permet dâ€™exÃ©cuter des requÃªtes Elasticsearch avec une syntaxe simple.\n\nFormat des requÃªtes\nMÃ‰THODE /chemin\n{\n  \"corps\": \"de la requÃªte en JSON\"\n}\n\n\nExemple pas Ã  pas\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  REST Query                                        [â–¶ Run]  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                              â”‚\nâ”‚  GET /clients/_search                                        â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"query\": {                                                â”‚\nâ”‚      \"match\": { \"pays\": \"France\" }                          â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ”‚                                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Response (200 OK)                                           â”‚\nâ”‚  {                                                           â”‚\nâ”‚    \"hits\": {                                                 â”‚\nâ”‚      \"total\": { \"value\": 2 },                                â”‚\nâ”‚      \"hits\": [...]                                           â”‚\nâ”‚    }                                                         â”‚\nâ”‚  }                                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nRaccourcis utiles\n\n\n\nRaccourci\nAction\n\n\n\n\nCtrl + Enter\nExÃ©cuter la requÃªte\n\n\nCtrl + /\nCommenter une ligne\n\n\nCtrl + Space\nAutocomplÃ©tion",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#crÃ©er-un-index-et-insÃ©rer-des-donnÃ©es",
    "title": "Elasticsearch for Data Engineers",
    "section": "4. CrÃ©er un index et insÃ©rer des donnÃ©es",
    "text": "4. CrÃ©er un index et insÃ©rer des donnÃ©es\n\n4.1 CrÃ©er lâ€™index clients\nDans lâ€™onglet REST, copier-coller :\nPUT /clients\n{\n  \"settings\": {\n    \"number_of_shards\": 1,\n    \"number_of_replicas\": 0\n  },\n  \"mappings\": {\n    \"properties\": {\n      \"nom\": { \"type\": \"text\" },\n      \"email\": { \"type\": \"keyword\" },\n      \"pays\": { \"type\": \"keyword\" },\n      \"age\": { \"type\": \"integer\" },\n      \"salaire\": { \"type\": \"float\" }\n    }\n  }\n}\nâœ… RÃ©ponse attendue : { \"acknowledged\": true }\n\n\n\n4.2 Comprendre le mapping\n\n\n\nType\nUsage\nRecherche full-text\nAgrÃ©gation\n\n\n\n\ntext\nTexte analysÃ©\nâœ… Oui\nâŒ Non\n\n\nkeyword\nValeur exacte\nâŒ Non\nâœ… Oui\n\n\ninteger, float\nNombres\nRange âœ…\nâœ… Oui\n\n\ndate\nDates\nRange âœ…\nâœ… Oui\n\n\nboolean\ntrue/false\nâœ…\nâœ… Oui\n\n\n\n\nğŸ’¡ RÃ¨gle : Utilise keyword pour les champs sur lesquels tu veux faire des agrÃ©gations (GROUP BY).",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#index-templates-automatiser-les-mappings",
    "title": "Elasticsearch for Data Engineers",
    "section": "Index Templates â€” Automatiser les mappings",
    "text": "Index Templates â€” Automatiser les mappings\n\nPourquoi utiliser des templates ?\nQuand tu gÃ¨res des logs ou des donnÃ©es temporelles, tu crÃ©es souvent des index par jour ou par mois :\nlogs-2024-01-01\nlogs-2024-01-02\nlogs-2024-01-03\n...\nProblÃ¨me : Sans template, chaque index utilise le mapping dynamique (Elasticsearch devine les types). Câ€™est risquÃ© !\nSolution : CrÃ©er un Index Template qui sâ€™applique automatiquement Ã  tous les index correspondant Ã  un pattern.\n\n\n\nCrÃ©er un Index Template\nPUT /_index_template/logs_template\n{\n  \"index_patterns\": [\"logs-*\"],\n  \"priority\": 1,\n  \"template\": {\n    \"settings\": {\n      \"number_of_shards\": 1,\n      \"number_of_replicas\": 1\n    },\n    \"mappings\": {\n      \"properties\": {\n        \"timestamp\": { \"type\": \"date\" },\n        \"level\": { \"type\": \"keyword\" },\n        \"message\": { \"type\": \"text\" },\n        \"service\": { \"type\": \"keyword\" },\n        \"host\": { \"type\": \"keyword\" },\n        \"response_time_ms\": { \"type\": \"integer\" }\n      }\n    }\n  }\n}\nMaintenant, tout index crÃ©Ã© avec le pattern logs-* aura automatiquement ce mapping !\n\n\n\nTester le template\n# CrÃ©er un index qui match le pattern\nPOST /logs-2024-01-15/_doc\n{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"level\": \"ERROR\",\n  \"message\": \"Connection timeout to database\",\n  \"service\": \"api-gateway\",\n  \"host\": \"server-01\",\n  \"response_time_ms\": 5000\n}\n# VÃ©rifier que le mapping a Ã©tÃ© appliquÃ©\nGET /logs-2024-01-15/_mapping\n\n\n\nLister les templates existants\nGET /_index_template\n\n\nSupprimer un template\nDELETE /_index_template/logs_template\n\n\n\nğŸ’¡ Bonnes pratiques Index Templates\n\n\n\n\n\n\n\nPratique\nExplication\n\n\n\n\nToujours dÃ©finir un mapping\nNe pas laisser ES deviner les types\n\n\nUtiliser keyword pour les agrÃ©gations\nlevel, service, host â†’ keyword\n\n\nUtiliser text pour la recherche\nmessage â†’ text\n\n\nDÃ©finir date explicitement\nÃ‰vite les erreurs de parsing\n\n\nNommer clairement\nlogs_template, metrics_template\n\n\nUtiliser priority\nQuand plusieurs templates matchent\n\n\n\n\n\n\n4.3 InsÃ©rer un document\nPOST /clients/_doc\n{\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@email.com\",\n  \"pays\": \"France\",\n  \"age\": 30,\n  \"salaire\": 55000\n}\n\n\n\n4.4 InsÃ©rer plusieurs documents (Bulk)\nPOST /_bulk\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Bob Martin\", \"email\": \"bob@email.com\", \"pays\": \"France\", \"age\": 25, \"salaire\": 48000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Charlie Konan\", \"email\": \"charlie@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 35, \"salaire\": 62000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Diana Schmidt\", \"email\": \"diana@email.com\", \"pays\": \"Allemagne\", \"age\": 28, \"salaire\": 51000 }\n{ \"index\": { \"_index\": \"clients\" } }\n{ \"nom\": \"Eve Kouassi\", \"email\": \"eve@email.com\", \"pays\": \"CÃ´te d'Ivoire\", \"age\": 32, \"salaire\": 58000 }\n\nâš ï¸ Attention : En bulk, chaque ligne doit Ãªtre sur une seule ligne (pas de formatage JSON multi-lignes).",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#requÃªtes-de-recherche",
    "title": "Elasticsearch for Data Engineers",
    "section": "5. RequÃªtes de recherche",
    "text": "5. RequÃªtes de recherche\n\n5.1 Afficher tous les documents\nGET /clients/_search\nâœ… SQL Ã©quivalent : SELECT * FROM clients\n\n\n\n5.2 Filtrer avec match (full-text)\nGET /clients/_search\n{\n  \"query\": {\n    \"match\": { \"nom\": \"Alice\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE nom LIKE '%Alice%'\n\n\n\n5.3 Filtrer avec term (exact match)\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France'\n\nğŸ’¡ Utilise term pour les champs keyword, match pour les champs text.\n\n\n\n\n5.4 Filtrer par plage de valeurs (range)\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"age\": {\n        \"gte\": 25,\n        \"lte\": 35\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE age BETWEEN 25 AND 35\n\n\n\nOpÃ©rateur\nSignification\n\n\n\n\ngt\n&gt; (greater than)\n\n\ngte\n&gt;= (greater than or equal)\n\n\nlt\n&lt; (less than)\n\n\nlte\n&lt;= (less than or equal)\n\n\n\n\n\n\n5.5 Conditions multiples (bool)\nLa requÃªte bool combine plusieurs conditions :\n\n\n\nClause\nComportement\nSQL Ã©quivalent\n\n\n\n\nmust\nToutes les conditions requises\nAND\n\n\nshould\nAu moins une condition\nOR\n\n\nmust_not\nExclure\nNOT / !=\n\n\nfilter\nComme must mais sans score\nWHERE (optimisÃ©)\n\n\n\n\nExemple : Clients franÃ§ais de plus de 25 ans\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"must\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"range\": { \"age\": { \"gt\": 25 } } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE pays = 'France' AND age &gt; 25\n\n\n\nExemple : Clients franÃ§ais OU ivoiriens, mais PAS Bob\nGET /clients/_search\n{\n  \"query\": {\n    \"bool\": {\n      \"should\": [\n        { \"term\": { \"pays\": \"France\" } },\n        { \"term\": { \"pays\": \"CÃ´te d'Ivoire\" } }\n      ],\n      \"minimum_should_match\": 1,\n      \"must_not\": [\n        { \"match\": { \"nom\": \"Bob\" } }\n      ]\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients WHERE (pays = 'France' OR pays = 'CÃ´te d''Ivoire') AND nom != 'Bob'\n\n\n\n\n5.6 Recherche floue (fuzzy)\nTrouve des rÃ©sultats mÃªme avec des fautes de frappe :\nGET /clients/_search\n{\n  \"query\": {\n    \"fuzzy\": {\n      \"nom\": {\n        \"value\": \"Alise\",\n        \"fuzziness\": \"AUTO\"\n      }\n    }\n  }\n}\nâœ… Trouve â€œAliceâ€ mÃªme si on tape â€œAliseâ€ !\n\nğŸ’¡ Pas dâ€™Ã©quivalent simple en SQL â€” câ€™est la force dâ€™Elasticsearch.\n\n\n\n\nğŸ“Œ 5.7 Trier et limiter les rÃ©sultats\nGET /clients/_search\n{\n  \"query\": { \"match_all\": {} },\n  \"sort\": [\n    { \"salaire\": \"desc\" }\n  ],\n  \"size\": 3\n}\nâœ… SQL Ã©quivalent : SELECT * FROM clients ORDER BY salaire DESC LIMIT 3",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#agrÃ©gations-group-by",
    "title": "Elasticsearch for Data Engineers",
    "section": "6. AgrÃ©gations (GROUP BY)",
    "text": "6. AgrÃ©gations (GROUP BY)\n\n6.1 Compter par catÃ©gorie (terms)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT pays, COUNT(*) FROM clients GROUP BY pays\n\nğŸ’¡ size: 0 = ne pas retourner les documents, seulement lâ€™agrÃ©gation.\n\n\n\n\n6.2 MÃ©triques (sum, avg, min, max)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n    \"salaire_max\": { \"max\": { \"field\": \"salaire\" } },\n    \"salaire_min\": { \"min\": { \"field\": \"salaire\" } },\n    \"salaire_total\": { \"sum\": { \"field\": \"salaire\" } }\n  }\n}\nâœ… SQL Ã©quivalent : SELECT AVG(salaire), MAX(salaire), MIN(salaire), SUM(salaire) FROM clients\n\n\n\n6.3 Stats complÃ¨tes en une requÃªte\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"stats_salaire\": {\n      \"stats\": { \"field\": \"salaire\" }\n    }\n  }\n}\nRÃ©sultat : count, min, max, avg, sum en une seule requÃªte !\n\n\n\n6.4 AgrÃ©gations imbriquÃ©es (GROUP BY + mÃ©triques)\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } },\n        \"age_moyen\": { \"avg\": { \"field\": \"age\" } }\n      }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, AVG(salaire) AS salaire_moyen, AVG(age) AS age_moyen\nFROM clients\nGROUP BY pays;\n\n\n\n6.5 Filtrer avant dâ€™agrÃ©ger\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"salaire\": { \"gte\": 50000 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\nâœ… SQL Ã©quivalent :\nSELECT pays, COUNT(*)\nFROM clients\nWHERE salaire &gt;= 50000\nGROUP BY pays;",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#modifier-et-supprimer",
    "title": "Elasticsearch for Data Engineers",
    "section": "7. Modifier et supprimer",
    "text": "7. Modifier et supprimer\n\n7.1 Mettre Ã  jour un document (par ID)\nPOST /clients/_update/1\n{\n  \"doc\": {\n    \"salaire\": 60000\n  }\n}\n\n\n\n7.2 Mettre Ã  jour par requÃªte\nPOST /clients/_update_by_query\n{\n  \"query\": {\n    \"term\": { \"pays\": \"France\" }\n  },\n  \"script\": {\n    \"source\": \"ctx._source.salaire += 1000\"\n  }\n}\nâœ… SQL Ã©quivalent : UPDATE clients SET salaire = salaire + 1000 WHERE pays = 'France'\n\n\n\n7.3 Supprimer un document (par ID)\nDELETE /clients/_doc/1\n\n\n\n7.4 Supprimer par requÃªte\nPOST /clients/_delete_by_query\n{\n  \"query\": {\n    \"range\": { \"age\": { \"lt\": 18 } }\n  }\n}\nâœ… SQL Ã©quivalent : DELETE FROM clients WHERE age &lt; 18\n\n\n\n7.5 Supprimer un index entier\nDELETE /clients\n\nâš ï¸ Attention : IrrÃ©versible !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#cheatsheet-elasticsearch",
    "title": "Elasticsearch for Data Engineers",
    "section": "Cheatsheet Elasticsearch",
    "text": "Cheatsheet Elasticsearch\n\nGestion des index\n\n\n\nAction\nRequÃªte\n\n\n\n\nCrÃ©er un index\nPUT /mon_index\n\n\nSupprimer\nDELETE /mon_index\n\n\nLister\nGET /_cat/indices?v\n\n\nVoir mapping\nGET /mon_index/_mapping\n\n\nSantÃ© cluster\nGET /_cluster/health\n\n\n\n\n\nCRUD Documents\n\n\n\nAction\nRequÃªte\n\n\n\n\nInsÃ©rer\nPOST /index/_doc\n\n\nLire (ID)\nGET /index/_doc/1\n\n\nMettre Ã  jour\nPOST /index/_update/1\n\n\nSupprimer\nDELETE /index/_doc/1\n\n\nBulk\nPOST /_bulk\n\n\n\n\n\nTypes de requÃªtes\n\n\n\nType\nUsage\n\n\n\n\nmatch\nFull-text (champs text)\n\n\nterm\nExact (champs keyword)\n\n\nrange\nPlage (nombres, dates)\n\n\nbool\nCombiner (must, should, must_not)\n\n\nfuzzy\nTolÃ©rant aux fautes\n\n\n\n\n\nAgrÃ©gations\n\n\n\nType\nSQL Ã©quivalent\n\n\n\n\nterms\nGROUP BY\n\n\nsum, avg, min, max\nFonctions dâ€™agrÃ©gation\n\n\nstats\nToutes les stats\n\n\ncardinality\nCOUNT(DISTINCT)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#exercices-pratiques",
    "title": "Elasticsearch for Data Engineers",
    "section": "Exercices pratiques",
    "text": "Exercices pratiques\nUtilise lâ€™onglet REST dâ€™Elasticvue pour rÃ©soudre ces exercices.\n\n\nExercice 1 â€” Facile\nAfficher tous les clients.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n\n\n\n\nExercice 2 â€” Facile\nTrouver les clients de CÃ´te dâ€™Ivoire.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"term\": { \"pays\": \"CÃ´te d'Ivoire\" }\n  }\n}\n\n\n\n\nExercice 3 â€” IntermÃ©diaire\nTrouver les clients avec un salaire entre 50000 et 60000, triÃ©s par Ã¢ge dÃ©croissant.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"query\": {\n    \"range\": {\n      \"salaire\": { \"gte\": 50000, \"lte\": 60000 }\n    }\n  },\n  \"sort\": [{ \"age\": \"desc\" }]\n}\n\n\n\n\nExercice 4 â€” IntermÃ©diaire\nCalculer le nombre de clients par pays.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" }\n    }\n  }\n}\n\n\n\n\nExercice 5 â€” AvancÃ©\nCalculer le salaire moyen par pays, seulement pour les clients de plus de 25 ans.\n\n\nğŸ’¡ Solution\n\nGET /clients/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"range\": { \"age\": { \"gt\": 25 } }\n  },\n  \"aggs\": {\n    \"par_pays\": {\n      \"terms\": { \"field\": \"pays\" },\n      \"aggs\": {\n        \"salaire_moyen\": { \"avg\": { \"field\": \"salaire\" } }\n      }\n    }\n  }\n}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#quiz",
    "title": "Elasticsearch for Data Engineers",
    "section": "Quiz",
    "text": "Quiz\n\n\nâ“ Q1. Quelle requÃªte crÃ©e un index ?\n\nPOST /clients\n\nPUT /clients\n\nGET /clients\n\nCREATE /clients\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” PUT /index crÃ©e un nouvel index.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre text et keyword ?\n\nAucune diffÃ©rence\n\ntext est analysÃ© (tokenisÃ©), keyword est stockÃ© tel quel\n\nkeyword est plus rapide\n\ntext est pour les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” text est dÃ©coupÃ© en tokens pour la recherche full-text, keyword reste intact pour les matchs exacts et agrÃ©gations.\n\n\n\n\nâ“ Q3. Quelle requÃªte utiliser pour un GROUP BY ?\n\nmatch\n\nbool\n\nterms (dans aggs)\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Lâ€™agrÃ©gation terms groupe les documents par valeur de champ.\n\n\n\n\nâ“ Q4. Que fait size: 0 dans une requÃªte dâ€™agrÃ©gation ?\n\nSupprime les donnÃ©es\n\nRetourne uniquement lâ€™agrÃ©gation, pas les documents\n\nLimite Ã  0 rÃ©sultat dâ€™agrÃ©gation\n\nErreur\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” size: 0 Ã©vite de retourner les documents, seulement les rÃ©sultats dâ€™agrÃ©gation.\n\n\n\n\nâ“ Q5. Quelle requÃªte permet de chercher â€œAliceâ€ mÃªme si on tape â€œAliseâ€ ?\n\nmatch\n\nterm\n\nfuzzy\n\nrange\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” fuzzy tolÃ¨re les fautes de frappe.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#ressources",
    "title": "Elasticsearch for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nElasticsearch Documentation\nElasticvue â€” Interface graphique utilisÃ©e dans ce cours\nKibana â€” Visualisation et dashboards\nElastic Cloud â€” Version cloud managÃ©e",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/10_elasticsearch_for_data_engineers.html#prochaine-Ã©tape",
    "title": "Elasticsearch for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant les bases NoSQL (MongoDB et Elasticsearch) ! Passons au traitement distribuÃ© avec PySpark.\nğŸ‘‰ Module suivant : 11_pyspark_for_data_engineering â€” PySpark pour le traitement Big Data\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Elasticsearch pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "10 Â· Elasticsearch"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html",
    "href": "notebooks/beginner/12_orchestration_pipelines.html",
    "title": "Orchestration de Pipelines Data",
    "section": "",
    "text": "Ce module prÃ©sente les outils dâ€™orchestration pour automatiser lâ€™exÃ©cution de vos pipelines de donnÃ©es.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prÃ©requis",
    "title": "Orchestration de Pipelines Data",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 11_pyspark_for_data_engineering\n\n\nâœ… Requis\nComprendre les pipelines ETL\n\n\nâœ… Requis\nMaÃ®triser Python (modules 04-05)\n\n\nâœ… Requis\nConnaÃ®tre les bases de Linux (ligne de commande)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#objectifs-du-module",
    "title": "Orchestration de Pipelines Data",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nComprendre ce quâ€™est lâ€™orchestration de pipelines\nUtiliser le Planificateur Windows (niveau dÃ©butant)\nConfigurer des tÃ¢ches avec Crontab (niveau intermÃ©diaire)\nComprendre lâ€™architecture dâ€™Apache Airflow\nCrÃ©er des DAGs avec Apache Airflow\nUtiliser les diffÃ©rents types dâ€™Operators\nGÃ©rer les dÃ©pendances et le passage de donnÃ©es (XCom)\nConfigurer les alertes et le monitoring\nChoisir le bon outil selon ton besoin",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#lorchestration-dans-lÃ©cosystÃ¨me-data-engineering",
    "title": "Orchestration de Pipelines Data",
    "section": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering",
    "text": "ğŸ¯ Lâ€™orchestration dans lâ€™Ã©cosystÃ¨me Data Engineering\nTu as appris Ã  crÃ©er des pipelines ETL avec PySpark. Mais comment les automatiser pour quâ€™ils sâ€™exÃ©cutent rÃ©guliÃ¨rement sans intervention manuelle ?\n\nLe problÃ¨me\nSans orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   ğŸ˜° \"Il faut que je lance mon script tous les jours...\"   â”‚\nâ”‚   ğŸ˜° \"J'ai oubliÃ© de lancer le pipeline hier !\"            â”‚\nâ”‚   ğŸ˜° \"Le script B a plantÃ© car A n'avait pas fini...\"      â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nAvec orchestration :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                             â”‚\nâ”‚   âœ… Scripts exÃ©cutÃ©s automatiquement                       â”‚\nâ”‚   âœ… Alertes en cas d'Ã©chec                                 â”‚\nâ”‚   âœ… DÃ©pendances respectÃ©es (A â†’ B â†’ C)                     â”‚\nâ”‚   âœ… Logs et monitoring centralisÃ©s                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPosition dans le pipeline Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     PIPELINE DATA                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Sources        ETL              Destination                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚\nâ”‚                                                                 â”‚\nâ”‚   APIs     â”€â”                 â”Œâ”€â–º  Data Warehouse               â”‚\nâ”‚   Fichiers â”€â”¼â”€â”€â–º  PySpark  â”€â”€â”€â”¼â”€â–º  Data Lake                    â”‚\nâ”‚   BDD      â”€â”˜                 â””â”€â–º  Dashboard                    â”‚\nâ”‚                                                                 â”‚\nâ”‚            â–²                                                    â”‚\nâ”‚            â”‚                                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚\nâ”‚   â”‚  ORCHESTRATION  â”‚  â—„â”€â”€ Quand ? Dans quel ordre ?            â”‚\nâ”‚   â”‚  (Airflow/Cron) â”‚                                           â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comparaison-rapide-des-outils",
    "title": "Orchestration de Pipelines Data",
    "section": "ğŸ“Š Comparaison rapide des outils",
    "text": "ğŸ“Š Comparaison rapide des outils\n\n\n\nCritÃ¨re\nWindows Task\nCrontab\nAirflow\n\n\n\n\nFacilitÃ©\nâ­â­â­\nâ­â­\nâ­\n\n\nInterface\nGUI\nCLI\nWeb\n\n\nDÃ©pendances\nâŒ\nâŒ\nâœ…\n\n\nMonitoring\nBasique\nNon\nComplet\n\n\nRetry auto\nâŒ\nâŒ\nâœ…\n\n\nIdÃ©al pour\n1-5 scripts\n5-15 scripts\n10+ pipelines",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi",
    "title": "Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn outil intÃ©grÃ© Ã  Windows pour exÃ©cuter des programmes automatiquement.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser",
    "title": "Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n\nOuvrir le planificateur :\nWindows + R â†’ Taper 'taskschd.msc' â†’ EntrÃ©e\n\n\nCrÃ©er une tÃ¢che :\n\nAction â†’ CrÃ©er une tÃ¢che de base\nNom : â€œMon script quotidienâ€\nDÃ©clencheur : Quotidien Ã  2h du matin\nAction : DÃ©marrer python.exe avec C:\\scripts\\mon_script.py\nTerminer\n\nâœ… VoilÃ  ! Votre script sâ€™exÃ©cutera automatiquement tous les jours Ã  2h.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces",
    "title": "Orchestration de Pipelines Data",
    "section": "âœ… Forces",
    "text": "âœ… Forces\nâœ… TrÃ¨s facile - Interface graphique intuitive\nâœ… DÃ©jÃ  installÃ© - Pas de setup\nâœ… Parfait pour dÃ©buter - Pas de code complexe",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses",
    "title": "Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses",
    "text": "âŒ Faiblesses\nâŒ Pas de dÃ©pendances - Si tÃ¢che A doit finir avant tÃ¢che B â†’ compliquÃ©\nâŒ Monitoring limitÃ© - Difficile de voir lâ€™Ã©tat global\nâŒ Windows uniquement - Ne fonctionne pas sur Linux",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser",
    "title": "Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nâœ… OUI : Vous avez 1-5 scripts simples sur Windows\nâŒ NON : Vous avez besoin que script B attende script A",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-1",
    "title": "Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nLe planificateur standard sur Linux/Mac.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#syntaxe-de-base",
    "title": "Orchestration de Pipelines Data",
    "section": "Syntaxe de base",
    "text": "Syntaxe de base\nminute heure jour mois jour_semaine commande\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande\n\nExemples simples :\n# Tous les jours Ã  2h du matin\n0 2 * * * python3 /home/user/script.py\n\n# Toutes les heures\n0 * * * * python3 /home/user/hourly.py\n\n# Lundi Ã  vendredi Ã  9h\n0 9 * * 1-5 python3 /home/user/weekday.py\n\n# Toutes les 15 minutes\n*/15 * * * * python3 /home/user/check.py\n\n# Le 1er de chaque mois Ã  minuit\n0 0 1 * * python3 /home/user/monthly.py",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#comment-lutiliser-1",
    "title": "Orchestration de Pipelines Data",
    "section": "Comment lâ€™utiliser ?",
    "text": "Comment lâ€™utiliser ?\n# Ã‰diter votre crontab\ncrontab -e\n\n# Voir les tÃ¢ches planifiÃ©es\ncrontab -l\n\n# Ajouter vos lignes\n0 2 * * * python3 /home/user/backup.py &gt;&gt; /var/log/backup.log 2&gt;&1\n\n# Sauvegarder et quitter\n# âœ… C'est fait !\n\nğŸ’¡ Astuce : Utilise crontab.guru pour tester tes expressions !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-1",
    "title": "Orchestration de Pipelines Data",
    "section": "Forces",
    "text": "Forces\nUniversel - Sur TOUS les serveurs Linux\nTrÃ¨s lÃ©ger - Presque pas de ressources\nSimple - Une ligne = une tÃ¢che\nGratuit - DÃ©jÃ  installÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-1",
    "title": "Orchestration de Pipelines Data",
    "section": "Faiblesses",
    "text": "Faiblesses\nPas de dÃ©pendances - MÃªme problÃ¨me que Windows\nPas de monitoring - Aucune interface\nPas de retry - Si Ã§a Ã©choue, il faut attendre le prochain run\nLogs manuels - Il faut les gÃ©rer soi-mÃªme",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-lutiliser-1",
    "title": "Orchestration de Pipelines Data",
    "section": "Quand lâ€™utiliser ?",
    "text": "Quand lâ€™utiliser ?\nOUI : Serveur Linux, 5-15 scripts indÃ©pendants\nNON : Scripts avec dÃ©pendances complexes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#cest-quoi-2",
    "title": "Orchestration de Pipelines Data",
    "section": "Câ€™est quoi ?",
    "text": "Câ€™est quoi ?\nUn orchestrateur professionnel pour gÃ©rer des workflows complexes, crÃ©Ã© par Airbnb en 2014 et devenu projet Apache en 2016.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    APACHE AIRFLOW                               â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   \"Airflow is a platform to programmatically author,           â”‚\nâ”‚    schedule, and monitor workflows.\"                            â”‚\nâ”‚                                                                 â”‚\nâ”‚   â€¢ CrÃ©Ã© par Airbnb (2014)                                      â”‚\nâ”‚   â€¢ Apache Top-Level Project (2019)                             â”‚\nâ”‚   â€¢ 30,000+ GitHub stars                                        â”‚\nâ”‚   â€¢ UtilisÃ© par : Airbnb, Netflix, Spotify, Twitter, Adobe...   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nLa grande diffÃ©rence avec Cron :\n# âŒ Avec Cron (problÃ¨me) :\n0 2 * * * python extract.py\n30 2 * * * python transform.py  # Et si extract prend plus de 30min ?\n0 3 * * * python load.py        # Et si transform a plantÃ© ?\n\n# âœ… Avec Airflow (solution) :\nextract &gt;&gt; transform &gt;&gt; load  # transform ATTEND que extract soit terminÃ© !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#architecture-dairflow",
    "title": "Orchestration de Pipelines Data",
    "section": "Architecture dâ€™Airflow",
    "text": "Architecture dâ€™Airflow\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        ARCHITECTURE AIRFLOW                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â”‚   WEB SERVER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    SCHEDULER    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  EXECUTOR   â”‚  â”‚\nâ”‚   â”‚   (Flask UI)    â”‚         â”‚  (Orchestrate)  â”‚         â”‚  (Workers)  â”‚  â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚         â”‚             â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚            â”‚                           â”‚                         â”‚         â”‚\nâ”‚            â”‚                           â–¼                         â”‚         â”‚\nâ”‚            â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚         â”‚\nâ”‚            â”‚                  â”‚                 â”‚                â”‚         â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    METADATA     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                               â”‚    DATABASE     â”‚                          â”‚\nâ”‚                               â”‚  (PostgreSQL)   â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                        â–²                                   â”‚\nâ”‚                                        â”‚                                   â”‚\nâ”‚                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â”‚    DAG FILES    â”‚                          â”‚\nâ”‚                               â”‚   (Python .py)  â”‚                          â”‚\nâ”‚                               â”‚                 â”‚                          â”‚\nâ”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nComposants clÃ©s\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nDescription\n\n\n\n\nWeb Server\nInterface UI\nDashboard Flask pour visualiser les DAGs\n\n\nScheduler\nPlanification\nDÃ©cide quand exÃ©cuter les tÃ¢ches\n\n\nExecutor\nExÃ©cution\nLance les tÃ¢ches (Local, Celery, K8sâ€¦)\n\n\nMetadata DB\nStockage\nÃ‰tat des DAGs, logs, historique\n\n\nDAG Files\nDÃ©finition\nFichiers Python dÃ©finissant les workflows\n\n\n\n\n\nTypes dâ€™Executors\n\n\n\nExecutor\nUsage\nScalabilitÃ©\n\n\n\n\nSequentialExecutor\nDev/Test\n1 tÃ¢che Ã  la fois\n\n\nLocalExecutor\nPetite prod\nParallÃ¨le sur 1 machine\n\n\nCeleryExecutor\nProduction\nWorkers distribuÃ©s\n\n\nKubernetesExecutor\nCloud\nPod par tÃ¢che",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#concepts-clÃ©s-dairflow",
    "title": "Orchestration de Pipelines Data",
    "section": "Concepts clÃ©s dâ€™Airflow",
    "text": "Concepts clÃ©s dâ€™Airflow\n\n1ï¸âƒ£ DAG (Directed Acyclic Graph)\nUn DAG est un graphe de tÃ¢ches sans cycle : les donnÃ©es vont toujours dans une direction.\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚ Extract â”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n          â–¼\n     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n     â”‚Transformâ”‚\n     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n          â”‚\n    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n    â–¼           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚Load DWâ”‚  â”‚Load S3 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2ï¸âƒ£ Task\nUne Task est une unitÃ© de travail dans un DAG (une Ã©tape).\n\n\n3ï¸âƒ£ Operator\nUn Operator dÃ©finit ce que fait une tÃ¢che.\n\n\n\nOperator\nUsage\n\n\n\n\nPythonOperator\nExÃ©cuter une fonction Python\n\n\nBashOperator\nExÃ©cuter une commande bash\n\n\nEmailOperator\nEnvoyer un email\n\n\nPostgresOperator\nExÃ©cuter du SQL\n\n\nS3ToRedshiftOperator\nCopier S3 â†’ Redshift\n\n\n\n\n\n4ï¸âƒ£ Task Instance\nUne Task Instance = une exÃ©cution spÃ©cifique dâ€™une Task Ã  une date donnÃ©e.\n\n\n5ï¸âƒ£ DAG Run\nUn DAG Run = une exÃ©cution complÃ¨te du DAG.\nDAG: etl_pipeline\nâ”œâ”€â”€ DAG Run 2024-01-15 âœ…\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) âœ…\nâ”‚   â””â”€â”€ load (Task Instance) âœ…\nâ”‚\nâ”œâ”€â”€ DAG Run 2024-01-16 â³\nâ”‚   â”œâ”€â”€ extract (Task Instance) âœ…\nâ”‚   â”œâ”€â”€ transform (Task Instance) â³ running\nâ”‚   â””â”€â”€ load (Task Instance) â¸ï¸ waiting",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#installation-locale-dairflow",
    "title": "Orchestration de Pipelines Data",
    "section": "Installation locale dâ€™Airflow",
    "text": "Installation locale dâ€™Airflow\n# CrÃ©er un environnement virtuel\npython -m venv airflow_venv\nsource airflow_venv/bin/activate  # Linux/Mac\n# ou : airflow_venv\\Scripts\\activate  # Windows\n\n# DÃ©finir le home Airflow\nexport AIRFLOW_HOME=~/airflow\n\n# Installer Airflow (version contrainte pour compatibilitÃ©)\nAIRFLOW_VERSION=2.8.1\nPYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\nCONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n\npip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n\n# Initialiser la base de donnÃ©es\nairflow db init\n\n# CrÃ©er un utilisateur admin\nairflow users create \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --role Admin \\\n    --email admin@example.com \\\n    --password admin\n\n# Lancer le webserver (Terminal 1)\nairflow webserver --port 8080\n\n# Lancer le scheduler (Terminal 2)\nairflow scheduler\nğŸ‘‰ AccÃ©der : http://localhost:8080 (login: admin / admin)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ton-premier-dag",
    "title": "Orchestration de Pipelines Data",
    "section": "Ton premier DAG",
    "text": "Ton premier DAG\nCrÃ©er le fichier ~/airflow/dags/mon_premier_dag.py :\n\n\nVoir le code\n# ~/airflow/dags/mon_premier_dag.py\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime, timedelta\n\n# Arguments par dÃ©faut pour toutes les tÃ¢ches\ndefault_args = {\n    'owner': 'data_engineer',\n    'depends_on_past': False,\n    'email': ['data-team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n}\n\n# DÃ©finir le DAG\ndag = DAG(\n    dag_id='mon_premier_dag',\n    default_args=default_args,\n    description='Mon premier pipeline ETL',\n    schedule_interval='@daily',  # ExÃ©cution quotidienne\n    start_date=datetime(2024, 1, 1),\n    catchup=False,  # Ne pas exÃ©cuter les runs passÃ©s\n    tags=['etl', 'tutorial'],\n)\n\n# Fonctions Python\ndef extract():\n    \"\"\"Extraire les donnÃ©es\"\"\"\n    print(\"ğŸ“¥ Extraction des donnÃ©es depuis l'API...\")\n    # Simuler extraction\n    data = {'records': 1000, 'source': 'api'}\n    return data  # RetournÃ© via XCom\n\ndef transform(**context):\n    \"\"\"Transformer les donnÃ©es\"\"\"\n    # RÃ©cupÃ©rer les donnÃ©es de extract via XCom\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ğŸ”„ Transformation de {data['records']} enregistrements\")\n    return {'records': data['records'], 'cleaned': True}\n\ndef load(**context):\n    \"\"\"Charger les donnÃ©es\"\"\"\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='transform')\n    print(f\"ğŸ’¾ Chargement de {data['records']} enregistrements nettoyÃ©s\")\n\n# CrÃ©er les tÃ¢ches\ntask_start = BashOperator(\n    task_id='start',\n    bash_command='echo \"ğŸš€ DÃ©marrage du pipeline Ã  $(date)\"',\n    dag=dag,\n)\n\ntask_extract = PythonOperator(\n    task_id='extract',\n    python_callable=extract,\n    dag=dag,\n)\n\ntask_transform = PythonOperator(\n    task_id='transform',\n    python_callable=transform,\n    dag=dag,\n)\n\ntask_load = PythonOperator(\n    task_id='load',\n    python_callable=load,\n    dag=dag,\n)\n\ntask_end = BashOperator(\n    task_id='end',\n    bash_command='echo \"âœ… Pipeline terminÃ© avec succÃ¨s !\"',\n    dag=dag,\n)\n\n# DÃ©finir les dÃ©pendances\ntask_start &gt;&gt; task_extract &gt;&gt; task_transform &gt;&gt; task_load &gt;&gt; task_end\n\n# Ã‰quivalent Ã  :\n# task_start.set_downstream(task_extract)\n# task_extract.set_downstream(task_transform)\n# etc.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#paramÃ¨tres-importants-du-dag",
    "title": "Orchestration de Pipelines Data",
    "section": "ParamÃ¨tres importants du DAG",
    "text": "ParamÃ¨tres importants du DAG\n\nSchedule Interval (frÃ©quence dâ€™exÃ©cution)\n\n\n\nPreset\nÃ‰quivalent Cron\nDescription\n\n\n\n\n@once\n-\nUne seule fois\n\n\n@hourly\n0 * * * *\nChaque heure\n\n\n@daily\n0 0 * * *\nChaque jour Ã  minuit\n\n\n@weekly\n0 0 * * 0\nChaque dimanche\n\n\n@monthly\n0 0 1 * *\nLe 1er du mois\n\n\n@yearly\n0 0 1 1 *\nLe 1er janvier\n\n\nNone\n-\nDÃ©clenchÃ© manuellement\n\n\n'0 6 * * 1-5'\nCron\nLun-Ven Ã  6h\n\n\n\n\n\nCatchup\n# catchup=True (dÃ©faut) :\n# Si start_date=2024-01-01 et on est le 2024-01-10,\n# Airflow va exÃ©cuter les 10 DAG Runs manquÃ©s !\n\n# catchup=False :\n# ExÃ©cute seulement Ã  partir de maintenant\n\n\nDefault Args importants\ndefault_args = {\n    'owner': 'data_team',           # PropriÃ©taire\n    'depends_on_past': False,       # DÃ©pend du run prÃ©cÃ©dent ?\n    'email_on_failure': True,       # Email si Ã©chec\n    'retries': 3,                   # Nombre de retry\n    'retry_delay': timedelta(minutes=5),  # DÃ©lai entre retries\n    'execution_timeout': timedelta(hours=1),  # Timeout\n    'sla': timedelta(hours=2),      # SLA (alerte si dÃ©passÃ©)\n}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#les-operators-les-plus-utilisÃ©s",
    "title": "Orchestration de Pipelines Data",
    "section": "Les Operators les plus utilisÃ©s",
    "text": "Les Operators les plus utilisÃ©s\n\nPythonOperator\nfrom airflow.operators.python import PythonOperator\n\ndef my_function(name, **context):\n    print(f\"Hello {name}!\")\n    print(f\"Execution date: {context['ds']}\")\n    return \"success\"\n\ntask = PythonOperator(\n    task_id='python_task',\n    python_callable=my_function,\n    op_kwargs={'name': 'World'},  # Arguments de la fonction\n)\n\n\nBashOperator\nfrom airflow.operators.bash import BashOperator\n\ntask = BashOperator(\n    task_id='bash_task',\n    bash_command='echo \"Date: {{ ds }}\" && python /scripts/etl.py',\n)\n\n\nPostgresOperator\nfrom airflow.providers.postgres.operators.postgres import PostgresOperator\n\ntask = PostgresOperator(\n    task_id='create_table',\n    postgres_conn_id='my_postgres',  # Connexion dÃ©finie dans UI\n    sql=\"\"\"\n        CREATE TABLE IF NOT EXISTS users (\n            id SERIAL PRIMARY KEY,\n            name VARCHAR(100)\n        );\n    \"\"\",\n)\n\n\nEmailOperator\nfrom airflow.operators.email import EmailOperator\n\ntask = EmailOperator(\n    task_id='send_report',\n    to='team@company.com',\n    subject='Pipeline Report - {{ ds }}',\n    html_content='&lt;h1&gt;Pipeline completed!&lt;/h1&gt;',\n)\n\n\nEmptyOperator (anciennement DummyOperator)\nfrom airflow.operators.empty import EmptyOperator\n\n# Utile pour crÃ©er des points de jonction\nstart = EmptyOperator(task_id='start')\nend = EmptyOperator(task_id='end')",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#xcom-passer-des-donnÃ©es-entre-tÃ¢ches",
    "title": "Orchestration de Pipelines Data",
    "section": "XCom â€” Passer des donnÃ©es entre tÃ¢ches",
    "text": "XCom â€” Passer des donnÃ©es entre tÃ¢ches\nXCom (Cross-Communication) permet de partager des donnÃ©es entre tÃ¢ches.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         XCom          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Task A  â”‚ â”€â”€â”€â”€â”€â”€â”€ data â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Task B  â”‚\nâ”‚  return  â”‚                       â”‚ xcom_pull â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nMÃ©thode 1 : Return (automatique)\n\n\nVoir le code\n# XCom avec return (automatique)\n\ndef extract():\n    data = {'records': 1000, 'status': 'ok'}\n    return data  # Automatiquement stockÃ© dans XCom\n\ndef transform(**context):\n    # RÃ©cupÃ©rer via ti (task instance)\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract')\n    print(f\"ReÃ§u: {data}\")\n    return {'records': data['records'], 'transformed': True}\n\n\n\n\nMÃ©thode 2 : Push/Pull explicite\ndef task_a(**context):\n    # Push explicite avec une clÃ©\n    context['ti'].xcom_push(key='my_data', value={'count': 42})\n    context['ti'].xcom_push(key='status', value='success')\n\ndef task_b(**context):\n    # Pull avec la clÃ©\n    data = context['ti'].xcom_pull(task_ids='task_a', key='my_data')\n    status = context['ti'].xcom_pull(task_ids='task_a', key='status')\n\n\nâš ï¸ Limites de XCom\n\n\n\nLimite\nDescription\n\n\n\n\nTaille\nMax ~48KB par dÃ©faut (stockÃ© en DB)\n\n\nSÃ©rialisation\nDoit Ãªtre JSON-sÃ©rialisable\n\n\nPas pour big data\nUtiliser S3/GCS pour gros fichiers\n\n\n\n# âŒ Mauvaise pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')  # 1GB\n    return df.to_dict()  # âŒ Trop gros pour XCom !\n\n# âœ… Bonne pratique\ndef extract():\n    df = pd.read_csv('big_file.csv')\n    path = '/data/output/extract_2024-01-15.parquet'\n    df.to_parquet(path)\n    return path  # âœ… Passer le chemin, pas les donnÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#taskflow-api-airflow-2.0",
    "title": "Orchestration de Pipelines Data",
    "section": "TaskFlow API (Airflow 2.0+)",
    "text": "TaskFlow API (Airflow 2.0+)\nSyntaxe moderne et plus Pythonic avec des dÃ©corateurs :\n\n\nVoir le code\n# TaskFlow API - Syntaxe moderne (Airflow 2.0+)\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='etl_taskflow',\n    schedule='@daily',\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    tags=['etl', 'taskflow'],\n)\ndef etl_pipeline():\n    \n    @task()\n    def extract():\n        \"\"\"Extraire les donnÃ©es\"\"\"\n        return {'records': 1000, 'source': 'api'}\n    \n    @task()\n    def transform(data: dict):\n        \"\"\"Transformer les donnÃ©es\"\"\"\n        return {\n            'records': data['records'],\n            'cleaned': True\n        }\n    \n    @task()\n    def load(data: dict):\n        \"\"\"Charger les donnÃ©es\"\"\"\n        print(f\"Loading {data['records']} records\")\n    \n    # DÃ©finir le flow - XCom automatique !\n    raw_data = extract()\n    clean_data = transform(raw_data)\n    load(clean_data)\n\n# Instancier le DAG\netl_pipeline()\n\n\n\nAvantages TaskFlow API\n\n\n\nAvantage\nDescription\n\n\n\n\nXCom automatique\nLes retours sont passÃ©s automatiquement\n\n\nCode plus lisible\nRessemble Ã  du Python normal\n\n\nType hints\nSupport des annotations de type\n\n\nMoins de boilerplate\nPas besoin de crÃ©er des Operators",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#sensors-attendre-une-condition",
    "title": "Orchestration de Pipelines Data",
    "section": "Sensors â€” Attendre une condition",
    "text": "Sensors â€” Attendre une condition\nLes Sensors attendent quâ€™une condition soit remplie avant de continuer.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                                                              â”‚\nâ”‚   FileSensor          S3KeySensor         HttpSensor         â”‚\nâ”‚   \"Fichier existe?\"   \"Fichier sur S3?\"   \"API disponible?\"  â”‚\nâ”‚                                                              â”‚\nâ”‚        â³                   â³                   â³            â”‚\nâ”‚        â”‚                    â”‚                    â”‚           â”‚\nâ”‚        â–¼                    â–¼                    â–¼           â”‚\nâ”‚       âœ…                   âœ…                   âœ…           â”‚\nâ”‚   Continuer            Continuer            Continuer        â”‚\nâ”‚                                                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nFileSensor\n\n\nVoir le code\n# FileSensor - Attendre qu'un fichier existe\n\nfrom airflow.sensors.filesystem import FileSensor\n\nwait_for_file = FileSensor(\n    task_id='wait_for_file',\n    filepath='/data/input/daily_export.csv',\n    poke_interval=60,      # VÃ©rifier toutes les 60 secondes\n    timeout=3600,          # Timeout aprÃ¨s 1 heure\n    mode='poke',           # poke ou reschedule\n)\n\n# Le pipeline attend le fichier avant de continuer\nwait_for_file &gt;&gt; process_file\n\n\n\n\nAutres Sensors utiles\n# S3KeySensor - Attendre un fichier sur S3\nfrom airflow.providers.amazon.aws.sensors.s3 import S3KeySensor\n\nwait_s3 = S3KeySensor(\n    task_id='wait_for_s3',\n    bucket_name='my-bucket',\n    bucket_key='data/{{ ds }}/export.csv',\n    aws_conn_id='aws_default',\n)\n\n# HttpSensor - Attendre qu'une API rÃ©ponde\nfrom airflow.providers.http.sensors.http import HttpSensor\n\nwait_api = HttpSensor(\n    task_id='wait_for_api',\n    http_conn_id='api_connection',\n    endpoint='health',\n    response_check=lambda response: response.status_code == 200,\n)\n\n# ExternalTaskSensor - Attendre un autre DAG\nfrom airflow.sensors.external_task import ExternalTaskSensor\n\nwait_other_dag = ExternalTaskSensor(\n    task_id='wait_upstream',\n    external_dag_id='upstream_dag',\n    external_task_id='final_task',\n)\n\n\nMode poke vs reschedule\n\n\n\nMode\nDescription\nRessources\n\n\n\n\npoke\nGarde un worker slot\nConsomme plus\n\n\nreschedule\nLibÃ¨re le slot entre checks\nRecommandÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#branching-logique-conditionnelle",
    "title": "Orchestration de Pipelines Data",
    "section": "Branching â€” Logique conditionnelle",
    "text": "Branching â€” Logique conditionnelle\nExÃ©cuter diffÃ©rentes tÃ¢ches selon une condition.\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Check   â”‚\n                    â”‚ Conditionâ”‚\n                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n                         â”‚\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚          â”‚          â”‚\n              â–¼          â–¼          â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚ Path A â”‚ â”‚ Path B â”‚ â”‚ Path C â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Branching - ExÃ©cution conditionnelle\n\nfrom airflow.operators.python import BranchPythonOperator\nfrom airflow.operators.empty import EmptyOperator\n\ndef choose_branch(**context):\n    \"\"\"DÃ©cider quelle branche exÃ©cuter\"\"\"\n    # Exemple : vÃ©rifier le jour de la semaine\n    day = context['ds_nodash']  # Format YYYYMMDD\n    \n    # Logique mÃ©tier\n    if int(day) % 2 == 0:\n        return 'process_even'  # Retourner le task_id Ã  exÃ©cuter\n    else:\n        return 'process_odd'\n\nbranch = BranchPythonOperator(\n    task_id='branch',\n    python_callable=choose_branch,\n)\n\nprocess_even = EmptyOperator(task_id='process_even')\nprocess_odd = EmptyOperator(task_id='process_odd')\nend = EmptyOperator(task_id='end', trigger_rule='none_failed_min_one_success')\n\n# DÃ©finir le flow\nbranch &gt;&gt; [process_even, process_odd] &gt;&gt; end",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#trigger-rules",
    "title": "Orchestration de Pipelines Data",
    "section": "Trigger Rules",
    "text": "Trigger Rules\nContrÃ´ler quand une tÃ¢che sâ€™exÃ©cute en fonction du statut des tÃ¢ches parentes.\n\n\n\nTrigger Rule\nExÃ©cute siâ€¦\n\n\n\n\nall_success\nTous les parents ont rÃ©ussi (dÃ©faut)\n\n\nall_failed\nTous les parents ont Ã©chouÃ©\n\n\nall_done\nTous les parents sont terminÃ©s (peu importe le statut)\n\n\none_success\nAu moins un parent a rÃ©ussi\n\n\none_failed\nAu moins un parent a Ã©chouÃ©\n\n\nnone_failed\nAucun parent nâ€™a Ã©chouÃ© (succÃ¨s ou skipped)\n\n\nnone_skipped\nAucun parent nâ€™a Ã©tÃ© skipped\n\n\n\nfrom airflow.utils.trigger_rule import TriggerRule\n\n# TÃ¢che de notification en cas d'Ã©chec\nnotify_failure = EmailOperator(\n    task_id='notify_failure',\n    to='team@company.com',\n    subject='Pipeline Failed!',\n    html_content='...',\n    trigger_rule=TriggerRule.ONE_FAILED,  # ExÃ©cute si un parent Ã©choue\n)\n\n# TÃ¢che finale qui s'exÃ©cute toujours\ncleanup = BashOperator(\n    task_id='cleanup',\n    bash_command='rm -rf /tmp/data/*',\n    trigger_rule=TriggerRule.ALL_DONE,  # Toujours exÃ©cuter\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#connections-et-variables",
    "title": "Orchestration de Pipelines Data",
    "section": "Connections et Variables",
    "text": "Connections et Variables\n\nConnections\nStocker les informations de connexion aux systÃ¨mes externes.\nDans lâ€™UI : Admin â†’ Connections â†’ +\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Add Connection                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  Connection Id:   â”‚ my_postgres                 â”‚           â”‚\nâ”‚  Connection Type: â”‚ Postgres           â–¼        â”‚           â”‚\nâ”‚  Host:            â”‚ localhost                   â”‚           â”‚\nâ”‚  Schema:          â”‚ mydb                        â”‚           â”‚\nâ”‚  Login:           â”‚ admin                       â”‚           â”‚\nâ”‚  Password:        â”‚ â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢                    â”‚           â”‚\nâ”‚  Port:            â”‚ 5432                        â”‚           â”‚\nâ”‚                                                             â”‚\nâ”‚                              [ Save ]                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nUtilisation dans le code :\nfrom airflow.hooks.postgres_hook import PostgresHook\n\ndef query_postgres():\n    hook = PostgresHook(postgres_conn_id='my_postgres')\n    df = hook.get_pandas_df('SELECT * FROM users')\n    return df\n\n\nVariables\nStocker des configurations rÃ©utilisables.\nDans lâ€™UI : Admin â†’ Variables â†’ +\nfrom airflow.models import Variable\n\n# RÃ©cupÃ©rer une variable\napi_key = Variable.get('API_KEY')\n\n# Variable JSON\nconfig = Variable.get('pipeline_config', deserialize_json=True)\n# config = {'batch_size': 1000, 'env': 'prod'}\n\n# Dans un template Jinja\n# {{ var.value.API_KEY }}\n# {{ var.json.pipeline_config.batch_size }}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#monitoring-et-alertes",
    "title": "Orchestration de Pipelines Data",
    "section": "Monitoring et Alertes",
    "text": "Monitoring et Alertes\n\nInterface Web\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Airflow - DAGs                                                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                             â”‚\nâ”‚  DAG                    Schedule    Owner    Runs   Recent Tasks            â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€    â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚  â–¶ etl_pipeline         @daily      team     125    âœ…âœ…âœ…âœ…âœ…                â”‚\nâ”‚  â–¶ data_quality_check   @hourly     team     560    âœ…âœ…âœ…âŒâœ…                â”‚\nâ”‚  â–¶ weekly_report        @weekly     team     52     âœ…âœ…âœ…âœ…âœ…                â”‚\nâ”‚  â¸ maintenance          None        admin    3      âœ…âœ…âœ…                   â”‚\nâ”‚                                                                             â”‚\nâ”‚  âœ… Success  âŒ Failed  â³ Running  â¸ Paused                                 â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVues disponibles\n\n\n\nVue\nDescription\n\n\n\n\nGrid\nVue matricielle des runs\n\n\nGraph\nGraphe du DAG\n\n\nCalendar\nHistorique par date\n\n\nGantt\nTimeline dâ€™exÃ©cution\n\n\nCode\nCode source du DAG\n\n\n\n\n\nConfigurer les alertes email\n# airflow.cfg\n[smtp]\nsmtp_host = smtp.gmail.com\nsmtp_port = 587\nsmtp_user = airflow@company.com\nsmtp_password = your_password\nsmtp_mail_from = airflow@company.com\n\n# Dans le DAG\ndefault_args = {\n    'email': ['team@company.com'],\n    'email_on_failure': True,\n    'email_on_retry': False,\n}\n\n\nAlertes Slack\nfrom airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator\n\ndef alert_slack_on_failure(context):\n    \"\"\"Callback en cas d'Ã©chec\"\"\"\n    slack_msg = f\"\"\"\n        :red_circle: Task Failed!\n        *DAG*: {context['dag'].dag_id}\n        *Task*: {context['task'].task_id}\n        *Execution Time*: {context['execution_date']}\n    \"\"\"\n    return SlackWebhookOperator(\n        task_id='slack_alert',\n        slack_webhook_conn_id='slack_webhook',\n        message=slack_msg,\n    ).execute(context)\n\n# Utiliser le callback\ndefault_args = {\n    'on_failure_callback': alert_slack_on_failure,\n}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#bonnes-pratiques-airflow",
    "title": "Orchestration de Pipelines Data",
    "section": "Bonnes pratiques Airflow",
    "text": "Bonnes pratiques Airflow\n\n1. Structure des DAGs\nairflow/\nâ”œâ”€â”€ dags/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ etl/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ daily_etl.py\nâ”‚   â”‚   â””â”€â”€ weekly_report.py\nâ”‚   â”œâ”€â”€ utils/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ helpers.py\nâ”‚   â””â”€â”€ config/\nâ”‚       â””â”€â”€ settings.py\nâ”œâ”€â”€ plugins/\nâ”‚   â””â”€â”€ custom_operators/\nâ””â”€â”€ tests/\n    â””â”€â”€ test_dags.py\n\n\n2. Idempotence\n# âŒ Non idempotent - accumule des donnÃ©es\ndef bad_load():\n    db.execute(\"INSERT INTO table VALUES (...)\")\n\n# âœ… Idempotent - mÃªme rÃ©sultat si relancÃ©\ndef good_load():\n    db.execute(\"DELETE FROM table WHERE date = '{{ ds }}'\")\n    db.execute(\"INSERT INTO table SELECT ... WHERE date = '{{ ds }}'\")\n\n\n3. AtomicitÃ© des tÃ¢ches\n# âŒ TÃ¢che trop grosse\ndef do_everything():\n    extract()\n    transform()\n    load()\n\n# âœ… TÃ¢ches atomiques\nextract &gt;&gt; transform &gt;&gt; load\n\n\n4. Ne pas mettre de logique dans le DAG\n# âŒ Code exÃ©cutÃ© Ã  chaque parsing\ndata = fetch_from_api()  # AppelÃ© toutes les 30s !\n\n# âœ… Logique dans les tasks\n@task\ndef fetch_data():\n    return fetch_from_api()\n\n\n5. Tester les DAGs\n# tests/test_dags.py\nimport pytest\nfrom airflow.models import DagBag\n\ndef test_dag_loaded():\n    dag_bag = DagBag()\n    assert len(dag_bag.import_errors) == 0\n\ndef test_dag_structure():\n    dag_bag = DagBag()\n    dag = dag_bag.get_dag('etl_pipeline')\n    assert dag is not None\n    assert len(dag.tasks) == 5",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#forces-dairflow",
    "title": "Orchestration de Pipelines Data",
    "section": "âœ… Forces dâ€™Airflow",
    "text": "âœ… Forces dâ€™Airflow\nGestion des dÃ©pendances - A &gt;&gt; B = B attend A\nRetry automatique - RÃ©essaie en cas dâ€™Ã©chec\nInterface web - Visualisation complÃ¨te\nMonitoring - Logs centralisÃ©s\nAlertes - Email/Slack en cas dâ€™Ã©chec\nScalable - GÃ¨re 100+ pipelines\nExtensible - Custom operators, hooks\nCommunautÃ© - TrÃ¨s active, beaucoup de providers",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#faiblesses-dairflow",
    "title": "Orchestration de Pipelines Data",
    "section": "âŒ Faiblesses dâ€™Airflow",
    "text": "âŒ Faiblesses dâ€™Airflow\nComplexe - Courbe dâ€™apprentissage\nRessources - Besoin de 4-8 GB RAM\nOverkill - Pour 1-3 scripts simples\nSetup - Installation et configuration\nPas pour le streaming - Batch only (utiliser Kafka)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quand-utiliser-airflow",
    "title": "Orchestration de Pipelines Data",
    "section": "Quand utiliser Airflow ?",
    "text": "Quand utiliser Airflow ?\nOUI : 10+ pipelines, dÃ©pendances complexes, production\nNON : 1-5 scripts simples sans dÃ©pendances",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#quel-outil-choisir",
    "title": "Orchestration de Pipelines Data",
    "section": "Quel outil choisir ?",
    "text": "Quel outil choisir ?\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nJâ€™ai 1-3 scripts sur Windows\nğŸªŸ Task Scheduler\n\n\nJâ€™ai 1-3 scripts sur Linux\nğŸ§ Crontab\n\n\nJâ€™ai 5-10 scripts indÃ©pendants\nğŸ§ Crontab\n\n\nJâ€™ai 10+ scripts avec dÃ©pendances\nğŸŒ¬ï¸ Airflow\n\n\nScript B doit attendre script A\nğŸŒ¬ï¸ Airflow\n\n\nJe veux un dashboard\nğŸŒ¬ï¸ Airflow\n\n\nJe dÃ©bute en automatisation\nğŸªŸ Task Scheduler\n\n\nProduction critique\nğŸŒ¬ï¸ Airflow",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#progression-naturelle",
    "title": "Orchestration de Pipelines Data",
    "section": "Progression naturelle",
    "text": "Progression naturelle\n1. DÃ©butez avec Task Scheduler ou Cron\n2. Quand vous avez 5+ scripts â†’ Pensez Ã  migrer\n3. Quand vous avez des dÃ©pendances â†’ Migrez vers Airflow",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#points-clÃ©s",
    "title": "Orchestration de Pipelines Data",
    "section": "Points clÃ©s",
    "text": "Points clÃ©s\n\nWindows Task Scheduler\n\nPour qui : DÃ©butants sur Windows\nForce : TrÃ¨s facile (GUI)\nFaiblesse : Pas de dÃ©pendances\nLimite : 5 scripts max\n\n\n\nCrontab\n\nPour qui : Utilisateurs Linux\nForce : Universel, lÃ©ger\nFaiblesse : Pas de monitoring\nLimite : 15 scripts max\n\n\n\nAirflow\n\nPour qui : Production, Ã©quipes data\nForce : DÃ©pendances, monitoring, scalable\nFaiblesse : Complexe, ressources\nLimite : Aucune (scalable)\n\n\n\nConcepts Airflow Ã  retenir\n\n\n\nConcept\nDescription\n\n\n\n\nDAG\nGraphe de tÃ¢ches (workflow)\n\n\nTask\nUnitÃ© de travail\n\n\nOperator\nType de tÃ¢che (Python, Bash, SQLâ€¦)\n\n\nXCom\nPassage de donnÃ©es entre tÃ¢ches\n\n\nSensor\nAttendre une condition\n\n\nConnection\nCredentials stockÃ©s\n\n\nVariable\nConfiguration stockÃ©e",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources",
    "title": "Orchestration de Pipelines Data",
    "section": "Ressources",
    "text": "Ressources\n\nCrontab : crontab.guru (tester expressions)\nAirflow : airflow.apache.org\nAirflow Tutorial : Documentation officielle",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#votre-score",
    "title": "Orchestration de Pipelines Data",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert orchestration !\n8-9/10 : ğŸŒŸ TrÃ¨s bien !\n6-7/10 : ğŸ’ª Bon dÃ©but !\n&lt; 6/10 : ğŸ“š Relisez le notebook",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#ressources-1",
    "title": "Orchestration de Pipelines Data",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nOutils\n\nCrontab Guru â€” Tester et gÃ©nÃ©rer des expressions cron\nApache Airflow â€” Documentation officielle\nAirflow Tutorial\nAstronomer â€” Guides et bonnes pratiques\n\n\n\nAlternatives Ã  Airflow\n\n\n\n\n\n\n\n\nOutil\nDescription\nCas dâ€™usage\n\n\n\n\nPrefect\nOrchestration moderne, Pythonic\nAlternative plus simple Ã  Airflow\n\n\nDagster\nData orchestration avec types\nPipelines ML\n\n\nLuigi\nPar Spotify, simple\nPipelines batch\n\n\nMage\nLow-code, moderne\nPrototypage rapide\n\n\nKestra\nEvent-driven, YAML\nWorkflows dÃ©claratifs\n\n\n\n\n\nCloud managed\n\n\n\nCloud\nService\n\n\n\n\nAWS\nMWAA (Managed Airflow), Step Functions\n\n\nGCP\nCloud Composer (Managed Airflow)\n\n\nAzure\nData Factory, Synapse Pipelines",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#fin-du-niveau-dÃ©butant",
    "title": "Orchestration de Pipelines Data",
    "section": "ğŸ‰ Fin du niveau dÃ©butant !",
    "text": "ğŸ‰ Fin du niveau dÃ©butant !\nTu as terminÃ© le parcours Data Engineering - From Zero to Hero niveau dÃ©butant ! ğŸ‰\n\nRÃ©capitulatif des modules\n\n\n\n#\nModule\nCompÃ©tence acquise\n\n\n\n\n01\nIntroduction\nVision du mÃ©tier\n\n\n02\nBash\nLigne de commande\n\n\n03\nGit\nVersioning\n\n\n04\nPython Basics\nProgrammation\n\n\n05\nPython Data Processing\nPandas, visualisation\n\n\n06\nIntro Bases Relationnelles\nConcepts relationnels\n\n\n07\nSQL\nRequÃªtes SQL\n\n\n08\nBig Data & NoSQL\nSystÃ¨mes distribuÃ©s\n\n\n09\nMongoDB\nBase NoSQL document\n\n\n10\nElasticsearch\nRecherche et indexation\n\n\n11\nPySpark\nTraitement distribuÃ©\n\n\n12\nOrchestration\nAirflow, pipelines\n\n\n\n\n\nModule BONUS disponible\n\n\n\n\n\n\n\n\n#\nModule\nDescription\n\n\n\n\n13\nBONUS FastAPI\nCrÃ©er des APIs REST pour exposer tes donnÃ©es\n\n\n\nğŸ‘‰ Parfait pour exposer les rÃ©sultats de tes pipelines via API !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "href": "notebooks/beginner/12_orchestration_pipelines.html#prochaine-Ã©tape-niveau-intermÃ©diaire",
    "title": "Orchestration de Pipelines Data",
    "section": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire",
    "text": "â¡ï¸ Prochaine Ã©tape : Niveau IntermÃ©diaire\nTu es maintenant prÃªt pour le niveau intermÃ©diaire qui couvrira :\n\n\n\nModule\nDescription\n\n\n\n\nDocker\nConteneurisation des pipelines\n\n\nData Lakes\nParquet, Delta Lake, Iceberg\n\n\nKafka\nStreaming en temps rÃ©el\n\n\ndbt\nTransformation dans le warehouse\n\n\nData Quality\nGreat Expectations, tests\n\n\nCloud\nAWS / GCP / Azure\n\n\nCI/CD\nGitHub Actions, tests automatisÃ©s\n\n\nKibana\nDashboards et monitoring\n\n\nProjet intÃ©grateur\nPipeline complet end-to-end\n\n\n\n\nğŸš€ Bravo ! Tu as maintenant toutes les bases pour construire des pipelines de donnÃ©es !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "âš¡ Spark & Orchestration",
      "12 Â· Orchestration de Pipelines"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html",
    "href": "notebooks/beginner/01_intro_data_engineering.html",
    "title": "Introduction au Data Engineering",
    "section": "",
    "text": "Bienvenue dans cette premiÃ¨re leÃ§on du parcours Data Engineering â€” From Zero to Hero.\nDans ce notebook, nous allons dÃ©couvrir :",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prÃ©requis",
    "title": "Introduction au Data Engineering",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\nAvant de commencer ce parcours, il est recommandÃ© dâ€™avoir :\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Basique\nSavoir utiliser un ordinateur et naviguer dans des fichiers\n\n\nâœ… Basique\nConnaÃ®tre les concepts de base dâ€™internet\n\n\n\n\nğŸ’¡ Pas de panique ! Ce parcours est conÃ§u pour les dÃ©butants. Nous couvrirons tout ce dont tu as besoin.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "href": "notebooks/beginner/01_intro_data_engineering.html#cest-quoi-le-data-engineering",
    "title": "Introduction au Data Engineering",
    "section": "1. Câ€™est quoi le Data Engineering ?",
    "text": "1. Câ€™est quoi le Data Engineering ?\nLe Data Engineering (ou ingÃ©nierie des donnÃ©es) est une discipline qui consiste Ã  concevoir, construire, maintenir et optimiser les systÃ¨mes de traitement de donnÃ©es.\nLe Data Engineer, spÃ©cialiste en gestion des donnÃ©es, conÃ§oit et maintient lâ€™infrastructure data (bases de donnÃ©es, entrepÃ´ts de donnÃ©es, lacs de donnÃ©es) et dÃ©veloppe des pipelines automatisÃ©s qui extraient, transforment et chargent les donnÃ©es dans des systÃ¨mes adaptÃ©s.\nCâ€™est le socle technique qui garantit la qualitÃ©, la disponibilitÃ© et la sÃ©curitÃ© des donnÃ©es utilisÃ©es par les Data Analysts et Data Scientists pour gÃ©nÃ©rer des insights et orienter les stratÃ©gies dâ€™entreprise.\n\nExemples concrets en entreprise\n\n\n\n\n\n\n\nEntreprise\nCas dâ€™usage Data Engineering\n\n\n\n\nNetflix\nPipeline qui collecte les donnÃ©es de visionnage de millions dâ€™utilisateurs pour alimenter le systÃ¨me de recommandation\n\n\nUber\nTraitement en temps rÃ©el des donnÃ©es GPS de milliers de chauffeurs pour optimiser les trajets et les prix\n\n\nSpotify\nAgrÃ©gation des donnÃ©es dâ€™Ã©coute pour gÃ©nÃ©rer les playlists â€œDiscover Weeklyâ€ personnalisÃ©es\n\n\nAirbnb\nPipeline de donnÃ©es pour analyser les prix du marchÃ© et suggÃ©rer des tarifs aux hÃ´tes\n\n\nE-commerce\nSynchronisation des stocks entre le site web, lâ€™ERP et les entrepÃ´ts en temps rÃ©el",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "href": "notebooks/beginner/01_intro_data_engineering.html#diffÃ©rences-entre-mÃ©tiers",
    "title": "Introduction au Data Engineering",
    "section": "2. DiffÃ©rences entre mÃ©tiers",
    "text": "2. DiffÃ©rences entre mÃ©tiers\n\n\n\n\n\n\n\n\n\nMÃ©tier\nRÃ´le Principal\nFocus\nOutils ClÃ©s\n\n\n\n\nData Engineer\nConstruire et maintenir lâ€™infrastructure de donnÃ©es\nInfrastructure & Pipelines\nApache Airflow, Apache Spark, Kafka, Snowflake, dbt, Python, SQL, Prefect, Docker, Kubernetes, etcâ€¦\n\n\nData Scientist\nExtraire des insights et crÃ©er des modÃ¨les prÃ©dictifs\nModÃ©lisation & ML\nPython, R, scikit-learn, TensorFlow, PyTorch, Jupyter, MLflow, Pandas, XGBoost, etcâ€¦\n\n\nData Analyst\nTransformer les donnÃ©es en insights actionnables\nBusiness Intelligence\nSQL, Excel, Power BI, Tableau, Looker, Google Analytics, Python (basique)\n\n\n\n\nComment ces rÃ´les collaborent ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Data Engineer  â”‚ â”€â”€â–¶ â”‚  Data Scientist â”‚ â”€â”€â–¶ â”‚  Data Analyst   â”‚\nâ”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚\nâ”‚ â€¢ Collecte      â”‚     â”‚ â€¢ ModÃ©lisation  â”‚     â”‚ â€¢ Visualisation â”‚\nâ”‚ â€¢ Pipelines     â”‚     â”‚ â€¢ PrÃ©dictions   â”‚     â”‚ â€¢ Reporting     â”‚\nâ”‚ â€¢ Infrastructureâ”‚     â”‚ â€¢ ML/AI         â”‚     â”‚ â€¢ Insights      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                                               â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Feedback & Besoins â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "href": "notebooks/beginner/01_intro_data_engineering.html#architecture-typique-dun-pipeline-de-donnÃ©es-moderne",
    "title": "Introduction au Data Engineering",
    "section": "3. Architecture typique dâ€™un pipeline de donnÃ©es moderne",
    "text": "3. Architecture typique dâ€™un pipeline de donnÃ©es moderne\nUn pipeline de donnÃ©es moderne suit gÃ©nÃ©ralement ce flux :\n                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                 â”‚                ğŸ›¡ï¸  GOUVERNANCE DATA                    â”‚\n                 â”‚ Catalogue, QualitÃ©, Lineage, SÃ©curitÃ©, RBAC, Privacy   â”‚\n                 â”‚ Compliance, MDM                                        â”‚\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   ğŸ“¥ SOURCES      â”‚   â†’     â”‚               ğŸ’¾ STOCKAGE                 â”‚\nâ”‚ API, DB, Logs,    â”‚         â”‚         (Data Lakehouse : S3, ADLSâ€¦)     â”‚\nâ”‚ CSV, IoT          â”‚         â”‚                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚   BRONZE : DonnÃ©es brutes (Raw)           â”‚\n                             â”‚   - IngÃ©rÃ©es telles quelles                â”‚\n                             â”‚   - Formats variÃ©s (JSON, CSV, Parquet)    â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   - Pas de qualitÃ© garantie                â”‚\n   â”‚   ğŸ”„ INGESTION        â”‚  â”‚                                           â”‚\n   â”‚ ETL/ELT, Streams     â”‚  â”‚   SILVER : DonnÃ©es nettoyÃ©es (Clean)       â”‚\n   â”‚ Airbyte, Fivetran    â”‚  â”‚   - NormalisÃ©es, typÃ©es, dÃ©dupliquÃ©es      â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   - QualitÃ© contrÃ´lÃ©e                      â”‚\n                             â”‚   - Jointures simples / enrichissement     â”‚\n                             â”‚                                           â”‚\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   GOLD : DonnÃ©es business (Curated)        â”‚\n   â”‚ âš™ï¸ TRANSFORMATION      â”‚  â”‚   - ModÃ¨les analytiques (Star Schema)      â”‚\n   â”‚ dbt, SQL, Spark,      â”‚  â”‚   - KPIs, mÃ©triques, tables prÃªtes BI     â”‚\n   â”‚ Pandas, MLflow        â”‚  â”‚   - Haute qualitÃ© et gouvernance forte     â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                              â”‚        ğŸ“Š EXPOSITION         â”‚\n                              â”‚ Dashboards, ML, APIs, Apps   â”‚\n                              â”‚ Power BI, Tableau, Looker    â”‚\n                              â”‚ (âš ï¸ Toujours Ã  partir du GOLD)â”‚\n                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nETL vs ELT â€” Quelle est la diffÃ©rence ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nETL (Extract â†’ Transform â†’ Load)\nELT (Extract â†’ Load â†’ Transform)\n\n\n\n\nğŸ”„ Ordre\nExtraction â†’ Transformation â†’ Chargement\nExtraction â†’ Chargement â†’ Transformation\n\n\nğŸ“ Lieu de la transformation\nEn dehors du stockage (dans un script ou un outil)\nDirectement dans le data warehouse\n\n\nâœ… Avantages\nPlus de contrÃ´le sur la transformation\nPlus rapide sur des gros volumes\n\n\nâš ï¸ InconvÃ©nients\nPeut surcharger les outils intermÃ©diaires\nBesoin dâ€™un entrepÃ´t puissant (coÃ»ts)\n\n\nğŸ› ï¸ Outils typiques\nInformatica, Talend, scripts Python, â€¦\ndbt, Snowflake, BigQuery, â€¦\n\n\nğŸ“… Cas dâ€™usage\nDonnÃ©es sensibles nÃ©cessitant un prÃ©-traitement\nAnalytics modernes sur le cloud",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "href": "notebooks/beginner/01_intro_data_engineering.html#pipelines-batch-vs-streaming",
    "title": "Introduction au Data Engineering",
    "section": "4. Pipelines batch vs streaming",
    "text": "4. Pipelines batch vs streaming\n\n\n\n\n\n\n\n\n\nMode\nDÃ©finition\nLatence\nExemples\n\n\n\n\nBatch\nTraitement pÃ©riodique (toutes les heures, tous les joursâ€¦)\nMinutes Ã  heures\nRapport quotidien, import CSV, agrÃ©gations nocturnes\n\n\nStreaming\nTraitement en temps rÃ©el, Ã©vÃ©nement par Ã©vÃ©nement\nMillisecondes Ã  secondes\nLogs serveurs, capteurs IoT, transactions bancaires, dÃ©tection de fraude\n\n\n\n\nComment choisir ?\n\n\n\n\n\n\n\nQuestion\nSi oui â†’\n\n\n\n\nLes donnÃ©es doivent-elles Ãªtre traitÃ©es immÃ©diatement ?\nStreaming\n\n\nLe volume est-il trÃ¨s Ã©levÃ© mais la latence peu critique ?\nBatch\n\n\nAvez-vous besoin de dÃ©tecter des anomalies en temps rÃ©el ?\nStreaming\n\n\nSâ€™agit-il de rapports quotidiens/hebdomadaires ?\nBatch\n\n\n\n\n\nFondations des pipelines de donnÃ©es\nTout pipeline de donnÃ©es repose sur plusieurs piliers fondamentaux :\n\n\n\n\n\n\n\n\nPilier\nDescription\nModule associÃ©\n\n\n\n\n1. Data Collecting\nComment collecter les donnÃ©es brutes (fichiers, API, capteursâ€¦)\nPython, APIs\n\n\n2. Data Ingestion\nComment les intÃ©grer dans un systÃ¨me (DB, data lakeâ€¦)\nETL, Airbyte\n\n\n3. Data Storage\nComment et oÃ¹ les stocker (SQL, NoSQL, S3â€¦)\nDatabases, Cloud\n\n\n4. Data Processing\nComment les transformer, nettoyer, agrÃ©ger\nPython, Spark, dbt\n\n\n5. Data Modeling\nComment organiser les donnÃ©es pour lâ€™analyse\nSQL, dbt\n\n\n6. Data Quality & Governance\nComment garantir la fiabilitÃ© et la traÃ§abilitÃ©\nGreat Expectations\n\n\n7. Data Orchestration\nComment automatiser les tÃ¢ches et gÃ©rer les dÃ©pendances\nAirflow, Prefect\n\n\n8. ScalabilitÃ© & Performance\nComment faire face Ã  de gros volumes ou Ã  la charge\nSpark, Kubernetes\n\n\n9. SÃ©curitÃ© des donnÃ©es\nChiffrement, contrÃ´le dâ€™accÃ¨s, audit\nIAM, Vault\n\n\n10. DevOps pour la Data\nConteneurisation, CI/CD, monitoring\nDocker, GitHub Actions\n\n\n\n\nğŸ“˜ Ces concepts seront abordÃ©s progressivement dans le parcours.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#panorama-des-outils-du-data-engineer",
    "title": "Introduction au Data Engineering",
    "section": "5. Panorama des outils du Data Engineer",
    "text": "5. Panorama des outils du Data Engineer\n\n\n\n\n\n\n\n\nDomaine\nOutils populaires\nNiveau\n\n\n\n\nğŸ“¥ Collecte\nPython, API REST, Scrapy, Kafka\nDÃ©butant â†’ IntermÃ©diaire\n\n\nğŸ”„ Ingestion\nAirbyte, Fivetran, Python scripts\nDÃ©butant\n\n\nğŸ’¾ Stockage\nPostgreSQL, Snowflake, S3, Delta Lake\nDÃ©butant â†’ AvancÃ©\n\n\nâš™ï¸ Traitement (Batch)\nPandas, Spark, dbt, SQL\nDÃ©butant â†’ AvancÃ©\n\n\nâš¡ Traitement (Streaming)\nKafka, Spark Streaming, Flink\nIntermÃ©diaire â†’ AvancÃ©\n\n\nğŸ¼ Orchestration\nApache Airflow, Prefect, Dagster\nIntermÃ©diaire\n\n\nğŸ³ DevOps & CI/CD\nDocker, GitHub Actions, Terraform\nIntermÃ©diaire\n\n\nğŸ“Š Monitoring\nGrafana, Prometheus, ELK Stack\nIntermÃ©diaire\n\n\n\n\nStack moderne typique (2024)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    STACK DATA MODERNE                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Orchestration    â”‚  Airflow / Prefect / Dagster            â”‚\nâ”‚  Transformation   â”‚  dbt / Spark / Python                   â”‚\nâ”‚  Warehouse        â”‚  Snowflake / BigQuery / Redshift        â”‚\nâ”‚  Ingestion        â”‚  Airbyte / Fivetran / Stitch            â”‚\nâ”‚  Sources          â”‚  APIs / Databases / SaaS / Files        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "href": "notebooks/beginner/01_intro_data_engineering.html#soft-skills-mindset-du-data-engineer",
    "title": "Introduction au Data Engineering",
    "section": "6. Soft Skills & Mindset du Data Engineer",
    "text": "6. Soft Skills & Mindset du Data Engineer\nLe mÃ©tier de Data Engineer nâ€™est pas uniquement technique. Pour rÃ©ussir dans ce domaine, il faut aussi dÃ©velopper des compÃ©tences humaines et professionnelles essentielles :\n\n\n\n\n\n\n\n\nCompÃ©tence\nDescription\nPourquoi câ€™est important\n\n\n\n\nğŸ“ Documenter\nÃ‰crire une documentation claire pour son code et ses pipelines\nFacilite la maintenance et lâ€™onboarding des nouveaux membres\n\n\nğŸ¤ Collaborer\nTravailler avec les Ã©quipes Data Science, BI, Produit, DevOps\nLes donnÃ©es traversent toute lâ€™organisation\n\n\nğŸ¯ ÃŠtre rigoureux\nGarantir la qualitÃ©, la fiabilitÃ© et la traÃ§abilitÃ© des donnÃ©es\nUne erreur de donnÃ©es peut avoir des consÃ©quences business majeures\n\n\nğŸ•µğŸ½â€â™‚ï¸ Investiguer\nSavoir dÃ©bugger des anomalies, logs ou Ã©checs de pipeline\nLes pipelines cassent, il faut savoir pourquoi rapidement\n\n\nğŸ“š Apprendre en continu\nSe tenir Ã  jour sur les nouveaux outils et pratiques\nLe domaine Ã©volue trÃ¨s rapidement\n\n\nğŸ’¬ Communiquer\nExpliquer des concepts techniques Ã  des non-techniques\nAlignement avec les Ã©quipes mÃ©tier",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/01_intro_data_engineering.html#quiz-de-fin-de-module",
    "title": "Introduction au Data Engineering",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour valider tes acquis ğŸ‘‡ğŸ½\n\n\nâ“ Q1. Quel est le rÃ´le principal dâ€™un Data Engineer ?\n\nCrÃ©er des modÃ¨les prÃ©dictifs\n\nVisualiser les donnÃ©es dans Power BI\n\nConcevoir et maintenir des pipelines de donnÃ©es\n\nFaire des analyses statistiques dans Excel\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le Data Engineer conÃ§oit et maintient des pipelines de donnÃ©es.\n\n\n\n\nâ“ Q2. Dans un pipeline ETL, que signifie le â€œTâ€ ?\n\nTransfer\n\nTrigger\n\nTransform\n\nTransport\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” â€œTâ€ signifie Transform, câ€™est lâ€™Ã©tape de transformation des donnÃ©es.\n\n\n\n\nâ“ Q3. Quelle est la principale diffÃ©rence entre ETL et ELT ?\n\nELT ne fait pas de transformation\n\nELT transforme les donnÃ©es aprÃ¨s les avoir chargÃ©es\n\nETL est utilisÃ© uniquement pour les fichiers CSV\n\nELT est un outil comme Apache Airflow\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ELT charge dâ€™abord les donnÃ©es, puis les transforme dans le data warehouse.\n\n\n\n\nâ“ Q4. Lequel de ces outils est utilisÃ© pour orchestrer des pipelines ?\n\nApache Kafka\n\nApache Airflow\n\nTableau\n\nPostgreSQL\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Apache Airflow est un outil dâ€™orchestration de pipelines.\n\n\n\n\nâ“ Q5. Le traitement batch consiste Ã  :\n\nTraiter les donnÃ©es en continu\n\nTraiter les donnÃ©es en petits lots Ã  la volÃ©e\n\nTraiter les donnÃ©es par groupe, Ã  intervalle rÃ©gulier\n\nTraiter uniquement les donnÃ©es texte\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le batch consiste Ã  traiter les donnÃ©es par lots Ã  intervalles dÃ©finis.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "href": "notebooks/beginner/01_intro_data_engineering.html#bonus-quiz-nouveaux-paradigmes-etlt-reverse-etl",
    "title": "Introduction au Data Engineering",
    "section": "Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL",
    "text": "Bonus Quiz â€” Nouveaux paradigmes : ETLt & Reverse ETL\n\n\nâ“ Q6. Que signifie ETLt ?\n\nUne erreur dans la chaÃ®ne ETL\n\nUne combinaison hybride entre ETL et ELT\n\nUne technique de transfert via email\n\nUne transformation uniquement aprÃ¨s chargement\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ETLt correspond Ã  : Extract â†’ Transform (1) â†’ Load â†’ Transform (2).\nCâ€™est une approche hybride oÃ¹ une partie des transformations est faite avant le chargement, et une autre aprÃ¨s.\n\n\n\n\nâ“ Q7. Le Reverse ETL est utilisÃ© pour :\n\nRecharger les donnÃ©es sources depuis le warehouse\n\nSupprimer les donnÃ©es invalides dans un lac de donnÃ©es\n\nPousser les donnÃ©es du data warehouse vers les outils mÃ©tiers\n\nTransformer les donnÃ©es en reverse-engineering\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Reverse ETL consiste Ã  extraire les donnÃ©es dâ€™un data warehouse (ex. BigQuery) pour les charger dans des outils mÃ©tiers comme Salesforce, Notion, Slack, etc.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/01_intro_data_engineering.html#ressources-pour-aller-plus-loin",
    "title": "Introduction au Data Engineering",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Lectures recommandÃ©es\n\nFundamentals of Data Engineering â€” Joe Reis & Matt Housley\nThe Data Warehouse Toolkit â€” Ralph Kimball\nDesigning Data-Intensive Applications â€” Martin Kleppmann\n\n\n\nğŸŒ Sites & Blogs\n\nData Engineering Weekly â€” Newsletter hebdomadaire\nSeattle Data Guy â€” ChaÃ®ne YouTube\nStart Data Engineering â€” Tutoriels pratiques\n\n\n\nğŸ“ Certifications\n\nGoogle Cloud Professional Data Engineer\nAWS Certified Data Engineer\nDatabricks Certified Data Engineer",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/01_intro_data_engineering.html#prochaine-Ã©tape",
    "title": "Introduction au Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu as une vue dâ€™ensemble du Data Engineering, passons Ã  la pratique !\nğŸ‘‰ Module suivant : 02_bash_for_data_engineers â€” MaÃ®triser la ligne de commande\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le premier module du parcours Data Engineering.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "01 Â· Introduction au Data Engineering"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "",
    "text": "Ce module bonus couvre FastAPI, le framework Python moderne pour crÃ©er des APIs REST performantes. En tant que Data Engineer, tu auras souvent besoin dâ€™exposer des donnÃ©es via des APIs.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#prÃ©requis",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 04_python_basics (fonctions, classes, type hints)\n\n\nâœ… Requis\nModule 05_python_data_processing (Pandas, JSON)\n\n\nâ­ Bonus\nModule 09_mongodb (pour les exemples avec DB)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#objectifs-du-module",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nCrÃ©er une API REST avec FastAPI\nDÃ©finir des modÃ¨les de donnÃ©es avec Pydantic\nCrÃ©er des endpoints CRUD (Create, Read, Update, Delete)\nValider automatiquement les donnÃ©es entrantes\nServir des donnÃ©es Pandas via API\nConnecter une API Ã  une base de donnÃ©es\nGÃ©nÃ©rer une documentation Swagger automatique\nDÃ©ployer une API avec Uvicorn\n\n\n\nğŸ’¡ Note : FastAPI sâ€™exÃ©cute comme un serveur. Certains exemples seront Ã  tester hors du notebook, dans des fichiers .py.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#pourquoi-fastapi-pour-un-data-engineer",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Pourquoi FastAPI pour un Data Engineer ?",
    "text": "Pourquoi FastAPI pour un Data Engineer ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              CAS D'USAGE DATA ENGINEERING                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚     Exposer des donnÃ©es traitÃ©es (datamart â†’ API)               â”‚\nâ”‚     CrÃ©er des webhooks pour dÃ©clencher des pipelines            â”‚\nâ”‚     Servir des prÃ©dictions ML en production                     â”‚\nâ”‚     Fournir des mÃ©triques pour les dashboards                   â”‚\nâ”‚     CrÃ©er des microservices data                                â”‚\nâ”‚     Valider des donnÃ©es avant ingestion                         â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nFastAPI vs autres frameworks\n\n\n\nFramework\nPerformance\nTyping\nDoc auto\nAsync\nComplexitÃ©\n\n\n\n\nFastAPI\nâ­â­â­\nâœ… Natif\nâœ… Swagger\nâœ…\nSimple\n\n\nFlask\nâ­â­\nâŒ Manuel\nâŒ Extension\nâŒ\nSimple\n\n\nDjango REST\nâ­â­\nâŒ Manuel\nâœ…\nâš ï¸\nComplexe\n\n\nExpress (Node)\nâ­â­â­\nâŒ\nâŒ\nâœ…\nSimple\n\n\n\n\nğŸ’¡ FastAPI combine le meilleur : performance, simplicitÃ©, et documentation automatique.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#installation",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#installation",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Installation",
    "text": "Installation\n# Installation de base\npip install fastapi uvicorn\n\n# Avec toutes les dÃ©pendances optionnelles\npip install \"fastapi[all]\"\n\n\n\nPackage\nRÃ´le\n\n\n\n\nfastapi\nLe framework\n\n\nuvicorn\nServeur ASGI (pour lancer lâ€™API)\n\n\npydantic\nValidation de donnÃ©es (inclus avec FastAPI)\n\n\n\n\n\nVoir le code\n# Installation\n!pip install fastapi uvicorn pydantic pandas --quiet\n\nprint(\"Packages installÃ©s !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#structure-minimale",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#structure-minimale",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Structure minimale",
    "text": "Structure minimale\nCrÃ©er un fichier main.py :\n# main.py\nfrom fastapi import FastAPI\n\n# CrÃ©er l'application\napp = FastAPI()\n\n# Premier endpoint\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Hello Data Engineer!\"}\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\"}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#lancer-le-serveur",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#lancer-le-serveur",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Lancer le serveur",
    "text": "Lancer le serveur\n# Dans le terminal\nuvicorn main:app --reload\n\n# main = fichier main.py\n# app = variable FastAPI()\n# --reload = redÃ©marre automatiquement si le code change\nRÃ©sultat :\nINFO:     Uvicorn running on http://127.0.0.1:8000\nINFO:     Started reloader process",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#tester-lapi",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#tester-lapi",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Tester lâ€™API",
    "text": "Tester lâ€™API\n\n\n\nURL\nRÃ©sultat\n\n\n\n\nhttp://127.0.0.1:8000\n{\"message\": \"Hello Data Engineer!\"}\n\n\nhttp://127.0.0.1:8000/health\n{\"status\": \"healthy\"}\n\n\nhttp://127.0.0.1:8000/docs\nDocumentation Swagger\n\n\nhttp://127.0.0.1:8000/redoc\nDocumentation ReDoc",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#les-mÃ©thodes-http",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Les mÃ©thodes HTTP",
    "text": "Les mÃ©thodes HTTP\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    MÃ‰THODES HTTP (CRUD)                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ MÃ©thode  â”‚ Action           â”‚ Exemple                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ GET      â”‚ Lire (Read)      â”‚ GET /users â†’ Liste des users      â”‚\nâ”‚ POST     â”‚ CrÃ©er (Create)   â”‚ POST /users â†’ CrÃ©er un user       â”‚\nâ”‚ PUT      â”‚ Remplacer        â”‚ PUT /users/1 â†’ Remplacer user 1   â”‚\nâ”‚ PATCH    â”‚ Modifier         â”‚ PATCH /users/1 â†’ Modifier user 1  â”‚\nâ”‚ DELETE   â”‚ Supprimer        â”‚ DELETE /users/1 â†’ Supprimer       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n@app.get(\"/items\")\ndef get_items():\n    return {\"action\": \"Liste des items\"}\n\n@app.post(\"/items\")\ndef create_item():\n    return {\"action\": \"CrÃ©ation d'un item\"}\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int):\n    return {\"action\": f\"Mise Ã  jour de l'item {item_id}\"}\n\n@app.delete(\"/items/{item_id}\")\ndef delete_item(item_id: int):\n    return {\"action\": f\"Suppression de l'item {item_id}\"}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#paramÃ¨tres-de-chemin-path-parameters",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "ParamÃ¨tres de chemin (Path Parameters)",
    "text": "ParamÃ¨tres de chemin (Path Parameters)\nLes paramÃ¨tres dans lâ€™URL permettent dâ€™identifier une ressource spÃ©cifique.\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n# ParamÃ¨tre simple\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):  # Typage automatique !\n    return {\"user_id\": user_id}\n\n# Plusieurs paramÃ¨tres\n@app.get(\"/users/{user_id}/orders/{order_id}\")\ndef get_user_order(user_id: int, order_id: int):\n    return {\n        \"user_id\": user_id,\n        \"order_id\": order_id\n    }\n\nValidation automatique\nGET /users/42        â†’ {\"user_id\": 42}       âœ…\nGET /users/abc       â†’ Erreur 422            âŒ (pas un int)\nGET /users/-1        â†’ {\"user_id\": -1}       âœ… (mais logiquement faux)\n\n\nValidation avancÃ©e avec Path\nfrom fastapi import FastAPI, Path\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(\n    user_id: int = Path(\n        ...,  # Requis\n        title=\"User ID\",\n        description=\"L'identifiant unique de l'utilisateur\",\n        ge=1,  # Greater or Equal (&gt;= 1)\n        le=10000  # Less or Equal (&lt;= 10000)\n    )\n):\n    return {\"user_id\": user_id}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#paramÃ¨tres-de-requÃªte-query-parameters",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "ParamÃ¨tres de requÃªte (Query Parameters)",
    "text": "ParamÃ¨tres de requÃªte (Query Parameters)\nLes paramÃ¨tres aprÃ¨s le ? dans lâ€™URL pour filtrer, paginer, etc.\nGET /users?skip=0&limit=10&active=true\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n              Query parameters\nfrom fastapi import FastAPI\nfrom typing import Optional\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(\n    skip: int = 0,           # DÃ©faut = 0\n    limit: int = 10,         # DÃ©faut = 10\n    active: bool = True,     # DÃ©faut = True\n    search: Optional[str] = None  # Optionnel\n):\n    return {\n        \"skip\": skip,\n        \"limit\": limit,\n        \"active\": active,\n        \"search\": search\n    }\n\nCas dâ€™usage Data Engineering : Pagination\n@app.get(\"/data\")\ndef get_data(\n    page: int = 1,\n    page_size: int = 100,\n    sort_by: str = \"created_at\",\n    order: str = \"desc\"\n):\n    # Calculer l'offset\n    offset = (page - 1) * page_size\n    \n    return {\n        \"page\": page,\n        \"page_size\": page_size,\n        \"offset\": offset,\n        \"sort_by\": sort_by,\n        \"order\": order\n    }",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#dÃ©finir-un-modÃ¨le",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "DÃ©finir un modÃ¨le",
    "text": "DÃ©finir un modÃ¨le\nfrom pydantic import BaseModel, Field, EmailStr\nfrom typing import Optional\nfrom datetime import datetime\n\nclass User(BaseModel):\n    \"\"\"ModÃ¨le utilisateur pour l'API.\"\"\"\n    \n    id: Optional[int] = None\n    nom: str = Field(..., min_length=2, max_length=50)\n    email: EmailStr\n    age: int = Field(..., ge=0, le=150)\n    actif: bool = True\n    created_at: datetime = Field(default_factory=datetime.now)\n    \n    class Config:\n        # Exemple pour la documentation\n        json_schema_extra = {\n            \"example\": {\n                \"nom\": \"Alice Dupont\",\n                \"email\": \"alice@example.com\",\n                \"age\": 30,\n                \"actif\": True\n            }\n        }\n\nValidateurs disponibles\n\n\n\nValidateur\nExemple\nDescription\n\n\n\n\nmin_length\nField(min_length=2)\nLongueur min string\n\n\nmax_length\nField(max_length=50)\nLongueur max string\n\n\nge\nField(ge=0)\nGreater or Equal\n\n\nle\nField(le=100)\nLess or Equal\n\n\ngt\nField(gt=0)\nGreater Than\n\n\nlt\nField(lt=100)\nLess Than\n\n\nregex\nField(regex='^[A-Z]')\nPattern regex\n\n\nEmailStr\nType spÃ©cial\nValide format email\n\n\n\n\n\nVoir le code\n# Exemple Pydantic dans le notebook\nfrom pydantic import BaseModel, Field, field_validator\nfrom typing import Optional\nfrom datetime import datetime\n\nclass Transaction(BaseModel):\n    \"\"\"ModÃ¨le de transaction pour pipeline ETL.\"\"\"\n    \n    id: Optional[int] = None\n    montant: float = Field(..., gt=0, description=\"Montant positif\")\n    devise: str = Field(..., min_length=3, max_length=3)\n    description: str = Field(..., min_length=1, max_length=200)\n    timestamp: datetime = Field(default_factory=datetime.now)\n    \n    @field_validator('devise')\n    @classmethod\n    def devise_uppercase(cls, v):\n        return v.upper()\n\n# Test de validation\ntry:\n    # Valide\n    tx1 = Transaction(montant=100.50, devise=\"eur\", description=\"Achat\")\n    print(f\" Transaction valide : {tx1}\")\n    print(f\" Devise convertie : {tx1.devise}\")\n    \n    # Invalide\n    tx2 = Transaction(montant=-50, devise=\"EUR\", description=\"Test\")\nexcept Exception as e:\n    print(f\"\\n Erreur de validation : {e}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#corps-de-requÃªte-request-body",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Corps de requÃªte (Request Body)",
    "text": "Corps de requÃªte (Request Body)\nPour les requÃªtes POST/PUT, les donnÃ©es sont envoyÃ©es dans le body (corps) en JSON.\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\nclass UserCreate(BaseModel):\n    nom: str\n    email: str\n    age: int\n\nclass UserResponse(BaseModel):\n    id: int\n    nom: str\n    email: str\n    age: int\n\n# Base de donnÃ©es fictive\nusers_db = []\nuser_id_counter = 1\n\n@app.post(\"/users\", response_model=UserResponse)\ndef create_user(user: UserCreate):\n    global user_id_counter\n    \n    # CrÃ©er le user avec un ID\n    new_user = {\n        \"id\": user_id_counter,\n        \"nom\": user.nom,\n        \"email\": user.email,\n        \"age\": user.age\n    }\n    \n    users_db.append(new_user)\n    user_id_counter += 1\n    \n    return new_user\n\nRequÃªte avec curl\ncurl -X POST \"http://127.0.0.1:8000/users\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}'\n\n\nRequÃªte avec Python requests\nimport requests\n\nresponse = requests.post(\n    \"http://127.0.0.1:8000/users\",\n    json={\"nom\": \"Alice\", \"email\": \"alice@test.com\", \"age\": 30}\n)\nprint(response.json())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#gestion-des-erreurs",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Gestion des erreurs",
    "text": "Gestion des erreurs\nFastAPI utilise HTTPException pour retourner des erreurs HTTP propres.\nfrom fastapi import FastAPI, HTTPException, status\n\napp = FastAPI()\n\n@app.get(\"/users/{user_id}\")\ndef get_user(user_id: int):\n    user = find_user_by_id(user_id)  # Fonction fictive\n    \n    if user is None:\n        raise HTTPException(\n            status_code=status.HTTP_404_NOT_FOUND,\n            detail=f\"User {user_id} non trouvÃ©\",\n            headers={\"X-Error\": \"User not found\"}\n        )\n    \n    return user\n\nCodes HTTP courants\n\n\n\nCode\nNom\nUsage\n\n\n\n\n200\nOK\nSuccÃ¨s (GET, PUT)\n\n\n201\nCreated\nCrÃ©ation rÃ©ussie (POST)\n\n\n204\nNo Content\nSuppression rÃ©ussie (DELETE)\n\n\n400\nBad Request\nRequÃªte invalide\n\n\n401\nUnauthorized\nNon authentifiÃ©\n\n\n403\nForbidden\nNon autorisÃ©\n\n\n404\nNot Found\nRessource introuvable\n\n\n422\nUnprocessable Entity\nErreur de validation\n\n\n500\nInternal Server Error\nErreur serveur",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#avec-sqlalchemy-postgresql-mysql-sqlite",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)",
    "text": "Avec SQLAlchemy (PostgreSQL, MySQL, SQLite)\n# database.py\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base\n\n# URL de connexion\nDATABASE_URL = \"postgresql://user:password@localhost:5432/mydb\"\n# Pour SQLite: \"sqlite:///./data.db\"\n\nengine = create_engine(DATABASE_URL)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\n\n# Dependency pour injecter la session\ndef get_db():\n    db = SessionLocal()\n    try:\n        yield db\n    finally:\n        db.close()\n# models.py\nfrom sqlalchemy import Column, Integer, String, Float, DateTime\nfrom database import Base\nfrom datetime import datetime\n\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(Integer, primary_key=True, index=True)\n    nom = Column(String(100), nullable=False)\n    email = Column(String(100), unique=True, index=True)\n    age = Column(Integer)\n    created_at = Column(DateTime, default=datetime.now)\n# main.py\nfrom fastapi import FastAPI, Depends\nfrom sqlalchemy.orm import Session\nfrom database import get_db\nimport models\n\napp = FastAPI()\n\n@app.get(\"/users\")\ndef get_users(db: Session = Depends(get_db), skip: int = 0, limit: int = 10):\n    users = db.query(models.User).offset(skip).limit(limit).all()\n    return users",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#avec-mongodb-pymongo",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Avec MongoDB (PyMongo)",
    "text": "Avec MongoDB (PyMongo)\n# mongodb_api.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel, Field\nfrom pymongo import MongoClient\nfrom bson import ObjectId\nfrom typing import Optional\n\napp = FastAPI(title=\"MongoDB API\")\n\n# Connexion MongoDB\nclient = MongoClient(\"mongodb://localhost:27017\")\ndb = client[\"data_engineering\"]\ncollection = db[\"pipelines\"]\n\n# Helper pour ObjectId\nclass PyObjectId(ObjectId):\n    @classmethod\n    def __get_validators__(cls):\n        yield cls.validate\n    \n    @classmethod\n    def validate(cls, v, field):\n        if not ObjectId.is_valid(v):\n            raise ValueError(\"Invalid ObjectId\")\n        return ObjectId(v)\n\n# ModÃ¨le\nclass Pipeline(BaseModel):\n    id: Optional[str] = Field(default=None, alias=\"_id\")\n    nom: str\n    description: str\n    schedule: str\n    \n    class Config:\n        populate_by_name = True\n\n# CRUD\n@app.get(\"/pipelines\")\ndef list_pipelines():\n    pipelines = []\n    for doc in collection.find():\n        doc[\"_id\"] = str(doc[\"_id\"])\n        pipelines.append(doc)\n    return pipelines\n\n@app.post(\"/pipelines\")\ndef create_pipeline(pipeline: Pipeline):\n    result = collection.insert_one(pipeline.model_dump(exclude={\"id\"}))\n    return {\"id\": str(result.inserted_id)}\n\n@app.get(\"/pipelines/{pipeline_id}\")\ndef get_pipeline(pipeline_id: str):\n    doc = collection.find_one({\"_id\": ObjectId(pipeline_id)})\n    if not doc:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    doc[\"_id\"] = str(doc[\"_id\"])\n    return doc\n\n@app.delete(\"/pipelines/{pipeline_id}\")\ndef delete_pipeline(pipeline_id: str):\n    result = collection.delete_one({\"_id\": ObjectId(pipeline_id)})\n    if result.deleted_count == 0:\n        raise HTTPException(status_code=404, detail=\"Pipeline non trouvÃ©\")\n    return {\"deleted\": True}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#background-tasks-tÃ¢ches-en-arriÃ¨re-plan",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Background Tasks (tÃ¢ches en arriÃ¨re-plan)",
    "text": "Background Tasks (tÃ¢ches en arriÃ¨re-plan)\nUtile pour les webhooks et le dÃ©clenchement de pipelines.\nfrom fastapi import FastAPI, BackgroundTasks\nimport time\n\napp = FastAPI()\n\ndef run_etl_pipeline(pipeline_id: int):\n    \"\"\"Simule l'exÃ©cution d'un pipeline ETL.\"\"\"\n    print(f\" DÃ©marrage du pipeline {pipeline_id}\")\n    time.sleep(10)  # Simule un traitement long\n    print(f\" Pipeline {pipeline_id} terminÃ©\")\n\n@app.post(\"/pipelines/{pipeline_id}/run\")\ndef trigger_pipeline(pipeline_id: int, background_tasks: BackgroundTasks):\n    \"\"\"DÃ©clenche un pipeline en arriÃ¨re-plan.\"\"\"\n    \n    # Ajoute la tÃ¢che en arriÃ¨re-plan\n    background_tasks.add_task(run_etl_pipeline, pipeline_id)\n    \n    # RÃ©pond immÃ©diatement\n    return {\n        \"message\": f\"Pipeline {pipeline_id} dÃ©clenchÃ©\",\n        \"status\": \"running\"\n    }\n\nWebhook pour dÃ©clencher un pipeline\n@app.post(\"/webhook/data-arrival\")\ndef webhook_data_arrival(\n    payload: dict,\n    background_tasks: BackgroundTasks\n):\n    \"\"\"Webhook appelÃ© quand de nouvelles donnÃ©es arrivent.\"\"\"\n    \n    source = payload.get(\"source\")\n    file_path = payload.get(\"file_path\")\n    \n    # DÃ©clencher le pipeline appropriÃ©\n    background_tasks.add_task(\n        process_new_data, \n        source=source, \n        file_path=file_path\n    )\n    \n    return {\"status\": \"accepted\", \"message\": \"Pipeline dÃ©clenchÃ©\"}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#upload-de-fichiers",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#upload-de-fichiers",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Upload de fichiers",
    "text": "Upload de fichiers\nPermet dâ€™ingÃ©rer des fichiers (CSV, JSON, Parquet) via API.\nfrom fastapi import FastAPI, UploadFile, File\nimport pandas as pd\nimport io\n\napp = FastAPI()\n\n@app.post(\"/upload/csv\")\nasync def upload_csv(file: UploadFile = File(...)):\n    \"\"\"Upload et analyse un fichier CSV.\"\"\"\n    \n    # VÃ©rifier l'extension\n    if not file.filename.endswith('.csv'):\n        raise HTTPException(status_code=400, detail=\"Fichier CSV requis\")\n    \n    # Lire le contenu\n    contents = await file.read()\n    \n    # Charger dans Pandas\n    df = pd.read_csv(io.StringIO(contents.decode('utf-8')))\n    \n    return {\n        \"filename\": file.filename,\n        \"rows\": len(df),\n        \"columns\": list(df.columns),\n        \"dtypes\": df.dtypes.astype(str).to_dict(),\n        \"preview\": df.head(5).to_dict(orient=\"records\")\n    }\n\n@app.post(\"/upload/batch\")\nasync def upload_multiple(files: list[UploadFile] = File(...)):\n    \"\"\"Upload plusieurs fichiers.\"\"\"\n    \n    results = []\n    for file in files:\n        results.append({\n            \"filename\": file.filename,\n            \"size\": len(await file.read())\n        })\n    \n    return {\"uploaded\": len(files), \"files\": results}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#documentation-automatique",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#documentation-automatique",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Documentation automatique",
    "text": "Documentation automatique\nFastAPI gÃ©nÃ¨re automatiquement une documentation interactive !\n\nURLs de documentation\n\n\n\nURL\nType\nDescription\n\n\n\n\n/docs\nSwagger UI\nInterface interactive pour tester\n\n\n/redoc\nReDoc\nDocumentation lisible\n\n\n/openapi.json\nOpenAPI\nSchÃ©ma JSON de lâ€™API\n\n\n\n\n\nPersonnaliser la documentation\nfrom fastapi import FastAPI\n\napp = FastAPI(\n    title=\"Data Pipeline API\",\n    description=\"\"\"\n    ## API pour gÃ©rer les pipelines de donnÃ©es\n    \n    Cette API permet de :\n    - GÃ©rer les pipelines ETL\n    - Consulter les donnÃ©es\n    - DÃ©clencher des exÃ©cutions\n    \n    ### Authentification\n    Utilise un token Bearer dans le header `Authorization`.\n    \"\"\",\n    version=\"1.0.0\",\n    contact={\n        \"name\": \"Data Team\",\n        \"email\": \"data@company.com\"\n    },\n    license_info={\n        \"name\": \"MIT\"\n    }\n)\n\n# Documenter un endpoint\n@app.get(\n    \"/pipelines\",\n    summary=\"Liste les pipelines\",\n    description=\"Retourne tous les pipelines avec pagination\",\n    response_description=\"Liste des pipelines\",\n    tags=[\"Pipelines\"]\n)\ndef list_pipelines():\n    pass",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#structure-de-projet-recommandÃ©e",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Structure de projet recommandÃ©e",
    "text": "Structure de projet recommandÃ©e\nmy_api/\nâ”œâ”€â”€ app/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ main.py              # Point d'entrÃ©e FastAPI\nâ”‚   â”œâ”€â”€ config.py            # Configuration (env vars)\nâ”‚   â”œâ”€â”€ database.py          # Connexion DB\nâ”‚   â”œâ”€â”€ models/              # ModÃ¨les SQLAlchemy/Pydantic\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ schemas/             # SchÃ©mas Pydantic (request/response)\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ pipeline.py\nâ”‚   â”œâ”€â”€ routers/             # Endpoints par domaine\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ pipelines.py\nâ”‚   â”‚   â””â”€â”€ data.py\nâ”‚   â””â”€â”€ services/            # Logique mÃ©tier\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ etl.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ test_pipelines.py\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ README.md",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#sÃ©curitÃ©",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "SÃ©curitÃ©",
    "text": "SÃ©curitÃ©\n# Ne jamais hardcoder les secrets !\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nDATABASE_URL = os.getenv(\"DATABASE_URL\")\nAPI_KEY = os.getenv(\"API_KEY\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#checklist-production",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#checklist-production",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "Checklist production",
    "text": "Checklist production\n\n\n\nâœ…\nItem\n\n\n\n\nâ˜\nVariables dâ€™environnement pour les secrets\n\n\nâ˜\nValidation des entrÃ©es (Pydantic)\n\n\nâ˜\nGestion des erreurs (HTTPException)\n\n\nâ˜\nLogging structurÃ©\n\n\nâ˜\nRate limiting\n\n\nâ˜\nCORS configurÃ©\n\n\nâ˜\nTests automatisÃ©s\n\n\nâ˜\nDocumentation Ã  jour\n\n\nâ˜\nHealth check endpoint",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#rÃ©sumÃ©",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "RÃ©sumÃ©",
    "text": "RÃ©sumÃ©\n\nCe que tu as appris\n\n\n\nConcept\nDescription\n\n\n\n\nFastAPI\nFramework moderne pour APIs REST\n\n\nPydantic\nValidation automatique des donnÃ©es\n\n\nEndpoints CRUD\nGET, POST, PUT, PATCH, DELETE\n\n\nPath/Query params\nParamÃ¨tres dâ€™URL et de requÃªte\n\n\nPandas + API\nServir des donnÃ©es via HTTP\n\n\nBackground tasks\nExÃ©cuter des tÃ¢ches asynchrones\n\n\nDocumentation\nSwagger automatique\n\n\n\n\n\nCas dâ€™usage Data Engineering\n\n\n\nCas\nExemple\n\n\n\n\nExposer des donnÃ©es\nAPI pour dashboard/BI\n\n\nWebhook\nDÃ©clencher un pipeline Ã  lâ€™arrivÃ©e de donnÃ©es\n\n\nValidation\nVÃ©rifier les donnÃ©es avant ingestion\n\n\nMonitoring\nAPI de statut des pipelines\n\n\nML Serving\nExposer des prÃ©dictions\n\n\n\n\n\nCommandes essentielles\n# Installation\npip install fastapi uvicorn\n\n# Lancer en dev\nuvicorn main:app --reload\n\n# Lancer en prod\nuvicorn main:app --host 0.0.0.0 --port 8000 --workers 4",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/13_fastapi_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/13_fastapi_for_data_engineers.html#ressources",
    "title": "BONUS : FastAPI pour Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation officielle\n\nFastAPI Documentation â€” Excellente et complÃ¨te\nPydantic Documentation\nUvicorn Documentation\n\n\n\nTutoriels\n\nFastAPI Tutorial â€” Tutoriel officiel\nReal Python - FastAPI\nTestDriven.io - FastAPI\n\n\n\nOutils complÃ©mentaires\n\nSQLModel â€” ORM par le crÃ©ateur de FastAPI\nStrawberry â€” GraphQL avec FastAPI\nCelery â€” TÃ¢ches asynchrones avancÃ©es\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant FastAPI pour le Data Engineering.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Bonus",
      "13 Â· FastAPI pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html",
    "title": "MongoDB for Data Engineers",
    "section": "",
    "text": "Ce module prÃ©sente MongoDB, la base de donnÃ©es NoSQL documentaire la plus populaire.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prÃ©requis",
    "title": "MongoDB for Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 08_intro_big_data_distributed\n\n\nâœ… Requis\nComprendre les 5V du Big Data\n\n\nâœ… Requis\nComprendre CAP\n\n\nâœ… Requis\nConnaÃ®tre le format JSON",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#objectifs-du-module",
    "title": "MongoDB for Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nConfigurer MongoDB Atlas (gratuit, cloud)\nMaÃ®triser le CRUD (Create, Read, Update, Delete)\nUtiliser les opÃ©rateurs de filtrage et comparaison\nÃ‰crire des agrÃ©gations\nCrÃ©er des index pour optimiser les performances",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#mongodb-dans-lÃ©cosystÃ¨me-big-data",
    "title": "MongoDB for Data Engineers",
    "section": "MongoDB dans lâ€™Ã©cosystÃ¨me Big Data",
    "text": "MongoDB dans lâ€™Ã©cosystÃ¨me Big Data\nTu as vu dans le module prÃ©cÃ©dent que MongoDB est une base NoSQL documentaire. Voici comment elle rÃ©pond aux dÃ©fis du Big Data :\n\nRappel : Les 5V\n\n\n\n\n\n\n\nV\nComment MongoDB rÃ©pond\n\n\n\n\nVolume\nSharding horizontal (donnÃ©es rÃ©parties sur plusieurs serveurs)\n\n\nVelocity\nÃ‰critures rapides, rÃ©plication temps rÃ©el\n\n\nVariety\nSchÃ©ma flexible (JSON), pas de structure rigide\n\n\nVeracity\nValidation de schÃ©ma optionnelle\n\n\nValue\nAgrÃ©gations puissantes, intÃ©gration BI\n\n\n\n\n\nRappel : CAP & BASE\n\n\n\nConcept\nMongoDB\n\n\n\n\nCAP\nCP (Consistency + Partition tolerance) par dÃ©faut\n\n\nBASE\nCohÃ©rence configurable (forte ou Ã©ventuelle)\n\n\n\n\nğŸ’¡ Pas dâ€™installation nÃ©cessaire : Tout se fait dans le cloud avec MongoDB Atlas !\nğŸ“ Note : Les commandes sâ€™exÃ©cutent dans MongoDB Shell ou lâ€™interface web Atlas.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#explication-simple",
    "title": "MongoDB for Data Engineers",
    "section": "1.1 Explication simple",
    "text": "1.1 Explication simple\nMongoDB est une base de donnÃ©es qui stocke les donnÃ©es au format JSON (comme des fichiers texte structurÃ©s).\n\nComparaison : SQL vs MongoDB\nBase de donnÃ©es SQL (MySQL, PostgreSQL) :\nTable : employes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom    â”‚ age â”‚ ville â”‚ salaire â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice    â”‚ 25  â”‚ Paris â”‚ 45000   â”‚\nâ”‚ 2  â”‚ Bob      â”‚ 30  â”‚ Lyon  â”‚ 50000   â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n-- RequÃªte SQL\nSELECT * FROM employes WHERE ville = 'Paris';\nMongoDB (NoSQL) :\nCollection : employes\n[\n  {\n    \"_id\": 1,\n    \"nom\": \"Alice\",\n    \"age\": 25,\n    \"ville\": \"Paris\",\n    \"salaire\": 45000,\n    \"competences\": [\"Python\", \"SQL\"]\n  },\n  {\n    \"_id\": 2,\n    \"nom\": \"Bob\",\n    \"age\": 30,\n    \"ville\": \"Lyon\",\n    \"salaire\": 50000,\n    \"competences\": [\"Java\", \"Docker\"]\n  }\n]\n\n// RequÃªte MongoDB\ndb.employes.find({ ville: \"Paris\" })\n\n\nDiffÃ©rences principales :\n\n\n\n\n\n\n\n\nAspect\nSQL\nMongoDB\n\n\n\n\nStructure\nTableaux avec colonnes fixes\nDocuments JSON flexibles\n\n\nSchÃ©ma\nRigide (dÃ©fini Ã  lâ€™avance)\nFlexible (peut changer)\n\n\nFormat\nLignes et colonnes\nDocuments JSON\n\n\nTableaux\nDifficile (tables sÃ©parÃ©es)\nFacile (intÃ©grÃ©)\n\n\nLangage\nSQL\nJavaScript/JSON",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#vocabulaire-de-base",
    "title": "MongoDB for Data Engineers",
    "section": "1.2 Vocabulaire de base",
    "text": "1.2 Vocabulaire de base\n\n\n\n\n\n\n\n\n\nSQL\nMongoDB\nExplication\nExemple\n\n\n\n\nDatabase\nDatabase\nConteneur principal\nentreprise\n\n\nTable\nCollection\nGroupe de donnÃ©es similaires\nemployes, produits\n\n\nRow\nDocument\nUne entrÃ©e de donnÃ©es\n{nom: \"Alice\", age: 25}\n\n\nColumn\nField\nUn attribut\nnom, age, ville\n\n\n\n\nStructure visuelle :\nMongoDB Server\n  â””â”€â”€ Database: ma_boutique\n       â”œâ”€â”€ Collection: clients\n       â”‚    â”œâ”€â”€ Document 1: { nom: \"Alice\", email: \"alice@email.com\" }\n       â”‚    â””â”€â”€ Document 2: { nom: \"Bob\", email: \"bob@email.com\" }\n       â”‚\n       â””â”€â”€ Collection: produits\n            â”œâ”€â”€ Document 1: { nom: \"Laptop\", prix: 899 }\n            â””â”€â”€ Document 2: { nom: \"Souris\", prix: 29 }",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#format-des-documents",
    "title": "MongoDB for Data Engineers",
    "section": "1.3 Format des documents",
    "text": "1.3 Format des documents\nMongoDB stocke les donnÃ©es en BSON (Binary JSON).\n\nExemple de document complet :\n{\n  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),  // ID unique automatique\n  \"nom\": \"Alice Dupont\",                        // String (texte)\n  \"age\": 28,                                     // Number (entier)\n  \"salaire\": 55000.50,                           // Number (dÃ©cimal)\n  \"actif\": true,                                 // Boolean (vrai/faux)\n  \"date_embauche\": ISODate(\"2022-01-15\"),       // Date\n  \"competences\": [\"Python\", \"SQL\", \"MongoDB\"],  // Array (tableau)\n  \"adresse\": {                                   // Object (objet imbriquÃ©)\n    \"rue\": \"123 Main St\",\n    \"ville\": \"Paris\",\n    \"code_postal\": \"75001\"\n  },\n  \"notes\": null                                  // Null (vide)\n}\n\n\nPoints importants :\n\nChaque document a un champ _id unique (crÃ©Ã© automatiquement si absent)\nOn peut imbriquer des objets et des tableaux\nPas besoin que tous les documents aient les mÃªmes champs\nLa syntaxe ressemble Ã  JavaScript/JSON",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-utiliser-mongodb-en-data-engineering",
    "title": "MongoDB for Data Engineers",
    "section": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?",
    "text": "1.4 Pourquoi utiliser MongoDB en Data Engineering ?\n\nAvantages :\n1. FlexibilitÃ© du schÃ©ma\n\nPas besoin de dÃ©finir la structure Ã  lâ€™avance\nParfait pour des donnÃ©es qui changent souvent\nChaque document peut Ãªtre diffÃ©rent\n\n// Document 1 : simple\n{ nom: \"Alice\", age: 25 }\n\n// Document 2 : plus dÃ©taillÃ© (dans la mÃªme collection !)\n{ nom: \"Bob\", age: 30, ville: \"Lyon\", competences: [\"Python\"], manager: \"Alice\" }\n2. Format JSON natif\n\nLes APIs web utilisent JSON\nPas de conversion nÃ©cessaire\nFacile Ã  lire et manipuler\n\n3. Performance\n\nLectures et Ã©critures trÃ¨s rapides\nGÃ¨re facilement des millions de documents\nScalabilitÃ© horizontale (ajouter des serveurs)\n\n4. DonnÃ©es imbriquÃ©es\n\nStocke des structures complexes facilement\nPas besoin de multiples tables et jointures\n\n// Tout dans un seul document !\n{\n  \"commande_id\": \"CMD001\",\n  \"client\": { \"nom\": \"Alice\", \"email\": \"alice@example.com\" },\n  \"articles\": [\n    { \"produit\": \"Laptop\", \"quantite\": 1, \"prix\": 899 },\n    { \"produit\": \"Souris\", \"quantite\": 2, \"prix\": 29 }\n  ],\n  \"total\": 957\n}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#quand-utiliser-mongodb",
    "title": "MongoDB for Data Engineers",
    "section": "1.5 Quand utiliser MongoDB ?",
    "text": "1.5 Quand utiliser MongoDB ?\n\nUtilisez MongoDB pour :\n\n\n\nCas dâ€™usage\nPourquoi\n\n\n\n\nApplications web/mobile\nAPI JSON, scalabilitÃ©\n\n\nCollecte de logs\nVolume Ã©levÃ©, structure flexible\n\n\nE-commerce (catalogues)\nProduits avec attributs variables\n\n\nDonnÃ©es IoT\nMillions dâ€™Ã©critures/seconde\n\n\nCMS (contenu)\nStructures diverses\n\n\nPrototypage rapide\nPas de schÃ©ma prÃ©dÃ©fini\n\n\nDonnÃ©es JSON dâ€™APIs\nFormat natif\n\n\n\n\n\nNâ€™utilisez PAS MongoDB pour :\n\n\n\nCas dâ€™usage\nPrÃ©fÃ©rez SQL\n\n\n\n\nTransactions bancaires\nBesoin dâ€™ACID strict\n\n\nRelations multiples complexes\nNombreuses jointures\n\n\nReporting BI traditionnel\nRequÃªtes SQL ad-hoc\n\n\nData Warehouse\nSchÃ©ma en Ã©toile\n\n\n\n\n\nğŸ’¡ RÃ¨gle simple :\n\nDonnÃ©es flexibles, JSON, volume Ã©levÃ© â†’ MongoDB\n\nDonnÃ©es structurÃ©es, relations strictes, transactions complexes â†’ SQL (PostgreSQL, MySQL)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-mongodb-atlas",
    "title": "MongoDB for Data Engineers",
    "section": "2.1 Pourquoi MongoDB Atlas ?",
    "text": "2.1 Pourquoi MongoDB Atlas ?\nMongoDB Atlas = MongoDB hÃ©bergÃ© dans le cloud (gratuit)\n\nAvantages :\nGratuit : Tier M0 avec 512 MB de stockage\nPas dâ€™installation : Tout dans le navigateur\nInterface graphique : Data Explorer facile\nSÃ©curisÃ© : Backup automatique\nMongoDB Shell intÃ©grÃ© : Testez vos commandes directement",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-votre-compte-5-minutes",
    "title": "MongoDB for Data Engineers",
    "section": "2.2 CrÃ©er votre compte (5 minutes)",
    "text": "2.2 CrÃ©er votre compte (5 minutes)\n\nâ–¶ï¸ Ã‰tape 1 : Inscription\n\nAllez sur ğŸ‘‰ cloud.mongodb.com\nCliquez sur â€œTry Freeâ€\nInscrivez-vous avec email/mot de passe (ou Google)\n\n\n\nâ–¶ï¸ Ã‰tape 2 : CrÃ©er un cluster gratuit\n\nChoisissez M0 (FREE) ğŸ‰\nProvider : AWS (ou Google Cloud/Azure)\nRÃ©gion : Choisissez proche de vous\n\nEurope : Frankfurt, Paris, London\nAmÃ©rique : N. Virginia, Oregon\nAsie : Singapore, Mumbai\n\nCluster Name : Cluster0 (par dÃ©faut, vous pouvez changer)\nCliquez â€œCreateâ€\n\nâ³ Attendez 2-3 minutes que le cluster se crÃ©e\n\n\nâ–¶ï¸ Ã‰tape 3 : CrÃ©er un utilisateur\n\nDans la popup de sÃ©curitÃ© :\nAuthentication Method : Username and Password\nUsername : admin (ou votre choix)\nPassword : CrÃ©ez un mot de passe fort\n\nâš ï¸ NOTEZ-LE BIEN quelque part !\n\nCliquez â€œCreate Userâ€\n\n\n\nâ–¶ï¸ Ã‰tape 4 : Autoriser lâ€™accÃ¨s\n\nDans la popup : â€œWhere would you like to connect from?â€\nChoisissez â€œMy Local Environmentâ€\nOption 1 (recommandÃ©e pour apprendre) :\n\nCliquez â€œAdd My Current IP Addressâ€\n\nOption 2 (plus simple mais moins sÃ©curisÃ©e) :\n\nMettez 0.0.0.0/0 pour autoriser toutes les IPs\n\nCliquez â€œFinish and Closeâ€\n\nâœ… Votre cluster est prÃªt !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#accÃ©der-Ã -mongodb-shell",
    "title": "MongoDB for Data Engineers",
    "section": "2.3 AccÃ©der Ã  MongoDB Shell",
    "text": "2.3 AccÃ©der Ã  MongoDB Shell\n\nMÃ©thode 1 : MongoDB Shell Web (le plus simple)\n\nDans MongoDB Atlas, allez sur â€œDatabaseâ€ (menu gauche)\nCliquez sur â€œBrowse Collectionsâ€ sur votre cluster\nEn bas de page, cliquez sur lâ€™onglet â€œMongoDB Shellâ€ ou â€œ&gt;_ mongoshâ€\nUne console sâ€™ouvre dans le navigateur\n\n\n\nMÃ©thode 2 : Data Explorer (interface graphique)\n\nCliquez sur â€œBrowse Collectionsâ€\nVous pouvez crÃ©er des bases de donnÃ©es et collections visuellement\nPratique pour visualiser, mais on va utiliser des commandes\n\n\n\nğŸ’¡ Dans ce tutoriel\nToutes les commandes ci-dessous sont Ã  taper dans : - MongoDB Shell Web (dans Atlas) - Ou mongosh (si vous lâ€™installez localement) - Ou Data Explorer â†’ Insert Document (pour ajouter des donnÃ©es)\nOn utilise la syntaxe JavaScript/MongoDB, pas Python !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-une-base-de-donnÃ©es-et-une-collection",
    "title": "MongoDB for Data Engineers",
    "section": "3.1 CrÃ©er une base de donnÃ©es et une collection",
    "text": "3.1 CrÃ©er une base de donnÃ©es et une collection\n\nCrÃ©er/SÃ©lectionner une base de donnÃ©es\n// CrÃ©er ou utiliser la base de donnÃ©es \"ma_premiere_db\"\nuse ma_premiere_db\nNote : La base de donnÃ©es nâ€™est crÃ©Ã©e rÃ©ellement que quand vous insÃ©rez des donnÃ©es.\n\n\nVoir la base de donnÃ©es actuelle\ndb\n\n\nLister toutes les bases de donnÃ©es\nshow dbs\n\n\nLister les collections\nshow collections",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#create---insÃ©rer-des-documents",
    "title": "MongoDB for Data Engineers",
    "section": "3.2 CREATE - InsÃ©rer des documents",
    "text": "3.2 CREATE - InsÃ©rer des documents\n\nInsÃ©rer UN document\n// CrÃ©er la collection \"employes\" et insÃ©rer un document\ndb.employes.insertOne({\n  nom: \"Alice Dupont\",\n  age: 28,\n  poste: \"Data Engineer\",\n  salaire: 55000,\n  ville: \"Paris\"\n})\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedId: ObjectId(\"507f1f77bcf86cd799439011\")\n}\n\n\nInsÃ©rer PLUSIEURS documents\ndb.employes.insertMany([\n  {\n    nom: \"Bob Martin\",\n    age: 32,\n    poste: \"Data Analyst\",\n    salaire: 48000,\n    ville: \"Lyon\"\n  },\n  {\n    nom: \"Charlie Dubois\",\n    age: 35,\n    poste: \"Data Scientist\",\n    salaire: 65000,\n    ville: \"Paris\"\n  },\n  {\n    nom: \"David Laurent\",\n    age: 29,\n    poste: \"Data Engineer\",\n    salaire: 58000,\n    ville: \"Marseille\"\n  },\n  {\n    nom: \"Eve Bernard\",\n    age: 26,\n    poste: \"Data Analyst\",\n    salaire: 45000,\n    ville: \"Lyon\"\n  }\n])\nRÃ©sultat :\n{\n  acknowledged: true,\n  insertedIds: {\n    '0': ObjectId(\"...\"),\n    '1': ObjectId(\"...\"),\n    '2': ObjectId(\"...\"),\n    '3': ObjectId(\"...\")\n  }\n}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#read---lire-des-documents",
    "title": "MongoDB for Data Engineers",
    "section": "3.3 READ - Lire des documents",
    "text": "3.3 READ - Lire des documents\n\nLire TOUS les documents\ndb.employes.find()\n\n\nLire avec un affichage formatÃ© (pretty)\ndb.employes.find().pretty()\n\n\nLire UN seul document\ndb.employes.findOne()\n\n\nFiltrer : employÃ©s Ã  Paris\ndb.employes.find({ ville: \"Paris\" })\n\n\nFiltrer : salaire supÃ©rieur Ã  50000\ndb.employes.find({ salaire: { $gt: 50000 } })\nOpÃ©rateurs de comparaison : - $gt : greater than (&gt;) - $gte : greater than or equal (&gt;=) - $lt : less than (&lt;) - $lte : less than or equal (&lt;=) - $eq : equal (=) - $ne : not equal (!=)\n\n\nFiltrer avec plusieurs conditions (AND)\n// EmployÃ©s Ã  Paris avec salaire &gt; 50000\ndb.employes.find({\n  ville: \"Paris\",\n  salaire: { $gt: 50000 }\n})\n\n\nFiltrer avec OR\n// EmployÃ©s Ã  Paris OU Lyon\ndb.employes.find({\n  $or: [\n    { ville: \"Paris\" },\n    { ville: \"Lyon\" }\n  ]\n})\n\n\nSÃ©lectionner certains champs seulement (projection)\n// Afficher seulement nom et salaire (sans _id)\ndb.employes.find(\n  {},\n  { nom: 1, salaire: 1, _id: 0 }\n)\nNote : 1 = inclure, 0 = exclure",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#trier-limiter-compter",
    "title": "MongoDB for Data Engineers",
    "section": "3.4 Trier, limiter, compter",
    "text": "3.4 Trier, limiter, compter\n\nTrier par salaire (croissant)\ndb.employes.find().sort({ salaire: 1 })\n\n\nTrier par salaire (dÃ©croissant)\ndb.employes.find().sort({ salaire: -1 })\nNote : 1 = croissant, -1 = dÃ©croissant\n\n\nLimiter les rÃ©sultats (top 3)\n// Top 3 salaires\ndb.employes.find().sort({ salaire: -1 }).limit(3)\n\n\nSauter des rÃ©sultats (pagination)\n// Sauter les 2 premiers, afficher les 3 suivants\ndb.employes.find().skip(2).limit(3)\n\n\nCompter les documents\n// Compter tous les employÃ©s\ndb.employes.countDocuments()\n\n// Compter les employÃ©s Ã  Paris\ndb.employes.countDocuments({ ville: \"Paris\" })\n\n// Compter les employÃ©s avec salaire &gt; 50000\ndb.employes.countDocuments({ salaire: { $gt: 50000 } })",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#update---modifier-des-documents",
    "title": "MongoDB for Data Engineers",
    "section": "3.5 UPDATE - Modifier des documents",
    "text": "3.5 UPDATE - Modifier des documents\n\nModifier UN document\n// Augmenter le salaire d'Alice\ndb.employes.updateOne(\n  { nom: \"Alice Dupont\" },           // Condition\n  { $set: { salaire: 60000 } }       // Modification\n)\nRÃ©sultat :\n{\n  acknowledged: true,\n  matchedCount: 1,\n  modifiedCount: 1\n}\n\n\nModifier PLUSIEURS documents\n// Augmenter tous les salaires de Lyon de 2000â‚¬\ndb.employes.updateMany(\n  { ville: \"Lyon\" },\n  { $inc: { salaire: 2000 } }       // $inc = incrÃ©menter\n)\n\n\nOpÃ©rateurs de modification\n// $set : dÃ©finir une valeur\n{ $set: { ville: \"Paris\" } }\n\n// $inc : incrÃ©menter\n{ $inc: { age: 1 } }\n\n// $mul : multiplier\n{ $mul: { salaire: 1.1 } }  // Augmentation de 10%\n\n// $unset : supprimer un champ\n{ $unset: { notes: \"\" } }\n\n// $rename : renommer un champ\n{ $rename: { \"nom\": \"nom_complet\" } }\n\n\nAjouter un champ Ã  tous les documents\ndb.employes.updateMany(\n  {},\n  { $set: { actif: true } }\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#delete---supprimer-des-documents",
    "title": "MongoDB for Data Engineers",
    "section": "3.6 DELETE - Supprimer des documents",
    "text": "3.6 DELETE - Supprimer des documents\n\nSupprimer UN document\ndb.employes.deleteOne({ nom: \"Test User\" })\n\n\nSupprimer PLUSIEURS documents\n// Supprimer tous les employÃ©s de Test\ndb.employes.deleteMany({ ville: \"Test\" })\n\n\nâš ï¸ Supprimer TOUS les documents\n// ATTENTION : Supprime tout !\ndb.employes.deleteMany({})\n\n\nSupprimer une collection entiÃ¨re\ndb.employes.drop()\n\n\nSupprimer une base de donnÃ©es\ndb.dropDatabase()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#grouper-et-compter",
    "title": "MongoDB for Data Engineers",
    "section": "4.1 Grouper et compter",
    "text": "4.1 Grouper et compter\n\nCompter les employÃ©s par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",              // Grouper par ville\n      nombre: { $sum: 1 }         // Compter\n    }\n  }\n])\nRÃ©sultat :\n[\n  { _id: \"Paris\", nombre: 2 },\n  { _id: \"Lyon\", nombre: 2 },\n  { _id: \"Marseille\", nombre: 1 }\n]\n\n\nSalaire moyen par ville\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  },\n  {\n    $sort: { salaire_moyen: -1 }   // Trier par salaire moyen dÃ©croissant\n  }\n])\n\n\nGrouper par poste\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: \"$poste\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#filtrer-avant-dagrÃ©ger",
    "title": "MongoDB for Data Engineers",
    "section": "4.2 Filtrer avant dâ€™agrÃ©ger",
    "text": "4.2 Filtrer avant dâ€™agrÃ©ger\n// Statistiques pour les employÃ©s avec salaire &gt; 50000\ndb.employes.aggregate([\n  {\n    $match: { salaire: { $gt: 50000 } }   // Filtrer d'abord\n  },\n  {\n    $group: {\n      _id: \"$ville\",\n      nombre: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#statistiques-globales",
    "title": "MongoDB for Data Engineers",
    "section": "4.3 Statistiques globales",
    "text": "4.3 Statistiques globales\n// Statistiques sur tous les salaires\ndb.employes.aggregate([\n  {\n    $group: {\n      _id: null,                          // null = pas de groupement\n      total_employes: { $sum: 1 },\n      salaire_moyen: { $avg: \"$salaire\" },\n      salaire_total: { $sum: \"$salaire\" },\n      salaire_max: { $max: \"$salaire\" },\n      salaire_min: { $min: \"$salaire\" }\n    }\n  }\n])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#ce-que-vous-avez-appris",
    "title": "MongoDB for Data Engineers",
    "section": "Ce que vous avez appris",
    "text": "Ce que vous avez appris\n\nConcepts\n\nMongoDB = Base de donnÃ©es NoSQL (documents JSON)\nVocabulaire : Database â†’ Collection â†’ Document â†’ Field\nDiffÃ©rence SQL/NoSQL : Structure fixe vs flexible\nQuand utiliser MongoDB : APIs, logs, IoT, e-commerce\n\n\n\nCommandes essentielles\n// BASES\nuse ma_db                    // CrÃ©er/utiliser DB\nshow dbs                     // Lister les DBs\nshow collections             // Lister les collections\n\n// CREATE\ndb.collection.insertOne({...})\ndb.collection.insertMany([{...}, {...}])\n\n// READ\ndb.collection.find()                     // Tout\ndb.collection.find({ ville: \"Paris\" })   // FiltrÃ©\ndb.collection.findOne()\ndb.collection.find().sort({ age: -1 })   // TriÃ©\ndb.collection.find().limit(5)            // LimitÃ©\ndb.collection.countDocuments()\n\n// UPDATE\ndb.collection.updateOne({ _id: 1 }, { $set: { age: 30 } })\ndb.collection.updateMany({ ville: \"Lyon\" }, { $inc: { salaire: 1000 } })\n\n// DELETE\ndb.collection.deleteOne({ _id: 1 })\ndb.collection.deleteMany({ ville: \"Test\" })\n\n// AGGREGATE\ndb.collection.aggregate([\n  { $group: { _id: \"$ville\", count: { $sum: 1 } } }\n])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-importants",
    "title": "MongoDB for Data Engineers",
    "section": "OpÃ©rateurs importants ğŸ“",
    "text": "OpÃ©rateurs importants ğŸ“\n\nComparaison\n\n$gt : &gt; (greater than)\n$gte : &gt;= (greater than or equal)\n$lt : &lt; (less than)\n$lte : &lt;= (less than or equal)\n$eq : = (equal)\n$ne : != (not equal)\n\n\n\nModification\n\n$set : DÃ©finir une valeur\n$inc : IncrÃ©menter\n$mul : Multiplier\n$unset : Supprimer un champ\n\n\n\nAgrÃ©gation\n\n$sum : Somme\n$avg : Moyenne\n$max : Maximum\n$min : Minimum\n$group : Grouper\n$match : Filtrer",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#pourquoi-les-index",
    "title": "MongoDB for Data Engineers",
    "section": "5.1 Pourquoi les index ?",
    "text": "5.1 Pourquoi les index ?\nSans index, MongoDB doit scanner tous les documents pour trouver ceux qui correspondent au filtre. Avec un index, la recherche est beaucoup plus rapide.\n\n\n\nSans index\nAvec index\n\n\n\n\nScan complet (lent)\nRecherche directe (rapide)\n\n\nO(n)\nO(log n)\n\n\n1M docs = 1M comparaisons\n1M docs = ~20 comparaisons",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#crÃ©er-un-index",
    "title": "MongoDB for Data Engineers",
    "section": "5.2 CrÃ©er un index",
    "text": "5.2 CrÃ©er un index\n\nIndex simple (un champ)\n// Index sur le champ \"email\" (croissant)\ndb.employes.createIndex({ email: 1 })\n\n// Index sur le champ \"salaire\" (dÃ©croissant)\ndb.employes.createIndex({ salaire: -1 })\n\n\nIndex composÃ© (plusieurs champs)\n// Index sur ville + poste (pour requÃªtes frÃ©quentes)\ndb.employes.createIndex({ ville: 1, poste: 1 })\n\n\nIndex unique\n// EmpÃªche les doublons sur email\ndb.employes.createIndex({ email: 1 }, { unique: true })",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#gÃ©rer-les-index",
    "title": "MongoDB for Data Engineers",
    "section": "5.3 GÃ©rer les index",
    "text": "5.3 GÃ©rer les index\n// Lister tous les index\ndb.employes.getIndexes()\n\n// Supprimer un index\ndb.employes.dropIndex({ email: 1 })\n\n// Supprimer tous les index (sauf _id)\ndb.employes.dropIndexes()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#analyser-les-performances",
    "title": "MongoDB for Data Engineers",
    "section": "5.4 Analyser les performances",
    "text": "5.4 Analyser les performances\n// Voir le plan d'exÃ©cution\ndb.employes.find({ ville: \"Paris\" }).explain(\"executionStats\")\nRegarder :\n\ntotalDocsExamined : Combien de documents scannÃ©s\nexecutionTimeMillis : Temps dâ€™exÃ©cution\nstage: \"IXSCAN\" = Index utilisÃ© âœ…\nstage: \"COLLSCAN\" = Scan complet âŒ",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#bonnes-pratiques",
    "title": "MongoDB for Data Engineers",
    "section": "5.5 Bonnes pratiques",
    "text": "5.5 Bonnes pratiques\n\n\n\n\n\n\n\nRÃ¨gle\nExplication\n\n\n\n\nIndexer les champs de filtrage frÃ©quents\nville, email, date\n\n\nIndexer les champs de tri\nORDER BY = index\n\n\nPas trop dâ€™index\nChaque index ralentit les Ã©critures\n\n\nIndex composÃ© : ordre important\nLe champ le plus filtrant en premier",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-avec-expressions-rÃ©guliÃ¨res",
    "title": "MongoDB for Data Engineers",
    "section": "6.1 Recherche avec expressions rÃ©guliÃ¨res",
    "text": "6.1 Recherche avec expressions rÃ©guliÃ¨res\n// Noms commenÃ§ant par \"A\"\ndb.employes.find({ nom: { $regex: \"^A\" } })\n\n// Noms contenant \"dupont\" (insensible Ã  la casse)\ndb.employes.find({ nom: { $regex: \"dupont\", $options: \"i\" } })\n\n// Emails Gmail\ndb.employes.find({ email: { $regex: \"@gmail\\.com$\" } })",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#opÃ©rateurs-de-tableau",
    "title": "MongoDB for Data Engineers",
    "section": "6.2 OpÃ©rateurs de tableau",
    "text": "6.2 OpÃ©rateurs de tableau\n// Document avec un tableau\ndb.employes.insertOne({\n  nom: \"Alice\",\n  competences: [\"Python\", \"SQL\", \"MongoDB\"]\n})\n\n// Trouver ceux qui ont \"Python\" dans leurs compÃ©tences\ndb.employes.find({ competences: \"Python\" })\n\n// Trouver ceux qui ont Python ET SQL\ndb.employes.find({ competences: { $all: [\"Python\", \"SQL\"] } })\n\n// Trouver ceux qui ont au moins 3 compÃ©tences\ndb.employes.find({ competences: { $size: 3 } })",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#recherche-dans-les-objets-imbriquÃ©s",
    "title": "MongoDB for Data Engineers",
    "section": "6.3 Recherche dans les objets imbriquÃ©s",
    "text": "6.3 Recherche dans les objets imbriquÃ©s\n// Document avec objet imbriquÃ©\ndb.employes.insertOne({\n  nom: \"Bob\",\n  adresse: {\n    ville: \"Paris\",\n    code_postal: \"75001\"\n  }\n})\n\n// Rechercher par ville imbriquÃ©e (notation pointÃ©e)\ndb.employes.find({ \"adresse.ville\": \"Paris\" })\n\n// Rechercher par code postal\ndb.employes.find({ \"adresse.code_postal\": { $regex: \"^75\" } })",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#cheatsheet-sql-vs-mongodb",
    "title": "MongoDB for Data Engineers",
    "section": "Cheatsheet : SQL vs MongoDB",
    "text": "Cheatsheet : SQL vs MongoDB\n\n\n\n\n\n\n\n\nOpÃ©ration\nSQL\nMongoDB\n\n\n\n\nTout sÃ©lectionner\nSELECT * FROM table\ndb.collection.find()\n\n\nFiltrer\nWHERE col = 'val'\nfind({ col: 'val' })\n\n\nSupÃ©rieur Ã \nWHERE col &gt; 10\nfind({ col: { $gt: 10 } })\n\n\nET\nWHERE a = 1 AND b = 2\nfind({ a: 1, b: 2 })\n\n\nOU\nWHERE a = 1 OR b = 2\nfind({ $or: [{a:1}, {b:2}] })\n\n\nIN\nWHERE col IN (1,2,3)\nfind({ col: { $in: [1,2,3] } })\n\n\nLIKE\nWHERE col LIKE '%val%'\nfind({ col: { $regex: 'val' } })\n\n\nProjection\nSELECT a, b FROM\nfind({}, { a:1, b:1 })\n\n\nTrier\nORDER BY col ASC\n.sort({ col: 1 })\n\n\nLimiter\nLIMIT 10\n.limit(10)\n\n\nCompter\nSELECT COUNT(*)\n.countDocuments()\n\n\nGrouper\nGROUP BY col\naggregate([{$group:{_id:'$col'}}])\n\n\nInsert\nINSERT INTO ... VALUES\ninsertOne({...})\n\n\nUpdate\nUPDATE ... SET ... WHERE\nupdateOne({filter}, {$set:{...}})\n\n\nDelete\nDELETE FROM ... WHERE\ndeleteOne({filter})",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#exercices-pratiques-Ã -toi-de-jouer",
    "title": "MongoDB for Data Engineers",
    "section": "Exercices pratiques â€” Ã€ toi de jouer !",
    "text": "Exercices pratiques â€” Ã€ toi de jouer !\n\nCollection disponible : employes\n{ nom, age, poste, salaire, ville, competences: [...] }\n\n\n\nExercice 1 â€” Facile\nTrouver tous les employÃ©s qui habitent Ã  Lyon.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({ ville: \"Lyon\" })\n\n\n\n\nExercice 2 â€” Facile\nCompter le nombre total dâ€™employÃ©s.\n\n\nğŸ’¡ Solution\n\ndb.employes.countDocuments()\n\n\n\n\nExercice 3 â€” IntermÃ©diaire\nTrouver les 3 employÃ©s les mieux payÃ©s (nom et salaire uniquement).\n\n\nğŸ’¡ Solution\n\ndb.employes.find({}, { nom: 1, salaire: 1, _id: 0 })\n  .sort({ salaire: -1 })\n  .limit(3)\n\n\n\n\nExercice 4 â€” IntermÃ©diaire\nAugmenter de 5% le salaire de tous les Data Engineers.\n\n\nğŸ’¡ Solution\n\ndb.employes.updateMany(\n  { poste: \"Data Engineer\" },\n  { $mul: { salaire: 1.05 } }\n)\n\n\n\n\nExercice 5 â€” AvancÃ©\nCalculer le salaire moyen par poste, triÃ© du plus Ã©levÃ© au plus bas.\n\n\nğŸ’¡ Solution\n\ndb.employes.aggregate([\n  { $group: { _id: \"$poste\", salaire_moyen: { $avg: \"$salaire\" } } },\n  { $sort: { salaire_moyen: -1 } }\n])\n\n\n\n\nExercice 6 â€” AvancÃ©\nTrouver les employÃ©s qui ont â€œPythonâ€ dans leurs compÃ©tences et gagnent plus de 50000â‚¬.\n\n\nğŸ’¡ Solution\n\ndb.employes.find({\n  competences: \"Python\",\n  salaire: { $gt: 50000 }\n})",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#votre-score",
    "title": "MongoDB for Data Engineers",
    "section": "ğŸ“Š Votre score",
    "text": "ğŸ“Š Votre score\n\n10/10 : ğŸ† Expert MongoDB ! Vous Ãªtes prÃªt pour le niveau intermÃ©diaire\n7-9/10 : ğŸŒŸ TrÃ¨s bien ! Relisez les sections oÃ¹ vous avez hÃ©sitÃ©\n5-6/10 : ğŸ’ª Bon dÃ©but ! Pratiquez les commandes dans Atlas\n&lt; 5/10 : ğŸ“š Relisez le notebook et testez les exemples",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/09_mongodb_for_data_engineers.html#prochaine-Ã©tape",
    "title": "MongoDB for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant MongoDB ! Passons Ã  une autre base NoSQL trÃ¨s utilisÃ©e en Data Engineering.\nğŸ‘‰ Module suivant : 10_elasticsearch_for_data_engineers â€” Elasticsearch pour la recherche et lâ€™analytics\n\n\nğŸ“š Ressources\n\nMongoDB Atlas â€” Votre compte\nMongoDB Documentation â€” Doc officielle\nMongoDB University â€” Cours gratuits\nMongoDB Cheat Sheet\n\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les bases de MongoDB.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "09 Â· MongoDB"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir les moteurs OLAP temps rÃ©el â€” des bases de donnÃ©es optimisÃ©es pour des requÃªtes analytiques ultra-rapides sur des donnÃ©es en streaming. Tu apprendras Ã  construire des dashboards live qui se rafraÃ®chissent en temps rÃ©el.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#prÃ©requis",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#prÃ©requis",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nKafka & Spark Streaming (M24)\n\n\nâœ… Requis\nSQL avancÃ©\n\n\nâœ… Requis\nDocker\n\n\nğŸ’¡ RecommandÃ©\nDistributed Messaging (M29)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#objectifs-du-module",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#objectifs-du-module",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre quand utiliser un OLAP engine vs Spark\nDÃ©ployer et configurer ClickHouse\nIngÃ©rer des donnÃ©es depuis Kafka vers ClickHouse\nCrÃ©er des Materialized Views pour prÃ©-agrÃ©gation\nConstruire des dashboards temps rÃ©el avec Grafana\nConnaÃ®tre les alternatives : Druid et Pinot",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#introduction-pourquoi-un-olap-engine",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#introduction-pourquoi-un-olap-engine",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "1. Introduction : Pourquoi un OLAP Engine ?",
    "text": "1. Introduction : Pourquoi un OLAP Engine ?\n\n1.1 Rappel : Architecture Streaming (M24)\nDans le module M24, tu as appris Ã  construire des pipelines streaming :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CE QU'ON A VU EN M24                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   Source â”€â”€â–¶ Kafka â”€â”€â–¶ Spark Streaming â”€â”€â–¶ Delta Lake                      â”‚\nâ”‚                              â”‚                                              â”‚\nâ”‚                              â””â”€â”€ Transformations                            â”‚\nâ”‚                                  AgrÃ©gations                                â”‚\nâ”‚                                  Windowing                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Ingestion temps rÃ©el                                                   â”‚\nâ”‚   âœ… Transformations complexes                                              â”‚\nâ”‚   âœ… Exactly-once semantics                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Le ProblÃ¨me : Queries Interactives\nSpark est excellent pour le traitement mais moins pour les queries interactives :\n\n\n\nBesoin\nSpark\nOLAP Engine\n\n\n\n\nQuery latency\nSecondes\nMillisecondes\n\n\nConcurrent users\n~10\n~1000\n\n\nAd-hoc queries\nLent Ã  dÃ©marrer\nInstantanÃ©\n\n\nDashboard refresh\nCoÃ»teux\nOptimisÃ©\n\n\n\n\n\n1.3 OLAP vs OLTP vs Streaming\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    OLTP vs OLAP vs STREAMING                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   OLTP (PostgreSQL)         STREAMING (Kafka+Spark)    OLAP (ClickHouse)   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Row-oriented            â€¢ Event processing         â€¢ Column-oriented   â”‚\nâ”‚   â€¢ Single row ops          â€¢ Continuous               â€¢ Analytical        â”‚\nâ”‚   â€¢ ACID transactions       â€¢ Transformations          â€¢ Fast aggregations â”‚\nâ”‚   â€¢ Low latency writes      â€¢ State management         â€¢ Low latency reads â”‚\nâ”‚                                                                             â”‚\nâ”‚   Use: Applications         Use: Pipelines             Use: Analytics      â”‚\nâ”‚   Ex: User signup           Ex: ETL, enrichment        Ex: Dashboards      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.4 Architecture ComplÃ¨te Real-Time Analytics\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    REAL-TIME ANALYTICS ARCHITECTURE                         â”‚\nâ”‚                                                                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚   â”‚  Apps   â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚   Spark     â”‚â”€â”€â”€â”€â–¶â”‚ Delta Lake  â”‚     â”‚\nâ”‚   â”‚  IoT    â”‚     â”‚         â”‚     â”‚  Streaming  â”‚     â”‚ (historique)â”‚     â”‚\nâ”‚   â”‚  Events â”‚     â”‚         â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚         â”‚                                              â”‚\nâ”‚                   â”‚         â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚                   â”‚         â”‚â”€â”€â”€â”€â–¶â”‚ ClickHouse  â”‚â”€â”€â”€â”€â–¶â”‚  Grafana    â”‚     â”‚\nâ”‚                   â”‚         â”‚     â”‚   (OLAP)    â”‚     â”‚ (Dashboard) â”‚     â”‚\nâ”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚                                                                             â”‚\nâ”‚   M24: Kafka + Spark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚\nâ”‚   M33: OLAP + Dashboards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â—€â”€â”€ CE MODULE                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#clickhouse-le-moteur-olap-ultra-rapide",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#clickhouse-le-moteur-olap-ultra-rapide",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "2. ClickHouse : Le Moteur OLAP Ultra-Rapide",
    "text": "2. ClickHouse : Le Moteur OLAP Ultra-Rapide\n\n2.1 Quâ€™est-ce que ClickHouse ?\nClickHouse est un SGBD OLAP open-source crÃ©Ã© par Yandex, conÃ§u pour : - RequÃªtes analytiques sur des milliards de lignes - Latence de millisecondes - Ingestion Ã  haute vitesse (millions de lignes/sec)\n\n\n2.2 Pourquoi ClickHouse est Rapide ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CLICKHOUSE : SECRETS DE PERFORMANCE                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. COLUMNAR STORAGE                                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\nâ”‚   Row-based:    [id, name, amount, date] [id, name, amount, date] ...      â”‚\nâ”‚   Column-based: [id, id, id...] [name, name...] [amount, amount...] âœ…     â”‚\nâ”‚                                                                             â”‚\nâ”‚   â†’ Lit uniquement les colonnes nÃ©cessaires                                â”‚\nâ”‚   â†’ Compression excellente (valeurs similaires groupÃ©es)                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   2. VECTORIZED EXECUTION                                                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚\nâ”‚   Traite les donnÃ©es par blocs (SIMD), pas ligne par ligne                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   3. DATA SKIPPING                                                          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚\nâ”‚   Indexes sparse + min/max par granule â†’ skip des blocs inutiles           â”‚\nâ”‚                                                                             â”‚\nâ”‚   4. COMPRESSION                                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   LZ4/ZSTD par dÃ©faut, 10-20x compression ratio                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.3 Architecture ClickHouse\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CLICKHOUSE ARCHITECTURE                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                         CLUSTER                                     â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚\nâ”‚   â”‚   â”‚   Shard 1   â”‚   â”‚   Shard 2   â”‚   â”‚   Shard 3   â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚             â”‚   â”‚             â”‚   â”‚             â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â”‚Replica 1â”‚ â”‚   â”‚ â”‚Replica 1â”‚ â”‚   â”‚ â”‚Replica 1â”‚ â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â”‚Replica 2â”‚ â”‚   â”‚ â”‚Replica 2â”‚ â”‚   â”‚ â”‚Replica 2â”‚ â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚              â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   Sharding: Distribution horizontale des donnÃ©es                   â”‚  â”‚\nâ”‚   â”‚   Replication: Haute disponibilitÃ©                                 â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                      ZOOKEEPER / CLICKHOUSE KEEPER                  â”‚  â”‚\nâ”‚   â”‚                      (coordination, replication)                    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.4 Installation avec Docker\n# DÃ©marrer ClickHouse (single node)\ndocker run -d \\\n    --name clickhouse-server \\\n    -p 8123:8123 \\\n    -p 9000:9000 \\\n    -v clickhouse_data:/var/lib/clickhouse \\\n    -v clickhouse_logs:/var/log/clickhouse-server \\\n    clickhouse/clickhouse-server:latest\n\n# AccÃ©der au client CLI\ndocker exec -it clickhouse-server clickhouse-client\n\n# Ou via HTTP (port 8123)\ncurl 'http://localhost:8123/?query=SELECT%201'\n\n\n2.5 Docker Compose (ClickHouse + Kafka + Grafana)\n# docker-compose.yaml\nversion: '3.8'\n\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n\n  clickhouse:\n    image: clickhouse/clickhouse-server:latest\n    ports:\n      - \"8123:8123\"  # HTTP\n      - \"9000:9000\"  # Native\n    volumes:\n      - clickhouse_data:/var/lib/clickhouse\n    ulimits:\n      nofile:\n        soft: 262144\n        hard: 262144\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      GF_INSTALL_PLUGINS: grafana-clickhouse-datasource\n    volumes:\n      - grafana_data:/var/lib/grafana\n\nvolumes:\n  clickhouse_data:\n  grafana_data:\n# DÃ©marrer tout\ndocker-compose up -d\n\n# AccÃ¨s :\n# - ClickHouse : http://localhost:8123\n# - Grafana : http://localhost:3000 (admin/admin)\n\n\n2.6 Table Engines\nClickHouse propose diffÃ©rents engines selon le use case :\n\n\n\n\n\n\n\n\nEngine\nUse Case\nCaractÃ©ristiques\n\n\n\n\nMergeTree\nAnalytics standard\nLe plus utilisÃ©, tri, partitioning\n\n\nReplacingMergeTree\nDÃ©duplication\nGarde derniÃ¨re version par clÃ©\n\n\nSummingMergeTree\nPrÃ©-agrÃ©gation\nSomme automatique par clÃ©\n\n\nAggregatingMergeTree\nAgrÃ©gations complexes\nStates dâ€™agrÃ©gation\n\n\nKafka\nIngestion Kafka\nConsomme directement un topic\n\n\nBuffer\nWrite buffering\nAccumule avant dâ€™Ã©crire\n\n\n\n\n\n2.7 CrÃ©er une Table MergeTree\n\n\nVoir le code\n# Exemple SQL ClickHouse\n\ncreate_table_sql = \"\"\"\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- TABLE : events (MergeTree)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCREATE TABLE IF NOT EXISTS events\n(\n    -- Colonnes\n    event_id        UUID DEFAULT generateUUIDv4(),\n    event_time      DateTime64(3),          -- Millisecond precision\n    event_date      Date DEFAULT toDate(event_time),\n    user_id         String,\n    event_type      LowCardinality(String), -- OptimisÃ© pour peu de valeurs distinctes\n    page            String,\n    country         LowCardinality(String),\n    device          LowCardinality(String),\n    session_id      String,\n    duration_ms     UInt32,\n    revenue         Decimal(10, 2) DEFAULT 0,\n    properties      String                   -- JSON stockÃ© comme String\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)           -- Partition par mois\nORDER BY (event_date, event_type, user_id)  -- ClÃ© de tri (crucial pour performance)\nTTL event_date + INTERVAL 90 DAY            -- Retention 90 jours\nSETTINGS index_granularity = 8192;          -- GranularitÃ© de l'index\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- InsÃ©rer des donnÃ©es\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nINSERT INTO events (event_time, user_id, event_type, page, country, device, session_id, duration_ms, revenue)\nVALUES\n    (now(), 'user_001', 'page_view', '/home', 'FR', 'mobile', 'sess_abc', 1500, 0),\n    (now(), 'user_001', 'click', '/products', 'FR', 'mobile', 'sess_abc', 200, 0),\n    (now(), 'user_002', 'purchase', '/checkout', 'US', 'desktop', 'sess_xyz', 5000, 99.99),\n    (now(), 'user_003', 'page_view', '/home', 'DE', 'tablet', 'sess_123', 800, 0);\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- RequÃªtes analytiques\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- Events par type aujourd'hui\nSELECT \n    event_type,\n    count() AS event_count,\n    uniq(user_id) AS unique_users,\n    avg(duration_ms) AS avg_duration\nFROM events\nWHERE event_date = today()\nGROUP BY event_type\nORDER BY event_count DESC;\n\n-- Revenue par pays (derniÃ¨re heure)\nSELECT \n    country,\n    sum(revenue) AS total_revenue,\n    count() AS purchases\nFROM events\nWHERE event_type = 'purchase'\n  AND event_time &gt;= now() - INTERVAL 1 HOUR\nGROUP BY country\nORDER BY total_revenue DESC;\n\n-- Funnel analysis\nSELECT\n    countIf(event_type = 'page_view') AS views,\n    countIf(event_type = 'click') AS clicks,\n    countIf(event_type = 'purchase') AS purchases,\n    round(clicks / views * 100, 2) AS click_rate,\n    round(purchases / clicks * 100, 2) AS conversion_rate\nFROM events\nWHERE event_date = today();\n\"\"\"\n\nprint(create_table_sql)\n\n\n\n\n2.8 ORDER BY : La ClÃ© de la Performance\nLe ORDER BY est crucial dans ClickHouse. Il dÃ©finit : - Lâ€™ordre physique des donnÃ©es sur disque - Lâ€™index primaire (sparse index) - Les colonnes Ã  utiliser dans les filtres WHERE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ORDER BY BEST PRACTICES                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃˆGLE 1 : Mettre les colonnes de filtre frÃ©quent en premier              â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚   ORDER BY (date, user_id, event_type)                                     â”‚\nâ”‚             ^^^^                                                            â”‚\nâ”‚   Si tu filtres souvent par date, mets-la en premier                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃˆGLE 2 : Du moins cardinal au plus cardinal                             â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚\nâ”‚   ORDER BY (country, city, user_id)                                        â”‚\nâ”‚             ~200      ~50K   ~10M  valeurs distinctes                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃˆGLE 3 : Ne pas mettre trop de colonnes                                 â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚\nâ”‚   3-5 colonnes max, sinon l'index grossit trop                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n-- BON : filtre sur les premiÃ¨res colonnes du ORDER BY\nSELECT * FROM events\nWHERE event_date = '2024-01-15' AND event_type = 'purchase';\n\n-- MOINS BON : filtre sur une colonne non dans ORDER BY\nSELECT * FROM events\nWHERE session_id = 'abc123';  -- Full scan possible",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#ingestion-kafka-clickhouse",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#ingestion-kafka-clickhouse",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "3. Ingestion Kafka â†’ ClickHouse",
    "text": "3. Ingestion Kafka â†’ ClickHouse\n\n3.1 Architecture dâ€™Ingestion\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    KAFKA â†’ CLICKHOUSE INGESTION                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚   â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚ Kafka Engineâ”‚â”€â”€â”€â”€â–¶â”‚ Materializedâ”‚â”€â”€â”€â”€â–¶â”‚ MergeTree   â”‚ â”‚\nâ”‚   â”‚  Topic  â”‚     â”‚   (source)  â”‚     â”‚    View     â”‚     â”‚  (storage)  â”‚ â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                                                             â”‚\nâ”‚   Le pattern recommandÃ© :                                                  â”‚\nâ”‚   1. Table Kafka Engine consomme le topic                                  â”‚\nâ”‚   2. Materialized View transforme et insÃ¨re dans la table finale          â”‚\nâ”‚   3. Table MergeTree stocke les donnÃ©es                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 Configuration ComplÃ¨te\n\n\nVoir le code\n# Configuration Kafka â†’ ClickHouse\n\nkafka_ingestion_sql = \"\"\"\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- Ã‰TAPE 1 : Table de stockage final (MergeTree)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCREATE TABLE IF NOT EXISTS events_final\n(\n    event_time      DateTime64(3),\n    event_date      Date DEFAULT toDate(event_time),\n    user_id         String,\n    event_type      LowCardinality(String),\n    page            String,\n    country         LowCardinality(String),\n    amount          Decimal(10, 2),\n    _kafka_topic    LowCardinality(String),\n    _kafka_offset   UInt64,\n    _inserted_at    DateTime DEFAULT now()\n)\nENGINE = MergeTree()\nPARTITION BY toYYYYMM(event_date)\nORDER BY (event_date, event_type, user_id)\nTTL event_date + INTERVAL 180 DAY;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- Ã‰TAPE 2 : Table Kafka Engine (source)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCREATE TABLE IF NOT EXISTS events_kafka\n(\n    raw String\n)\nENGINE = Kafka\nSETTINGS\n    kafka_broker_list = 'kafka:29092',\n    kafka_topic_list = 'events',\n    kafka_group_name = 'clickhouse_consumer',\n    kafka_format = 'JSONAsString',\n    kafka_num_consumers = 2,                    -- ParallÃ©lisme\n    kafka_max_block_size = 65536,\n    kafka_skip_broken_messages = 100;           -- TolÃ©rance aux erreurs\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- Ã‰TAPE 3 : Materialized View (transformation + insertion)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS events_kafka_mv\nTO events_final\nAS SELECT\n    -- Parser le JSON\n    parseDateTime64BestEffort(JSONExtractString(raw, 'timestamp')) AS event_time,\n    JSONExtractString(raw, 'user_id') AS user_id,\n    JSONExtractString(raw, 'event_type') AS event_type,\n    JSONExtractString(raw, 'page') AS page,\n    JSONExtractString(raw, 'country') AS country,\n    toDecimal64(JSONExtractFloat(raw, 'amount'), 2) AS amount,\n    _topic AS _kafka_topic,\n    _offset AS _kafka_offset\nFROM events_kafka;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- VÃ©rifier l'ingestion\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- Nombre d'events ingÃ©rÃ©s\nSELECT count() FROM events_final;\n\n-- Lag Kafka (derniers offsets)\nSELECT \n    _kafka_topic,\n    max(_kafka_offset) AS latest_offset,\n    max(_inserted_at) AS last_insert\nFROM events_final\nGROUP BY _kafka_topic;\n\n-- Events par minute (monitoring)\nSELECT \n    toStartOfMinute(event_time) AS minute,\n    count() AS events\nFROM events_final\nWHERE event_time &gt;= now() - INTERVAL 1 HOUR\nGROUP BY minute\nORDER BY minute DESC\nLIMIT 10;\n\"\"\"\n\nprint(kafka_ingestion_sql)\n\n\n\n\n3.3 Producer Python pour Tester\n\n\nVoir le code\n# Producer Kafka pour envoyer des events\n\nproducer_code = '''\nfrom kafka import KafkaProducer\nimport json\nimport random\nfrom datetime import datetime\nimport time\n\nproducer = KafkaProducer(\n    bootstrap_servers=['localhost:9092'],\n    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n)\n\nevent_types = ['page_view', 'click', 'scroll', 'purchase', 'signup']\npages = ['/home', '/products', '/cart', '/checkout', '/profile']\ncountries = ['FR', 'US', 'DE', 'GB', 'ES', 'IT']\n\ndef generate_event():\n    event_type = random.choice(event_types)\n    return {\n        'timestamp': datetime.utcnow().isoformat(),\n        'user_id': f'user_{random.randint(1, 1000):04d}',\n        'event_type': event_type,\n        'page': random.choice(pages),\n        'country': random.choice(countries),\n        'amount': round(random.uniform(10, 500), 2) if event_type == 'purchase' else 0\n    }\n\n# Envoyer des events en continu\nprint(\"Sending events to Kafka...\")\ntry:\n    while True:\n        event = generate_event()\n        producer.send('events', value=event)\n        print(f\"Sent: {event['event_type']} from {event['country']}\")\n        time.sleep(0.1)  # 10 events/sec\nexcept KeyboardInterrupt:\n    print(\"Stopped\")\nfinally:\n    producer.close()\n'''\n\nprint(\"# kafka_producer.py\")\nprint(producer_code)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#materialized-views-pour-prÃ©-agrÃ©gation",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#materialized-views-pour-prÃ©-agrÃ©gation",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "4. Materialized Views pour PrÃ©-AgrÃ©gation",
    "text": "4. Materialized Views pour PrÃ©-AgrÃ©gation\n\n4.1 Pourquoi PrÃ©-AgrÃ©ger ?\n\n\n\n\n\n\n\n\n\nApproche\nQuery Time\nStorage\nFlexibilitÃ©\n\n\n\n\nRaw data + query Ã  la volÃ©e\nLent sur gros volumes\nMinimal\nMaximum\n\n\nMaterialized View\nUltra-rapide\nModÃ©rÃ©\nPrÃ©-dÃ©fini\n\n\n\n\n\n4.2 SummingMergeTree : AgrÃ©gation Automatique\n\n\nVoir le code\n# Materialized View avec SummingMergeTree\n\nmv_summing_sql = \"\"\"\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- AGRÃ‰GATION HORAIRE : events par type, pays, heure\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- Table de destination (SummingMergeTree)\nCREATE TABLE IF NOT EXISTS events_hourly\n(\n    event_hour      DateTime,\n    event_type      LowCardinality(String),\n    country         LowCardinality(String),\n    event_count     UInt64,\n    unique_users    AggregateFunction(uniq, String),  -- HyperLogLog\n    total_amount    Decimal(18, 2),\n    avg_amount      AggregateFunction(avg, Decimal(10, 2))\n)\nENGINE = SummingMergeTree((event_count, total_amount))\nPARTITION BY toYYYYMM(event_hour)\nORDER BY (event_hour, event_type, country);\n\n-- Materialized View qui alimente la table\nCREATE MATERIALIZED VIEW IF NOT EXISTS events_hourly_mv\nTO events_hourly\nAS SELECT\n    toStartOfHour(event_time) AS event_hour,\n    event_type,\n    country,\n    count() AS event_count,\n    uniqState(user_id) AS unique_users,        -- State pour merge\n    sum(amount) AS total_amount,\n    avgState(amount) AS avg_amount             -- State pour merge\nFROM events_final\nGROUP BY event_hour, event_type, country;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- REQUÃŠTES SUR L'AGRÃ‰GAT (ultra-rapides !)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- Events par heure (derniÃ¨res 24h)\nSELECT \n    event_hour,\n    sum(event_count) AS total_events,\n    uniqMerge(unique_users) AS unique_users,   -- Merge les HLL\n    sum(total_amount) AS revenue\nFROM events_hourly\nWHERE event_hour &gt;= now() - INTERVAL 24 HOUR\nGROUP BY event_hour\nORDER BY event_hour;\n\n-- Top pays par revenue\nSELECT \n    country,\n    sum(event_count) AS events,\n    sum(total_amount) AS revenue,\n    avgMerge(avg_amount) AS avg_order_value\nFROM events_hourly\nWHERE event_type = 'purchase'\n  AND event_hour &gt;= today()\nGROUP BY country\nORDER BY revenue DESC;\n\"\"\"\n\nprint(mv_summing_sql)\n\n\n\n\nVoir le code\n# Materialized View pour mÃ©triques temps rÃ©el (par minute)\n\nmv_realtime_sql = \"\"\"\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- MÃ‰TRIQUES TEMPS RÃ‰EL (par minute, pour dashboards)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nCREATE TABLE IF NOT EXISTS events_minute\n(\n    event_minute    DateTime,\n    event_type      LowCardinality(String),\n    event_count     UInt64,\n    unique_users    UInt64,\n    total_amount    Decimal(18, 2)\n)\nENGINE = SummingMergeTree((event_count, unique_users, total_amount))\nORDER BY (event_minute, event_type)\nTTL event_minute + INTERVAL 7 DAY;  -- Garder 7 jours seulement\n\nCREATE MATERIALIZED VIEW IF NOT EXISTS events_minute_mv\nTO events_minute\nAS SELECT\n    toStartOfMinute(event_time) AS event_minute,\n    event_type,\n    count() AS event_count,\n    uniq(user_id) AS unique_users,\n    sum(amount) AS total_amount\nFROM events_final\nGROUP BY event_minute, event_type;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- QUERIES POUR DASHBOARD TEMPS RÃ‰EL\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n-- DerniÃ¨res 5 minutes (refresh toutes les 5 sec)\nSELECT \n    event_minute,\n    sum(event_count) AS events,\n    sum(unique_users) AS users,\n    sum(total_amount) AS revenue\nFROM events_minute\nWHERE event_minute &gt;= now() - INTERVAL 5 MINUTE\nGROUP BY event_minute\nORDER BY event_minute;\n\n-- Events par seconde (approximation)\nSELECT \n    sum(event_count) / 60 AS events_per_second\nFROM events_minute\nWHERE event_minute &gt;= now() - INTERVAL 1 MINUTE;\n\n-- Comparaison vs mÃªme heure hier\nSELECT \n    'today' AS period,\n    sum(event_count) AS events,\n    sum(total_amount) AS revenue\nFROM events_minute\nWHERE event_minute &gt;= toStartOfHour(now())\n\nUNION ALL\n\nSELECT \n    'yesterday' AS period,\n    sum(event_count) AS events,\n    sum(total_amount) AS revenue\nFROM events_minute\nWHERE event_minute &gt;= toStartOfHour(now() - INTERVAL 1 DAY)\n  AND event_minute &lt; toStartOfHour(now() - INTERVAL 1 DAY) + INTERVAL 1 HOUR;\n\"\"\"\n\nprint(mv_realtime_sql)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#alternatives-apache-druid-pinot",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#alternatives-apache-druid-pinot",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "5. Alternatives : Apache Druid & Pinot",
    "text": "5. Alternatives : Apache Druid & Pinot\n\n5.1 Apache Druid\nDruid est un OLAP engine optimisÃ© pour les time-series et le real-time.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    APACHE DRUID ARCHITECTURE                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚   â”‚   Kafka     â”‚â”€â”€â–¶â”‚   Middle    â”‚â”€â”€â–¶â”‚   Historicalâ”‚                      â”‚\nâ”‚   â”‚   (stream)  â”‚   â”‚   Manager   â”‚   â”‚   (segments)â”‚                      â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚                              â”‚\nâ”‚   â”‚   Batch     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\nâ”‚   â”‚   (HDFS/S3) â”‚                                                           â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                           â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                              â”‚\nâ”‚                           â”‚     Broker      â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€ Queries              â”‚\nâ”‚                           â”‚   (scatter/gather)                              â”‚\nâ”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCaractÃ©ristiques Druid : - Columnar storage avec compression - Ingestion real-time ET batch - Roll-up automatique (prÃ©-agrÃ©gation Ã  lâ€™ingestion) - OptimisÃ© pour GROUP BY sur time-series\nQuand utiliser Druid : - Time-series analytics (monitoring, IoT) - TrÃ¨s hauts volumes (trillions de rows) - Besoin de roll-up Ã  lâ€™ingestion\n\n\n5.2 Apache Pinot\nPinot est un OLAP engine crÃ©Ã© par LinkedIn, optimisÃ© pour les user-facing analytics.\nCaractÃ©ristiques Pinot : - Latence ultra-basse (&lt;100ms P99) - OptimisÃ© pour queries concurrentes (1000+ QPS) - Star-tree index pour agrÃ©gations prÃ©-calculÃ©es - Upsert support (contrairement Ã  Druid)\nQuand utiliser Pinot : - Analytics user-facing (dashboards clients) - TrÃ¨s haute concurrence - Besoin dâ€™upserts\n\n\n5.3 Comparaison ClickHouse vs Druid vs Pinot\n\n\n\nFeature\nClickHouse\nDruid\nPinot\n\n\n\n\nType\nOLAP DB\nTime-series OLAP\nUser-facing OLAP\n\n\nSQL\nFull SQL\nDruid SQL (limitÃ©)\nPQL + SQL\n\n\nLatency\n~10-100ms\n~100-500ms\n~10-50ms\n\n\nConcurrency\n~100\n~100\n~1000+\n\n\nUpserts\nOui (ReplacingMergeTree)\nNon\nOui\n\n\nJoins\nOui\nLimitÃ©\nLimitÃ©\n\n\nComplexity\nSimple\nComplexe\nMedium\n\n\nBest for\nGeneral analytics\nTime-series\nUser-facing\n\n\n\n\n\n5.4 Recommandations\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    QUEL OLAP CHOISIR ?                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  Tu veux de l'analytics interne (dashboards, ad-hoc) ?              â”‚  â”‚\nâ”‚   â”‚  â†’ ClickHouse (simple, SQL complet, flexible)                       â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  Tu as des time-series Ã  trÃ¨s haut volume avec roll-up ?            â”‚  â”‚\nâ”‚   â”‚  â†’ Druid (optimisÃ© pour Ã§a, mais plus complexe)                     â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  Tu as des dashboards user-facing avec 1000+ users concurrents ?    â”‚  â”‚\nâ”‚   â”‚  â†’ Pinot (conÃ§u pour Ã§a, latence garantie)                          â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   Dans le doute : commence par ClickHouse (plus simple Ã  opÃ©rer)           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#real-time-dashboards",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#real-time-dashboards",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "6. Real-Time Dashboards",
    "text": "6. Real-Time Dashboards\n\n6.1 Architecture Dashboard Temps RÃ©el\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    REAL-TIME DASHBOARD ARCHITECTURE                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   Option 1: PULL (Polling)                                                  â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   Dashboard â”€â”€(every 5s)â”€â”€â–¶ ClickHouse â”€â”€â–¶ Response                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Simple                                                                 â”‚\nâ”‚   âŒ Latence = intervalle de refresh                                        â”‚\nâ”‚   âŒ Charge DB si beaucoup de clients                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Option 2: PUSH (WebSocket/SSE)                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   Kafka â”€â”€â–¶ Stream Processor â”€â”€â–¶ WebSocket â”€â”€â–¶ Dashboard                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Latence minimale                                                       â”‚\nâ”‚   âœ… Efficient (pas de polling)                                             â”‚\nâ”‚   âŒ Plus complexe Ã  implÃ©menter                                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   Option 3: HYBRIDE (recommandÃ©)                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ DonnÃ©es historiques : Query ClickHouse                                 â”‚\nâ”‚   â€¢ MÃ©triques live : WebSocket depuis Kafka                                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.2 Grafana + ClickHouse\nGrafana est lâ€™outil le plus populaire pour les dashboards temps rÃ©el.\n\nConfiguration du Data Source\n# grafana/provisioning/datasources/clickhouse.yaml\napiVersion: 1\n\ndatasources:\n  - name: ClickHouse\n    type: grafana-clickhouse-datasource\n    access: proxy\n    url: http://clickhouse:8123\n    jsonData:\n      defaultDatabase: default\n      port: 9000\n      server: clickhouse\n      username: default\n      tlsSkipVerify: true\n    secureJsonData:\n      password: \"\"\n\n\nExemples de Queries pour Panels\n\n\nVoir le code\n# Queries Grafana pour ClickHouse\n\ngrafana_queries = \"\"\"\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 1 : Time Series - Events par minute\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT \n    $__timeInterval(event_minute) AS time,\n    sum(event_count) AS events\nFROM events_minute\nWHERE $__timeFilter(event_minute)\nGROUP BY time\nORDER BY time;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 2 : Stat - Total events (derniÃ¨re heure)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT sum(event_count) AS total_events\nFROM events_minute\nWHERE event_minute &gt;= now() - INTERVAL 1 HOUR;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 3 : Pie Chart - Events par type\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT \n    event_type,\n    sum(event_count) AS count\nFROM events_minute\nWHERE event_minute &gt;= now() - INTERVAL 1 HOUR\nGROUP BY event_type\nORDER BY count DESC;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 4 : Bar Chart - Top 10 pays par revenue\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT \n    country,\n    sum(total_amount) AS revenue\nFROM events_hourly\nWHERE event_hour &gt;= today()\n  AND event_type = 'purchase'\nGROUP BY country\nORDER BY revenue DESC\nLIMIT 10;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 5 : Gauge - Conversion rate (temps rÃ©el)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT \n    round(\n        sumIf(event_count, event_type = 'purchase') / \n        sumIf(event_count, event_type = 'page_view') * 100, \n        2\n    ) AS conversion_rate\nFROM events_minute\nWHERE event_minute &gt;= now() - INTERVAL 1 HOUR;\n\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n-- PANEL 6 : Table - Derniers events (live)\n-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSELECT \n    event_time,\n    user_id,\n    event_type,\n    country,\n    amount\nFROM events_final\nWHERE event_time &gt;= now() - INTERVAL 5 MINUTE\nORDER BY event_time DESC\nLIMIT 100;\n\"\"\"\n\nprint(grafana_queries)\n\n\n\n\n\n6.3 Dashboard JSON (Import dans Grafana)\n\n\nVoir le code\nimport json\n\ngrafana_dashboard = {\n    \"dashboard\": {\n        \"title\": \"Real-Time Analytics\",\n        \"tags\": [\"clickhouse\", \"realtime\"],\n        \"timezone\": \"browser\",\n        \"refresh\": \"5s\",  # Auto-refresh toutes les 5 secondes\n        \"panels\": [\n            {\n                \"id\": 1,\n                \"title\": \"Events per Minute\",\n                \"type\": \"timeseries\",\n                \"gridPos\": {\"x\": 0, \"y\": 0, \"w\": 12, \"h\": 8},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT $__timeInterval(event_minute) AS time, sum(event_count) AS events FROM events_minute WHERE $__timeFilter(event_minute) GROUP BY time ORDER BY time\",\n                    \"format\": \"time_series\"\n                }]\n            },\n            {\n                \"id\": 2,\n                \"title\": \"Total Events (1h)\",\n                \"type\": \"stat\",\n                \"gridPos\": {\"x\": 12, \"y\": 0, \"w\": 4, \"h\": 4},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT sum(event_count) AS total FROM events_minute WHERE event_minute &gt;= now() - INTERVAL 1 HOUR\",\n                    \"format\": \"table\"\n                }],\n                \"options\": {\n                    \"colorMode\": \"value\",\n                    \"graphMode\": \"none\"\n                }\n            },\n            {\n                \"id\": 3,\n                \"title\": \"Revenue (1h)\",\n                \"type\": \"stat\",\n                \"gridPos\": {\"x\": 16, \"y\": 0, \"w\": 4, \"h\": 4},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT sum(total_amount) AS revenue FROM events_minute WHERE event_minute &gt;= now() - INTERVAL 1 HOUR\",\n                    \"format\": \"table\"\n                }],\n                \"options\": {\n                    \"colorMode\": \"value\"\n                },\n                \"fieldConfig\": {\n                    \"defaults\": {\n                        \"unit\": \"currencyEUR\"\n                    }\n                }\n            },\n            {\n                \"id\": 4,\n                \"title\": \"Events by Type\",\n                \"type\": \"piechart\",\n                \"gridPos\": {\"x\": 12, \"y\": 4, \"w\": 8, \"h\": 8},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT event_type, sum(event_count) AS count FROM events_minute WHERE event_minute &gt;= now() - INTERVAL 1 HOUR GROUP BY event_type\",\n                    \"format\": \"table\"\n                }]\n            },\n            {\n                \"id\": 5,\n                \"title\": \"Top Countries by Revenue\",\n                \"type\": \"barchart\",\n                \"gridPos\": {\"x\": 0, \"y\": 8, \"w\": 12, \"h\": 8},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT country, sum(total_amount) AS revenue FROM events_hourly WHERE event_hour &gt;= today() AND event_type = 'purchase' GROUP BY country ORDER BY revenue DESC LIMIT 10\",\n                    \"format\": \"table\"\n                }]\n            },\n            {\n                \"id\": 6,\n                \"title\": \"Live Events\",\n                \"type\": \"table\",\n                \"gridPos\": {\"x\": 12, \"y\": 12, \"w\": 12, \"h\": 8},\n                \"datasource\": \"ClickHouse\",\n                \"targets\": [{\n                    \"rawSql\": \"SELECT event_time, user_id, event_type, country, amount FROM events_final WHERE event_time &gt;= now() - INTERVAL 5 MINUTE ORDER BY event_time DESC LIMIT 50\",\n                    \"format\": \"table\"\n                }]\n            }\n        ]\n    },\n    \"overwrite\": True\n}\n\nprint(\"ğŸ“Š Grafana Dashboard JSON:\")\nprint(json.dumps(grafana_dashboard, indent=2)[:2000] + \"...\")\n\n\n\n\n6.4 Apache Superset (Alternative)\nApache Superset est une alternative open-source Ã  Grafana, plus orientÃ©e BI.\n# Docker Compose pour Superset\ndocker run -d -p 8088:8088 \\\n    --name superset \\\n    -e SUPERSET_SECRET_KEY='your-secret-key' \\\n    apache/superset\n\n# Setup initial\ndocker exec -it superset superset fab create-admin \\\n    --username admin \\\n    --firstname Admin \\\n    --lastname User \\\n    --email admin@example.com \\\n    --password admin\n\ndocker exec -it superset superset db upgrade\ndocker exec -it superset superset init\n\n# AccÃ¨s : http://localhost:8088\nConnexion ClickHouse dans Superset :\nclickhousedb://default:@clickhouse:8123/default\n\n\n6.5 Comparaison Grafana vs Superset\n\n\n\nFeature\nGrafana\nSuperset\n\n\n\n\nFocus\nMonitoring, time-series\nBI, exploration\n\n\nRefresh\nExcellent (auto, push)\nBon (polling)\n\n\nSQL Editor\nBasique\nExcellent\n\n\nExploration\nLimitÃ©e\nTrÃ¨s bonne\n\n\nAlerting\nIntÃ©grÃ©\nVia plugin\n\n\nBest for\nOps dashboards\nBusiness analytics",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#patterns-et-best-practices",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#patterns-et-best-practices",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "7. Patterns et Best Practices",
    "text": "7. Patterns et Best Practices\n\n7.1 Pre-Aggregation Patterns\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PRE-AGGREGATION PATTERNS                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   PATTERN 1 : Multi-Level Aggregation                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   Raw Events â”€â”€â–¶ Per-Minute â”€â”€â–¶ Per-Hour â”€â”€â–¶ Per-Day                       â”‚\nâ”‚   (dÃ©tail)       (7 jours)      (90 jours)   (1+ an)                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   â†’ Queries rapides Ã  chaque niveau                                        â”‚\nâ”‚   â†’ Retention diffÃ©rente selon granularitÃ©                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   PATTERN 2 : Dimension-Specific Tables                                     â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   events_by_country  (agrÃ©gÃ© par pays)                                     â”‚\nâ”‚   events_by_product  (agrÃ©gÃ© par produit)                                  â”‚\nâ”‚   events_by_user     (agrÃ©gÃ© par user)                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   â†’ Ultra-rapide pour les dimensions connues                               â”‚\nâ”‚   â†’ Moins flexible pour ad-hoc                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n7.2 Tiered Storage\n-- ClickHouse : Storage policies\nCREATE TABLE events_tiered\n(\n    event_time DateTime,\n    event_type String,\n    data String\n)\nENGINE = MergeTree()\nORDER BY event_time\nTTL \n    event_time + INTERVAL 7 DAY TO VOLUME 'hot',     -- SSD\n    event_time + INTERVAL 30 DAY TO VOLUME 'warm',   -- HDD\n    event_time + INTERVAL 365 DAY TO VOLUME 'cold';  -- S3\n\n\n7.3 Retention Policies\n-- TTL pour auto-delete\nALTER TABLE events_minute\nMODIFY TTL event_minute + INTERVAL 7 DAY;\n\n-- Voir l'espace utilisÃ©\nSELECT \n    table,\n    formatReadableSize(sum(bytes_on_disk)) AS size,\n    sum(rows) AS rows\nFROM system.parts\nWHERE active\nGROUP BY table\nORDER BY sum(bytes_on_disk) DESC;\n\n\n7.4 Monitoring des Pipelines\n-- Lag d'ingestion (Kafka offset vs current)\nSELECT \n    max(_kafka_offset) AS latest_offset,\n    max(event_time) AS latest_event,\n    dateDiff('second', max(event_time), now()) AS lag_seconds\nFROM events_final;\n\n-- Throughput (events/sec)\nSELECT \n    toStartOfMinute(event_time) AS minute,\n    count() / 60 AS events_per_second\nFROM events_final\nWHERE event_time &gt;= now() - INTERVAL 10 MINUTE\nGROUP BY minute\nORDER BY minute DESC;\n\n-- Alerter si lag &gt; 5 minutes\nSELECT \n    CASE \n        WHEN dateDiff('minute', max(event_time), now()) &gt; 5 \n        THEN 'ALERT: Ingestion lag &gt; 5 min!'\n        ELSE 'OK'\n    END AS status\nFROM events_final;\n\n\n7.5 Cost Optimization\n\n\n\nTechnique\nImpact\nImplÃ©mentation\n\n\n\n\nCompression\n-80% storage\nLZ4 (dÃ©faut) ou ZSTD\n\n\nTTL\nLimite le volume\nTTL date + INTERVAL X DAY\n\n\nPartitioning\nPruning efficace\nPARTITION BY toYYYYMM(date)\n\n\nMaterialized Views\n-90% query cost\nPrÃ©-agrÃ©gation\n\n\nLowCardinality\n-50% pour enums\nLowCardinality(String)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#client-python-pour-clickhouse",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#client-python-pour-clickhouse",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "8. Client Python pour ClickHouse",
    "text": "8. Client Python pour ClickHouse\n\n\nVoir le code\n# Client Python pour ClickHouse\n\npython_client_code = '''\n# pip install clickhouse-connect\n\nimport clickhouse_connect\nfrom datetime import datetime, timedelta\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# CONNEXION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nclient = clickhouse_connect.get_client(\n    host='localhost',\n    port=8123,\n    username='default',\n    password=''\n)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# QUERIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Query simple\nresult = client.query(\"SELECT count() FROM events_final\")\nprint(f\"Total events: {result.result_rows[0][0]}\")\n\n# Query avec paramÃ¨tres\nresult = client.query(\n    \"SELECT event_type, count() FROM events_final \"\n    \"WHERE event_time &gt;= {start:DateTime} \"\n    \"GROUP BY event_type\",\n    parameters={\"start\": datetime.now() - timedelta(hours=1)}\n)\n\nfor row in result.result_rows:\n    print(f\"{row[0]}: {row[1]}\")\n\n# Query vers DataFrame\ndf = client.query_df(\n    \"SELECT toStartOfMinute(event_time) AS minute, count() AS events \"\n    \"FROM events_final \"\n    \"WHERE event_time &gt;= now() - INTERVAL 1 HOUR \"\n    \"GROUP BY minute ORDER BY minute\"\n)\nprint(df.head())\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# INSERT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Insert batch\ndata = [\n    [datetime.now(), \"user_001\", \"page_view\", \"FR\", 0],\n    [datetime.now(), \"user_002\", \"click\", \"US\", 0],\n    [datetime.now(), \"user_003\", \"purchase\", \"DE\", 99.99],\n]\n\nclient.insert(\n    \"events_final\",\n    data,\n    column_names=[\"event_time\", \"user_id\", \"event_type\", \"country\", \"amount\"]\n)\n\n# Insert depuis DataFrame\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"event_time\": [datetime.now()] * 100,\n    \"user_id\": [f\"user_{i:04d}\" for i in range(100)],\n    \"event_type\": [\"page_view\"] * 100,\n    \"country\": [\"FR\"] * 100,\n    \"amount\": [0.0] * 100\n})\n\nclient.insert_df(\"events_final\", df)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ASYNC (pour haute performance)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport asyncio\nimport clickhouse_connect.driver.asyncclient as async_client\n\nasync def async_queries():\n    client = await async_client.create_async_client(host=\"localhost\")\n    \n    result = await client.query(\"SELECT count() FROM events_final\")\n    print(f\"Async count: {result.result_rows[0][0]}\")\n    \n    await client.close()\n\n# asyncio.run(async_queries())\n'''\n\nprint(python_client_code)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#exercices-pratiques",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#exercices-pratiques",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "9. Exercices Pratiques",
    "text": "9. Exercices Pratiques\n\nExercice 1 : Setup ClickHouse + Kafka Ingestion\n\nDÃ©marrer ClickHouse et Kafka avec Docker Compose\nCrÃ©er un topic Kafka events\nCrÃ©er la table events_final (MergeTree)\nCrÃ©er la table Kafka Engine + Materialized View\nEnvoyer des events avec le producer Python\nVÃ©rifier lâ€™ingestion dans ClickHouse\n\n\n\n\nExercice 2 : Materialized Views Multi-Niveaux\nCrÃ©er 3 niveaux dâ€™agrÃ©gation : - events_minute : TTL 7 jours - events_hourly : TTL 90 jours - events_daily : TTL 2 ans\nVÃ©rifier que les queries sur chaque niveau sont rapides.\n\n\n\nExercice 3 : Dashboard Grafana\nCrÃ©er un dashboard avec : - Time series : events par minute - Stats : total events, revenue, unique users - Pie chart : events par type - Table : derniers events live\nConfigurer auto-refresh Ã  5 secondes.\n\n\n\nExercice 4 : Alerting\nCrÃ©er des alertes Grafana pour : - Lag dâ€™ingestion &gt; 5 minutes - Events/sec &lt; 10 (drop de trafic) - Revenue = 0 depuis 30 minutes\n\n\n\nExercice 5 : Benchmark\n\nGÃ©nÃ©rer 10 millions dâ€™events\nComparer les temps de query sur :\n\nTable raw (MergeTree)\nMaterialized View minute\nMaterialized View horaire\n\nDocumenter les gains de performance",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#mini-projet-real-time-analytics-platform",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#mini-projet-real-time-analytics-platform",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "10. Mini-Projet : Real-Time Analytics Platform",
    "text": "10. Mini-Projet : Real-Time Analytics Platform\n\nObjectif\nConstruire une plateforme dâ€™analytics temps rÃ©el complÃ¨te.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   MINI-PROJET : REAL-TIME ANALYTICS                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚   â”‚   Event     â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚ ClickHouse  â”‚                      â”‚\nâ”‚   â”‚  Generator  â”‚     â”‚  Topic  â”‚     â”‚             â”‚                      â”‚\nâ”‚   â”‚  (Python)   â”‚     â”‚         â”‚     â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚  Raw    â”‚ â”‚â”€â”€â”€â”€â–¶â”‚   Grafana   â”‚ â”‚\nâ”‚                                       â”‚ â”‚  Table  â”‚ â”‚     â”‚  Dashboard  â”‚ â”‚\nâ”‚   Throughput:                         â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â”‚     â”‚             â”‚ â”‚\nâ”‚   1000 events/sec                     â”‚      â”‚      â”‚     â”‚  â€¢ Live     â”‚ â”‚\nâ”‚                                       â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â” â”‚     â”‚  â€¢ Refresh  â”‚ â”‚\nâ”‚                                       â”‚ â”‚  MVs    â”‚ â”‚     â”‚    5 sec    â”‚ â”‚\nâ”‚                                       â”‚ â”‚ min/hr  â”‚ â”‚     â”‚             â”‚ â”‚\nâ”‚                                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   MÃ©triques Ã  afficher :                                                   â”‚\nâ”‚   â€¢ Events/minute (time series)                                            â”‚\nâ”‚   â€¢ Revenue temps rÃ©el                                                     â”‚\nâ”‚   â€¢ Top pays, Top produits                                                 â”‚\nâ”‚   â€¢ Conversion funnel                                                      â”‚\nâ”‚   â€¢ Alertes si anomalie                                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLivrables\n\ndocker-compose.yaml : Kafka + ClickHouse + Grafana\nEvent Generator : Python script gÃ©nÃ©rant 1000 events/sec\nClickHouse Schema : Tables + Materialized Views\nGrafana Dashboard : 6+ panels avec auto-refresh\nAlerting : 3+ alertes configurÃ©es\nDocumentation : README avec architecture et setup\n\n\n\nStructure du Projet\nrealtime-analytics/\nâ”œâ”€â”€ docker-compose.yaml\nâ”œâ”€â”€ clickhouse/\nâ”‚   â””â”€â”€ init.sql                # Schema + MVs\nâ”œâ”€â”€ producer/\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ event_generator.py\nâ”œâ”€â”€ grafana/\nâ”‚   â”œâ”€â”€ provisioning/\nâ”‚   â”‚   â”œâ”€â”€ datasources/\nâ”‚   â”‚   â”‚   â””â”€â”€ clickhouse.yaml\nâ”‚   â”‚   â””â”€â”€ dashboards/\nâ”‚   â”‚       â””â”€â”€ realtime.json\nâ”‚   â””â”€â”€ dashboards/\nâ”‚       â””â”€â”€ realtime_analytics.json\nâ””â”€â”€ README.md\n\n\nCritÃ¨res de SuccÃ¨s\n\nIngestion Kafka â†’ ClickHouse fonctionne\nMaterialized Views crÃ©Ã©es (minute + heure)\nDashboard avec 6+ panels\nAuto-refresh 5 secondes\nAlertes configurÃ©es\nLatence &lt; 10 secondes end-to-end",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#ressources",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#ressources",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation\n\nClickHouse Documentation\nGrafana ClickHouse Plugin\nApache Druid Docs\nApache Pinot Docs\n\n\n\nArticles\n\nClickHouse vs Druid vs Pinot\nReal-Time Analytics at Scale\nMaterialized Views Best Practices\n\n\n\nTutoriels\n\nClickHouse + Kafka Tutorial\nGrafana + ClickHouse Setup",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/33_realtime_olap_dashboards.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/33_realtime_olap_dashboards.html#prochaine-Ã©tape",
    "title": "âš¡ Real-Time OLAP & Dashboards",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 34_cloud_data_platform â€” Cloud Data Platforms (AWS/GCP/Azure)\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les OLAP engines et les dashboards temps rÃ©el.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "33 Â· Realtime OLAP & Dashboards"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html",
    "href": "notebooks/advanced/32_data_mesh_contracts.html",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas dÃ©couvrir les architectures Data Mesh et les Data Contracts â€” deux concepts essentiels pour scaler les organisations data. Tu apprendras Ã  passer dâ€™une architecture centralisÃ©e Ã  une approche dÃ©centralisÃ©e oÃ¹ chaque domaine est responsable de ses donnÃ©es.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#prÃ©requis",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#prÃ©requis",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nData Lakehouse (M20)\n\n\nâœ… Requis\nData Quality (M23)\n\n\nâœ… Requis\nOrchestration (M22, M28)\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience en organisation data",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#objectifs-du-module",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#objectifs-du-module",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les 4 principes du Data Mesh\nConcevoir des Data Products avec ownership clair\nÃ‰crire des Data Contracts complets\nImplÃ©menter la validation de contracts avec Soda\nConfigurer DataHub comme Data Catalog\nGÃ©rer la Schema Evolution sans casser les consommateurs\nMettre en place une gouvernance fÃ©dÃ©rÃ©e",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#introduction-au-data-mesh",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#introduction-au-data-mesh",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "1. Introduction au Data Mesh",
    "text": "1. Introduction au Data Mesh\n\n1.1 ProblÃ¨mes de lâ€™Architecture CentralisÃ©e\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ARCHITECTURE CENTRALISÃ‰E (MONOLITHE)                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                               â”‚\nâ”‚   â”‚ Sales   â”‚â”€â”€â”                                                            â”‚\nâ”‚   â”‚ Domain  â”‚  â”‚                                                            â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚                  â”‚      â”‚                 â”‚        â”‚\nâ”‚   â”‚Marketingâ”‚â”€â”€â”¼â”€â”€â”€â”€â”€â–¶â”‚  Central Data    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Data Lake/    â”‚        â”‚\nâ”‚   â”‚ Domain  â”‚  â”‚      â”‚     Team         â”‚      â”‚   Warehouse     â”‚        â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚                  â”‚      â”‚                 â”‚        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  (bottleneck!)   â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚   â”‚ Finance â”‚â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\nâ”‚   â”‚ Domain  â”‚                                                               â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   PROBLÃˆMES :                                                               â”‚\nâ”‚   âŒ Ã‰quipe data = bottleneck (tout passe par eux)                          â”‚\nâ”‚   âŒ Pas de ownership clair (\"c'est pas mes donnÃ©es\")                       â”‚\nâ”‚   âŒ Time-to-market lent (file d'attente de demandes)                       â”‚\nâ”‚   âŒ Ã‰quipe data ne connaÃ®t pas le contexte mÃ©tier                          â”‚\nâ”‚   âŒ ScalabilitÃ© organisationnelle limitÃ©e                                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Les 4 Principes du Data Mesh\nLe Data Mesh est une approche architecturale proposÃ©e par Zhamak Dehghani (Thoughtworks) en 2019.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    LES 4 PRINCIPES DU DATA MESH                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  1ï¸âƒ£ DOMAIN OWNERSHIP                                                 â”‚  â”‚\nâ”‚   â”‚     Chaque domaine mÃ©tier est responsable de ses donnÃ©es             â”‚  â”‚\nâ”‚   â”‚     â†’ Sales gÃ¨re les donnÃ©es sales, Marketing gÃ¨re les donnÃ©es mktg â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  2ï¸âƒ£ DATA AS A PRODUCT                                                â”‚  â”‚\nâ”‚   â”‚     Les donnÃ©es sont traitÃ©es comme un produit avec des clients      â”‚  â”‚\nâ”‚   â”‚     â†’ SLOs, documentation, support, qualitÃ©                          â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  3ï¸âƒ£ SELF-SERVE DATA PLATFORM                                         â”‚  â”‚\nâ”‚   â”‚     Une plateforme qui permet aux domaines d'Ãªtre autonomes          â”‚  â”‚\nâ”‚   â”‚     â†’ Infra, outils, templates, pas besoin de l'Ã©quipe centrale      â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  4ï¸âƒ£ FEDERATED COMPUTATIONAL GOVERNANCE                               â”‚  â”‚\nâ”‚   â”‚     Gouvernance dÃ©centralisÃ©e avec standards globaux                 â”‚  â”‚\nâ”‚   â”‚     â†’ Policies automatisÃ©es, interopÃ©rabilitÃ©, compliance            â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.3 Data Mesh vs Data Lake vs Data Lakehouse\n\n\n\n\n\n\n\n\n\nAspect\nData Lake\nData Lakehouse\nData Mesh\n\n\n\n\nType\nArchitecture technique\nArchitecture technique\nArchitecture organisationnelle\n\n\nFocus\nStockage\nStockage + Analytics\nOrganisation + Ownership\n\n\nCentralisation\nCentralisÃ©\nCentralisÃ©\nDÃ©centralisÃ©\n\n\nOwnership\nÃ‰quipe data\nÃ‰quipe data\nDomaines mÃ©tier\n\n\nCompatibilitÃ©\nâ€”\nâ€”\nPeut utiliser Lakehouse underneath\n\n\n\n\nâš ï¸ Important : Data Mesh nâ€™est PAS une technologie, câ€™est une approche organisationnelle. Tu peux implÃ©menter un Data Mesh avec un Data Lakehouse comme infrastructure.\n\n\n\n1.4 Quand Adopter le Data Mesh ?\nâœ… Data Mesh est adaptÃ© si : - Organisation large (&gt;100 personnes dans la data) - Plusieurs domaines mÃ©tier distincts - Ã‰quipe data centrale = bottleneck avÃ©rÃ© - Domaines ont des Ã©quipes techniques capables - Culture dâ€™ownership et dâ€™autonomie\nâŒ Data Mesh nâ€™est PAS adaptÃ© si : - Petite organisation (&lt;20 personnes data) - Un seul domaine mÃ©tier - Pas assez de maturitÃ© technique dans les domaines - Besoin de contrÃ´le centralisÃ© fort - Ressources limitÃ©es pour la plateforme",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#domain-ownership-data-products",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#domain-ownership-data-products",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "2. Domain Ownership & Data Products",
    "text": "2. Domain Ownership & Data Products\n\n2.1 Identifier les Domaines\nUn domaine correspond gÃ©nÃ©ralement Ã  un bounded context (DDD - Domain-Driven Design).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    EXEMPLE : E-COMMERCE                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚   ORDERS    â”‚   â”‚  CUSTOMERS  â”‚   â”‚  PRODUCTS   â”‚   â”‚  MARKETING  â”‚   â”‚\nâ”‚   â”‚   Domain    â”‚   â”‚   Domain    â”‚   â”‚   Domain    â”‚   â”‚   Domain    â”‚   â”‚\nâ”‚   â”‚             â”‚   â”‚             â”‚   â”‚             â”‚   â”‚             â”‚   â”‚\nâ”‚   â”‚ - orders    â”‚   â”‚ - customers â”‚   â”‚ - products  â”‚   â”‚ - campaigns â”‚   â”‚\nâ”‚   â”‚ - payments  â”‚   â”‚ - addresses â”‚   â”‚ - inventory â”‚   â”‚ - emails    â”‚   â”‚\nâ”‚   â”‚ - refunds   â”‚   â”‚ - segments  â”‚   â”‚ - pricing   â”‚   â”‚ - analytics â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                                             â”‚\nâ”‚   Chaque domaine :                                                          â”‚\nâ”‚   âœ“ A son Ã©quipe dÃ©diÃ©e                                                    â”‚\nâ”‚   âœ“ PossÃ¨de ses donnÃ©es                                                    â”‚\nâ”‚   âœ“ Expose des Data Products                                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 Quâ€™est-ce quâ€™un Data Product ?\nUn Data Product est un ensemble de donnÃ©es exposÃ© par un domaine pour Ãªtre consommÃ© par dâ€™autres.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         DATA PRODUCT                                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                      orders_fact                                    â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚\nâ”‚   â”‚   â”‚   DATA      â”‚   â”‚   CODE      â”‚   â”‚  METADATA   â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚             â”‚   â”‚             â”‚   â”‚             â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ Delta Table â”‚   â”‚ Spark Jobs  â”‚   â”‚ Schema      â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚ (storage)   â”‚   â”‚ Airflow DAG â”‚   â”‚ Contract    â”‚              â”‚  â”‚\nâ”‚   â”‚   â”‚             â”‚   â”‚ dbt models  â”‚   â”‚ SLOs        â”‚              â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ Docs        â”‚              â”‚  â”‚\nâ”‚   â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚\nâ”‚   â”‚   â”‚                    OUTPUT PORTS                             â”‚  â”‚  â”‚\nâ”‚   â”‚   â”‚   SQL (Snowflake) â”‚ API (REST) â”‚ File (S3) â”‚ Stream (Kafka) â”‚  â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   Owner: orders-team@company.com                                           â”‚\nâ”‚   SLA: 99.9% availability, &lt;1h freshness                                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.3 CaractÃ©ristiques dâ€™un Bon Data Product (DATSIS)\n\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\nExemple\n\n\n\n\nDiscoverable\nFacile Ã  trouver\nRÃ©fÃ©rencÃ© dans DataHub\n\n\nAddressable\nAccÃ¨s standardisÃ©\nsnowflake://prod.orders.orders_fact\n\n\nTrustworthy\nQualitÃ© garantie\nSLOs, tests automatisÃ©s\n\n\nSelf-describing\nDocumentation complÃ¨te\nSchema, descriptions, exemples\n\n\nInteroperable\nStandards communs\nFormats, naming conventions\n\n\nSecure\nAccÃ¨s contrÃ´lÃ©\nRBAC, encryption, audit\n\n\n\n\n\n2.4 Ã‰quipe Data Product\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Ã‰QUIPE DATA PRODUCT (par domaine)                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   Product Owner â”€â”€â”€ DÃ©finit les prioritÃ©s, roadmap                  â”‚  â”‚\nâ”‚   â”‚        â”‚                                                            â”‚  â”‚\nâ”‚   â”‚        â”œâ”€â”€ Data Engineer â”€â”€â”€ Pipelines, infrastructure              â”‚  â”‚\nâ”‚   â”‚        â”‚                                                            â”‚  â”‚\nâ”‚   â”‚        â”œâ”€â”€ Analytics Engineer â”€â”€â”€ ModÃ¨les, transformations          â”‚  â”‚\nâ”‚   â”‚        â”‚                                                            â”‚  â”‚\nâ”‚   â”‚        â””â”€â”€ Domain Expert â”€â”€â”€ Connaissances mÃ©tier, validation       â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   ResponsabilitÃ©s :                                                         â”‚\nâ”‚   âœ“ DÃ©velopper et maintenir les Data Products du domaine                   â”‚\nâ”‚   âœ“ Garantir la qualitÃ© (SLOs)                                             â”‚\nâ”‚   âœ“ Supporter les consommateurs                                            â”‚\nâ”‚   âœ“ Documenter et communiquer les changements                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#self-serve-data-platform",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#self-serve-data-platform",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "3. Self-Serve Data Platform",
    "text": "3. Self-Serve Data Platform\n\n3.1 RÃ´le de la Plateforme\nLa plateforme est un enabler, pas un bottleneck. Elle fournit les outils pour que les domaines soient autonomes.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    SELF-SERVE DATA PLATFORM                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    DOMAIN TEAMS                                     â”‚  â”‚\nâ”‚   â”‚   Orders â”‚ Customers â”‚ Products â”‚ Marketing â”‚ Finance              â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                â”‚                                            â”‚\nâ”‚                                â–¼                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    PLATFORM SERVICES                                â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\nâ”‚   â”‚   â”‚ Data Productâ”‚  â”‚   Data      â”‚  â”‚   Access    â”‚                â”‚  â”‚\nâ”‚   â”‚   â”‚  Templates  â”‚  â”‚  Catalog    â”‚  â”‚ Management  â”‚                â”‚  â”‚\nâ”‚   â”‚   â”‚             â”‚  â”‚  (DataHub)  â”‚  â”‚   (RBAC)    â”‚                â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\nâ”‚   â”‚   â”‚   Quality   â”‚  â”‚ Observabilityâ”‚  â”‚  Contract  â”‚                â”‚  â”‚\nâ”‚   â”‚   â”‚   (Soda)    â”‚  â”‚ (Monitoring) â”‚  â”‚ Validation â”‚                â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                â”‚                                            â”‚\nâ”‚                                â–¼                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    INFRASTRUCTURE                                   â”‚  â”‚\nâ”‚   â”‚   Compute (Spark) â”‚ Storage (S3/Delta) â”‚ Orchestration (Airflow)   â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 Composants Essentiels\n\n\n\n\n\n\n\n\nComposant\nFonction\nOutils\n\n\n\n\nInfrastructure\nCompute, storage, networking\nKubernetes, S3, Snowflake\n\n\nTemplates\nBootstrapper un Data Product\nCookiecutter, Backstage\n\n\nCatalog\nDÃ©couverte, lineage, metadata\nDataHub, OpenMetadata, Atlan\n\n\nAccess Management\nRBAC, policies\nUnity Catalog, Ranger\n\n\nQuality\nTests, monitoring\nSoda, Great Expectations\n\n\nObservability\nMÃ©triques, alertes\nDatadog, Monte Carlo\n\n\n\n\n\n3.3 Technologies de Data Catalog\n\n\n\nOutil\nType\nPoints forts\n\n\n\n\nDataHub\nOpen Source\nLineage, search, extensible\n\n\nOpenMetadata\nOpen Source\nUI moderne, collaboration\n\n\nAtlan\nCommercial\nUX, collaboration, governance\n\n\nUnity Catalog\nDatabricks\nIntÃ©grÃ© Databricks, ACL\n\n\nAWS Glue Catalog\nAWS\nIntÃ©grÃ© AWS, serverless",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#data-contracts",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#data-contracts",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "4. Data Contracts",
    "text": "4. Data Contracts\n\n4.1 Pourquoi les Data Contracts ?\nSans contracts, câ€™est le chaos :\nâŒ SANS DATA CONTRACT :\n\nProducer: \"J'ai renommÃ© la colonne 'user_id' en 'customer_id', c'Ã©tait plus clair.\"\nConsumer: \"Tu as cassÃ© tous nos dashboards !\" ğŸ˜¡\nProducer: \"C'est pas ma faute si vous dÃ©pendez de mes donnÃ©es sans me prÃ©venir.\"\n\nâœ… AVEC DATA CONTRACT :\n\nProducer: \"Je veux renommer 'user_id' en 'customer_id'.\"\nContract: \"Breaking change dÃ©tectÃ©. 3 consommateurs impactÃ©s.\"\nProducer: \"OK, je crÃ©e une nouvelle version avec pÃ©riode de deprecation.\"\n\n\n4.2 Anatomie dâ€™un Data Contract\nUn Data Contract dÃ©finit lâ€™interface entre un producteur et ses consommateurs.\n\n\nVoir le code\n# Exemple de Data Contract complet (YAML)\n\ndata_contract_yaml = \"\"\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# DATA CONTRACT : orders_fact\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndataContractSpecification: 0.9.3\nid: orders-domain.orders-fact\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# INFO : MÃ©tadonnÃ©es gÃ©nÃ©rales\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ninfo:\n  title: Orders Fact Table\n  version: 1.0.0\n  status: active  # draft | active | deprecated | retired\n  description: |\n    Table de faits contenant toutes les commandes.\n    Une ligne par commande. Mise Ã  jour en near real-time.\n  owner: orders-team@company.com\n  contact:\n    slack: \"#orders-data\"\n    oncall: \"https://pagerduty.com/orders-data\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# SCHEMA : Structure des donnÃ©es\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nschema:\n  type: table\n  fields:\n    - name: order_id\n      type: string\n      required: true\n      unique: true\n      description: Identifiant unique de la commande (UUID v4)\n      example: \"550e8400-e29b-41d4-a716-446655440000\"\n      pii: false\n      \n    - name: customer_id\n      type: string\n      required: true\n      description: RÃ©fÃ©rence au domaine customers\n      pii: true\n      \n    - name: order_date\n      type: timestamp\n      required: true\n      description: Date et heure de la commande (UTC)\n      \n    - name: amount\n      type: decimal(10,2)\n      required: true\n      description: Montant total en EUR\n      constraints:\n        minimum: 0\n        maximum: 1000000\n        \n    - name: status\n      type: string\n      required: true\n      description: Statut actuel de la commande\n      enum:\n        - pending\n        - confirmed\n        - shipped\n        - delivered\n        - cancelled\n        \n    - name: shipping_country\n      type: string\n      required: false\n      description: Code pays ISO 3166-1 alpha-2\n      pattern: \"^[A-Z]{2}$\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# SEMANTICS : Signification mÃ©tier\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsemantics:\n  granularity: one row per order\n  temporality: event time (order_date)\n  updateFrequency: near real-time (&lt; 15 min)\n  businessRules:\n    - \"amount = somme des lignes + taxes - remises\"\n    - \"Les commandes annulÃ©es restent dans la table (status=cancelled)\"\n    - \"order_date est en UTC, pas en heure locale\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# QUALITY : SLOs de qualitÃ©\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nquality:\n  freshness:\n    threshold: 1 hour\n    column: order_date\n    \n  completeness:\n    - column: order_id\n      threshold: 100%\n    - column: customer_id\n      threshold: 100%\n    - column: amount\n      threshold: 99.9%\n      \n  validity:\n    - column: amount\n      rule: \"&gt;= 0\"\n      threshold: 100%\n    - column: status\n      rule: \"in enum values\"\n      threshold: 100%\n      \n  uniqueness:\n    - column: order_id\n      threshold: 100%\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# SLA : Service Level Agreements\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsla:\n  availability: 99.9%\n  latency: \"&lt; 5 minutes from source\"\n  retention: 7 years\n  supportHours: \"24/7\"\n  incidentResponse: \"&lt; 1 hour\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# ACCESS : Qui peut accÃ©der\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\naccess:\n  classification: internal\n  location: \"snowflake://prod.orders.orders_fact\"\n  consumers:\n    - team: analytics\n      purpose: dashboards\n    - team: marketing\n      purpose: segmentation\n    - team: finance\n      purpose: revenue reporting\n\"\"\"\n\nprint(\"ğŸ“„ Data Contract Example:\")\nprint(data_contract_yaml)\n\n\n\n\n4.3 DataContract CLI (Open Source)\n# Installation\npip install datacontract-cli\n\n# Initialiser un nouveau contract\ndatacontract init orders-fact\n\n# Valider la syntaxe\ndatacontract lint datacontract.yaml\n\n# Tester contre des donnÃ©es rÃ©elles\ndatacontract test datacontract.yaml \\\n  --source snowflake://prod.orders.orders_fact\n\n# GÃ©nÃ©rer la documentation HTML\ndatacontract export datacontract.yaml --format html &gt; docs/orders.html\n\n# DÃ©tecter les breaking changes\ndatacontract diff v1/datacontract.yaml v2/datacontract.yaml\n\n\n4.4 Validation avec Soda\n\n\nVoir le code\n# Soda checks gÃ©nÃ©rÃ©s depuis le Data Contract\n\nsoda_checks_yaml = \"\"\"\n# soda_checks.yaml\n# GÃ©nÃ©rÃ© depuis le Data Contract orders-domain.orders-fact\n\nchecks for orders_fact:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # Freshness : donnÃ©es &lt; 1 heure\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  - freshness(order_date) &lt; 1h:\n      name: \"Data freshness SLO\"\n      fail: when &gt; 1h\n      warn: when &gt; 30m\n\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # Completeness : pas de nulls sur colonnes required\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  - missing_count(order_id) = 0:\n      name: \"order_id completeness\"\n      \n  - missing_count(customer_id) = 0:\n      name: \"customer_id completeness\"\n      \n  - missing_percent(amount) &lt; 0.1:\n      name: \"amount completeness (99.9%)\"\n\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # Validity : valeurs dans les plages attendues\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  - invalid_count(amount) = 0:\n      name: \"amount must be &gt;= 0\"\n      valid min: 0\n      \n  - invalid_count(status) = 0:\n      name: \"status must be in enum\"\n      valid values: [pending, confirmed, shipped, delivered, cancelled]\n      \n  - invalid_count(shipping_country) = 0:\n      name: \"shipping_country format\"\n      valid regex: \"^[A-Z]{2}$\"\n\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # Uniqueness : pas de doublons\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  - duplicate_count(order_id) = 0:\n      name: \"order_id uniqueness\"\n\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # Volume : anomaly detection\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  - anomaly detection for row_count:\n      name: \"Volume anomaly detection\"\n      warn: when diff &gt; 20%\n      fail: when diff &gt; 50%\n\"\"\"\n\nprint(\"âœ… Soda Checks (from Data Contract):\")\nprint(soda_checks_yaml)\n\n\n\n\n4.5 ExÃ©cuter Soda\n# Installation\npip install soda-core-spark  # ou soda-core-snowflake, soda-core-postgres\n\n# Configuration (soda_config.yaml)\n# data_source my_warehouse:\n#   type: snowflake\n#   account: mycompany.us-east-1\n#   username: ${SNOWFLAKE_USER}\n#   password: ${SNOWFLAKE_PASSWORD}\n#   database: PROD\n#   schema: ORDERS\n\n# ExÃ©cuter les checks\nsoda scan -d my_warehouse -c soda_config.yaml soda_checks.yaml\n\n# RÃ©sultat :\n# Scan summary:\n# 8/8 checks PASSED\n# 0 checks WARNED\n# 0 checks FAILED\n\n\n4.6 Contract Validation dans CI/CD\n# .github/workflows/contract-validation.yaml\nname: Data Contract Validation\n\non:\n  push:\n    paths:\n      - 'contracts/**'\n  schedule:\n    - cron: '0 * * * *'  # Toutes les heures\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Install tools\n        run: pip install datacontract-cli soda-core\n        \n      - name: Lint contracts\n        run: datacontract lint contracts/*.yaml\n        \n      - name: Check for breaking changes\n        run: |\n          datacontract diff contracts/orders.yaml \\\n            --against main:contracts/orders.yaml\n            \n      - name: Run Soda checks\n        run: soda scan -d warehouse soda_checks.yaml\n        \n      - name: Alert on failure\n        if: failure()\n        run: |\n          curl -X POST $SLACK_WEBHOOK \\\n            -d '{\"text\": \"ğŸš¨ Data Contract validation failed!\"}'",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#schema-evolution-versioning",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#schema-evolution-versioning",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "5. Schema Evolution & Versioning",
    "text": "5. Schema Evolution & Versioning\n\n5.1 Breaking vs Non-Breaking Changes\n\n\n\n\n\n\n\n\nType de changement\nBreaking ?\nAction recommandÃ©e\n\n\n\n\nAjouter colonne nullable\nâŒ Non\nâœ… OK direct\n\n\nAjouter colonne required\nâœ… Oui\nNouvelle version majeure\n\n\nRenommer colonne\nâœ… Oui\nDeprecation + alias\n\n\nChanger type (ex: stringâ†’int)\nâœ… Oui\nNouvelle version majeure\n\n\nSupprimer colonne\nâœ… Oui\nDeprecation period (30-90 jours)\n\n\nChanger enum (ajouter valeur)\nâš ï¸ DÃ©pend\nPrÃ©venir les consommateurs\n\n\nChanger contrainte (min/max)\nâš ï¸ DÃ©pend\nPrÃ©venir les consommateurs\n\n\n\n\n\n5.2 StratÃ©gies de Versioning\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    SEMANTIC VERSIONING                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚                         MAJOR.MINOR.PATCH                                   â”‚\nâ”‚                           â”‚     â”‚     â”‚                                     â”‚\nâ”‚                           â”‚     â”‚     â””â”€â”€ Bug fixes, no schema change       â”‚\nâ”‚                           â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€ New features, backward compatible â”‚\nâ”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Breaking changes                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   Exemples :                                                                â”‚\nâ”‚   1.0.0 â†’ 1.0.1 : Fix bug dans calcul                                      â”‚\nâ”‚   1.0.1 â†’ 1.1.0 : Ajout colonne nullable                                   â”‚\nâ”‚   1.1.0 â†’ 2.0.0 : Renommage colonne (breaking)                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.3 Backward vs Forward Compatibility\n\n\n\n\n\n\n\n\nType\nDescription\nQui peut lire quoi\n\n\n\n\nBackward\nNouveau code lit ancien format\nâœ… v2 reader â†’ v1 data\n\n\nForward\nAncien code lit nouveau format\nâœ… v1 reader â†’ v2 data\n\n\nFull\nLes deux\nâœ… Best practice\n\n\n\n\n\n5.4 Processus de Deprecation\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PROCESSUS DE DEPRECATION                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   Jour 0          Jour 30         Jour 60         Jour 90                  â”‚\nâ”‚     â”‚               â”‚               â”‚               â”‚                       â”‚\nâ”‚     â–¼               â–¼               â–¼               â–¼                       â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”                  â”‚\nâ”‚   â”‚Annonceâ”‚â”€â”€â”€â”€â”€â”€â”€â”‚Rappelâ”‚â”€â”€â”€â”€â”€â”€â”€â”‚Warningâ”‚â”€â”€â”€â”€â”€â”€â”€â”‚Removeâ”‚                  â”‚\nâ”‚   â”‚       â”‚       â”‚      â”‚        â”‚       â”‚       â”‚      â”‚                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”˜                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Annoncer la deprecation dans le contract                               â”‚\nâ”‚   â€¢ Notifier tous les consommateurs                                        â”‚\nâ”‚   â€¢ Fournir un guide de migration                                          â”‚\nâ”‚   â€¢ Logger les accÃ¨s Ã  la colonne deprecated                               â”‚\nâ”‚   â€¢ Supprimer aprÃ¨s la pÃ©riode                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.5 Exemple de Deprecation dans le Contract\nschema:\n  fields:\n    - name: user_id\n      type: string\n      deprecated: true\n      deprecationDate: \"2024-01-15\"\n      removalDate: \"2024-04-15\"\n      replacedBy: customer_id\n      description: \"[DEPRECATED] Use customer_id instead\"\n      \n    - name: customer_id\n      type: string\n      required: true\n      description: \"Unique customer identifier (replaces user_id)\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#data-catalog-discovery",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#data-catalog-discovery",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "6. Data Catalog & Discovery",
    "text": "6. Data Catalog & Discovery\n\n6.1 Pourquoi un Data Catalog ?\n\n\n\nProblÃ¨me\nSolution Catalog\n\n\n\n\nâ€œOÃ¹ trouver les donnÃ©es clients ?â€\nSearch\n\n\nâ€œDâ€™oÃ¹ viennent ces donnÃ©es ?â€\nLineage\n\n\nâ€œQui est responsable de cette table ?â€\nOwnership\n\n\nâ€œCette colonne contient quoi exactement ?â€\nDocumentation\n\n\nâ€œLes donnÃ©es sont-elles fiables ?â€\nQuality scores\n\n\n\n\n\n6.2 Types de Metadata\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    TYPES DE METADATA                                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  TECHNICAL METADATA                                                 â”‚  â”‚\nâ”‚   â”‚  â€¢ Schema (colonnes, types)                                         â”‚  â”‚\nâ”‚   â”‚  â€¢ Lineage (sources, transformations)                               â”‚  â”‚\nâ”‚   â”‚  â€¢ Partitioning, format, location                                   â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  BUSINESS METADATA                                                  â”‚  â”‚\nâ”‚   â”‚  â€¢ Descriptions, dÃ©finitions                                        â”‚  â”‚\nâ”‚   â”‚  â€¢ Ownership, contact                                               â”‚  â”‚\nâ”‚   â”‚  â€¢ Business glossary terms                                          â”‚  â”‚\nâ”‚   â”‚  â€¢ Classification (PII, confidential)                               â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  OPERATIONAL METADATA                                               â”‚  â”‚\nâ”‚   â”‚  â€¢ Freshness, last update                                           â”‚  â”‚\nâ”‚   â”‚  â€¢ Quality scores                                                   â”‚  â”‚\nâ”‚   â”‚  â€¢ Usage statistics                                                 â”‚  â”‚\nâ”‚   â”‚  â€¢ Query patterns                                                   â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.3 DataHub : Setup et Utilisation\nDataHub est le Data Catalog open source le plus populaire (crÃ©Ã© par LinkedIn).\n# Installation avec Docker\ngit clone https://github.com/datahub-project/datahub.git\ncd datahub/docker\n./quickstart.sh\n\n# AccÃ©der Ã  l'UI\n# http://localhost:9002\n# Login: datahub / datahub\n\n\n6.4 Ingestion de Metadata\n# recipes/snowflake_ingestion.yaml\nsource:\n  type: snowflake\n  config:\n    account_id: \"mycompany.us-east-1\"\n    username: ${SNOWFLAKE_USER}\n    password: ${SNOWFLAKE_PASSWORD}\n    warehouse: COMPUTE_WH\n    database_pattern:\n      allow:\n        - \"PROD\"\n    include_table_lineage: true\n    profiling:\n      enabled: true\n\nsink:\n  type: datahub-rest\n  config:\n    server: \"http://localhost:8080\"\n# Installer le CLI et exÃ©cuter l'ingestion\npip install 'acryl-datahub[snowflake]'\ndatahub ingest -c recipes/snowflake_ingestion.yaml\n\n\n6.5 Lineage Automatique\n# Spark avec DataHub Lineage\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"OrdersETL\") \\\n    .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\") \\\n    .config(\"spark.datahub.rest.server\", \"http://localhost:8080\") \\\n    .getOrCreate()\n\n# Ces opÃ©rations crÃ©ent automatiquement le lineage dans DataHub\norders = spark.read.parquet(\"s3://bucket/raw/orders\")\ncustomers = spark.read.parquet(\"s3://bucket/dim/customers\")\n\nenriched = orders.join(customers, \"customer_id\")\nenriched.write.parquet(\"s3://bucket/gold/orders_enriched\")\n\n# Lineage capturÃ© :\n# raw/orders â”€â”€â”\n#              â”œâ”€â”€â–¶ gold/orders_enriched\n# dim/customersâ”˜\n\n\n6.6 Alternatives Ã  DataHub\n\n\n\nOutil\nType\nPoints forts\n\n\n\n\nDataHub\nOpen Source\nLineage, extensible, actif\n\n\nOpenMetadata\nOpen Source\nUI moderne, tout-en-un\n\n\nAtlan\nCommercial\nUX, collaboration\n\n\nAlation\nCommercial\nRecherche, ML-powered\n\n\nUnity Catalog\nDatabricks\nIntÃ©grÃ©, ACL avancÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#federated-governance",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#federated-governance",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "7. Federated Governance",
    "text": "7. Federated Governance\n\n7.1 Gouvernance CentralisÃ©e vs FÃ©dÃ©rÃ©e\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    GOUVERNANCE CENTRALISÃ‰E vs FÃ‰DÃ‰RÃ‰E                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   CENTRALISÃ‰E                         FÃ‰DÃ‰RÃ‰E (Data Mesh)                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚   â”‚  Central Team   â”‚                 â”‚  Global Policiesâ”‚                  â”‚\nâ”‚   â”‚  decides ALL    â”‚                 â”‚  (standards)    â”‚                  â”‚\nâ”‚   â”‚  policies       â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚                           â”‚\nâ”‚            â”‚                                   â–¼                           â”‚\nâ”‚            â–¼                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   Domain A        â”‚                â”‚\nâ”‚   â”‚  All Domains    â”‚                 â”‚   (local policies)â”‚                â”‚\nâ”‚   â”‚  follow blindly â”‚                 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚   Domain B        â”‚                â”‚\nâ”‚                                       â”‚   (local policies)â”‚                â”‚\nâ”‚   âŒ Slow                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚   âŒ One-size-fits-all                                                     â”‚\nâ”‚   âŒ Bottleneck                       âœ… Fast                               â”‚\nâ”‚                                       âœ… Context-aware                      â”‚\nâ”‚                                       âœ… Scalable                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n7.2 Policies Globales vs Locales\n\n\n\n\n\n\n\n\nScope\nExemples\nQui dÃ©cide\n\n\n\n\nGlobal\nNaming conventions, formats de dates, compliance GDPR\nPlatform team\n\n\nLocal\nSchema spÃ©cifique, SLOs, refresh frequency\nDomain team\n\n\n\n\n\n7.3 Standards Communs (InteropÃ©rabilitÃ©)\n# global_standards.yaml\n# Standards que TOUS les domaines doivent respecter\n\nnaming:\n  tables: \"{domain}_{entity}_{type}\"  # orders_customers_dim\n  columns: snake_case\n  timestamps: \"*_at\" suffix  # created_at, updated_at\n\nformats:\n  dates: ISO 8601 (YYYY-MM-DD)\n  timestamps: ISO 8601 with timezone (UTC)\n  currency: ISO 4217 codes (EUR, USD)\n  country: ISO 3166-1 alpha-2 (FR, US)\n\nquality:\n  minimumFreshness: 24h\n  minimumCompleteness: 95%\n  requiredTests:\n    - uniqueness on primary key\n    - not null on required columns\n\ncompliance:\n  piiColumns:\n    mustBeTagged: true\n    defaultRetention: 3 years\n    encryption: required\n\n\n7.4 Data Quality as Code\n# Policies automatisÃ©es via CI/CD\n\ndef validate_global_standards(contract):\n    \"\"\"Valider qu'un contract respecte les standards globaux.\"\"\"\n    errors = []\n    \n    # 1. Naming convention\n    if not re.match(r'^[a-z]+_[a-z]+_(fact|dim|event)$', contract['info']['title']):\n        errors.append(\"Table name must follow {domain}_{entity}_{type} convention\")\n    \n    # 2. Required metadata\n    if 'owner' not in contract['info']:\n        errors.append(\"Owner is required\")\n    \n    # 3. Quality SLOs\n    if contract.get('quality', {}).get('freshness', {}).get('threshold', '999h') &gt; '24h':\n        errors.append(\"Freshness SLO must be &lt;= 24h\")\n    \n    # 4. PII tagging\n    for field in contract.get('schema', {}).get('fields', []):\n        if 'pii' not in field:\n            errors.append(f\"Field {field['name']} must have PII tag\")\n    \n    return errors\n\n\n7.5 Compliance (GDPR, PII)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PII MANAGEMENT                                           â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. IDENTIFICATION                                                         â”‚\nâ”‚      Tag PII columns in contract: pii: true                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   2. CLASSIFICATION                                                         â”‚\nâ”‚      â€¢ Direct PII: email, phone, SSN                                       â”‚\nâ”‚      â€¢ Indirect PII: customer_id, IP address                               â”‚\nâ”‚      â€¢ Sensitive: health, financial, political                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   3. PROTECTION                                                             â”‚\nâ”‚      â€¢ Encryption at rest                                                  â”‚\nâ”‚      â€¢ Access control (need-to-know)                                       â”‚\nâ”‚      â€¢ Masking for non-prod environments                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   4. RETENTION                                                              â”‚\nâ”‚      â€¢ Define retention period per classification                          â”‚\nâ”‚      â€¢ Automated deletion                                                  â”‚\nâ”‚      â€¢ Right to be forgotten (GDPR)                                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#implÃ©mentation-pratique",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#implÃ©mentation-pratique",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "8. ImplÃ©mentation Pratique",
    "text": "8. ImplÃ©mentation Pratique\n\n8.1 Par OÃ¹ Commencer ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ROADMAP D'ADOPTION DATA MESH                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   PHASE 1: PILOT (3-6 mois)                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚\nâ”‚   â€¢ Choisir 1-2 domaines pilotes                                           â”‚\nâ”‚   â€¢ DÃ©finir les premiers Data Products                                     â”‚\nâ”‚   â€¢ Ã‰crire les premiers Data Contracts                                     â”‚\nâ”‚   â€¢ Setup basique du catalog (DataHub)                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   PHASE 2: EXPAND (6-12 mois)                                              â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚\nâ”‚   â€¢ Ajouter 3-5 domaines                                                   â”‚\nâ”‚   â€¢ DÃ©velopper la self-serve platform                                      â”‚\nâ”‚   â€¢ Automatiser la validation des contracts                                â”‚\nâ”‚   â€¢ DÃ©finir les standards globaux                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   PHASE 3: SCALE (12+ mois)                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚\nâ”‚   â€¢ Tous les domaines on-boardÃ©s                                           â”‚\nâ”‚   â€¢ Plateforme mature et self-serve                                        â”‚\nâ”‚   â€¢ Gouvernance fÃ©dÃ©rÃ©e opÃ©rationnelle                                     â”‚\nâ”‚   â€¢ MÃ©triques et amÃ©lioration continue                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n8.2 CritÃ¨res de SÃ©lection des Domaines Pilotes\n\n\n\nCritÃ¨re\nBon candidat\nMauvais candidat\n\n\n\n\nMaturitÃ© Ã©quipe\nÃ‰quipe tech capable\nPas de compÃ©tences data\n\n\nComplexitÃ©\nModÃ©rÃ©e\nTrop simple ou trop complexe\n\n\nVisibilitÃ©\nImpact business visible\nProjet interne invisible\n\n\nDÃ©pendances\nPeu de dÃ©pendances\nDÃ©pend de tout le monde\n\n\nSponsor\nManagement engagÃ©\nPas de sponsor\n\n\n\n\n\n8.3 Anti-Patterns Ã  Ã‰viter\n\n\n\n\n\n\n\n\nAnti-Pattern\nProblÃ¨me\nSolution\n\n\n\n\nBig Bang\nTout migrer dâ€™un coup\nApproche incrÃ©mentale\n\n\nPlatform-first\nConstruire la plateforme avant les besoins\nUse-case driven\n\n\nNo Governance\nChaque domaine fait ce quâ€™il veut\nStandards globaux\n\n\nOver-Governance\nTrop de rÃ¨gles, retour au bottleneck\nBalance global/local\n\n\nCopy-Paste\nCopier les donnÃ©es au lieu de les consommer\nData Products vrais\n\n\n\n\n\n8.4 MÃ©triques de SuccÃ¨s\n\n\n\n\n\n\n\n\nMÃ©trique\nDescription\nTarget\n\n\n\n\nTime to Data\nTemps pour accÃ©der Ã  une nouvelle donnÃ©e\n&lt; 1 jour\n\n\nData Product Count\nNombre de Data Products publiÃ©s\nCroissance\n\n\nContract Coverage\n% de tables avec contract\n&gt; 80%\n\n\nQuality Score\nScore moyen de qualitÃ©\n&gt; 95%\n\n\nConsumer Satisfaction\nNPS des consommateurs\n&gt; 50",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#exercices-pratiques",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#exercices-pratiques",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "9. Exercices Pratiques",
    "text": "9. Exercices Pratiques\n\nExercice 1 : Identifier les Domaines\nPour une entreprise de e-commerce avec les Ã©quipes suivantes : - Ã‰quipe Produit (catalogue, pricing) - Ã‰quipe Ventes (commandes, paiements) - Ã‰quipe Logistique (livraisons, stock) - Ã‰quipe Marketing (campagnes, analytics) - Ã‰quipe Support (tickets, satisfaction)\nQuestions : 1. Identifier les 5 domaines Data Mesh 2. Lister 2-3 Data Products par domaine 3. Identifier les dÃ©pendances entre domaines\n\n\n\nExercice 2 : Ã‰crire un Data Contract\nÃ‰crire un Data Contract complet pour la table customers_dim avec : - 6+ colonnes (id, name, email, country, created_at, segment) - PII identifiÃ© - SLOs de qualitÃ© - SLA de disponibilitÃ©\n\n\n\nExercice 3 : Setup DataHub Local\n\nInstaller DataHub avec Docker\nCrÃ©er une recette dâ€™ingestion pour un fichier Parquet local\nExÃ©cuter lâ€™ingestion\nExplorer le lineage dans lâ€™UI\n\n\n\n\nExercice 4 : Validation de Contract avec Soda\n\nConvertir le Data Contract de lâ€™exercice 2 en checks Soda\nCrÃ©er des donnÃ©es de test (certaines valides, certaines invalides)\nExÃ©cuter Soda et interprÃ©ter les rÃ©sultats\n\n\n\n\nExercice 5 : Schema Evolution\nSimuler une schema evolution : 1. Version 1.0.0 du contract orders_fact 2. Ajouter une colonne nullable discount_code (non-breaking) 3. Renommer user_id en customer_id (breaking) 4. Documenter le processus de deprecation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#mini-projet-data-mesh-pilot",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#mini-projet-data-mesh-pilot",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "10. Mini-Projet : Data Mesh Pilot",
    "text": "10. Mini-Projet : Data Mesh Pilot\n\nObjectif\nImplÃ©menter un pilot Data Mesh avec 2 domaines.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      MINI-PROJET : DATA MESH PILOT                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚\nâ”‚   â”‚  ORDERS DOMAIN  â”‚         â”‚ CUSTOMERS DOMAINâ”‚                          â”‚\nâ”‚   â”‚                 â”‚         â”‚                 â”‚                          â”‚\nâ”‚   â”‚  Data Product:  â”‚         â”‚  Data Product:  â”‚                          â”‚\nâ”‚   â”‚  - orders_fact  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  - customers_dimâ”‚                          â”‚\nâ”‚   â”‚  - Contract âœ“   â”‚         â”‚  - Contract âœ“   â”‚                          â”‚\nâ”‚   â”‚  - SLOs âœ“       â”‚         â”‚  - SLOs âœ“       â”‚                          â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚            â”‚                           â”‚                                    â”‚\nâ”‚            â–¼                           â–¼                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                     SELF-SERVE PLATFORM                             â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚\nâ”‚   â”‚   â”‚   DataHub   â”‚  â”‚   Contract  â”‚  â”‚   Quality   â”‚                â”‚  â”‚\nâ”‚   â”‚   â”‚   Catalog   â”‚  â”‚  Validation â”‚  â”‚  (Soda)     â”‚                â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLivrables\n\nData Contracts : 2 contracts complets (orders_fact, customers_dim)\nSoda Checks : Validation automatisÃ©e pour chaque contract\nDataHub : Catalog avec les 2 Data Products\nLineage : Visualisation des dÃ©pendances\nCI/CD : Pipeline de validation des contracts\nDocumentation : README avec architecture et standards\n\n\n\nStructure du Projet\ndata-mesh-pilot/\nâ”œâ”€â”€ domains/\nâ”‚   â”œâ”€â”€ orders/\nâ”‚   â”‚   â”œâ”€â”€ contracts/\nâ”‚   â”‚   â”‚   â””â”€â”€ orders_fact.yaml\nâ”‚   â”‚   â”œâ”€â”€ soda/\nâ”‚   â”‚   â”‚   â””â”€â”€ orders_checks.yaml\nâ”‚   â”‚   â””â”€â”€ pipelines/\nâ”‚   â”‚       â””â”€â”€ orders_etl.py\nâ”‚   â””â”€â”€ customers/\nâ”‚       â”œâ”€â”€ contracts/\nâ”‚       â”‚   â””â”€â”€ customers_dim.yaml\nâ”‚       â”œâ”€â”€ soda/\nâ”‚       â”‚   â””â”€â”€ customers_checks.yaml\nâ”‚       â””â”€â”€ pipelines/\nâ”‚           â””â”€â”€ customers_etl.py\nâ”œâ”€â”€ platform/\nâ”‚   â”œâ”€â”€ datahub/\nâ”‚   â”‚   â””â”€â”€ docker-compose.yaml\nâ”‚   â”œâ”€â”€ standards/\nâ”‚   â”‚   â””â”€â”€ global_standards.yaml\nâ”‚   â””â”€â”€ ci/\nâ”‚       â””â”€â”€ contract_validation.yaml\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ orders.parquet\nâ”‚   â””â”€â”€ customers.parquet\nâ””â”€â”€ README.md\n\n\nCritÃ¨res de SuccÃ¨s\n\n2 Data Contracts complets et valides\nSoda checks passent Ã  100%\nDataHub affiche les 2 Data Products\nLineage visible entre orders et customers\nCI/CD dÃ©tecte les breaking changes\nDocumentation claire",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#ressources",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#ressources",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation\n\nData Mesh Principles (Zhamak Dehghani)\nData Contract Specification\nDataHub Documentation\nSoda Documentation\n\n\n\nLivres\n\nData Mesh: Delivering Data-Driven Value at Scale â€” Zhamak Dehghani\nFundamentals of Data Engineering â€” Joe Reis, Matt Housley\n\n\n\nOutils\n\nDataHub â€” Data Catalog open source\nOpenMetadata â€” Alternative open source\nSoda â€” Data Quality\nDataContract CLI â€” Contract validation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/32_data_mesh_contracts.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/32_data_mesh_contracts.html#prochaine-Ã©tape",
    "title": "ğŸ—ï¸ Data Mesh & Data Contracts",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 33_real_time_analytics â€” Real-Time Analytics\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les concepts de Data Mesh et Data Contracts.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "32 Â· Data Mesh & Contracts"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas maÃ®triser Scala pour Spark et comprendre les internals de Spark pour Ã©crire des jobs performants. Tu apprendras Ã  dÃ©velopper, builder et dÃ©ployer des applications Spark professionnelles.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#prÃ©requis",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#prÃ©requis",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nPySpark DataFrame API (M19)\n\n\nâœ… Requis\nSpark sur Kubernetes (M21)\n\n\nâœ… Requis\nNotions de programmation fonctionnelle\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec un IDE (VS Code, PyCharm)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#objectifs-du-module",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#objectifs-du-module",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nÃ‰crire du code Scala idiomatique pour Spark\nUtiliser les ADT (Algebraic Data Types) et Either/Try pour des pipelines robustes\nConfigurer un environnement de dÃ©veloppement complet (Notebook + IntelliJ)\nBuilder et dÃ©ployer des applications Spark avec sbt et spark-submit\nComprendre Catalyst, AQE et Tungsten pour optimiser tes jobs\nDiagnostiquer et rÃ©soudre les problÃ¨mes de performance",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#scala-pour-data-engineers",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#scala-pour-data-engineers",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "1. Scala pour Data Engineers",
    "text": "1. Scala pour Data Engineers\n\n1.1 Pourquoi Scala pour Spark ?\n\n\n\n\n\n\n\n\nAspect\nScala\nPython\n\n\n\n\nPerformance\nNatif JVM, pas de sÃ©rialisation\nSÃ©rialisation Python â†”ï¸ JVM\n\n\nTypage\nStatique, erreurs Ã  la compilation\nDynamique, erreurs au runtime\n\n\nAPI Spark\nAPI native, toutes les features\nWrapper, parfois en retard\n\n\nÃ‰cosystÃ¨me\nKafka, Flink, Akka\nML, Data Science\n\n\nCourbe dâ€™apprentissage\nPlus raide\nPlus accessible\n\n\n\nRÃ¨gle pratique : - Python : Exploration, prototypage, Data Science, petits pipelines - Scala : Production, gros volumes, performance critique, Ã©quipe backend Java/Scala\n\n\n1.2 Syntaxe Essentielle\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Variables : val (immutable) vs var (mutable)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nval name: String = \"Spark\"     // Immutable (prÃ©fÃ©rÃ©)\nvar counter: Int = 0           // Mutable (Ã  Ã©viter)\nval inferred = 42              // Type infÃ©rÃ© automatiquement\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Fonctions\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n// Fonction classique\ndef add(a: Int, b: Int): Int = {\n  a + b\n}\n\n// Fonction one-liner (return implicite)\ndef multiply(a: Int, b: Int): Int = a * b\n\n// Fonction anonyme (lambda)\nval double = (x: Int) =&gt; x * 2\n\n// ParamÃ¨tres par dÃ©faut\ndef greet(name: String, greeting: String = \"Hello\"): String = \n  s\"$greeting, $name!\"\n\ngreet(\"Alice\")              // \"Hello, Alice!\"\ngreet(\"Bob\", \"Bonjour\")     // \"Bonjour, Bob!\"\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// String interpolation\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nval version = 3.5\nprintln(s\"Spark version: $version\")           // Simple\nprintln(s\"Next version: ${version + 0.1}\")    // Expression\nprintln(f\"Pi = ${math.Pi}%.2f\")               // FormatÃ©\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Conditions et boucles\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n// if est une expression (retourne une valeur)\nval status = if (counter &gt; 0) \"positive\" else \"zero or negative\"\n\n// for-comprehension\nval squares = for (i &lt;- 1 to 5) yield i * i  // Vector(1, 4, 9, 16, 25)\n\n// for avec filtres\nval evenSquares = for {\n  i &lt;- 1 to 10\n  if i % 2 == 0\n} yield i * i  // Vector(4, 16, 36, 64, 100)\n\n\n1.3 Collections Fonctionnelles\nLes collections Scala sont la base pour comprendre les transformations Spark (RDD, DataFrame).\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Types de collections\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nval list = List(1, 2, 3, 4, 5)           // Immutable, linked list\nval vector = Vector(1, 2, 3, 4, 5)       // Immutable, indexed\nval set = Set(1, 2, 3, 3, 3)             // Immutable, unique: Set(1, 2, 3)\nval map = Map(\"a\" -&gt; 1, \"b\" -&gt; 2)        // Immutable, key-value\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Transformations (comme Spark !)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nval numbers = List(1, 2, 3, 4, 5)\n\n// map : transformer chaque Ã©lÃ©ment\nnumbers.map(x =&gt; x * 2)              // List(2, 4, 6, 8, 10)\nnumbers.map(_ * 2)                   // Syntaxe courte\n\n// filter : garder les Ã©lÃ©ments qui matchent\nnumbers.filter(x =&gt; x &gt; 2)           // List(3, 4, 5)\nnumbers.filter(_ &gt; 2)                // Syntaxe courte\n\n// flatMap : map + flatten\nval words = List(\"hello world\", \"scala spark\")\nwords.flatMap(_.split(\" \"))          // List(\"hello\", \"world\", \"scala\", \"spark\")\n\n// reduce : agrÃ©ger en une valeur\nnumbers.reduce((a, b) =&gt; a + b)      // 15\nnumbers.reduce(_ + _)                // Syntaxe courte\n\n// fold : reduce avec valeur initiale\nnumbers.fold(0)(_ + _)               // 15\nnumbers.fold(10)(_ + _)              // 25 (commence Ã  10)\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ChaÃ®nage (comme les pipelines Spark)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nval result = numbers\n  .filter(_ % 2 == 0)    // Garder les pairs\n  .map(_ * 10)           // Multiplier par 10\n  .sum                   // Sommer : 60 (20 + 40)\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// groupBy (comme Spark groupBy !)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncase class Sale(product: String, amount: Double)\n\nval sales = List(\n  Sale(\"laptop\", 1000),\n  Sale(\"phone\", 500),\n  Sale(\"laptop\", 1200),\n  Sale(\"phone\", 600)\n)\n\nval byProduct = sales.groupBy(_.product)\n// Map(\n//   \"laptop\" -&gt; List(Sale(\"laptop\", 1000), Sale(\"laptop\", 1200)),\n//   \"phone\"  -&gt; List(Sale(\"phone\", 500), Sale(\"phone\", 600))\n// )\n\nval totalByProduct = sales\n  .groupBy(_.product)\n  .map { case (product, sales) =&gt; \n    (product, sales.map(_.amount).sum) \n  }\n// Map(\"laptop\" -&gt; 2200.0, \"phone\" -&gt; 1100.0)\n\n\n1.4 Option : GÃ©rer les valeurs absentes\n// Option = Some(valeur) ou None (jamais null !)\n\ndef findUser(id: Int): Option[String] = {\n  val users = Map(1 -&gt; \"Alice\", 2 -&gt; \"Bob\")\n  users.get(id)  // Retourne Option[String]\n}\n\nfindUser(1)  // Some(\"Alice\")\nfindUser(99) // None\n\n// Pattern matching\nfindUser(1) match {\n  case Some(name) =&gt; println(s\"Found: $name\")\n  case None       =&gt; println(\"User not found\")\n}\n\n// getOrElse : valeur par dÃ©faut\nval user = findUser(99).getOrElse(\"Unknown\")\n\n// map sur Option (sÃ»r, pas d'exception)\nval upperName = findUser(1).map(_.toUpperCase)  // Some(\"ALICE\")\nval noName = findUser(99).map(_.toUpperCase)    // None (pas d'erreur !)\n\n// flatMap pour chaÃ®ner\ndef findEmail(name: String): Option[String] = {\n  if (name == \"Alice\") Some(\"alice@example.com\") else None\n}\n\nval email = findUser(1).flatMap(findEmail)  // Some(\"alice@example.com\")\n\n\n1.5 Case Classes et Pattern Matching\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Case Class = classe de donnÃ©es immuable\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncase class Customer(\n  id: Long,\n  name: String,\n  email: String,\n  country: String = \"France\"  // Valeur par dÃ©faut\n)\n\n// CrÃ©ation (pas besoin de \"new\")\nval alice = Customer(1, \"Alice\", \"alice@example.com\")\nval bob = Customer(2, \"Bob\", \"bob@example.com\", \"USA\")\n\n// Accesseurs automatiques\nprintln(alice.name)    // \"Alice\"\nprintln(alice.country) // \"France\"\n\n// equals automatique (compare les valeurs)\nval alice2 = Customer(1, \"Alice\", \"alice@example.com\")\nprintln(alice == alice2)  // true !\n\n// copy : crÃ©er une copie modifiÃ©e (immutabilitÃ©)\nval aliceUSA = alice.copy(country = \"USA\")\n\n// toString automatique\nprintln(alice)  // Customer(1,Alice,alice@example.com,France)\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Pattern Matching (switch++ ultra puissant)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef describeCustomer(c: Customer): String = c match {\n  case Customer(_, \"Alice\", _, _)     =&gt; \"C'est Alice !\"\n  case Customer(id, _, _, \"France\")   =&gt; s\"Client franÃ§ais #$id\"\n  case Customer(_, name, _, country)  =&gt; s\"$name de $country\"\n}\n\n// Pattern matching avec guards\ndef customerTier(c: Customer, totalSpent: Double): String = c match {\n  case Customer(_, _, _, \"France\") if totalSpent &gt; 10000 =&gt; \"VIP France\"\n  case _ if totalSpent &gt; 5000                             =&gt; \"Gold\"\n  case _                                                  =&gt; \"Standard\"\n}\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Case classes et Spark\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n// Spark utilise les case classes pour crÃ©er des Datasets typÃ©s !\nimport spark.implicits._\n\ncase class Sale(product: String, amount: Double, date: String)\n\nval sales = Seq(\n  Sale(\"laptop\", 1000, \"2024-01-15\"),\n  Sale(\"phone\", 500, \"2024-01-16\")\n).toDS()  // Dataset[Sale] - typÃ© !\n\n// AccÃ¨s typÃ©\nsales.filter(_.amount &gt; 600).show()\n\n\n1.6 Sealed Traits et ADT (Algebraic Data Types) ğŸ”¥\nLes ADT permettent de modÃ©liser tous les Ã©tats possibles dâ€™un systÃ¨me. Le compilateur garantit lâ€™exhaustivitÃ© !\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// sealed trait = toutes les sous-classes dans le mÃªme fichier\n// Le compilateur CONNAÃT tous les cas possibles\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n// ModÃ©liser le rÃ©sultat d'un job ETL\nsealed trait JobResult\ncase class Success(recordsProcessed: Long, durationMs: Long) extends JobResult\ncase class Failure(error: String, stage: String) extends JobResult\ncase object Skipped extends JobResult  // object = singleton\n\ndef handleResult(result: JobResult): Unit = result match {\n  case Success(n, d) =&gt; println(s\"âœ… $n records traitÃ©s en ${d}ms\")\n  case Failure(e, s) =&gt; println(s\"âŒ Erreur Ã  l'Ã©tape $s: $e\")\n  case Skipped       =&gt; println(s\"â­ï¸ Job ignorÃ©\")\n}\n// Si tu oublies un cas, le compilateur te PRÃ‰VIENT !\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Exemple : Sources de donnÃ©es\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsealed trait DataSource\ncase class JdbcSource(url: String, table: String, user: String) extends DataSource\ncase class S3Source(bucket: String, path: String, format: String) extends DataSource\ncase class KafkaSource(brokers: String, topic: String) extends DataSource\ncase class LocalSource(path: String) extends DataSource\n\ndef readData(source: DataSource)(implicit spark: SparkSession): DataFrame = source match {\n  case JdbcSource(url, table, user) =&gt;\n    spark.read\n      .format(\"jdbc\")\n      .option(\"url\", url)\n      .option(\"dbtable\", table)\n      .option(\"user\", user)\n      .load()\n      \n  case S3Source(bucket, path, format) =&gt;\n    spark.read\n      .format(format)\n      .load(s\"s3a://$bucket/$path\")\n      \n  case KafkaSource(brokers, topic) =&gt;\n    spark.read\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", brokers)\n      .option(\"subscribe\", topic)\n      .load()\n      \n  case LocalSource(path) =&gt;\n    spark.read.parquet(path)\n}\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Exemple : Modes de traitement\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsealed trait WriteMode\ncase object Overwrite extends WriteMode\ncase object Append extends WriteMode\ncase class MergeByKey(keys: Seq[String]) extends WriteMode\n\ndef writeData(df: DataFrame, path: String, mode: WriteMode): Unit = mode match {\n  case Overwrite       =&gt; df.write.mode(\"overwrite\").parquet(path)\n  case Append          =&gt; df.write.mode(\"append\").parquet(path)\n  case MergeByKey(keys) =&gt; \n    // Logique Delta Lake MERGE\n    println(s\"Merge on keys: ${keys.mkString(\", \")}\")\n}\n\n\n1.7 Gestion dâ€™Erreurs : Either et Try ğŸ”¥\nFini les try/catch partout ! Scala offre des types pour gÃ©rer les erreurs proprement.\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Try[T] : capturer les exceptions (code legacy, I/O)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport scala.util.{Try, Success, Failure}\n\ndef parseNumber(s: String): Try[Int] = Try {\n  s.toInt  // Peut lancer NumberFormatException\n}\n\nparseNumber(\"42\")   // Success(42)\nparseNumber(\"abc\")  // Failure(NumberFormatException)\n\n// Pattern matching\nparseNumber(\"42\") match {\n  case Success(n)  =&gt; println(s\"Nombre: $n\")\n  case Failure(ex) =&gt; println(s\"Erreur: ${ex.getMessage}\")\n}\n\n// getOrElse\nval num = parseNumber(\"abc\").getOrElse(0)  // 0\n\n// map / flatMap (chaÃ®nage sÃ»r)\nval doubled = parseNumber(\"21\").map(_ * 2)  // Success(42)\n\n// recover : transformer l'erreur\nval safe = parseNumber(\"abc\").recover {\n  case _: NumberFormatException =&gt; -1\n}  // Success(-1)\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Either[L, R] : erreurs mÃ©tier typÃ©es (pas d'exceptions)\n// Left = erreur, Right = succÃ¨s\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n// DÃ©finir les types d'erreurs\nsealed trait ETLError\ncase class SourceNotFound(path: String) extends ETLError\ncase class SchemaError(expected: String, actual: String) extends ETLError\ncase class WriteError(message: String) extends ETLError\ncase class ConfigError(param: String) extends ETLError\n\n// Fonctions qui retournent Either\ndef readSource(path: String): Either[ETLError, DataFrame] = {\n  if (!new java.io.File(path).exists())\n    Left(SourceNotFound(path))\n  else\n    Right(spark.read.parquet(path))\n}\n\ndef validateSchema(df: DataFrame, expected: Seq[String]): Either[ETLError, DataFrame] = {\n  val actual = df.columns.toSeq\n  if (expected.forall(actual.contains))\n    Right(df)\n  else\n    Left(SchemaError(expected.mkString(\",\"), actual.mkString(\",\")))\n}\n\ndef writeOutput(df: DataFrame, path: String): Either[ETLError, Long] = {\n  try {\n    df.write.parquet(path)\n    Right(df.count())\n  } catch {\n    case e: Exception =&gt; Left(WriteError(e.getMessage))\n  }\n}\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ChaÃ®nage avec for-comprehension (le pattern ultime !)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef runETL(inputPath: String, outputPath: String): Either[ETLError, Long] = {\n  for {\n    rawData      &lt;- readSource(inputPath)\n    validated    &lt;- validateSchema(rawData, Seq(\"id\", \"name\", \"amount\"))\n    transformed  = validated.filter(col(\"amount\") &gt; 0)  // = pour les transformations pures\n    recordCount  &lt;- writeOutput(transformed, outputPath)\n  } yield recordCount\n}\n\n// Utilisation\nrunETL(\"data/input\", \"data/output\") match {\n  case Right(count) =&gt; println(s\"âœ… ETL terminÃ© : $count records\")\n  case Left(SourceNotFound(p)) =&gt; println(s\"âŒ Source introuvable : $p\")\n  case Left(SchemaError(e, a)) =&gt; println(s\"âŒ SchÃ©ma invalide. Attendu: $e, ReÃ§u: $a\")\n  case Left(WriteError(msg))   =&gt; println(s\"âŒ Erreur d'Ã©criture : $msg\")\n  case Left(ConfigError(p))    =&gt; println(s\"âŒ Config manquante : $p\")\n}\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// Quand utiliser quoi ?\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\n\nOutil\nQuand lâ€™utiliser\nExemple\n\n\n\n\nOption[T]\nValeur absente (pas une erreur)\nfindUser(id)\n\n\nTry[T]\nExceptions Java/legacy, I/O\nTry { file.read() }\n\n\nEither[E, T]\nErreurs mÃ©tier typÃ©es\nPipeline ETL, validation",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#environnements-de-dÃ©veloppement",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#environnements-de-dÃ©veloppement",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "2. Environnements de DÃ©veloppement",
    "text": "2. Environnements de DÃ©veloppement\n\n2.1 Scala dans Jupyter Notebook avec Almond\nAlmond est un kernel Scala pour Jupyter, idÃ©al pour lâ€™exploration et la formation.\n\nInstallation\n# 1. Installer Coursier (gestionnaire de packages Scala)\ncurl -fL https://github.com/coursier/coursier/releases/latest/download/cs-x86_64-pc-linux.gz | gzip -d &gt; cs\nchmod +x cs\n./cs setup  # Ajoute au PATH\n\n# 2. Installer Almond\ncs launch --fork almond -- --install\n\n# 3. VÃ©rifier\njupyter kernelspec list\n# Devrait afficher : scala  /home/user/.local/share/jupyter/kernels/scala\n\n# 4. Lancer Jupyter\njupyter notebook\n# â†’ Nouveau notebook â†’ Kernel \"Scala\"\n\n\nConfiguration Spark dans Almond\n// Dans une cellule du notebook Scala\n\n// Importer les dÃ©pendances avec Ammonite\nimport $ivy.`org.apache.spark::spark-sql:3.5.0`\nimport $ivy.`io.delta::delta-spark:3.1.0`\n\n// CrÃ©er la SparkSession\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder()\n  .appName(\"Almond Spark\")\n  .master(\"local[*]\")\n  .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n  .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n  .getOrCreate()\n\nimport spark.implicits._\n\n// Test\nval df = Seq((1, \"Alice\"), (2, \"Bob\")).toDF(\"id\", \"name\")\ndf.show()\n\n\nAlternatives\n\n\n\nKernel\nAvantages\nInconvÃ©nients\n\n\n\n\nAlmond\nModerne, Ammonite, bien maintenu\nSetup manuel\n\n\nspylon-kernel\nSimple, pip install\nMoins de features\n\n\nApache Toree\nOfficiel Apache Spark\nPlus lourd, moins actif\n\n\n\n\n\n\n2.2 IntelliJ IDEA\nPour les projets de production, IntelliJ IDEA est lâ€™IDE de rÃ©fÃ©rence pour Scala.\n\nInstallation\n# 1. TÃ©lÃ©charger IntelliJ IDEA Community Edition (gratuit)\n# https://www.jetbrains.com/idea/download/\n\n# 2. Linux : extraire et lancer\ntar -xzf ideaIC-*.tar.gz\ncd idea-IC-*/bin\n./idea.sh\n\n# 3. Installer le plugin Scala\n# File â†’ Settings â†’ Plugins â†’ Marketplace â†’ Rechercher \"Scala\" â†’ Install\n\n# 4. Configurer le JDK\n# File â†’ Project Structure â†’ SDKs â†’ + â†’ Download JDK â†’ Version 11 ou 17\n\n\n\n2.3 Quand utiliser quoi ?\n\n\n\n\n\n\n\nEnvironnement\nUse case\n\n\n\n\nNotebook (Almond)\nExploration, prototypage, formation, dÃ©mos\n\n\nIntelliJ + sbt\nDÃ©veloppement, tests, refactoring, projets production\n\n\nspark-submit\nExÃ©cution sur cluster (YARN, K8s)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#projet-complet-avec-intellij-step-by-step",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#projet-complet-avec-intellij-step-by-step",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "3. Projet Complet avec IntelliJ (Step-by-Step)",
    "text": "3. Projet Complet avec IntelliJ (Step-by-Step)\nOn va crÃ©er un projet Spark/Scala complet de A Ã  Z.\n\n3.1 CrÃ©er le projet\n\nFile â†’ New â†’ Project\nSÃ©lectionner sbt (Ã  gauche)\nConfigurer :\n\nName : spark-etl-project\nLocation : /home/user/projects/spark-etl-project\nJDK : 11 ou 17\nsbt version : 1.9.x\nScala version : 2.12.18 (compatible Spark 3.5)\n\nCreate\n\n\n\n3.2 Structure du projet\nspark-etl-project/\nâ”œâ”€â”€ build.sbt                      # DÃ©pendances et config\nâ”œâ”€â”€ project/\nâ”‚   â”œâ”€â”€ build.properties           # Version sbt\nâ”‚   â””â”€â”€ plugins.sbt                # Plugins (sbt-assembly)\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ main/\nâ”‚   â”‚   â”œâ”€â”€ scala/\nâ”‚   â”‚   â”‚   â””â”€â”€ com/\nâ”‚   â”‚   â”‚       â””â”€â”€ example/\nâ”‚   â”‚   â”‚           â”œâ”€â”€ Main.scala\nâ”‚   â”‚   â”‚           â”œâ”€â”€ config/\nâ”‚   â”‚   â”‚           â”‚   â””â”€â”€ AppConfig.scala\nâ”‚   â”‚   â”‚           â”œâ”€â”€ jobs/\nâ”‚   â”‚   â”‚           â”‚   â””â”€â”€ SalesETL.scala\nâ”‚   â”‚   â”‚           â”œâ”€â”€ models/\nâ”‚   â”‚   â”‚           â”‚   â””â”€â”€ Models.scala\nâ”‚   â”‚   â”‚           â””â”€â”€ utils/\nâ”‚   â”‚   â”‚               â””â”€â”€ SparkSessionWrapper.scala\nâ”‚   â”‚   â””â”€â”€ resources/\nâ”‚   â”‚       â”œâ”€â”€ application.conf\nâ”‚   â”‚       â””â”€â”€ log4j2.properties\nâ”‚   â””â”€â”€ test/\nâ”‚       â””â”€â”€ scala/\nâ”‚           â””â”€â”€ com/\nâ”‚               â””â”€â”€ example/\nâ”‚                   â””â”€â”€ jobs/\nâ”‚                       â””â”€â”€ SalesETLSpec.scala\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ input/\nâ”‚       â””â”€â”€ sales.csv\nâ””â”€â”€ output/                        # GÃ©nÃ©rÃ©\n\n\n3.3 Fichiers de configuration\nbuild.sbt :\nname := \"spark-etl-project\"\nversion := \"1.0.0\"\nscalaVersion := \"2.12.18\"\n\n// Spark 3.5 (provided = dÃ©jÃ  sur le cluster)\nval sparkVersion = \"3.5.0\"\n\nlibraryDependencies ++= Seq(\n  // Spark (provided pour le cluster, compile pour local)\n  \"org.apache.spark\" %% \"spark-sql\"  % sparkVersion % \"provided\",\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion % \"provided\",\n  \n  // Delta Lake\n  \"io.delta\" %% \"delta-spark\" % \"3.1.0\",\n  \n  // Config\n  \"com.typesafe\" % \"config\" % \"1.4.3\",\n  \n  // Tests\n  \"org.scalatest\" %% \"scalatest\" % \"3.2.17\" % Test\n)\n\n// Pour exÃ©cuter en local dans IntelliJ (override provided)\nCompile / run := Defaults.runTask(\n  Compile / fullClasspath,\n  Compile / run / mainClass,\n  Compile / run / runner\n).evaluated\n\n// Assembly config (fat JAR)\nassembly / assemblyMergeStrategy := {\n  case PathList(\"META-INF\", xs @ _*) =&gt; MergeStrategy.discard\n  case \"reference.conf\"              =&gt; MergeStrategy.concat\n  case x                              =&gt; MergeStrategy.first\n}\n\nassembly / assemblyJarName := s\"${name.value}-${version.value}.jar\"\nproject/build.properties :\nsbt.version=1.9.8\nproject/plugins.sbt :\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"2.1.5\")\n\n\n3.4 Code Scala\nsrc/main/scala/com/example/utils/SparkSessionWrapper.scala :\npackage com.example.utils\n\nimport org.apache.spark.sql.SparkSession\n\ntrait SparkSessionWrapper {\n  \n  lazy val spark: SparkSession = SparkSession.builder()\n    .appName(\"SparkETL\")\n    .master(sys.env.getOrElse(\"SPARK_MASTER\", \"local[*]\"))\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\"spark.sql.catalog.spark_catalog\", \n            \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n    .config(\"spark.sql.adaptive.enabled\", \"true\")\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n    .getOrCreate()\n    \n  // RÃ©duire les logs Spark\n  spark.sparkContext.setLogLevel(\"WARN\")\n}\nsrc/main/scala/com/example/models/Models.scala :\npackage com.example.models\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ModÃ¨les de donnÃ©es (case classes)\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ncase class Sale(\n  transactionId: String,\n  productCategory: String,\n  amount: Double,\n  quantity: Int,\n  date: String,\n  customerId: String\n)\n\ncase class SalesSummary(\n  productCategory: String,\n  totalSales: Double,\n  totalQuantity: Long,\n  avgSale: Double,\n  transactionCount: Long\n)\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ADT pour les rÃ©sultats de job\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsealed trait JobResult\ncase class JobSuccess(\n  recordsRead: Long,\n  recordsWritten: Long,\n  durationMs: Long\n) extends JobResult\n\ncase class JobFailure(\n  stage: String,\n  error: String\n) extends JobResult\n\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n// ADT pour les erreurs ETL\n// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nsealed trait ETLError\ncase class SourceNotFound(path: String) extends ETLError\ncase class InvalidSchema(message: String) extends ETLError\ncase class TransformError(message: String) extends ETLError\ncase class WriteError(message: String) extends ETLError\nsrc/main/scala/com/example/jobs/SalesETL.scala :\npackage com.example.jobs\n\nimport com.example.models._\nimport com.example.utils.SparkSessionWrapper\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport org.apache.spark.sql.functions._\n\nobject SalesETL extends SparkSessionWrapper {\n  \n  import spark.implicits._\n  \n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  // EXTRACT\n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  \n  def extract(inputPath: String): Either[ETLError, Dataset[Sale]] = {\n    try {\n      val df = spark.read\n        .option(\"header\", \"true\")\n        .option(\"inferSchema\", \"true\")\n        .csv(inputPath)\n        \n      // Renommer les colonnes pour matcher la case class\n      val cleaned = df\n        .withColumnRenamed(\"transaction_id\", \"transactionId\")\n        .withColumnRenamed(\"product_category\", \"productCategory\")\n        .withColumnRenamed(\"customer_id\", \"customerId\")\n        \n      Right(cleaned.as[Sale])\n    } catch {\n      case e: org.apache.spark.sql.AnalysisException =&gt;\n        Left(SourceNotFound(inputPath))\n      case e: Exception =&gt;\n        Left(InvalidSchema(e.getMessage))\n    }\n  }\n  \n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  // TRANSFORM\n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  \n  def transform(sales: Dataset[Sale]): Either[ETLError, Dataset[SalesSummary]] = {\n    try {\n      val summary = sales\n        .filter(_.amount &gt; 0)\n        .groupByKey(_.productCategory)\n        .agg(\n          sum($\"amount\").as[Double],\n          sum($\"quantity\").as[Long],\n          avg($\"amount\").as[Double],\n          count(\"*\").as[Long]\n        )\n        .map { case (category, total, qty, avgSale, count) =&gt;\n          SalesSummary(category, total, qty, avgSale, count)\n        }\n        \n      Right(summary)\n    } catch {\n      case e: Exception =&gt;\n        Left(TransformError(e.getMessage))\n    }\n  }\n  \n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  // LOAD\n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  \n  def load(data: Dataset[SalesSummary], outputPath: String): Either[ETLError, Long] = {\n    try {\n      data.write\n        .format(\"delta\")\n        .mode(\"overwrite\")\n        .save(outputPath)\n        \n      Right(data.count())\n    } catch {\n      case e: Exception =&gt;\n        Left(WriteError(e.getMessage))\n    }\n  }\n  \n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  // RUN (orchestration avec for-comprehension)\n  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n  \n  def run(inputPath: String, outputPath: String): Either[ETLError, JobSuccess] = {\n    val startTime = System.currentTimeMillis()\n    \n    for {\n      sales   &lt;- extract(inputPath)\n      _       = println(s\"ğŸ“¥ Extracted ${sales.count()} records\")\n      \n      summary &lt;- transform(sales)\n      _       = println(s\"ğŸ”„ Transformed to ${summary.count()} categories\")\n      \n      count   &lt;- load(summary, outputPath)\n      _       = println(s\"ğŸ“¤ Loaded $count records to $outputPath\")\n      \n    } yield JobSuccess(\n      recordsRead = sales.count(),\n      recordsWritten = count,\n      durationMs = System.currentTimeMillis() - startTime\n    )\n  }\n}\nsrc/main/scala/com/example/Main.scala :\npackage com.example\n\nimport com.example.jobs.SalesETL\nimport com.example.models._\n\nobject Main {\n  \n  def main(args: Array[String]): Unit = {\n    \n    val inputPath = args.lift(0).getOrElse(\"data/input/sales.csv\")\n    val outputPath = args.lift(1).getOrElse(\"output/sales_summary\")\n    \n    println(s\"ğŸš€ Starting ETL Job\")\n    println(s\"   Input:  $inputPath\")\n    println(s\"   Output: $outputPath\")\n    println()\n    \n    SalesETL.run(inputPath, outputPath) match {\n      case Right(JobSuccess(read, written, duration)) =&gt;\n        println()\n        println(s\"âœ… Job completed successfully!\")\n        println(s\"   Records read:    $read\")\n        println(s\"   Records written: $written\")\n        println(s\"   Duration:        ${duration}ms\")\n        \n      case Left(SourceNotFound(path)) =&gt;\n        System.err.println(s\"âŒ Source not found: $path\")\n        System.exit(1)\n        \n      case Left(InvalidSchema(msg)) =&gt;\n        System.err.println(s\"âŒ Invalid schema: $msg\")\n        System.exit(1)\n        \n      case Left(TransformError(msg)) =&gt;\n        System.err.println(s\"âŒ Transform error: $msg\")\n        System.exit(1)\n        \n      case Left(WriteError(msg)) =&gt;\n        System.err.println(s\"âŒ Write error: $msg\")\n        System.exit(1)\n    }\n    \n    // ArrÃªter Spark proprement\n    SalesETL.spark.stop()\n  }\n}\n\n\n3.5 DonnÃ©es de test\ndata/input/sales.csv :\ntransaction_id,product_category,amount,quantity,date,customer_id\nTXN001,Electronics,1299.99,1,2024-01-15,CUST001\nTXN002,Clothing,89.99,3,2024-01-15,CUST002\nTXN003,Electronics,599.99,1,2024-01-16,CUST001\nTXN004,Food,45.50,10,2024-01-16,CUST003\nTXN005,Clothing,129.99,2,2024-01-17,CUST002\nTXN006,Electronics,199.99,2,2024-01-17,CUST004\nTXN007,Food,78.25,5,2024-01-18,CUST001\nTXN008,Clothing,299.99,1,2024-01-18,CUST005\nTXN009,Electronics,899.99,1,2024-01-19,CUST003\nTXN010,Food,156.00,8,2024-01-19,CUST002\n\n\n3.6 ExÃ©cuter dans IntelliJ\n\nClic droit sur Main.scala â†’ Run â€˜Mainâ€™\nOu crÃ©er une Run Configuration :\n\nRun â†’ Edit Configurations â†’ + â†’ Application\nMain class : com.example.Main\nProgram arguments : data/input/sales.csv output/sales_summary\nVM options : --add-opens java.base/sun.nio.ch=ALL-UNNAMED (Java 17)\n\n\n\n\n3.7 Builder le JAR\n# Dans le terminal IntelliJ (View â†’ Tool Windows â†’ Terminal)\n\n# Compiler\nsbt compile\n\n# CrÃ©er le fat JAR (inclut toutes les dÃ©pendances sauf Spark)\nsbt assembly\n\n# Le JAR est crÃ©Ã© dans :\n# target/scala-2.12/spark-etl-project-1.0.0.jar\n\n\n3.8 ExÃ©cuter avec spark-submit (local)\nspark-submit \\\n  --master local[*] \\\n  --driver-memory 2g \\\n  --class com.example.Main \\\n  --packages io.delta:delta-spark_2.12:3.1.0 \\\n  target/scala-2.12/spark-etl-project-1.0.0.jar \\\n  data/input/sales.csv \\\n  output/sales_summary\n\n\n3.9 ExÃ©cuter sur cluster (YARN/K8s)\n# YARN\nspark-submit \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --driver-memory 4g \\\n  --executor-memory 8g \\\n  --executor-cores 4 \\\n  --num-executors 10 \\\n  --class com.example.Main \\\n  --packages io.delta:delta-spark_2.12:3.1.0 \\\n  spark-etl-project-1.0.0.jar \\\n  hdfs:///data/input/sales.csv \\\n  hdfs:///data/output/sales_summary\n\n# Kubernetes (voir M27)\nspark-submit \\\n  --master k8s://https://k8s-api:6443 \\\n  --deploy-mode cluster \\\n  --conf spark.kubernetes.container.image=my-spark:3.5.0 \\\n  --class com.example.Main \\\n  local:///opt/spark/jars/spark-etl-project-1.0.0.jar",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#connecteurs-jars-et-configuration",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#connecteurs-jars-et-configuration",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "4. Connecteurs : JARs et Configuration",
    "text": "4. Connecteurs : JARs et Configuration\n\n4.1 JARs requis par source de donnÃ©es\n\n\n\n\n\n\n\n\nSource\nJAR\nMaven coordinates\n\n\n\n\nPostgreSQL\npostgresql-42.7.0.jar\norg.postgresql:postgresql:42.7.0\n\n\nMySQL\nmysql-connector-j-8.0.33.jar\ncom.mysql:mysql-connector-j:8.0.33\n\n\nSQL Server\nmssql-jdbc-12.4.0.jar\ncom.microsoft.sqlserver:mssql-jdbc:12.4.0\n\n\nOracle\nojdbc11-23.3.0.0.jar\ncom.oracle.database.jdbc:ojdbc11:23.3.0.0\n\n\nKafka\nspark-sql-kafka-0-10_2.12-3.5.0.jar\norg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\n\n\nDelta Lake\ndelta-spark_2.12-3.1.0.jar\nio.delta:delta-spark_2.12:3.1.0\n\n\nIceberg\niceberg-spark-runtime-3.5_2.12-1.4.0.jar\norg.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0\n\n\nAWS S3\nhadoop-aws-3.3.4.jar + aws-java-sdk-bundle\norg.apache.hadoop:hadoop-aws:3.3.4\n\n\nGCS\ngcs-connector-hadoop3-2.2.17.jar\nâ€” (tÃ©lÃ©chargement manuel)\n\n\n\n\n\n4.2 Ajouter dans build.sbt\nlibraryDependencies ++= Seq(\n  // Spark (provided = dÃ©jÃ  sur le cluster)\n  \"org.apache.spark\" %% \"spark-sql\" % \"3.5.0\" % \"provided\",\n  \n  // Connecteurs (compile = inclus dans le JAR)\n  \"org.postgresql\" % \"postgresql\" % \"42.7.0\",\n  \"org.apache.spark\" %% \"spark-sql-kafka-0-10\" % \"3.5.0\",\n  \"io.delta\" %% \"delta-spark\" % \"3.1.0\",\n)\n\n\n4.3 spark-submit : â€“jars vs â€“packages\n\n\n\nOption\nUsage\n\n\n\n\n--jars\nChemins locaux ou HDFS vers des JARs\n\n\n--packages\nCoordonnÃ©es Maven (tÃ©lÃ©chargement automatique)\n\n\n--repositories\nRepos Maven custom\n\n\n--driver-class-path\nJARs pour le driver uniquement\n\n\n\n# Avec JARs locaux\nspark-submit \\\n  --jars /path/to/postgresql-42.7.0.jar,/path/to/delta-spark_2.12-3.1.0.jar \\\n  my-app.jar\n\n# Avec tÃ©lÃ©chargement Maven\nspark-submit \\\n  --packages org.postgresql:postgresql:42.7.0,io.delta:delta-spark_2.12:3.1.0 \\\n  my-app.jar\n\n# Repo custom\nspark-submit \\\n  --repositories https://my-company.jfrog.io/artifactory/libs-release \\\n  --packages com.mycompany:my-lib:1.0.0 \\\n  my-app.jar\n\n\n4.4 Exemples de connexion\n\nâ„¹ï¸ Note : Ces exemples supposent que vous avez accÃ¨s aux services correspondants.\n\nPostgreSQL :\nval df = spark.read\n  .format(\"jdbc\")\n  .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\")\n  .option(\"dbtable\", \"customers\")\n  .option(\"user\", \"postgres\")\n  .option(\"password\", \"secret\")\n  .option(\"driver\", \"org.postgresql.Driver\")\n  .load()\nKafka :\nval df = spark.readStream\n  .format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n  .option(\"subscribe\", \"events\")\n  .option(\"startingOffsets\", \"earliest\")\n  .load()\nDelta Lake :\n// Lecture\nval df = spark.read.format(\"delta\").load(\"/path/to/delta\")\n\n// Ã‰criture\ndf.write.format(\"delta\").mode(\"overwrite\").save(\"/path/to/delta\")\n\n// Time travel\nval dfV2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"/path/to/delta\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#catalyst-optimizer",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#catalyst-optimizer",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "5. Catalyst Optimizer",
    "text": "5. Catalyst Optimizer\nCatalyst est le moteur dâ€™optimisation de Spark SQL. Il transforme ton code en un plan dâ€™exÃ©cution optimal.\n\n5.1 Phases dâ€™optimisation\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         CATALYST OPTIMIZER                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   Code SQL/DataFrame                                                        â”‚\nâ”‚         â”‚                                                                   â”‚\nâ”‚         â–¼                                                                   â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚    ANALYSIS     â”‚  RÃ©soudre les noms de colonnes, tables               â”‚\nâ”‚   â”‚                 â”‚  VÃ©rifier les types                                   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ LOGICAL OPTIM   â”‚  Predicate pushdown, Column pruning                  â”‚\nâ”‚   â”‚                 â”‚  Constant folding, Filter reordering                 â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ PHYSICAL PLAN   â”‚  Choisir les algorithmes (Sort-Merge vs Broadcast)   â”‚\nâ”‚   â”‚                 â”‚  Cost-Based Optimization                             â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ CODE GENERATION â”‚  GÃ©nÃ©rer du bytecode Java optimisÃ©                   â”‚\nâ”‚   â”‚   (Tungsten)    â”‚  Whole-Stage Code Generation                         â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚        EXECUTION                                                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.2 Lire un plan dâ€™exÃ©cution\nval df = spark.read.parquet(\"data/sales\")\n  .filter($\"amount\" &gt; 100)\n  .groupBy(\"category\")\n  .agg(sum(\"amount\"))\n\n// Plan simple\ndf.explain()\n\n// Plan dÃ©taillÃ© (Parsed â†’ Analyzed â†’ Optimized â†’ Physical)\ndf.explain(true)\n\n// Plan formatÃ© (Spark 3.0+)\ndf.explain(\"formatted\")\n\n// Plan avec coÃ»ts (si CBO activÃ©)\ndf.explain(\"cost\")\n\n\n5.3 Exemple de plan\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[category#12], functions=[sum(amount#14)])\n   +- Exchange hashpartitioning(category#12, 200)        â† SHUFFLE\n      +- HashAggregate(keys=[category#12], functions=[partial_sum(amount#14)])\n         +- Project [category#12, amount#14]             â† Colonnes sÃ©lectionnÃ©es\n            +- Filter (amount#14 &gt; 100)                  â† Filtre pushdown\n               +- FileScan parquet [category#12,amount#14]  â† Lecture\nCe quâ€™il faut regarder : - Exchange = Shuffle (coÃ»teux !) - BroadcastHashJoin vs SortMergeJoin (broadcast = plus rapide si petite table) - FileScan : colonnes pruned, partitions pruned - Filter : poussÃ© au plus prÃ¨s de la source\n\n\n5.4 Cost-Based Optimization (CBO)\n// Activer CBO\nspark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\nspark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", \"true\")\n\n// Collecter les statistiques (important !)\nspark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS\")\nspark.sql(\"ANALYZE TABLE sales COMPUTE STATISTICS FOR COLUMNS amount, category\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#adaptive-query-execution-aqe",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#adaptive-query-execution-aqe",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "6. Adaptive Query Execution (AQE)",
    "text": "6. Adaptive Query Execution (AQE)\nAQE (Spark 3.0+) optimise le plan dâ€™exÃ©cution pendant lâ€™exÃ©cution, en se basant sur les statistiques rÃ©elles.\n\n6.1 FonctionnalitÃ©s\n\n\n\n\n\n\n\nFeature\nDescription\n\n\n\n\nCoalesce Partitions\nFusionner les petites partitions aprÃ¨s shuffle\n\n\nBroadcast Join Conversion\nConvertir Sort-Merge â†’ Broadcast si table petite\n\n\nSkew Join Optimization\nSplitter les partitions dÃ©sÃ©quilibrÃ©es\n\n\n\n\n\n6.2 Configuration\n// Activer AQE (activÃ© par dÃ©faut depuis Spark 3.2)\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n\n// Coalesce automatique\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\")\n\n// Broadcast dynamique\nspark.conf.set(\"spark.sql.adaptive.autoBroadcastJoinThreshold\", \"10MB\")\n\n// Skew join\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"5\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"256MB\")\n\n\n6.3 Voir AQE en action\n// Le plan montre \"AdaptiveSparkPlan\"\ndf.explain()\n\n// == Physical Plan ==\n// AdaptiveSparkPlan isFinalPlan=true    â† AQE activÃ© !\n// +- ...\n\n// AprÃ¨s exÃ©cution, voir le plan final\ndf.collect()  // ExÃ©cuter d'abord\ndf.explain()  // Maintenant isFinalPlan=true",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#tungsten-engine",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#tungsten-engine",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "7. Tungsten Engine",
    "text": "7. Tungsten Engine\nTungsten est le moteur dâ€™exÃ©cution bas-niveau de Spark, optimisÃ© pour le CPU et la mÃ©moire.\n\n7.1 Optimisations Tungsten\n\n\n\nOptimisation\nDescription\n\n\n\n\nOff-heap Memory\nStockage hors JVM heap (Ã©vite GC)\n\n\nBinary Format\nDonnÃ©es en format binaire compact\n\n\nCache-aware\nAlgorithmes optimisÃ©s pour le cache CPU\n\n\nWhole-Stage CodeGen\nCompile le plan en bytecode Java\n\n\n\n\n\n7.2 Whole-Stage Code Generation\nAu lieu dâ€™appeler des fonctions virtuelles pour chaque row, Tungsten gÃ©nÃ¨re du code spÃ©cialisÃ©.\nSANS CODEGEN                          AVEC CODEGEN\n                                      \nfor (row in data) {                   // Code gÃ©nÃ©rÃ© automatiquement\n  filter.eval(row)   â† virtual call   for (row in data) {\n  project.eval(row)  â† virtual call     if (row.amount &gt; 100) {\n  agg.update(row)    â† virtual call       sum += row.amount\n}                                       }\n                                      }\n\n\n7.3 Voir le code gÃ©nÃ©rÃ©\n// Activer le debug\nspark.conf.set(\"spark.sql.codegen.wholeStage\", \"true\")\n\n// Voir le code gÃ©nÃ©rÃ©\ndf.queryExecution.debug.codegen()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#memory-management-tuning",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#memory-management-tuning",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "8. Memory Management & Tuning",
    "text": "8. Memory Management & Tuning\n\n8.1 Architecture MÃ©moire Spark\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     EXECUTOR MEMORY (spark.executor.memory)                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                SPARK MEMORY (spark.memory.fraction = 0.6)           â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚\nâ”‚   â”‚   â”‚   STORAGE MEMORY    â”‚   â”‚      EXECUTION MEMORY           â”‚    â”‚  â”‚\nâ”‚   â”‚   â”‚                     â”‚   â”‚                                 â”‚    â”‚  â”‚\nâ”‚   â”‚   â”‚   Cache, Broadcast  â”‚ âŸ· â”‚   Shuffle, Join, Sort, Agg     â”‚    â”‚  â”‚\nâ”‚   â”‚   â”‚                     â”‚   â”‚                                 â”‚    â”‚  â”‚\nâ”‚   â”‚   â”‚   (storageFraction  â”‚   â”‚   (1 - storageFraction          â”‚    â”‚  â”‚\nâ”‚   â”‚   â”‚    = 0.5)           â”‚   â”‚    = 0.5)                       â”‚    â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   â† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Unified Memory (frontiÃ¨re flexible) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    USER MEMORY (0.4)                                â”‚  â”‚\nâ”‚   â”‚   Structures internes, UDFs, mÃ©tadonnÃ©es                            â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    RESERVED MEMORY (300MB fixe)                     â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n8.2 Configurations MÃ©moire\n// MÃ©moire totale executor\nspark.conf.set(\"spark.executor.memory\", \"8g\")\n\n// Fraction pour Spark (vs User)\nspark.conf.set(\"spark.memory.fraction\", \"0.6\")  // 60% pour Spark\n\n// Fraction Storage dans Spark Memory\nspark.conf.set(\"spark.memory.storageFraction\", \"0.5\")  // 50% Storage, 50% Execution\n\n// Off-heap (Tungsten)\nspark.conf.set(\"spark.memory.offHeap.enabled\", \"true\")\nspark.conf.set(\"spark.memory.offHeap.size\", \"4g\")\n\n\n8.3 Pourquoi 80% des Jobs Spark sont Lents ğŸ”¥\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚            LES 5 CAUSES DE 80% DES JOBS SPARK LENTS                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   1ï¸âƒ£ DATA SKEW (40%)                                                        â”‚\nâ”‚      â””â”€ 1 partition avec 10M rows, les autres avec 1K                       â”‚\nâ”‚      â””â”€ SymptÃ´me : 199 tasks Ã  2s, 1 task Ã  45min                          â”‚\nâ”‚      â””â”€ Fix : salting, repartition par clÃ©, AQE skew join                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   2ï¸âƒ£ SHUFFLE EXCESSIF (25%)                                                 â”‚\nâ”‚      â””â”€ Trop de groupBy/join, pas de broadcast                              â”‚\nâ”‚      â””â”€ SymptÃ´me : \"Shuffle Write\" Ã©norme dans Spark UI                    â”‚\nâ”‚      â””â”€ Fix : broadcast join, rÃ©duire colonnes avant shuffle               â”‚\nâ”‚                                                                             â”‚\nâ”‚   3ï¸âƒ£ MAUVAIS PARTITIONING (15%)                                             â”‚\nâ”‚      â””â”€ 200 partitions par dÃ©faut, fichiers trop petits/gros                â”‚\nâ”‚      â””â”€ SymptÃ´me : 10000 tasks de 100KB ou 2 tasks de 50GB                 â”‚\nâ”‚      â””â”€ Fix : repartition/coalesce, AQE coalesce, tuning shuffle.partitionsâ”‚\nâ”‚                                                                             â”‚\nâ”‚   4ï¸âƒ£ SPILL TO DISK (10%)                                                    â”‚\nâ”‚      â””â”€ MÃ©moire insuffisante, donnÃ©es Ã©crites sur disque                    â”‚\nâ”‚      â””â”€ SymptÃ´me : \"Spill (Memory)\" dans Stage details                     â”‚\nâ”‚      â””â”€ Fix : augmenter executor memory, rÃ©duire taille partitions         â”‚\nâ”‚                                                                             â”‚\nâ”‚   5ï¸âƒ£ SMALL FILES PROBLEM (10%)                                              â”‚\nâ”‚      â””â”€ Lire 10000 fichiers de 1MB au lieu de 100 de 100MB                  â”‚\nâ”‚      â””â”€ SymptÃ´me : temps de listing S3 trÃ¨s long, driver OOM               â”‚\nâ”‚      â””â”€ Fix : compaction Delta, maxPartitionBytes, bin-packing             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n8.4 Diagnostic Toolkit\n// 1. VÃ©rifier le plan d'exÃ©cution\ndf.explain(true)          // Plan complet\ndf.explain(\"cost\")        // Avec coÃ»ts estimÃ©s\n\n// 2. Voir le nombre de partitions\nprintln(s\"Partitions: ${df.rdd.getNumPartitions}\")\n\n// 3. DÃ©tecter le skew (distribution par partition)\nimport org.apache.spark.sql.functions._\ndf.withColumn(\"partition_id\", spark_partition_id())\n  .groupBy(\"partition_id\")\n  .count()\n  .orderBy(desc(\"count\"))\n  .show()\n\n// 4. Voir la taille des partitions\ndf.rdd.mapPartitionsWithIndex { case (idx, iter) =&gt;\n  Iterator((idx, iter.size))\n}.toDF(\"partition\", \"rows\").show()\n\n// 5. MÃ©triques pendant l'exÃ©cution\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n\n\n8.5 Spark UI : Ce quâ€™il faut regarder\n\n\n\nOnglet\nCe quâ€™on cherche\n\n\n\n\nJobs\nDurÃ©e totale, jobs qui traÃ®nent\n\n\nStages\nShuffle Read/Write, Spill, tÃ¢ches en retard\n\n\nTasks\nDistribution (min/median/max), stragglers\n\n\nStorage\nCache utilisÃ©, mÃ©moire disponible\n\n\nSQL\nPlan physique, mÃ©triques par opÃ©rateur\n\n\nExecutors\nMÃ©moire, GC time, shuffle read/write\n\n\n\n\n\n8.6 Debugging OOM\njava.lang.OutOfMemoryError: Java heap space\n\nCAUSES PROBABLES :\nâ”œâ”€â”€ collect() sur un gros DataFrame\nâ”œâ”€â”€ broadcast() d'une table trop grande\nâ”œâ”€â”€ Trop de partitions small â†’ overhead\nâ”œâ”€â”€ Skew â†’ 1 executor avec trop de donnÃ©es\nâ””â”€â”€ Driver OOM â†’ trop de metadata / rÃ©sultats\n\nSOLUTIONS :\nâ”œâ”€â”€ Augmenter spark.executor.memory\nâ”œâ”€â”€ Augmenter spark.driver.memory (si driver OOM)\nâ”œâ”€â”€ Repartitionner les donnÃ©es\nâ”œâ”€â”€ Ã‰viter collect(), utiliser take() ou write()\nâ””â”€â”€ RÃ©duire spark.sql.autoBroadcastJoinThreshold",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#exercices-pratiques",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#exercices-pratiques",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "9. Exercices Pratiques",
    "text": "9. Exercices Pratiques\n\nExercice 1 : Pratique Scala â€” ADT Pipeline\nModÃ©liser un pipeline de traitement avec des ADT.\n// TODO: CrÃ©er un sealed trait pour les Ã©tapes du pipeline\n// Ã‰tapes : Extract, Validate, Transform, Load\n// Chaque Ã©tape peut Success ou Fail\n\n// TODO: ImplÃ©menter une fonction qui exÃ©cute le pipeline\n// et retourne un rapport dÃ©taillÃ© de chaque Ã©tape\n\n\nğŸ’¡ Solution\n\nsealed trait PipelineStep\ncase object Extract extends PipelineStep\ncase object Validate extends PipelineStep\ncase object Transform extends PipelineStep\ncase object Load extends PipelineStep\n\nsealed trait StepResult\ncase class StepSuccess(step: PipelineStep, durationMs: Long, records: Long) extends StepResult\ncase class StepFailure(step: PipelineStep, error: String) extends StepResult\n\ncase class PipelineReport(results: List[StepResult]) {\n  def isSuccess: Boolean = results.forall(_.isInstanceOf[StepSuccess])\n  def failedStep: Option[StepFailure] = results.collectFirst { case f: StepFailure =&gt; f }\n}\n\n\n\n\nExercice 2 : Either pour ETL\nImplÃ©menter un mini-ETL avec gestion dâ€™erreurs typÃ©e.\n// TODO: CrÃ©er une fonction qui :\n// 1. Lit un fichier CSV (peut Ã©chouer : FileNotFound)\n// 2. Valide que la colonne \"amount\" existe (peut Ã©chouer : SchemaError)\n// 3. Filtre les montants &gt; 0 (peut Ã©chouer : DataError si 0 records)\n// 4. Ã‰crit en Parquet\n//\n// Utiliser Either[ETLError, DataFrame] Ã  chaque Ã©tape\n// ChaÃ®ner avec for-comprehension\n\n\n\nExercice 3 : Setup Almond + Spark\n\nInstaller Almond (suivre les instructions section 2.1)\nCrÃ©er un notebook Scala\nImporter Spark et Delta Lake\nCrÃ©er un DataFrame, le transformer, lâ€™Ã©crire en Delta\n\n\n\n\nExercice 4 : Projet IntelliJ Complet\n\nCrÃ©er le projet selon la structure section 3\nAjouter les fichiers de config\nImplÃ©menter le code\nCrÃ©er le fichier CSV de test\nExÃ©cuter dans IntelliJ\nBuilder avec sbt assembly\nLancer avec spark-submit\n\n\n\n\nExercice 5 : Diagnostiquer un Job Lent\n// Ce code est intentionnellement lent. Pourquoi ?\nval df1 = spark.read.parquet(\"big_table\")  // 100M rows\nval df2 = spark.read.parquet(\"small_table\")  // 1000 rows\n\nval result = df1\n  .join(df2, \"key\")  // Quel type de join ?\n  .groupBy(\"category\")\n  .agg(sum(\"amount\"))\n  .collect()  // ProblÃ¨me ?\n\n// TODO: \n// 1. Identifier les problÃ¨mes\n// 2. Proposer des optimisations\n// 3. RÃ©Ã©crire le code optimisÃ©\n\n\nğŸ’¡ Solution\n\nProblÃ¨mes : 1. Join sans broadcast â†’ Sort-Merge Join (shuffle de 100M rows) 2. collect() sur un rÃ©sultat potentiellement gros\nSolution :\nimport org.apache.spark.sql.functions.broadcast\n\nval result = df1\n  .join(broadcast(df2), \"key\")  // Broadcast join !\n  .groupBy(\"category\")\n  .agg(sum(\"amount\"))\n  .write.parquet(\"output\")  // Ã‰crire au lieu de collect",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#mini-projet-etl-scala-bronze-silver-gold",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#mini-projet-etl-scala-bronze-silver-gold",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "10. Mini-Projet : ETL Scala Bronze â†’ Silver â†’ Gold",
    "text": "10. Mini-Projet : ETL Scala Bronze â†’ Silver â†’ Gold\n\nObjectif\nConstruire un pipeline ETL complet en Scala avec architecture Medallion.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         MEDALLION ARCHITECTURE                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚   â”‚   BRONZE    â”‚ â”€â”€â”€â–¶ â”‚   SILVER    â”‚ â”€â”€â”€â–¶ â”‚    GOLD     â”‚                â”‚\nâ”‚   â”‚   (Raw)     â”‚      â”‚  (Cleaned)  â”‚      â”‚ (Aggregated)â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                                             â”‚\nâ”‚   CSV/JSON           Delta Lake           Delta Lake                        â”‚\nâ”‚   + metadata         + validation         + business KPIs                   â”‚\nâ”‚   + ingestion_ts     + dedup              + reporting ready                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLivrables\n\nProjet IntelliJ complet avec structure\nADT pour les erreurs et rÃ©sultats\n3 jobs : BronzeJob, SilverJob, GoldJob\nMain qui orchestre avec Either\nTests avec ScalaTest\nFat JAR et script spark-submit\n\n\n\nDonnÃ©es\ndata/raw/transactions_*.json (plusieurs fichiers) :\n{\"transaction_id\": \"TXN001\", \"customer_id\": \"C001\", \"amount\": 150.0, \"category\": \"Electronics\", \"timestamp\": \"2024-01-15T10:30:00Z\"}\n\n\nJobs Ã  implÃ©menter\nBronzeJob : - Lire les JSON - Ajouter _ingestion_timestamp, _source_file - Ã‰crire en Delta (append)\nSilverJob : - Lire Bronze - Valider schÃ©ma - DÃ©dupliquer par transaction_id - Filtrer amount &gt; 0 - Ã‰crire en Delta avec MERGE\nGoldJob : - Lire Silver - AgrÃ©gations : ventes par catÃ©gorie, par jour - Top customers - Ã‰crire plusieurs tables Gold",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#ressources",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#ressources",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation\n\nScala Documentation\nSpark Scala API\nsbt Documentation\nDelta Lake Scala API\n\n\n\nLivres\n\nProgramming in Scala â€” Martin Odersky\nFunctional Programming in Scala â€” Paul Chiusano, RÃºnar Bjarnason\nSpark: The Definitive Guide â€” Bill Chambers, Matei Zaharia\n\n\n\nOutils\n\nIntelliJ IDEA â€” IDE\nAlmond â€” Kernel Scala pour Jupyter\nMetals â€” LSP pour VS Code",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/30_spark_scala_deep_dive.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/30_spark_scala_deep_dive.html#prochaine-Ã©tape",
    "title": "ğŸš€ Spark & Scala Deep Dive",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 31_ml_engineering â€” ML Engineering\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant Scala pour Spark et les optimisations avancÃ©es.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "âš¡ Processing AvancÃ©",
      "30 Â· Spark & Scala Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html",
    "href": "notebooks/advanced/28_advanced_orchestration.html",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas maÃ®triser lâ€™orchestration de pipelines data Ã  lâ€™Ã©chelle. Tu apprendras Ã  dÃ©ployer Airflow sur Kubernetes, Ã  exploiter la TaskFlow API, Ã  comparer les orchestrateurs modernes, et Ã  mettre en place le data lineage avec OpenLineage â€” des compÃ©tences essentielles pour un Data Engineer Senior !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#prÃ©requis",
    "href": "notebooks/advanced/28_advanced_orchestration.html#prÃ©requis",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 12_orchestration_pipelines (Airflow basics)\n\n\nâœ… Requis\nAvoir suivi les modules 15_kubernetes_fundamentals et 27_kubernetes_deep_dive\n\n\nâœ… Requis\nMaÃ®triser les DAGs, Operators, XCom dans Airflow\n\n\nâœ… Requis\nConnaissances solides en Python et Docker\n\n\nğŸ’¡ RecommandÃ©\nUn cluster K8s accessible (Minikube, kind, ou cloud)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#objectifs-du-module",
    "href": "notebooks/advanced/28_advanced_orchestration.html#objectifs-du-module",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nDÃ©ployer et configurer Airflow sur Kubernetes\nUtiliser le KubernetesExecutor et le KubernetesPodOperator\nÃ‰crire des DAGs modernes avec la TaskFlow API\nImplÃ©menter le Dynamic Task Mapping\nComparer et choisir entre Airflow, Dagster et Prefect\nMettre en place le data lineage avec OpenLineage\nUtiliser Astronomer (Astro CLI et Astro SDK) pour industrialiser",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#rappel-ce-quon-a-vu-vs-ce-quon-va-approfondir",
    "href": "notebooks/advanced/28_advanced_orchestration.html#rappel-ce-quon-a-vu-vs-ce-quon-va-approfondir",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "Rappel : Ce quâ€™on a vu vs Ce quâ€™on va approfondir",
    "text": "Rappel : Ce quâ€™on a vu vs Ce quâ€™on va approfondir\n\n\n\nModule M12 (Beginner)\nCe module M28 (Advanced)\n\n\n\n\nArchitecture Airflow basique\nAirflow sur Kubernetes\n\n\nDAGs, Operators simples\nTaskFlow API, Dynamic Mapping\n\n\nXCom manuel\nXCom automatique avec TaskFlow\n\n\nMention des alternatives\nComparatif dÃ©taillÃ© + exemples de code\n\n\nâ€”\nOpenLineage (data lineage)\n\n\nâ€”\nAstronomer (plateforme enterprise)\n\n\n\n\nSchÃ©ma : De lâ€™orchestration basique Ã  lâ€™orchestration avancÃ©e\nM12 Orchestration Basics              M28 Advanced Orchestration (ce module)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ â€¢ Cron / Task Scheduler â”‚           â”‚ â€¢ Airflow sur Kubernetes            â”‚\nâ”‚ â€¢ DAGs basics           â”‚           â”‚ â€¢ KubernetesExecutor                â”‚\nâ”‚ â€¢ Operators simples     â”‚  â”€â”€â”€â”€â”€â”€â–¶  â”‚ â€¢ TaskFlow API                      â”‚\nâ”‚ â€¢ XCom manuel           â”‚           â”‚ â€¢ Dynamic Task Mapping              â”‚\nâ”‚ â€¢ SequentialExecutor    â”‚           â”‚ â€¢ Dagster, Prefect (comparatif)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ â€¢ OpenLineage (lineage)             â”‚\n                                      â”‚ â€¢ Astronomer (enterprise)           â”‚\n                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Ce module est orientÃ© â€œproduction Ã  lâ€™Ã©chelleâ€ â€” tu vas apprendre Ã  faire tourner des centaines de DAGs avec des milliers de tÃ¢ches sur Kubernetes.\n\n\nâ„¹ï¸ Le savais-tu ?\nApache Airflow a Ã©tÃ© crÃ©Ã© par Airbnb en 2014 pour orchestrer leurs pipelines data. Il a Ã©tÃ© donnÃ© Ã  la Apache Foundation en 2016.\nAujourdâ€™hui, Airflow est utilisÃ© par des milliers dâ€™entreprises dont Uber, Lyft, Twitter, Slack, Adobe, et bien dâ€™autres.\nAstronomer, fondÃ© en 2018, est devenu le leader des solutions Airflow managÃ©es, levant plus de 200 millions de dollars et Ã©tant le principal contributeur au projet open-source Airflow.\nğŸ“– History of Apache Airflow",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#airflow-sur-kubernetes",
    "href": "notebooks/advanced/28_advanced_orchestration.html#airflow-sur-kubernetes",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "1. Airflow sur Kubernetes",
    "text": "1. Airflow sur Kubernetes\nDÃ©ployer Airflow sur Kubernetes permet de bÃ©nÃ©ficier de lâ€™Ã©lasticitÃ©, de lâ€™isolation et de la scalabilitÃ© native de K8s.\n\nPourquoi Airflow sur K8s ?\n\n\n\nAspect\nSans K8s (Celery/Local)\nAvec K8s\n\n\n\n\nScaling\nWorkers fixes\nPods Ã  la demande\n\n\nIsolation\nDÃ©pendances partagÃ©es\nChaque tÃ¢che dans son pod\n\n\nRessources\nAllocation statique\nRequests/Limits par tÃ¢che\n\n\nCoÃ»t\nServeurs 24/7\nPay-per-use (pods Ã©phÃ©mÃ¨res)\n\n\nMaintenance\nGÃ©rer les workers\nK8s gÃ¨re tout\n\n\n\n\n\nArchitecture Airflow sur Kubernetes\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        AIRFLOW ON KUBERNETES                                â”‚\nâ”‚                                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚                      Namespace: airflow                              â”‚   â”‚\nâ”‚  â”‚                                                                      â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚  â”‚   â”‚  Scheduler  â”‚   â”‚  Webserver  â”‚   â”‚   Triggerer (Airflow 2) â”‚   â”‚   â”‚\nâ”‚  â”‚   â”‚   (Pod)     â”‚   â”‚   (Pod)     â”‚   â”‚       (Pod)             â”‚   â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚  â”‚          â”‚                                                           â”‚   â”‚\nâ”‚  â”‚          â”‚ CrÃ©e des pods pour chaque tÃ¢che                          â”‚   â”‚\nâ”‚  â”‚          â–¼                                                           â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚\nâ”‚  â”‚   â”‚              Worker Pods (Ã©phÃ©mÃ¨res)                         â”‚   â”‚   â”‚\nâ”‚  â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚   â”‚\nâ”‚  â”‚   â”‚   â”‚Task 1 â”‚  â”‚Task 2 â”‚  â”‚Task 3 â”‚  â”‚Task 4 â”‚  â”‚Task 5 â”‚    â”‚   â”‚   â”‚\nâ”‚  â”‚   â”‚   â”‚ Pod   â”‚  â”‚ Pod   â”‚  â”‚ Pod   â”‚  â”‚ Pod   â”‚  â”‚ Pod   â”‚    â”‚   â”‚   â”‚\nâ”‚  â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚\nâ”‚  â”‚                                                                      â”‚   â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚\nâ”‚  â”‚   â”‚   PostgreSQL    â”‚   â”‚            DAGs (PVC/Git-Sync)      â”‚     â”‚   â”‚\nâ”‚  â”‚   â”‚  (Metadata DB)  â”‚   â”‚                                     â”‚     â”‚   â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLes Executors Airflow\n\n\n\n\n\n\n\n\nExecutor\nDescription\nQuand lâ€™utiliser\n\n\n\n\nSequentialExecutor\n1 tÃ¢che Ã  la fois\nDev/test uniquement\n\n\nLocalExecutor\nParallÃ¨le sur 1 machine\nPetite production\n\n\nCeleryExecutor\nWorkers Celery distribuÃ©s\nProduction classique\n\n\nKubernetesExecutor\n1 pod K8s par tÃ¢che\nProduction K8s\n\n\nCeleryKubernetesExecutor\nHybrid Celery + K8s\nWorkloads mixtes\n\n\n\n\n\nInstallation avec le Helm Chart Officiel\n# Ajouter le repo Helm officiel Airflow\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n# CrÃ©er le namespace\nkubectl create namespace airflow\n\n# Installer Airflow avec KubernetesExecutor\nhelm install airflow apache-airflow/airflow \\\n  --namespace airflow \\\n  --set executor=KubernetesExecutor \\\n  --set webserver.defaultUser.password=admin \\\n  --set dags.persistence.enabled=true \\\n  --set dags.gitSync.enabled=true \\\n  --set dags.gitSync.repo=https://github.com/myorg/airflow-dags.git \\\n  --set dags.gitSync.branch=main \\\n  --set dags.gitSync.subPath=dags\n\n# VÃ©rifier l'installation\nkubectl get pods -n airflow\n\n# AccÃ©der au webserver\nkubectl port-forward svc/airflow-webserver -n airflow 8080:8080\n# http://localhost:8080 (admin / admin)\n\n\nConfiguration values.yaml avancÃ©e\n# values-production.yaml\nexecutor: KubernetesExecutor\n\n# Webserver\nwebserver:\n  replicas: 2\n  resources:\n    requests:\n      cpu: \"500m\"\n      memory: \"1Gi\"\n    limits:\n      cpu: \"1000m\"\n      memory: \"2Gi\"\n\n# Scheduler\nscheduler:\n  replicas: 2    # HA Scheduler (Airflow 2.0+)\n  resources:\n    requests:\n      cpu: \"500m\"\n      memory: \"1Gi\"\n\n# Configuration KubernetesExecutor\nconfig:\n  kubernetes:\n    # Namespace pour les worker pods\n    namespace: airflow\n    # Supprimer les pods aprÃ¨s exÃ©cution\n    delete_worker_pods: \"True\"\n    delete_worker_pods_on_failure: \"False\"  # Garder pour debug\n    # Image par dÃ©faut pour les workers\n    worker_container_repository: apache/airflow\n    worker_container_tag: 2.8.0-python3.11\n\n# Git-Sync pour les DAGs\ndags:\n  persistence:\n    enabled: false\n  gitSync:\n    enabled: true\n    repo: git@github.com:myorg/airflow-dags.git\n    branch: main\n    subPath: dags\n    sshKeySecret: airflow-git-ssh-key\n    wait: 60  # Sync toutes les 60 secondes\n\n# Logs dans un stockage externe\nlogs:\n  persistence:\n    enabled: true\n    size: 10Gi\n\n# PostgreSQL (ou utiliser un service externe)\npostgresql:\n  enabled: true\n  auth:\n    postgresPassword: airflow\n    username: airflow\n    password: airflow\n    database: airflow\n\n\nKubernetesExecutor : Comment Ã§a marche\n1. DAG est schedulÃ©\n       â”‚\n       â–¼\n2. Scheduler parse le DAG et identifie les tÃ¢ches Ã  exÃ©cuter\n       â”‚\n       â–¼\n3. Pour chaque tÃ¢che, le Scheduler crÃ©e un Pod K8s\n       â”‚\n   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚  Pod Spec gÃ©nÃ©rÃ© automatiquement :        â”‚\n   â”‚  - Image: airflow (ou custom)             â”‚\n   â”‚  - Command: airflow tasks run ...         â”‚\n   â”‚  - Env: connexions, variables             â”‚\n   â”‚  - Resources: requests/limits             â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n       â”‚\n       â–¼\n4. K8s schedule le pod sur un node\n       â”‚\n       â–¼\n5. La tÃ¢che s'exÃ©cute\n       â”‚\n       â–¼\n6. Pod terminÃ© â†’ supprimÃ© (si delete_worker_pods=True)\n\n\nKubernetesPodOperator\nPour exÃ©cuter des tÃ¢ches avec des images Docker personnalisÃ©es :\nfrom airflow import DAG\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\nfrom datetime import datetime\n\nwith DAG(\n    dag_id=\"etl_with_custom_image\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@daily\",\n    catchup=False,\n) as dag:\n    \n    # TÃ¢che avec image Spark\n    spark_job = KubernetesPodOperator(\n        task_id=\"run_spark_etl\",\n        name=\"spark-etl-job\",\n        namespace=\"airflow\",\n        image=\"my-registry/spark-etl:1.0\",\n        cmds=[\"spark-submit\"],\n        arguments=[\n            \"--master\", \"k8s://https://kubernetes.default.svc\",\n            \"/app/etl_job.py\"\n        ],\n        # Ressources\n        container_resources={\n            \"requests\": {\"cpu\": \"1\", \"memory\": \"2Gi\"},\n            \"limits\": {\"cpu\": \"2\", \"memory\": \"4Gi\"},\n        },\n        # Variables d'environnement\n        env_vars={\n            \"SOURCE_PATH\": \"s3://bucket/raw/\",\n            \"DEST_PATH\": \"s3://bucket/processed/\",\n        },\n        # Secrets\n        secrets=[\n            {\"secret\": \"aws-credentials\", \"key\": \"AWS_ACCESS_KEY_ID\", \"env\": \"AWS_ACCESS_KEY_ID\"},\n            {\"secret\": \"aws-credentials\", \"key\": \"AWS_SECRET_ACCESS_KEY\", \"env\": \"AWS_SECRET_ACCESS_KEY\"},\n        ],\n        # Configuration K8s\n        is_delete_operator_pod=True,\n        get_logs=True,\n        startup_timeout_seconds=300,\n        # Affinity/Tolerations\n        affinity={\n            \"nodeAffinity\": {\n                \"requiredDuringSchedulingIgnoredDuringExecution\": {\n                    \"nodeSelectorTerms\": [{\n                        \"matchExpressions\": [{\n                            \"key\": \"node-type\",\n                            \"operator\": \"In\",\n                            \"values\": [\"compute\"]\n                        }]\n                    }]\n                }\n            }\n        },\n    )\n    \n    # TÃ¢che avec image Python custom\n    python_job = KubernetesPodOperator(\n        task_id=\"run_python_transform\",\n        name=\"python-transform\",\n        namespace=\"airflow\",\n        image=\"my-registry/python-etl:2.0\",\n        cmds=[\"python\", \"/app/transform.py\"],\n        container_resources={\n            \"requests\": {\"cpu\": \"500m\", \"memory\": \"1Gi\"},\n            \"limits\": {\"cpu\": \"1\", \"memory\": \"2Gi\"},\n        },\n        is_delete_operator_pod=True,\n        get_logs=True,\n    )\n    \n    spark_job &gt;&gt; python_job\n\n\npod_template_file : Personnalisation avancÃ©e\nPour des configurations complexes, utiliser un template YAML :\n# pod_template.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: airflow-worker\n  labels:\n    app: airflow-worker\nspec:\n  serviceAccountName: airflow-worker\n  containers:\n    - name: base\n      image: apache/airflow:2.8.0-python3.11\n      imagePullPolicy: IfNotPresent\n      env:\n        - name: AIRFLOW__CORE__EXECUTOR\n          value: \"LocalExecutor\"\n      resources:\n        requests:\n          cpu: \"500m\"\n          memory: \"512Mi\"\n        limits:\n          cpu: \"1000m\"\n          memory: \"1Gi\"\n      volumeMounts:\n        - name: dags\n          mountPath: /opt/airflow/dags\n          readOnly: true\n  volumes:\n    - name: dags\n      persistentVolumeClaim:\n        claimName: airflow-dags\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n# Dans le DAG, rÃ©fÃ©rencer le template\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\ntask = KubernetesPodOperator(\n    task_id=\"task_with_template\",\n    name=\"custom-task\",\n    namespace=\"airflow\",\n    pod_template_file=\"/opt/airflow/pod_templates/pod_template.yaml\",\n    # Override l'image du template\n    image=\"my-custom-image:1.0\",\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#taskflow-api",
    "href": "notebooks/advanced/28_advanced_orchestration.html#taskflow-api",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "2. TaskFlow API",
    "text": "2. TaskFlow API\n\nLa TaskFlow API (introduite dans Airflow 2.0) permet dâ€™Ã©crire des DAGs de maniÃ¨re Pythonic avec des dÃ©corateurs @dag et @task, en gÃ©rant automatiquement les XComs.\n\n\nPourquoi TaskFlow ?\n\n\n\nApproche Traditionnelle\nTaskFlow API\n\n\n\n\nPythonOperator(python_callable=fn)\n@task sur la fonction\n\n\nxcom_push / xcom_pull manuels\nPassage de donnÃ©es automatique\n\n\nVerbeux\nConcis et lisible\n\n\nDÃ©pendances explicites &gt;&gt;\nDÃ©pendances implicites par appel\n\n\n\n\n\nExemple comparatif\nAvant (approche traditionnelle) :\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime\n\ndef extract(**context):\n    data = {\"users\": 100, \"orders\": 500}\n    context['ti'].xcom_push(key='extracted_data', value=data)\n    return data\n\ndef transform(**context):\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='extract_task', key='extracted_data')\n    transformed = {\"total\": data['users'] + data['orders']}\n    ti.xcom_push(key='transformed_data', value=transformed)\n    return transformed\n\ndef load(**context):\n    ti = context['ti']\n    data = ti.xcom_pull(task_ids='transform_task', key='transformed_data')\n    print(f\"Loading: {data}\")\n\nwith DAG(\n    dag_id='traditional_etl',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n) as dag:\n    \n    extract_task = PythonOperator(\n        task_id='extract_task',\n        python_callable=extract,\n    )\n    \n    transform_task = PythonOperator(\n        task_id='transform_task',\n        python_callable=transform,\n    )\n    \n    load_task = PythonOperator(\n        task_id='load_task',\n        python_callable=load,\n    )\n    \n    extract_task &gt;&gt; transform_task &gt;&gt; load_task\nAprÃ¨s (TaskFlow API) :\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='taskflow_etl',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef taskflow_etl():\n    \n    @task\n    def extract() -&gt; dict:\n        return {\"users\": 100, \"orders\": 500}\n    \n    @task\n    def transform(data: dict) -&gt; dict:\n        return {\"total\": data['users'] + data['orders']}\n    \n    @task\n    def load(data: dict):\n        print(f\"Loading: {data}\")\n    \n    # DÃ©pendances implicites par appel de fonction !\n    raw_data = extract()\n    transformed_data = transform(raw_data)\n    load(transformed_data)\n\n# Instancier le DAG\ntaskflow_etl()\n\n\nAvantages TaskFlow\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nCode Pythonic\nRessemble Ã  du Python standard\n\n\nXCom automatique\nLes retours de fonction sont automatiquement passÃ©s\n\n\nType hints\nSupport des annotations de type\n\n\nMoins de boilerplate\nPas de PythonOperator explicite\n\n\nDÃ©pendances claires\nLe flux de donnÃ©es dÃ©finit les dÃ©pendances\n\n\n\n\n\nTaskFlow avec plusieurs outputs\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='taskflow_multiple_outputs',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef etl_pipeline():\n    \n    @task(multiple_outputs=True)\n    def extract() -&gt; dict:\n        \"\"\"Retourne plusieurs valeurs nommÃ©es\"\"\"\n        return {\n            \"users\": [{\"id\": 1, \"name\": \"Alice\"}],\n            \"orders\": [{\"id\": 101, \"amount\": 99.99}],\n            \"metadata\": {\"source\": \"api\", \"timestamp\": \"2024-01-01\"}\n        }\n    \n    @task\n    def process_users(users: list) -&gt; list:\n        return [u['name'].upper() for u in users]\n    \n    @task\n    def process_orders(orders: list) -&gt; float:\n        return sum(o['amount'] for o in orders)\n    \n    @task\n    def combine(users: list, total: float, metadata: dict):\n        print(f\"Users: {users}\")\n        print(f\"Total orders: {total}\")\n        print(f\"Source: {metadata['source']}\")\n    \n    # Extraire les donnÃ©es\n    data = extract()\n    \n    # AccÃ©der aux outputs individuels\n    processed_users = process_users(data['users'])\n    orders_total = process_orders(data['orders'])\n    \n    # Combiner\n    combine(processed_users, orders_total, data['metadata'])\n\netl_pipeline()\n\n\nMixing TaskFlow avec des Operators classiques\nfrom airflow.decorators import dag, task\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\n@dag(\n    dag_id='mixed_taskflow',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef mixed_pipeline():\n    \n    # TaskFlow task\n    @task\n    def prepare_config() -&gt; dict:\n        return {\"batch_size\": 1000, \"date\": \"2024-01-01\"}\n    \n    # Operator classique\n    run_spark = KubernetesPodOperator(\n        task_id=\"run_spark\",\n        name=\"spark-job\",\n        namespace=\"airflow\",\n        image=\"apache/spark:3.5.0\",\n        cmds=[\"spark-submit\", \"/app/job.py\"],\n    )\n    \n    # TaskFlow task qui dÃ©pend d'un Operator\n    @task\n    def validate_output():\n        print(\"Validating Spark output...\")\n        return True\n    \n    # Bash operator\n    notify = BashOperator(\n        task_id=\"notify\",\n        bash_command=\"echo 'Pipeline completed!'\",\n    )\n    \n    # DÃ©finir les dÃ©pendances\n    config = prepare_config()\n    config &gt;&gt; run_spark &gt;&gt; validate_output() &gt;&gt; notify\n\nmixed_pipeline()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#dynamic-task-mapping",
    "href": "notebooks/advanced/28_advanced_orchestration.html#dynamic-task-mapping",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "3. Dynamic Task Mapping",
    "text": "3. Dynamic Task Mapping\n\nLe Dynamic Task Mapping (Airflow 2.3+) permet de crÃ©er un nombre variable de tÃ¢ches Ã  runtime, basÃ© sur les donnÃ©es.\n\n\nPourquoi Dynamic Mapping ?\n\n\n\n\n\n\n\nProblÃ¨me\nSolution\n\n\n\n\nNombre de fichiers inconnu Ã  lâ€™avance\n.expand() sur la liste de fichiers\n\n\nTraiter N partitions dynamiquement\nMap sur les partitions\n\n\nParallÃ©liser sur une liste variable\nCrÃ©er N tÃ¢ches automatiquement\n\n\n\n\n\nSchÃ©ma Dynamic Mapping\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  list_files()   â”‚\n                    â”‚ [f1, f2, f3, f4]â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n            â”‚                â”‚                â”‚\n            â–¼                â–¼                â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ process(f1)   â”‚ â”‚ process(f2)   â”‚ â”‚ process(f3)   â”‚ ...\n    â”‚   [mapped]    â”‚ â”‚   [mapped]    â”‚ â”‚   [mapped]    â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n            â”‚                 â”‚                 â”‚\n            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â”‚\n                              â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   aggregate()   â”‚\n                    â”‚ Combine results â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nExemple : Traiter des fichiers dynamiquement\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='dynamic_file_processing',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef process_files_dynamically():\n    \n    @task\n    def list_files() -&gt; list[str]:\n        \"\"\"Liste les fichiers Ã  traiter (nombre variable)\"\"\"\n        # En rÃ©alitÃ© : lister depuis S3, GCS, etc.\n        return [\n            \"s3://bucket/data/file1.parquet\",\n            \"s3://bucket/data/file2.parquet\",\n            \"s3://bucket/data/file3.parquet\",\n            \"s3://bucket/data/file4.parquet\",\n        ]\n    \n    @task\n    def process_file(file_path: str) -&gt; dict:\n        \"\"\"Traite UN fichier â€” sera mappÃ© dynamiquement\"\"\"\n        print(f\"Processing: {file_path}\")\n        # Simuler le traitement\n        row_count = len(file_path) * 100  # Fake\n        return {\"file\": file_path, \"rows\": row_count}\n    \n    @task\n    def aggregate_results(results: list[dict]) -&gt; dict:\n        \"\"\"AgrÃ¨ge tous les rÃ©sultats\"\"\"\n        total_rows = sum(r['rows'] for r in results)\n        return {\n            \"files_processed\": len(results),\n            \"total_rows\": total_rows,\n        }\n    \n    # RÃ©cupÃ©rer la liste de fichiers\n    files = list_files()\n    \n    # ğŸ¯ DYNAMIC MAPPING : .expand() crÃ©e N tÃ¢ches\n    processed = process_file.expand(file_path=files)\n    \n    # AgrÃ©ger les rÃ©sultats\n    aggregate_results(processed)\n\nprocess_files_dynamically()\n\n\nexpand() avec plusieurs paramÃ¨tres\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='dynamic_multi_param',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef multi_param_mapping():\n    \n    @task\n    def get_partitions() -&gt; list[dict]:\n        return [\n            {\"date\": \"2024-01-01\", \"region\": \"EU\"},\n            {\"date\": \"2024-01-01\", \"region\": \"US\"},\n            {\"date\": \"2024-01-02\", \"region\": \"EU\"},\n            {\"date\": \"2024-01-02\", \"region\": \"US\"},\n        ]\n    \n    @task\n    def process_partition(date: str, region: str) -&gt; dict:\n        print(f\"Processing {region} for {date}\")\n        return {\"date\": date, \"region\": region, \"status\": \"done\"}\n    \n    partitions = get_partitions()\n    \n    # expand_kwargs pour mapper plusieurs paramÃ¨tres\n    process_partition.expand_kwargs(partitions)\n\nmulti_param_mapping()\n\n\nLimiter le parallÃ©lisme\n@task(max_active_tis_per_dag=5)  # Max 5 instances en parallÃ¨le\ndef process_file(file_path: str) -&gt; dict:\n    # ...\n    pass\n\n\nMapping sur un Operator (non-TaskFlow)\nfrom airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\nwith DAG(\n    dag_id='dynamic_bash',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n) as dag:\n    \n    # Liste statique (ou XCom d'une tÃ¢che prÃ©cÃ©dente)\n    files = [\"file1.csv\", \"file2.csv\", \"file3.csv\"]\n    \n    process = BashOperator.partial(\n        task_id=\"process_files\",\n    ).expand(\n        bash_command=[f\"python process.py {f}\" for f in files]\n    )",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#airflow-vs-dagster-vs-prefect",
    "href": "notebooks/advanced/28_advanced_orchestration.html#airflow-vs-dagster-vs-prefect",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "4. Airflow vs Dagster vs Prefect",
    "text": "4. Airflow vs Dagster vs Prefect\nIl existe plusieurs orchestrateurs modernes. Voici une comparaison approfondie.\n\nVue dâ€™ensemble\n\n\n\nCritÃ¨re\nAirflow\nDagster\nPrefect\n\n\n\n\nCrÃ©Ã© par\nAirbnb (2014)\nElementl (2018)\nPrefect (2018)\n\n\nPhilosophie\nDAG-centric\nAsset-centric\nFlow-centric\n\n\nMaturitÃ©\nâ­â­â­â­â­\nâ­â­â­â­\nâ­â­â­â­\n\n\nCommunautÃ©\nTrÃ¨s large\nEn croissance\nEn croissance\n\n\nAdoption\nStandard industrie\nStartups, ML\nStartups, Data\n\n\nLearning curve\nMoyenne\nPlus raide\nPlus douce\n\n\n\n\n\nComparaison dÃ©taillÃ©e\n\n\n\n\n\n\n\n\n\nAspect\nAirflow\nDagster\nPrefect\n\n\n\n\nDÃ©finition\nDAGs Python\nAssets + Ops\nFlows + Tasks\n\n\nScheduling\nCron-like\nCron + Sensors\nCron + Events\n\n\nData Lineage\nVia OpenLineage\nNatif (Assets)\nVia intÃ©grations\n\n\nTesting\nDifficile\nExcellent\nBon\n\n\nType checking\nNon natif\nNatif (I/O types)\nPydantic\n\n\nUI\nFonctionnelle\nModerne\nModerne\n\n\nLocal dev\nComplexe\nExcellent\nExcellent\n\n\nCloud offering\nMWAA, Composer, Astronomer\nDagster Cloud\nPrefect Cloud\n\n\n\n\n\nExemples de code comparÃ©s\nMÃªme pipeline dans les 3 outils :\n\n\nAirflow (TaskFlow)\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n@dag(\n    dag_id='etl_pipeline',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef etl_pipeline():\n    \n    @task\n    def extract() -&gt; dict:\n        return {\"data\": [1, 2, 3, 4, 5]}\n    \n    @task\n    def transform(raw: dict) -&gt; dict:\n        return {\"data\": [x * 2 for x in raw['data']]}\n    \n    @task\n    def load(transformed: dict):\n        print(f\"Loading: {transformed}\")\n    \n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n\netl_pipeline()\n\n\nDagster (Asset-based)\nfrom dagster import asset, Definitions, define_asset_job\n\n@asset\ndef raw_data() -&gt; dict:\n    \"\"\"Asset: donnÃ©es brutes\"\"\"\n    return {\"data\": [1, 2, 3, 4, 5]}\n\n@asset\ndef transformed_data(raw_data: dict) -&gt; dict:\n    \"\"\"Asset: donnÃ©es transformÃ©es (dÃ©pend de raw_data)\"\"\"\n    return {\"data\": [x * 2 for x in raw_data['data']]}\n\n@asset\ndef loaded_data(transformed_data: dict) -&gt; None:\n    \"\"\"Asset: donnÃ©es chargÃ©es\"\"\"\n    print(f\"Loading: {transformed_data}\")\n\n# Job pour exÃ©cuter tous les assets\netl_job = define_asset_job(\"etl_job\", selection=\"*\")\n\n# DÃ©finitions Dagster\ndefs = Definitions(\n    assets=[raw_data, transformed_data, loaded_data],\n    jobs=[etl_job],\n)\nDagster : Concepts clÃ©s\n\n\n\nConcept\nDescription\n\n\n\n\nAsset\nObjet de donnÃ©es persistant (table, fichier)\n\n\nOp\nUnitÃ© de calcul (comme un Operator)\n\n\nGraph\nComposition dâ€™Ops\n\n\nJob\nGraph exÃ©cutable avec config\n\n\nResource\nConnexion externe (DB, S3)\n\n\nI/O Manager\nGÃ¨re la persistance des assets\n\n\n\n\n\nPrefect (Flow-based)\nfrom prefect import flow, task\n\n@task\ndef extract() -&gt; dict:\n    return {\"data\": [1, 2, 3, 4, 5]}\n\n@task\ndef transform(raw: dict) -&gt; dict:\n    return {\"data\": [x * 2 for x in raw['data']]}\n\n@task\ndef load(transformed: dict):\n    print(f\"Loading: {transformed}\")\n\n@flow(name=\"ETL Pipeline\")\ndef etl_pipeline():\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n\n# ExÃ©cuter\nif __name__ == \"__main__\":\n    etl_pipeline()\nPrefect : Concepts clÃ©s\n\n\n\nConcept\nDescription\n\n\n\n\nFlow\nPipeline de tÃ¢ches\n\n\nTask\nUnitÃ© de travail\n\n\nDeployment\nFlow dÃ©ployÃ© et schedulable\n\n\nWork Pool\nGroupe de workers\n\n\nBlock\nCredentials et configs rÃ©utilisables\n\n\n\n\n\nQuand utiliser quoi ?\n\n\n\n\n\n\n\n\nScÃ©nario\nRecommandation\nPourquoi\n\n\n\n\nGrande entreprise, Ã©quipe mature\nAirflow\nStandard, trÃ¨s documentÃ©, Ã©cosystÃ¨me large\n\n\nPipelines ML avec assets\nDagster\nAsset-centric, excellent testing, types\n\n\nStartup, itÃ©ration rapide\nPrefect\nSimple, moderne, local dev facile\n\n\nDÃ©jÃ  sur Airflow\nRester sur Airflow\nMigration coÃ»teuse, TaskFlow moderne\n\n\nNouveau projet, Ã©quipe data\nDagster ou Prefect\nApproches modernes\n\n\nK8s natif requis\nAirflow ou Dagster\nKubernetesExecutor mature\n\n\n\n\n\nTableau de dÃ©cision\n                            ComplexitÃ© du pipeline\n                     Faible â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Ã‰levÃ©e\n                        â”‚                          â”‚\n    Taille Ã©quipe       â”‚                          â”‚\n         â”‚              â”‚                          â”‚\n     Petite  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Prefect    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Dagster\n         â”‚              â”‚                          â”‚\n         â”‚              â”‚                          â”‚\n    Grande  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Prefect    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Airflow\n         â”‚              â”‚    ou Dagster            â”‚\n         â–¼              â”‚                          â”‚",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#openlineage-data-lineage",
    "href": "notebooks/advanced/28_advanced_orchestration.html#openlineage-data-lineage",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "5. OpenLineage â€” Data Lineage",
    "text": "5. OpenLineage â€” Data Lineage\n\nOpenLineage est un standard ouvert pour collecter et partager les mÃ©tadonnÃ©es de lineage des pipelines data. Il rÃ©pond Ã  la question : â€œDâ€™oÃ¹ viennent mes donnÃ©es et oÃ¹ vont-elles ?â€\n\n\nPourquoi le Data Lineage ?\n\n\n\n\n\n\n\nQuestion\nLineage rÃ©pond\n\n\n\n\nDâ€™oÃ¹ viennent les donnÃ©es de ce dashboard ?\nâœ… TraÃ§abilitÃ© amont\n\n\nSi cette table change, quoi dâ€™autre est impactÃ© ?\nâœ… Impact analysis\n\n\nCe job a Ã©chouÃ©, quelles donnÃ©es sont corrompues ?\nâœ… Root cause analysis\n\n\nSommes-nous conformes RGPD ?\nâœ… Audit et gouvernance\n\n\n\n\n\nSchÃ©ma OpenLineage\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         OPENLINEAGE ECOSYSTEM                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚   Airflow   â”‚   â”‚   Spark     â”‚   â”‚    dbt      â”‚   â”‚   Flink     â”‚    â”‚\nâ”‚   â”‚  + OL Pluginâ”‚   â”‚  + OL Lib   â”‚   â”‚  + OL Pluginâ”‚   â”‚  + OL Lib   â”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚          â”‚                 â”‚                 â”‚                 â”‚            â”‚\nâ”‚          â”‚    OpenLineage Events (JSON)      â”‚                 â”‚            â”‚\nâ”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\nâ”‚                                    â”‚                                        â”‚\nâ”‚                                    â–¼                                        â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\nâ”‚                    â”‚     OpenLineage Backend       â”‚                       â”‚\nâ”‚                    â”‚  (Marquez, Atlan, DataHub...) â”‚                       â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\nâ”‚                                    â”‚                                        â”‚\nâ”‚                                    â–¼                                        â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\nâ”‚                    â”‚    Lineage Visualization      â”‚                       â”‚\nâ”‚                    â”‚    Impact Analysis            â”‚                       â”‚\nâ”‚                    â”‚    Data Catalog               â”‚                       â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nConcepts OpenLineage\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nJob\nPipeline ou transformation\nDAG Airflow, job Spark\n\n\nRun\nExÃ©cution dâ€™un job\nDAG run avec ID unique\n\n\nDataset\nSource ou destination de donnÃ©es\nTable SQL, fichier S3\n\n\nFacet\nMÃ©tadonnÃ©es additionnelles\nSchema, stats, owner\n\n\n\n\n\nActiver OpenLineage dans Airflow\n# Installer le provider\npip install apache-airflow-providers-openlineage\n# airflow.cfg ou variables d'environnement\n[openlineage]\ntransport = '{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\nnamespace = \"my_airflow_instance\"\nOu via environnement :\nexport AIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\nexport AIRFLOW__OPENLINEAGE__NAMESPACE=\"my_airflow_instance\"\n\n\nMarquez : Backend OpenLineage\nMarquez est le backend de rÃ©fÃ©rence pour OpenLineage (open-source par WeWork/Linux Foundation).\n# DÃ©ployer Marquez avec Docker Compose\ngit clone https://github.com/MarquezProject/marquez.git\ncd marquez\ndocker-compose up -d\n\n# UI disponible sur http://localhost:3000\n# API sur http://localhost:5000\n\n\nDAG avec lineage explicite\nfrom airflow.decorators import dag, task\nfrom airflow.lineage.entities import Table, File\nfrom datetime import datetime\n\n@dag(\n    dag_id='etl_with_lineage',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef etl_with_lineage():\n    \n    @task(\n        inlets=[File(url=\"s3://bucket/raw/\")],\n        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"staging_orders\")],\n    )\n    def extract_and_stage():\n        \"\"\"Extract depuis S3 et charge dans staging\"\"\"\n        print(\"Extracting from S3 to staging...\")\n    \n    @task(\n        inlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"staging_orders\")],\n        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"fact_orders\")],\n    )\n    def transform_to_fact():\n        \"\"\"Transforme staging en fact table\"\"\"\n        print(\"Transforming to fact table...\")\n    \n    extract_and_stage() &gt;&gt; transform_to_fact()\n\netl_with_lineage()\n\n\nLineage automatique avec certains Operators\nCertains operators extraient automatiquement le lineage :\n\n\n\nOperator\nLineage auto\n\n\n\n\nSnowflakeOperator\nâœ… (parse SQL)\n\n\nBigQueryOperator\nâœ…\n\n\nPostgresOperator\nâœ…\n\n\nSparkSubmitOperator\nâœ… (avec Spark OL)\n\n\nPythonOperator\nâŒ (manuel)\n\n\n\n\n\nAlternatives Ã  Marquez\n\n\n\nOutil\nType\nDescription\n\n\n\n\nMarquez\nOpen-source\nBackend de rÃ©fÃ©rence\n\n\nDataHub\nOpen-source\nCatalogue + lineage (LinkedIn)\n\n\nAtlan\nSaaS\nPlateforme data catalog complÃ¨te\n\n\nCollibra\nEnterprise\nGouvernance et lineage\n\n\nAlation\nEnterprise\nData catalog",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#astronomer-airflow-enterprise",
    "href": "notebooks/advanced/28_advanced_orchestration.html#astronomer-airflow-enterprise",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "6. Astronomer â€” Airflow Enterprise",
    "text": "6. Astronomer â€” Airflow Enterprise\n\nAstronomer est la plateforme enterprise pour Apache Airflow. Elle fournit des outils, un hosting managÃ©, et des SDKs pour industrialiser Airflow.\n\n\nPourquoi Astronomer ?\n\n\n\n\n\n\n\nProblÃ¨me\nSolution Astronomer\n\n\n\n\nInstaller/maintenir Airflow est complexe\nAstro Cloud (fully managed)\n\n\nDev local difficile\nAstro CLI (environment identique)\n\n\nÃ‰crire du code Airflow verbeux\nAstro SDK (API simplifiÃ©e)\n\n\nPas de support\nSupport enterprise 24/7\n\n\n\n\n\nÃ‰cosystÃ¨me Astronomer\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         ASTRONOMER ECOSYSTEM                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                        Astro Cloud                                   â”‚  â”‚\nâ”‚   â”‚  â€¢ Airflow fully managed                                            â”‚  â”‚\nâ”‚   â”‚  â€¢ Auto-scaling                                                      â”‚  â”‚\nâ”‚   â”‚  â€¢ Observability intÃ©grÃ©e                                           â”‚  â”‚\nâ”‚   â”‚  â€¢ CI/CD intÃ©grÃ©                                                    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚\nâ”‚   â”‚      Astro CLI        â”‚   â”‚      Astro SDK        â”‚                    â”‚\nâ”‚   â”‚  â€¢ Dev local          â”‚   â”‚  â€¢ API Pythonic       â”‚                    â”‚\nâ”‚   â”‚  â€¢ Deploy en 1 cmd    â”‚   â”‚  â€¢ SQL/Python tasks   â”‚                    â”‚\nâ”‚   â”‚  â€¢ Tests intÃ©grÃ©s     â”‚   â”‚  â€¢ Data quality       â”‚                    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    Astronomer Registry                               â”‚  â”‚\nâ”‚   â”‚  â€¢ Providers certifiÃ©s                                               â”‚  â”‚\nâ”‚   â”‚  â€¢ DAGs de rÃ©fÃ©rence                                                 â”‚  â”‚\nâ”‚   â”‚  â€¢ Documentation enrichie                                            â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nAstro CLI\nLâ€™outil en ligne de commande pour dÃ©velopper et dÃ©ployer Airflow.\n# Installation\n# macOS\nbrew install astro\n\n# Linux\ncurl -sSL install.astronomer.io | sudo bash -s\n\n# Windows (WSL)\ncurl -sSL install.astronomer.io | sudo bash -s\n# CrÃ©er un nouveau projet Airflow\nmkdir my-airflow-project && cd my-airflow-project\nastro dev init\n\n# Structure gÃ©nÃ©rÃ©e :\n# .\n# â”œâ”€â”€ dags/                 # Tes DAGs\n# â”‚   â””â”€â”€ example_dag.py\n# â”œâ”€â”€ include/              # Fichiers additionnels\n# â”œâ”€â”€ plugins/              # Plugins custom\n# â”œâ”€â”€ tests/                # Tests\n# â”œâ”€â”€ Dockerfile            # Image Airflow custom\n# â”œâ”€â”€ packages.txt          # Packages systÃ¨me\n# â”œâ”€â”€ requirements.txt      # DÃ©pendances Python\n# â””â”€â”€ airflow_settings.yaml # Variables, connexions\n\n# DÃ©marrer l'environnement local\nastro dev start\n# UI sur http://localhost:8080 (admin/admin)\n\n# Voir les logs\nastro dev logs\n\n# ExÃ©cuter les tests\nastro dev pytest\n\n# Parser les DAGs (vÃ©rifier les erreurs)\nastro dev parse\n\n# ArrÃªter\nastro dev stop\n\n# DÃ©ployer sur Astro Cloud\nastro deploy\n\n\nAstro SDK\nUne API Python simplifiÃ©e pour Ã©crire des DAGs, particuliÃ¨rement pour les opÃ©rations SQL/dataframe.\npip install astro-sdk-python\nExemple : ETL SQL simplifiÃ©\nfrom datetime import datetime\nfrom airflow.decorators import dag\nfrom astro import sql as aql\nfrom astro.files import File\nfrom astro.sql.table import Table\n\n@dag(\n    dag_id='astro_sdk_etl',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n)\ndef astro_sdk_etl():\n    \n    # Charger un CSV dans une table\n    raw_orders = aql.load_file(\n        input_file=File(\"s3://bucket/orders.csv\"),\n        output_table=Table(\n            name=\"raw_orders\",\n            conn_id=\"postgres_conn\",\n        ),\n    )\n    \n    # Transformation SQL (inline)\n    @aql.transform\n    def transform_orders(input_table: Table):\n        return \"\"\"\n            SELECT \n                order_id,\n                customer_id,\n                order_date,\n                amount,\n                amount * 1.2 as amount_with_tax\n            FROM {{ input_table }}\n            WHERE amount &gt; 0\n        \"\"\"\n    \n    # Appliquer la transformation\n    transformed = transform_orders(\n        input_table=raw_orders,\n        output_table=Table(\n            name=\"transformed_orders\",\n            conn_id=\"postgres_conn\",\n        ),\n    )\n    \n    # Exporter vers S3\n    aql.export_to_file(\n        input_data=transformed,\n        output_file=File(\"s3://bucket/processed/orders.parquet\"),\n        if_exists=\"replace\",\n    )\n    \n    # Nettoyer les tables temporaires\n    aql.cleanup()\n\nastro_sdk_etl()\n\n\nFonctionnalitÃ©s Astro SDK\n\n\n\nFonction\nDescription\n\n\n\n\naql.load_file()\nCharger CSV/Parquet/JSON dans une table\n\n\naql.transform()\nSQL transformation avec Jinja\n\n\naql.run_raw_sql()\nExÃ©cuter du SQL brut\n\n\naql.export_to_file()\nExporter une table vers fichier\n\n\naql.merge()\nMerge/Upsert entre tables\n\n\naql.append()\nAppend data to table\n\n\naql.dataframe()\nTransformer avec Pandas\n\n\naql.cleanup()\nSupprimer les tables temporaires\n\n\n\n\n\nAstro SDK : Transformation DataFrame\nfrom astro import sql as aql\nfrom astro.sql.table import Table\nimport pandas as pd\n\n@aql.dataframe\ndef process_with_pandas(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transformation avec Pandas\"\"\"\n    df['total'] = df['quantity'] * df['price']\n    df['processed_at'] = pd.Timestamp.now()\n    return df\n\n# Dans le DAG :\nprocessed = process_with_pandas(\n    df=some_table,\n    output_table=Table(name=\"processed\", conn_id=\"postgres\"),\n)\n\n\nAstro Cloud vs Self-Hosted\n\n\n\nAspect\nAstro Cloud\nSelf-Hosted (Helm)\n\n\n\n\nSetup\nMinutes\nHeures/jours\n\n\nMaintenance\nAstronomer\nTon Ã©quipe\n\n\nScaling\nAutomatique\nManuel\n\n\nUpdates\nAutomatiques\nManuels\n\n\nCoÃ»t\nPar usage\nInfrastructure\n\n\nContrÃ´le\nMoyen\nTotal\n\n\nSÃ©curitÃ©\nSOC 2, HIPAA\nÃ€ implÃ©menter",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#exercices-pratiques",
    "href": "notebooks/advanced/28_advanced_orchestration.html#exercices-pratiques",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "7. Exercices Pratiques",
    "text": "7. Exercices Pratiques\n\nExercice 1 : DAG TaskFlow avec Dynamic Mapping\nObjectif : CrÃ©er un DAG qui traite dynamiquement une liste de fichiers.\nInstructions : 1. CrÃ©er une tÃ¢che list_files() qui retourne une liste de chemins 2. CrÃ©er une tÃ¢che process_file(path) mappÃ©e dynamiquement 3. CrÃ©er une tÃ¢che report(results) qui agrÃ¨ge les rÃ©sultats 4. Limiter Ã  3 tÃ¢ches parallÃ¨les maximum\n\n\nğŸ’¡ Indice\n\nUtilise @task(max_active_tis_per_dag=3) et .expand()\n\n\n\n\nExercice 2 : KubernetesPodOperator avec Spark\nObjectif : CrÃ©er un DAG qui exÃ©cute un job Spark via KubernetesPodOperator.\nInstructions : 1. Utiliser lâ€™image apache/spark:3.5.0 2. Configurer les ressources : 2 CPU, 4Gi RAM 3. Passer des variables dâ€™environnement pour la config 4. Utiliser une node affinity pour cibler les nodes compute\n\n\n\nExercice 3 : Comparatif Airflow/Dagster/Prefect\nObjectif : ImplÃ©menter le mÃªme pipeline simple dans les 3 outils.\nPipeline : 1. Lire un fichier JSON 2. Filtrer les enregistrements (amount &gt; 100) 3. Calculer une somme 4. Ã‰crire le rÃ©sultat\nComparer : - Nombre de lignes de code - FacilitÃ© de test local - LisibilitÃ©\n\n\n\nExercice 4 : OpenLineage avec Marquez\nObjectif : Configurer OpenLineage et visualiser le lineage.\nInstructions : 1. DÃ©ployer Marquez en local (Docker Compose) 2. Configurer Airflow pour envoyer les events Ã  Marquez 3. CrÃ©er un DAG avec inlets et outlets explicites 4. Visualiser le lineage dans lâ€™UI Marquez\n\n\n\nExercice 5 : Astro CLI Project\nObjectif : CrÃ©er un projet Airflow complet avec Astro CLI.\nInstructions : 1. Initialiser un projet avec astro dev init 2. CrÃ©er un DAG utilisant Astro SDK 3. Ajouter des tests dans /tests 4. Valider avec astro dev pytest",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#mini-projet-pipeline-data-production-ready",
    "href": "notebooks/advanced/28_advanced_orchestration.html#mini-projet-pipeline-data-production-ready",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "8. Mini-Projet : Pipeline Data Production-Ready",
    "text": "8. Mini-Projet : Pipeline Data Production-Ready\n\nObjectif\nCrÃ©er un pipeline ETL production-ready dÃ©ployÃ© sur Kubernetes avec monitoring et lineage.\n\n\nArchitecture cible\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          MINI-PROJET M28                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    Airflow sur Kubernetes                            â”‚  â”‚\nâ”‚   â”‚                                                                      â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚   â”‚   â”‚            DAG: data_pipeline_advanced                       â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚                                                              â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚   â”‚ Extract â”‚â”€â”€â”€â–¶â”‚  Transform  â”‚â”€â”€â”€â–¶â”‚      Load       â”‚     â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚   â”‚ (K8sPod)â”‚    â”‚ (TaskFlow)  â”‚    â”‚  (Astro SDK)    â”‚     â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚                         â”‚                                    â”‚   â”‚  â”‚\nâ”‚   â”‚   â”‚            Dynamic Mapping (N fichiers)                      â”‚   â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚   â”‚                                                                      â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚   â”‚   â”‚  Scheduler  â”‚   â”‚  Webserver  â”‚   â”‚    Worker Pods (K8s)    â”‚   â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                    â”‚                                        â”‚\nâ”‚                         OpenLineage Events                                  â”‚\nâ”‚                                    â–¼                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                         Marquez                                      â”‚  â”‚\nâ”‚   â”‚              (Data Lineage Visualization)                           â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nInstructions\nÃ‰tape 1 : Setup Astro Project\nmkdir advanced-pipeline && cd advanced-pipeline\nastro dev init\nÃ‰tape 2 : CrÃ©er le DAG principal\nLe DAG doit : - Lister des fichiers depuis S3 (simulÃ©) - Traiter chaque fichier avec Dynamic Mapping - Utiliser TaskFlow API - Avoir des inlets/outlets pour le lineage\nÃ‰tape 3 : Configurer OpenLineage\n\nAjouter la config dans airflow_settings.yaml\nVÃ©rifier que les events arrivent dans Marquez\n\nÃ‰tape 4 : Tests\n\nÃ‰crire des tests unitaires pour les tÃ¢ches\nValider avec astro dev pytest\n\nÃ‰tape 5 : DÃ©ployer (optionnel)\n\nDÃ©ployer sur un cluster K8s avec le Helm chart\nOu utiliser Astro Cloud\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. Structure du projet Astro\nadvanced-pipeline/\nâ”œâ”€â”€ dags/\nâ”‚   â””â”€â”€ data_pipeline_advanced.py\nâ”œâ”€â”€ include/\nâ”‚   â””â”€â”€ sql/\nâ”‚       â””â”€â”€ transform.sql\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_pipeline.py\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ airflow_settings.yaml\n2. requirements.txt\nastro-sdk-python&gt;=1.5.0\napache-airflow-providers-openlineage&gt;=1.0.0\napache-airflow-providers-cncf-kubernetes&gt;=7.0.0\npandas&gt;=2.0.0\n3. airflow_settings.yaml\nairflow:\n  connections:\n    - conn_id: postgres_conn\n      conn_type: postgres\n      host: postgres\n      login: airflow\n      password: airflow\n      schema: airflow\n      port: 5432\n\n  variables:\n    - variable_name: data_bucket\n      variable_value: s3://my-data-bucket\n\n  # OpenLineage config\n  openlineage:\n    transport: '{\"type\": \"http\", \"url\": \"http://marquez:5000\", \"endpoint\": \"api/v1/lineage\"}'\n    namespace: advanced-pipeline\n4. dags/data_pipeline_advanced.py\nfrom datetime import datetime\nfrom airflow.decorators import dag, task\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\nfrom airflow.lineage.entities import File, Table\nfrom astro import sql as aql\nfrom astro.sql.table import Table as AstroTable\nimport pandas as pd\n\n@dag(\n    dag_id='data_pipeline_advanced',\n    start_date=datetime(2024, 1, 1),\n    schedule='@daily',\n    catchup=False,\n    tags=['production', 'etl'],\n    doc_md=\"\"\"## Pipeline Data AvancÃ©\n    \n    Ce pipeline:\n    1. Liste les fichiers Ã  traiter\n    2. Extrait chaque fichier (Dynamic Mapping)\n    3. Transforme avec Astro SDK\n    4. Charge dans PostgreSQL\n    \"\"\",\n)\ndef data_pipeline_advanced():\n    \n    # --- Ã‰TAPE 1 : Lister les fichiers ---\n    @task\n    def list_files() -&gt; list[str]:\n        \"\"\"Liste les fichiers Ã  traiter (simulÃ©)\"\"\"\n        # En production: lister depuis S3\n        return [\n            \"orders_2024_01.parquet\",\n            \"orders_2024_02.parquet\",\n            \"orders_2024_03.parquet\",\n        ]\n    \n    # --- Ã‰TAPE 2 : Extraire chaque fichier ---\n    @task(\n        max_active_tis_per_dag=3,\n        inlets=[File(url=\"s3://bucket/raw/\")],\n    )\n    def extract_file(file_name: str) -&gt; dict:\n        \"\"\"Extrait un fichier (simulÃ©)\"\"\"\n        print(f\"Extracting: {file_name}\")\n        # Simuler des donnÃ©es\n        return {\n            \"file\": file_name,\n            \"rows\": 1000,\n            \"data\": [\n                {\"order_id\": 1, \"amount\": 100},\n                {\"order_id\": 2, \"amount\": 200},\n            ]\n        }\n    \n    # --- Ã‰TAPE 3 : Transformer avec Pandas ---\n    @task(\n        outlets=[Table(cluster=\"warehouse\", database=\"analytics\", name=\"orders_processed\")],\n    )\n    def transform_data(extracted_list: list[dict]) -&gt; list[dict]:\n        \"\"\"AgrÃ¨ge et transforme les donnÃ©es\"\"\"\n        all_data = []\n        for extracted in extracted_list:\n            for record in extracted['data']:\n                record['source_file'] = extracted['file']\n                record['amount_with_tax'] = record['amount'] * 1.2\n                all_data.append(record)\n        \n        print(f\"Transformed {len(all_data)} records\")\n        return all_data\n    \n    # --- Ã‰TAPE 4 : Charger dans PostgreSQL ---\n    @aql.dataframe\n    def load_to_postgres(data: list[dict]) -&gt; pd.DataFrame:\n        \"\"\"Charge dans PostgreSQL via Astro SDK\"\"\"\n        df = pd.DataFrame(data)\n        df['loaded_at'] = pd.Timestamp.now()\n        return df\n    \n    # --- Ã‰TAPE 5 : Rapport final ---\n    @task\n    def generate_report(row_count: int):\n        \"\"\"GÃ©nÃ¨re un rapport\"\"\"\n        print(f\"Pipeline completed. Total rows: {row_count}\")\n    \n    # --- ORCHESTRATION ---\n    files = list_files()\n    \n    # Dynamic mapping sur les fichiers\n    extracted = extract_file.expand(file_name=files)\n    \n    # Transformer\n    transformed = transform_data(extracted)\n    \n    # Charger\n    loaded = load_to_postgres(\n        data=transformed,\n        output_table=AstroTable(\n            name=\"orders_processed\",\n            conn_id=\"postgres_conn\",\n        ),\n    )\n    \n    # Rapport\n    generate_report(len(transformed))\n    \n    # Cleanup Astro SDK\n    aql.cleanup()\n\n# Instancier le DAG\ndata_pipeline_advanced()\n5. tests/test_pipeline.py\nimport pytest\nfrom dags.data_pipeline_advanced import data_pipeline_advanced\n\ndef test_dag_loads():\n    \"\"\"Test que le DAG se charge sans erreur\"\"\"\n    dag = data_pipeline_advanced()\n    assert dag is not None\n    assert dag.dag_id == \"data_pipeline_advanced\"\n\ndef test_dag_has_expected_tasks():\n    \"\"\"Test que le DAG a les bonnes tÃ¢ches\"\"\"\n    dag = data_pipeline_advanced()\n    task_ids = [t.task_id for t in dag.tasks]\n    \n    assert \"list_files\" in task_ids\n    assert \"transform_data\" in task_ids\n\ndef test_extract_file():\n    \"\"\"Test unitaire de la fonction extract\"\"\"\n    # Import direct de la fonction\n    from dags.data_pipeline_advanced import extract_file\n    \n    # Appeler la fonction wrapped\n    result = extract_file.function(\"test.parquet\")\n    \n    assert \"file\" in result\n    assert \"rows\" in result\n    assert result[\"file\"] == \"test.parquet\"\n6. Commandes pour tester\n# DÃ©marrer l'environnement\nastro dev start\n\n# Parser les DAGs\nastro dev parse\n\n# Lancer les tests\nastro dev pytest\n\n# Voir l'UI\nopen http://localhost:8080\n\n# DÃ©clencher le DAG\nastro dev run dags trigger data_pipeline_advanced\n7. docker-compose.override.yml (pour Marquez)\nversion: \"3\"\nservices:\n  marquez:\n    image: marquezproject/marquez:latest\n    ports:\n      - \"5000:5000\"\n      - \"5001:5001\"\n    environment:\n      - MARQUEZ_PORT=5000\n      - MARQUEZ_ADMIN_PORT=5001\n  \n  marquez-web:\n    image: marquezproject/marquez-web:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      - MARQUEZ_HOST=marquez\n      - MARQUEZ_PORT=5000",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/advanced/28_advanced_orchestration.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nApache Airflow Docs â€” Documentation Airflow\nAirflow Helm Chart â€” DÃ©ploiement K8s\nDagster Docs â€” Documentation Dagster\nPrefect Docs â€” Documentation Prefect\nOpenLineage â€” Standard de lineage\nAstronomer Docs â€” Documentation Astronomer\n\n\n\nğŸ® Pratique\n\nAstronomer Academy â€” Cours gratuits Airflow\nAstro CLI Quickstart â€” DÃ©marrer avec Astro\nMarquez Demo â€” Essayer OpenLineage\n\n\n\nğŸ“– Livres & Articles\n\nData Pipelines with Apache Airflow â€” Bas Harenslak & Julian de Ruiter\nFundamentals of Data Engineering â€” Joe Reis & Matt Housley\nAstronomer Blog â€” Best practices Airflow\n\n\n\nğŸ”§ Outils\n\nAstronomer Registry â€” Providers et DAGs\nMarquez â€” Backend OpenLineage\nDataHub â€” Data catalog avec lineage",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/28_advanced_orchestration.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/28_advanced_orchestration.html#prochaine-Ã©tape",
    "title": "ğŸ¼ Advanced Orchestration pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises lâ€™orchestration avancÃ©e, passons au messaging distribuÃ© !\nğŸ‘‰ Module suivant : 29_distributed_messaging â€” Kafka avancÃ©, RabbitMQ, Pulsar, Debezium\nTu vas apprendre : - Kafka avancÃ© (Quotas, Tiered Storage) - Alternatives : RabbitMQ, Apache Pulsar - Change Data Capture avec Debezium - Patterns de messaging distribuÃ©\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Advanced Orchestration pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "28 Â· Orchestration AvancÃ©e"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  construire lâ€™infrastructure data qui alimente les systÃ¨mes de Machine Learning. En tant que Data Engineer, tu ne crÃ©es pas les modÃ¨les, mais tu construis les pipelines, Feature Stores, et systÃ¨mes de monitoring qui rendent le ML possible en production.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#prÃ©requis",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#prÃ©requis",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nPySpark DataFrame API (M19)\n\n\nâœ… Requis\nDelta Lake (M20)\n\n\nâœ… Requis\nAirflow (M22, M28)\n\n\nâœ… Requis\nData Quality avec Great Expectations (M23)\n\n\nğŸ’¡ RecommandÃ©\nNotions de base en ML (features, training, inference)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#objectifs-du-module",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#objectifs-du-module",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nConstruire des Feature Pipelines robustes avec Spark\nDÃ©ployer et alimenter un Feature Store (Feast)\nCrÃ©er des Training Datasets sans data leakage\nImplÃ©menter la Data Validation spÃ©cifique au ML\nMettre en place le Data Monitoring (drift detection)\nComprendre lâ€™intÃ©gration avec MLflow (cÃ´tÃ© data)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#le-rÃ´le-du-data-engineer-dans-le-ml",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#le-rÃ´le-du-data-engineer-dans-le-ml",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "1. Le RÃ´le du Data Engineer dans le ML",
    "text": "1. Le RÃ´le du Data Engineer dans le ML\n\n1.1 ML Lifecycle vu par le Data Engineer\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     DATA ENGINEER SCOPE IN ML                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    DATA ENGINEER CONSTRUIT                          â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   Raw Data â”€â”€â–¶ Data Pipelines â”€â”€â–¶ Feature Pipelines â”€â”€â–¶ Feature    â”‚  â”‚\nâ”‚   â”‚                                                          Store     â”‚  â”‚\nâ”‚   â”‚                                                            â”‚        â”‚  â”‚\nâ”‚   â”‚   Training Data â—€â”€â”€ Serving Data â—€â”€â”€ Data Validation â—€â”€â”€â”€â”€â”˜        â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                          â”‚                                  â”‚\nâ”‚                                          â–¼                                  â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    DATA SCIENTIST UTILISE                           â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   Feature Store â”€â”€â–¶ Model Training â”€â”€â–¶ Model Registry â”€â”€â–¶ Serving  â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Data Engineer vs Data Scientist vs ML Engineer\n\n\n\n\n\n\n\nRÃ´le\nResponsabilitÃ©s\n\n\n\n\nData Engineer\nPipelines de donnÃ©es, Feature Store infra, Data Quality, Monitoring data\n\n\nData Scientist\nFeature design, Model training, Experimentation, Evaluation\n\n\nML Engineer\nModel deployment, Model serving, Model monitoring, MLOps\n\n\n\n\n\n1.3 ProblÃ¨mes classiques que le DE doit rÃ©soudre\n\n\n\n\n\n\n\n\nProblÃ¨me\nDescription\nSolution DE\n\n\n\n\nTraining-Serving Skew\nFeatures diffÃ©rentes en training vs production\nFeature Store unique\n\n\nData Leakage\nUtiliser des donnÃ©es du futur pour prÃ©dire le passÃ©\nPoint-in-time joins\n\n\nReproducibility\nImpossible de recrÃ©er un training dataset\nDataset versioning\n\n\nFeature Inconsistency\nCalcul diffÃ©rent selon les Ã©quipes\nFeature pipelines centralisÃ©s\n\n\nStale Features\nFeatures pas Ã  jour en production\nRefresh pipelines, CDC",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#feature-pipelines-avec-spark",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#feature-pipelines-avec-spark",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "2. Feature Pipelines avec Spark",
    "text": "2. Feature Pipelines avec Spark\n\n2.1 Quâ€™est-ce quâ€™une Feature ?\nUne feature est une variable dÃ©rivÃ©e des donnÃ©es brutes, utilisÃ©e comme input pour un modÃ¨le ML.\n\n\n\n\n\n\n\nRaw Data\nFeatures dÃ©rivÃ©es\n\n\n\n\nTransactions individuelles\ntotal_transactions_30d, avg_amount_30d\n\n\nClics sur un site\npages_viewed_7d, time_on_site_avg\n\n\nHistorique dâ€™achats\ndays_since_last_purchase, favorite_category\n\n\n\n\n\n2.2 Transformations courantes\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\n\nspark = SparkSession.builder \\\n    .appName(\"FeaturePipeline\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\n\nVoir le code\n# CrÃ©er des donnÃ©es d'exemple\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom datetime import datetime, timedelta\n\nspark = SparkSession.builder \\\n    .appName(\"FeaturePipeline\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\nspark.sparkContext.setLogLevel(\"WARN\")\n\n# DonnÃ©es de transactions\ntransactions_data = [\n    (\"C001\", \"TXN001\", 150.0, \"Electronics\", \"2024-01-15\"),\n    (\"C001\", \"TXN002\", 25.0, \"Food\", \"2024-01-18\"),\n    (\"C001\", \"TXN003\", 200.0, \"Electronics\", \"2024-01-25\"),\n    (\"C002\", \"TXN004\", 75.0, \"Clothing\", \"2024-01-10\"),\n    (\"C002\", \"TXN005\", 50.0, \"Food\", \"2024-01-20\"),\n    (\"C003\", \"TXN006\", 500.0, \"Electronics\", \"2024-01-05\"),\n    (\"C003\", \"TXN007\", 30.0, \"Food\", \"2024-01-08\"),\n    (\"C003\", \"TXN008\", 120.0, \"Clothing\", \"2024-01-22\"),\n    (\"C001\", \"TXN009\", 80.0, \"Food\", \"2024-02-01\"),\n    (\"C002\", \"TXN010\", 300.0, \"Electronics\", \"2024-02-05\"),\n]\n\ntransactions_schema = StructType([\n    StructField(\"customer_id\", StringType(), False),\n    StructField(\"transaction_id\", StringType(), False),\n    StructField(\"amount\", DoubleType(), False),\n    StructField(\"category\", StringType(), False),\n    StructField(\"transaction_date\", StringType(), False),\n])\n\ntransactions_df = spark.createDataFrame(transactions_data, transactions_schema) \\\n    .withColumn(\"transaction_date\", F.to_date(\"transaction_date\"))\n\nprint(\"ğŸ“¦ Transactions brutes :\")\ntransactions_df.show()\n\n\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FEATURE 1 : AgrÃ©gations simples\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\naggregation_features = transactions_df \\\n    .groupBy(\"customer_id\") \\\n    .agg(\n        F.count(\"*\").alias(\"total_transactions\"),\n        F.sum(\"amount\").alias(\"total_spent\"),\n        F.avg(\"amount\").alias(\"avg_transaction_amount\"),\n        F.min(\"amount\").alias(\"min_transaction_amount\"),\n        F.max(\"amount\").alias(\"max_transaction_amount\"),\n        F.stddev(\"amount\").alias(\"stddev_transaction_amount\"),\n        F.countDistinct(\"category\").alias(\"unique_categories\"),\n        F.max(\"transaction_date\").alias(\"last_transaction_date\"),\n        F.min(\"transaction_date\").alias(\"first_transaction_date\"),\n    )\n\nprint(\"ğŸ“Š Features d'agrÃ©gation :\")\naggregation_features.show(truncate=False)\n\n\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FEATURE 2 : Window Functions (features temporelles)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# DÃ©finir les fenÃªtres\nwindow_30d = Window.partitionBy(\"customer_id\") \\\n    .orderBy(F.col(\"transaction_date\").cast(\"long\")) \\\n    .rangeBetween(-30 * 86400, 0)  # 30 jours en secondes\n\nwindow_7d = Window.partitionBy(\"customer_id\") \\\n    .orderBy(F.col(\"transaction_date\").cast(\"long\")) \\\n    .rangeBetween(-7 * 86400, 0)\n\n# Features rolling\nrolling_features = transactions_df \\\n    .withColumn(\"transaction_ts\", F.col(\"transaction_date\").cast(\"timestamp\")) \\\n    .withColumn(\"amount_sum_30d\", F.sum(\"amount\").over(window_30d)) \\\n    .withColumn(\"amount_avg_30d\", F.avg(\"amount\").over(window_30d)) \\\n    .withColumn(\"txn_count_30d\", F.count(\"*\").over(window_30d)) \\\n    .withColumn(\"amount_sum_7d\", F.sum(\"amount\").over(window_7d)) \\\n    .withColumn(\"txn_count_7d\", F.count(\"*\").over(window_7d))\n\nprint(\"ğŸ“ˆ Features avec Window Functions :\")\nrolling_features.select(\n    \"customer_id\", \"transaction_date\", \"amount\",\n    \"amount_sum_30d\", \"txn_count_30d\", \"amount_sum_7d\"\n).show()\n\n\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FEATURE 3 : Encoding catÃ©goriel\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# One-Hot Encoding manuel (pivot)\ncategory_pivot = transactions_df \\\n    .groupBy(\"customer_id\") \\\n    .pivot(\"category\") \\\n    .agg(F.count(\"*\")) \\\n    .fillna(0)\n\nprint(\"ğŸ·ï¸ One-Hot Encoding des catÃ©gories :\")\ncategory_pivot.show()\n\n# CatÃ©gorie favorite\nfavorite_category = transactions_df \\\n    .groupBy(\"customer_id\", \"category\") \\\n    .agg(F.sum(\"amount\").alias(\"category_total\")) \\\n    .withColumn(\n        \"rank\",\n        F.row_number().over(\n            Window.partitionBy(\"customer_id\").orderBy(F.desc(\"category_total\"))\n        )\n    ) \\\n    .filter(F.col(\"rank\") == 1) \\\n    .select(\"customer_id\", F.col(\"category\").alias(\"favorite_category\"))\n\nprint(\"â­ CatÃ©gorie favorite par client :\")\nfavorite_category.show()\n\n\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FEATURE 4 : Features de rÃ©cence (RFM)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nreference_date = \"2024-02-10\"\n\nrfm_features = transactions_df \\\n    .groupBy(\"customer_id\") \\\n    .agg(\n        F.max(\"transaction_date\").alias(\"last_transaction\"),\n        F.count(\"*\").alias(\"frequency\"),\n        F.sum(\"amount\").alias(\"monetary\")\n    ) \\\n    .withColumn(\n        \"recency_days\",\n        F.datediff(F.lit(reference_date), F.col(\"last_transaction\"))\n    ) \\\n    .withColumn(\n        \"is_active_30d\",\n        F.when(F.col(\"recency_days\") &lt;= 30, 1).otherwise(0)\n    )\n\nprint(\"ğŸ“… Features RFM (Recency, Frequency, Monetary) :\")\nrfm_features.show()\n\n\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# PIPELINE COMPLET : Assembler toutes les features\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef build_customer_features(transactions_df, reference_date):\n    \"\"\"\n    Pipeline complet de feature engineering pour les clients.\n    \n    Args:\n        transactions_df: DataFrame des transactions\n        reference_date: Date de rÃ©fÃ©rence pour les calculs de rÃ©cence\n    \n    Returns:\n        DataFrame avec toutes les features client\n    \"\"\"\n    \n    # 1. AgrÃ©gations de base\n    base_features = transactions_df \\\n        .groupBy(\"customer_id\") \\\n        .agg(\n            F.count(\"*\").alias(\"total_transactions\"),\n            F.sum(\"amount\").alias(\"total_spent\"),\n            F.avg(\"amount\").alias(\"avg_transaction_amount\"),\n            F.stddev(\"amount\").alias(\"stddev_amount\"),\n            F.countDistinct(\"category\").alias(\"unique_categories\"),\n            F.max(\"transaction_date\").alias(\"last_transaction_date\"),\n            F.min(\"transaction_date\").alias(\"first_transaction_date\"),\n        )\n    \n    # 2. Features de rÃ©cence\n    recency_features = base_features \\\n        .withColumn(\n            \"recency_days\",\n            F.datediff(F.lit(reference_date), F.col(\"last_transaction_date\"))\n        ) \\\n        .withColumn(\n            \"customer_tenure_days\",\n            F.datediff(F.lit(reference_date), F.col(\"first_transaction_date\"))\n        ) \\\n        .withColumn(\n            \"is_active_30d\",\n            F.when(F.col(\"recency_days\") &lt;= 30, 1).otherwise(0)\n        )\n    \n    # 3. CatÃ©gorie favorite\n    favorite_cat = transactions_df \\\n        .groupBy(\"customer_id\", \"category\") \\\n        .agg(F.count(\"*\").alias(\"cat_count\")) \\\n        .withColumn(\n            \"rank\",\n            F.row_number().over(\n                Window.partitionBy(\"customer_id\").orderBy(F.desc(\"cat_count\"))\n            )\n        ) \\\n        .filter(F.col(\"rank\") == 1) \\\n        .select(\"customer_id\", F.col(\"category\").alias(\"favorite_category\"))\n    \n    # 4. One-hot des catÃ©gories\n    category_ohe = transactions_df \\\n        .groupBy(\"customer_id\") \\\n        .pivot(\"category\") \\\n        .agg(F.count(\"*\")) \\\n        .fillna(0)\n    \n    # Renommer les colonnes OHE\n    for col_name in category_ohe.columns:\n        if col_name != \"customer_id\":\n            category_ohe = category_ohe.withColumnRenamed(\n                col_name, f\"category_{col_name.lower()}_count\"\n            )\n    \n    # 5. Joindre toutes les features\n    final_features = recency_features \\\n        .join(favorite_cat, \"customer_id\", \"left\") \\\n        .join(category_ohe, \"customer_id\", \"left\") \\\n        .withColumn(\"feature_timestamp\", F.lit(reference_date).cast(\"timestamp\"))\n    \n    return final_features\n\n# ExÃ©cuter le pipeline\ncustomer_features = build_customer_features(transactions_df, \"2024-02-10\")\n\nprint(\"ğŸ¯ Features client complÃ¨tes :\")\ncustomer_features.show(truncate=False)\ncustomer_features.printSchema()\n\n\n\n\n2.3 Point-in-Time Correctness (Ã‰viter le Data Leakage)\nData Leakage = utiliser des informations du futur pour prÃ©dire le passÃ©.\nâŒ MAUVAIS (Data Leakage) :\n   Pour prÃ©dire si le client achÃ¨te le 15 janvier,\n   on utilise ses transactions du 20 janvier â†’ TRICHE !\n\nâœ… BON (Point-in-Time Correct) :\n   Pour prÃ©dire si le client achÃ¨te le 15 janvier,\n   on utilise UNIQUEMENT ses transactions AVANT le 15 janvier.\ndef build_features_as_of(transactions_df, as_of_date):\n    \"\"\"\n    Construire les features en utilisant UNIQUEMENT les donnÃ©es\n    disponibles AVANT as_of_date.\n    \"\"\"\n    # Filtrer les transactions AVANT la date\n    filtered = transactions_df.filter(\n        F.col(\"transaction_date\") &lt; as_of_date\n    )\n    \n    return build_customer_features(filtered, as_of_date)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#infrastructure-feature-store",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#infrastructure-feature-store",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "3. Infrastructure Feature Store",
    "text": "3. Infrastructure Feature Store\n\n3.1 Pourquoi un Feature Store ?\nProblÃ¨me sans Feature Store | Solution avec Feature Store |\n|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€“|| | Features calculÃ©es diffÃ©remment en training vs serving | Single source of truth | | Duplication du code de features | RÃ©utilisation | | Pas de dÃ©couverte des features existantes | Feature discovery & catalog | | Point-in-time joins complexes | Built-in time-travel | | Latence Ã©levÃ©e en serving | Online store low-latency |\n\n\n3.2 Architecture Feature Store\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         FEATURE STORE ARCHITECTURE                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚  Feature        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  Pipelines      â”‚â”€â”€â”€â”€â–¶â”‚           OFFLINE STORE                     â”‚  â”‚\nâ”‚   â”‚  (Spark/Airflow)â”‚     â”‚  (Data Warehouse / Delta Lake / Parquet)    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  - Historical features                      â”‚  â”‚\nâ”‚                           â”‚  - Training data generation                 â”‚  â”‚\nâ”‚                           â”‚  - Backfill support                         â”‚  â”‚\nâ”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                                    Materialization Job                      â”‚\nâ”‚                                              â”‚                              â”‚\nâ”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚                           â”‚           ONLINE STORE                      â”‚  â”‚\nâ”‚                           â”‚  (Redis / DynamoDB / Cassandra)             â”‚  â”‚\nâ”‚                           â”‚  - Latest feature values only               â”‚  â”‚\nâ”‚                           â”‚  - Low-latency serving (&lt;10ms)              â”‚  â”‚\nâ”‚                           â”‚  - Real-time inference                      â”‚  â”‚\nâ”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.3 Feast : Feature Store Open Source\nFeast (Feature Store) est le Feature Store open-source le plus populaire.\n\nInstallation\npip install feast[redis]\n\n# CrÃ©er un projet Feast\nfeast init my_feature_store\ncd my_feature_store\n\n\nStructure du projet\nmy_feature_store/\nâ”œâ”€â”€ feature_store.yaml      # Configuration\nâ”œâ”€â”€ features.py             # DÃ©finition des features\nâ””â”€â”€ data/\n    â””â”€â”€ customer_features.parquet\n\n\nConfiguration feature_store.yaml\nproject: my_ml_project\nregistry: data/registry.db\nprovider: local\n\noffline_store:\n  type: file\n  # En production : type: snowflake / bigquery / redshift\n\nonline_store:\n  type: redis\n  connection_string: localhost:6379\n  # Alternatives : dynamodb, datastore, sqlite (local)\n\nentity_key_serialization_version: 2\n\n\nDÃ©finition des features features.py\nfrom datetime import timedelta\nfrom feast import Entity, Feature, FeatureView, FileSource, ValueType\nfrom feast.types import Float64, Int64, String\n\n# 1. DÃ©finir l'entitÃ© (la clÃ©)\ncustomer = Entity(\n    name=\"customer_id\",\n    value_type=ValueType.STRING,\n    description=\"Unique customer identifier\"\n)\n\n# 2. DÃ©finir la source de donnÃ©es\ncustomer_features_source = FileSource(\n    path=\"data/customer_features.parquet\",\n    timestamp_field=\"feature_timestamp\",\n)\n\n# 3. DÃ©finir la Feature View\ncustomer_features_view = FeatureView(\n    name=\"customer_features\",\n    entities=[customer],\n    ttl=timedelta(days=1),  # Time-to-live dans l'online store\n    schema=[\n        Feature(name=\"total_transactions\", dtype=Int64),\n        Feature(name=\"total_spent\", dtype=Float64),\n        Feature(name=\"avg_transaction_amount\", dtype=Float64),\n        Feature(name=\"recency_days\", dtype=Int64),\n        Feature(name=\"is_active_30d\", dtype=Int64),\n        Feature(name=\"favorite_category\", dtype=String),\n    ],\n    source=customer_features_source,\n    online=True,  # MatÃ©rialiser dans l'online store\n)\n\n\nCommandes Feast\n# Appliquer les dÃ©finitions\nfeast apply\n\n# MatÃ©rialiser dans l'online store\nfeast materialize 2024-01-01 2024-02-10\n\n# MatÃ©rialisation incrÃ©mentale\nfeast materialize-incremental $(date +%Y-%m-%d)\n\n\nUtilisation Python\nfrom feast import FeatureStore\nfrom datetime import datetime\nimport pandas as pd\n\nstore = FeatureStore(repo_path=\".\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# OFFLINE : RÃ©cupÃ©rer des features historiques (training)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nentity_df = pd.DataFrame({\n    \"customer_id\": [\"C001\", \"C002\", \"C003\"],\n    \"event_timestamp\": [datetime(2024, 2, 1)] * 3\n})\n\ntraining_df = store.get_historical_features(\n    entity_df=entity_df,\n    features=[\n        \"customer_features:total_transactions\",\n        \"customer_features:total_spent\",\n        \"customer_features:recency_days\",\n    ]\n).to_df()\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ONLINE : RÃ©cupÃ©rer les features en temps rÃ©el (serving)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nonline_features = store.get_online_features(\n    features=[\n        \"customer_features:total_spent\",\n        \"customer_features:is_active_30d\",\n    ],\n    entity_rows=[{\"customer_id\": \"C001\"}]\n).to_dict()\n\nprint(online_features)\n# {'customer_id': ['C001'], 'total_spent': [375.0], 'is_active_30d': [1]}\n\n\n\n3.4 Pipeline dâ€™alimentation du Feature Store\n# feature_pipeline_dag.py (Airflow)\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    'owner': 'data-engineering',\n    'depends_on_past': True,\n    'start_date': datetime(2024, 1, 1),\n    'retries': 2,\n    'retry_delay': timedelta(minutes=5),\n}\n\nwith DAG(\n    'feature_pipeline',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n) as dag:\n    \n    # 1. Calculer les features avec Spark\n    compute_features = SparkSubmitOperator(\n        task_id='compute_customer_features',\n        application='/jobs/compute_features.py',\n        application_args=['--date', '{{ ds }}'],\n        conf={\n            'spark.executor.memory': '4g',\n            'spark.executor.cores': '2',\n        },\n    )\n    \n    # 2. Valider les features\n    validate_features = PythonOperator(\n        task_id='validate_features',\n        python_callable=run_feature_validation,\n        op_kwargs={'date': '{{ ds }}'},\n    )\n    \n    # 3. MatÃ©rialiser dans le Feature Store\n    materialize = PythonOperator(\n        task_id='materialize_features',\n        python_callable=materialize_to_feast,\n        op_kwargs={'end_date': '{{ ds }}'},\n    )\n    \n    compute_features &gt;&gt; validate_features &gt;&gt; materialize",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#training-data-pipelines",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#training-data-pipelines",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "4. Training Data Pipelines",
    "text": "4. Training Data Pipelines\n\n4.1 GÃ©nÃ©rer des Datasets Reproductibles\nUn bon training dataset doit Ãªtre : - Reproductible : on peut le recrÃ©er exactement - VersionnÃ© : on sait quelle version a Ã©tÃ© utilisÃ©e - Point-in-time correct : pas de data leakage\n\n\n4.2 Point-in-Time Joins\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       POINT-IN-TIME JOIN                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   Events (ce qu'on prÃ©dit)          Features (inputs du modÃ¨le)            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚\nâ”‚                                                                             â”‚\nâ”‚   customer_id | event_date          customer_id | feature_date | features  â”‚\nâ”‚   C001        | 2024-02-01          C001        | 2024-01-15   | {...}     â”‚\nâ”‚   C001        | 2024-02-15          C001        | 2024-02-01   | {...}     â”‚\nâ”‚                                     C001        | 2024-02-10   | {...}     â”‚\nâ”‚                                                                             â”‚\nâ”‚   Pour l'event du 2024-02-01 :                                             â”‚\nâ”‚   â†’ Utiliser les features du 2024-01-15 (la plus rÃ©cente AVANT l'event)    â”‚\nâ”‚                                                                             â”‚\nâ”‚   Pour l'event du 2024-02-15 :                                             â”‚\nâ”‚   â†’ Utiliser les features du 2024-02-10 (la plus rÃ©cente AVANT l'event)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# POINT-IN-TIME JOIN avec Spark\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# CrÃ©er des donnÃ©es d'exemple\n\n# Events : ce qu'on veut prÃ©dire (ex: churn, achat)\nevents_data = [\n    (\"C001\", \"2024-02-01\", 1),  # Client C001 a churnÃ© le 1er fÃ©vrier\n    (\"C002\", \"2024-02-05\", 0),  # Client C002 n'a pas churnÃ©\n    (\"C003\", \"2024-02-10\", 1),  # Client C003 a churnÃ© le 10 fÃ©vrier\n]\nevents_df = spark.createDataFrame(\n    events_data, \n    [\"customer_id\", \"event_date\", \"label\"]\n).withColumn(\"event_date\", F.to_date(\"event_date\"))\n\n# Features avec timestamps (plusieurs versions par client)\nfeatures_data = [\n    (\"C001\", \"2024-01-15\", 5, 500.0),\n    (\"C001\", \"2024-01-25\", 6, 550.0),\n    (\"C002\", \"2024-01-20\", 3, 200.0),\n    (\"C002\", \"2024-02-01\", 4, 280.0),\n    (\"C003\", \"2024-01-10\", 10, 1000.0),\n    (\"C003\", \"2024-02-05\", 11, 1100.0),\n]\nfeatures_df = spark.createDataFrame(\n    features_data,\n    [\"customer_id\", \"feature_date\", \"total_transactions\", \"total_spent\"]\n).withColumn(\"feature_date\", F.to_date(\"feature_date\"))\n\nprint(\"ğŸ“… Events (labels) :\")\nevents_df.show()\n\nprint(\"ğŸ“Š Features (avec historique) :\")\nfeatures_df.show()\n\n\n\n\nVoir le code\ndef point_in_time_join(events_df, features_df, entity_col, event_ts_col, feature_ts_col):\n    \"\"\"\n    Join point-in-time correct : pour chaque event, rÃ©cupÃ©rer\n    les features les plus rÃ©centes AVANT l'event.\n    \n    Args:\n        events_df: DataFrame avec les events et leurs timestamps\n        features_df: DataFrame avec les features et leurs timestamps\n        entity_col: Colonne de jointure (ex: customer_id)\n        event_ts_col: Colonne timestamp dans events_df\n        feature_ts_col: Colonne timestamp dans features_df\n    \n    Returns:\n        DataFrame avec events + features point-in-time correct\n    \"\"\"\n    \n    # 1. Joindre sur l'entitÃ© + feature_date &lt; event_date\n    joined = events_df.alias(\"e\").join(\n        features_df.alias(\"f\"),\n        (F.col(f\"e.{entity_col}\") == F.col(f\"f.{entity_col}\")) &\n        (F.col(f\"f.{feature_ts_col}\") &lt; F.col(f\"e.{event_ts_col}\")),\n        \"left\"\n    )\n    \n    # 2. Garder uniquement la feature la plus rÃ©cente avant l'event\n    window = Window.partitionBy(f\"e.{entity_col}\", f\"e.{event_ts_col}\") \\\n                   .orderBy(F.col(f\"f.{feature_ts_col}\").desc())\n    \n    result = joined \\\n        .withColumn(\"_rank\", F.row_number().over(window)) \\\n        .filter(F.col(\"_rank\") == 1) \\\n        .drop(\"_rank\", f\"f.{entity_col}\")\n    \n    return result\n\n# ExÃ©cuter le join point-in-time\ntraining_data = point_in_time_join(\n    events_df, \n    features_df,\n    entity_col=\"customer_id\",\n    event_ts_col=\"event_date\",\n    feature_ts_col=\"feature_date\"\n)\n\nprint(\"ğŸ¯ Training Dataset (Point-in-Time Correct) :\")\ntraining_data.select(\n    \"customer_id\", \"event_date\", \"label\", \n    \"feature_date\", \"total_transactions\", \"total_spent\"\n).show()\n\n# VÃ©rification : feature_date est toujours &lt; event_date âœ“\n\n\n\n\n4.3 Dataset Versioning avec Delta Lake\n# Sauvegarder le training dataset avec versioning\n\ntraining_data.write \\\n    .format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"overwriteSchema\", \"true\") \\\n    .save(\"data/training_datasets/churn_model\")\n\n# Ajouter des mÃ©tadonnÃ©es\nfrom delta.tables import DeltaTable\n\ndelta_table = DeltaTable.forPath(spark, \"data/training_datasets/churn_model\")\n\n# Voir l'historique des versions\ndelta_table.history().select(\n    \"version\", \"timestamp\", \"operation\", \"operationMetrics\"\n).show(truncate=False)\n\n# Time travel : rÃ©cupÃ©rer une version spÃ©cifique\ntraining_v2 = spark.read \\\n    .format(\"delta\") \\\n    .option(\"versionAsOf\", 2) \\\n    .load(\"data/training_datasets/churn_model\")\n\n# Ou par timestamp\ntraining_at_date = spark.read \\\n    .format(\"delta\") \\\n    .option(\"timestampAsOf\", \"2024-01-15 10:00:00\") \\\n    .load(\"data/training_datasets/churn_model\")\n\n\n4.4 Data Splits\ndef create_train_val_test_split(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n    \"\"\"\n    Split un dataset en train/validation/test de maniÃ¨re reproductible.\n    \n    Pour les donnÃ©es temporelles, prÃ©fÃ©rer un split par date !\n    \"\"\"\n    assert abs(train_ratio + val_ratio + test_ratio - 1.0) &lt; 0.001\n    \n    # Random split\n    train_df, val_df, test_df = df.randomSplit(\n        [train_ratio, val_ratio, test_ratio], \n        seed=seed\n    )\n    \n    return train_df, val_df, test_df\n\ndef create_temporal_split(df, date_col, train_end, val_end):\n    \"\"\"\n    Split temporel (recommandÃ© pour Ã©viter le leakage) :\n    - Train : donnÃ©es avant train_end\n    - Val : donnÃ©es entre train_end et val_end  \n    - Test : donnÃ©es aprÃ¨s val_end\n    \"\"\"\n    train_df = df.filter(F.col(date_col) &lt; train_end)\n    val_df = df.filter(\n        (F.col(date_col) &gt;= train_end) & \n        (F.col(date_col) &lt; val_end)\n    )\n    test_df = df.filter(F.col(date_col) &gt;= val_end)\n    \n    return train_df, val_df, test_df",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#data-validation-pour-ml",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#data-validation-pour-ml",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "5. Data Validation pour ML",
    "text": "5. Data Validation pour ML\n\n5.1 Pourquoi la Data Quality est Critique pour ML\n\"Garbage In, Garbage Out\" â€” mais en pire pour le ML !\n\nProblÃ¨me de data â†’ ModÃ¨le apprend le bruit â†’ PrÃ©dictions fausses en production\n\n\n\n\n\n\n\nProblÃ¨me de donnÃ©es\nImpact sur le ML\n\n\n\n\nMissing values\nModÃ¨le biaisÃ© ou crash\n\n\nOutliers extrÃªmes\nPoids aberrants\n\n\nData leakage\nMÃ©triques sur-optimistes, crash en prod\n\n\nDistribution drift\nPerformance dÃ©gradÃ©e en prod\n\n\nClass imbalance non dÃ©tectÃ©\nModÃ¨le prÃ©dit toujours la classe majoritaire\n\n\n\n\n\n5.2 Great Expectations pour ML Data\nimport great_expectations as gx\n\n# CrÃ©er le contexte\ncontext = gx.get_context()\n\n# CrÃ©er une expectation suite pour les features ML\nsuite = context.add_expectation_suite(\"ml_features_validation\")\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# VALIDATIONS ML avec Great Expectations (conceptuel)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nml_feature_expectations = {\n    \"expectations\": [\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 1. ComplÃ©tude : pas de valeurs manquantes sur les features critiques\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n            \"kwargs\": {\"column\": \"customer_id\"}\n        },\n        {\n            \"expectation_type\": \"expect_column_values_to_not_be_null\",\n            \"kwargs\": {\"column\": \"total_transactions\"}\n        },\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 2. Plage de valeurs : dÃ©tecter les outliers\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_column_values_to_be_between\",\n            \"kwargs\": {\n                \"column\": \"total_spent\",\n                \"min_value\": 0,\n                \"max_value\": 100000  # Alerter si &gt; 100k\n            }\n        },\n        {\n            \"expectation_type\": \"expect_column_values_to_be_between\",\n            \"kwargs\": {\n                \"column\": \"recency_days\",\n                \"min_value\": 0,\n                \"max_value\": 365\n            }\n        },\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 3. Distribution : moyennes et Ã©carts-types attendus\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_column_mean_to_be_between\",\n            \"kwargs\": {\n                \"column\": \"avg_transaction_amount\",\n                \"min_value\": 50,\n                \"max_value\": 500\n            }\n        },\n        {\n            \"expectation_type\": \"expect_column_stdev_to_be_between\",\n            \"kwargs\": {\n                \"column\": \"total_spent\",\n                \"min_value\": 10,\n                \"max_value\": 5000\n            }\n        },\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 4. CardinalitÃ© : vÃ©rifier les catÃ©gories\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_column_values_to_be_in_set\",\n            \"kwargs\": {\n                \"column\": \"favorite_category\",\n                \"value_set\": [\"Electronics\", \"Clothing\", \"Food\", \"Other\", None]\n            }\n        },\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 5. UnicitÃ© : pas de doublons\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_column_values_to_be_unique\",\n            \"kwargs\": {\"column\": \"customer_id\"}\n        },\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # 6. Volume : nombre de lignes attendu\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        {\n            \"expectation_type\": \"expect_table_row_count_to_be_between\",\n            \"kwargs\": {\n                \"min_value\": 1000,\n                \"max_value\": 1000000\n            }\n        },\n    ]\n}\n\nprint(\"âœ… Expectations ML dÃ©finies :\")\nfor exp in ml_feature_expectations[\"expectations\"]:\n    print(f\"  - {exp['expectation_type']} sur {exp['kwargs'].get('column', 'table')}\")\n\n\n\n\n5.3 Validations SpÃ©cifiques ML\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Validation du label (target)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef validate_classification_labels(df, label_col, expected_classes):\n    \"\"\"\n    Valider les labels pour un problÃ¨me de classification.\n    \"\"\"\n    # 1. Pas de nulls dans le label\n    null_count = df.filter(F.col(label_col).isNull()).count()\n    assert null_count == 0, f\"Found {null_count} null labels!\"\n    \n    # 2. Labels dans les classes attendues\n    actual_classes = set(df.select(label_col).distinct().toPandas()[label_col].tolist())\n    unexpected = actual_classes - set(expected_classes)\n    assert len(unexpected) == 0, f\"Unexpected labels: {unexpected}\"\n    \n    # 3. VÃ©rifier le class imbalance\n    class_counts = df.groupBy(label_col).count().toPandas()\n    min_count = class_counts['count'].min()\n    max_count = class_counts['count'].max()\n    imbalance_ratio = max_count / min_count\n    \n    if imbalance_ratio &gt; 10:\n        print(f\"âš ï¸ WARNING: Class imbalance ratio = {imbalance_ratio:.1f}\")\n        print(f\"   Consider using class weights or resampling.\")\n    \n    return True\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Validation anti-leakage\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef validate_no_leakage(df, event_date_col, feature_date_col):\n    \"\"\"\n    VÃ©rifier qu'aucune feature n'est du futur par rapport Ã  l'event.\n    \"\"\"\n    leakage_count = df.filter(\n        F.col(feature_date_col) &gt;= F.col(event_date_col)\n    ).count()\n    \n    if leakage_count &gt; 0:\n        raise ValueError(f\"ğŸš¨ DATA LEAKAGE DETECTED! {leakage_count} rows have future features!\")\n    \n    print(\"âœ… No data leakage detected\")\n    return True",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#serving-data-infrastructure",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#serving-data-infrastructure",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "6. Serving Data Infrastructure",
    "text": "6. Serving Data Infrastructure\n\n6.1 Patterns de Serving\n\n\n\n\n\n\n\n\nPattern\nLatence\nUse Case\n\n\n\n\nBatch precompute\nMinutes-Hours\nScoring quotidien, recommendations\n\n\nOnline store lookup\n&lt;10ms\nPersonnalisation temps rÃ©el\n\n\nOn-demand compute\n100ms+\nFeatures complexes Ã  la demande\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     SERVING DATA PATTERNS                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  BATCH PRECOMPUTE                                                   â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  Features â”€â”€â–¶ Batch Scoring â”€â”€â–¶ Predictions Table â”€â”€â–¶ Application  â”‚  â”‚\nâ”‚   â”‚  (Spark)       (Spark MLlib)     (Delta/Postgres)      (lookup)    â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  â±ï¸ Latency: Minutes/Hours    ğŸ‘ Simple, scalable                   â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  ONLINE STORE LOOKUP                                                â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  Request â”€â”€â–¶ Feature Store â”€â”€â–¶ ML Model â”€â”€â–¶ Response               â”‚  â”‚\nâ”‚   â”‚              (Redis &lt;10ms)     (API)        (real-time)            â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  â±ï¸ Latency: &lt;50ms total      ğŸ‘ Real-time personalization         â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚  ON-DEMAND COMPUTE                                                  â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  Request â”€â”€â–¶ Compute Features â”€â”€â–¶ ML Model â”€â”€â–¶ Response            â”‚  â”‚\nâ”‚   â”‚              (on-the-fly)         (API)        (computed)          â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚  â±ï¸ Latency: 100ms+           ğŸ‘ Always fresh, complex features    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.2 Batch Scoring Pipeline\n# batch_scoring_dag.py (Airflow)\n\nfrom airflow import DAG\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\n\nwith DAG('batch_scoring', schedule_interval='@daily') as dag:\n    \n    # 1. RÃ©cupÃ©rer les features du jour\n    prepare_features = SparkSubmitOperator(\n        task_id='prepare_scoring_features',\n        application='/jobs/prepare_features.py',\n    )\n    \n    # 2. Scorer avec le modÃ¨le\n    score = SparkSubmitOperator(\n        task_id='batch_score',\n        application='/jobs/batch_score.py',\n        application_args=['--model-uri', 'models:/churn_model/Production'],\n    )\n    \n    # 3. Ã‰crire les prÃ©dictions\n    write_predictions = SparkSubmitOperator(\n        task_id='write_predictions',\n        application='/jobs/write_predictions.py',\n    )\n    \n    prepare_features &gt;&gt; score &gt;&gt; write_predictions\n\n\n6.3 Online Store avec Redis\nimport redis\nimport json\n\n# Connexion Redis\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Ã‰criture : MatÃ©rialisation des features\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef materialize_to_redis(features_df):\n    \"\"\"Ã‰crire les features dans Redis pour serving temps rÃ©el.\"\"\"\n    \n    for row in features_df.collect():\n        customer_id = row['customer_id']\n        features = {\n            'total_transactions': row['total_transactions'],\n            'total_spent': row['total_spent'],\n            'recency_days': row['recency_days'],\n            'is_active_30d': row['is_active_30d'],\n        }\n        \n        # ClÃ© : customer_features:{customer_id}\n        r.hset(f\"customer_features:{customer_id}\", mapping=features)\n        \n        # TTL : 24 heures\n        r.expire(f\"customer_features:{customer_id}\", 86400)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# Lecture : Serving temps rÃ©el\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef get_features_for_scoring(customer_id):\n    \"\"\"RÃ©cupÃ©rer les features pour le scoring temps rÃ©el.\"\"\"\n    \n    features = r.hgetall(f\"customer_features:{customer_id}\")\n    \n    if not features:\n        return None\n    \n    # Convertir bytes â†’ types Python\n    return {\n        k.decode(): float(v.decode())\n        for k, v in features.items()\n    }\n\n# Usage\n# features = get_features_for_scoring(\"C001\")\n# prediction = model.predict([features])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#data-monitoring-pour-ml",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#data-monitoring-pour-ml",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "7. Data Monitoring pour ML",
    "text": "7. Data Monitoring pour ML\n\n7.1 Types de Drift\n\n\n\n\n\n\n\n\nType\nDescription\nCe que le DE monitore\n\n\n\n\nData Drift\nDistribution des inputs change\nâœ… Features distributions\n\n\nConcept Drift\nRelation inputâ†’output change\nâš ï¸ Alerter le DS\n\n\nPrediction Drift\nDistribution des outputs change\nâš ï¸ Alerter le DS\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         DATA DRIFT DETECTION                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   Training Data Distribution          Production Data Distribution          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\nâ”‚                                                                             â”‚\nâ”‚   amount:                             amount:                               â”‚\nâ”‚   mean = 150                          mean = 280  â† DRIFT!                 â”‚\nâ”‚   std = 50                            std = 120   â† DRIFT!                 â”‚\nâ”‚                                                                             â”‚\nâ”‚        â–²                                    â–²                               â”‚\nâ”‚       â•±â•²                                  â•±    â•²                            â”‚\nâ”‚      â•±  â•²                               â•±      â•²                            â”‚\nâ”‚     â•±    â•²                            â•±         â•²                           â”‚\nâ”‚   â”€â•±â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â–¶                    â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²â”€â”€â–¶                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Si drift dÃ©tectÃ© â†’ Alerter â†’ Potentiellement retrainer le modÃ¨le         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n7.2 Evidently AI pour Data Monitoring\nfrom evidently import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metrics import (\n    DataDriftTable,\n    DatasetDriftMetric,\n    ColumnDriftMetric,\n)\nfrom evidently.test_suite import TestSuite\nfrom evidently.tests import (\n    TestColumnDrift,\n    TestShareOfDriftedColumns,\n)\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# RAPPORT DE DRIFT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# reference_data = donnÃ©es de training\n# current_data = donnÃ©es de production rÃ©centes\n\ncolumn_mapping = ColumnMapping(\n    numerical_features=['total_transactions', 'total_spent', 'avg_transaction_amount'],\n    categorical_features=['favorite_category'],\n)\n\n# CrÃ©er le rapport de drift\ndrift_report = Report(metrics=[\n    DatasetDriftMetric(),\n    DataDriftTable(),\n])\n\ndrift_report.run(\n    reference_data=reference_df,\n    current_data=current_df,\n    column_mapping=column_mapping\n)\n\n# Sauvegarder en HTML\ndrift_report.save_html(\"reports/drift_report.html\")\n\n# Ou obtenir les rÃ©sultats en dict\nresults = drift_report.as_dict()\ndataset_drift = results['metrics'][0]['result']['dataset_drift']\nprint(f\"Dataset drift detected: {dataset_drift}\")\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TESTS AUTOMATISÃ‰S (pour CI/CD)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndrift_tests = TestSuite(tests=[\n    TestShareOfDriftedColumns(lt=0.3),  # Moins de 30% de colonnes en drift\n    TestColumnDrift(column_name='total_spent'),\n    TestColumnDrift(column_name='recency_days'),\n])\n\ndrift_tests.run(\n    reference_data=reference_df,\n    current_data=current_df,\n    column_mapping=column_mapping\n)\n\n# VÃ©rifier si les tests passent\ntest_results = drift_tests.as_dict()\nall_passed = all(t['status'] == 'SUCCESS' for t in test_results['tests'])\n\nif not all_passed:\n    print(\"ğŸš¨ DRIFT ALERT: Some tests failed!\")\n    # Envoyer une alerte (Slack, PagerDuty, etc.)\n\n\nVoir le code\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MONITORING PIPELINE SIMPLIFIÃ‰ (sans Evidently)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef compute_feature_stats(df, feature_cols):\n    \"\"\"\n    Calculer les statistiques de base pour le monitoring.\n    \"\"\"\n    stats = {}\n    \n    for col in feature_cols:\n        col_stats = df.select(\n            F.mean(col).alias(\"mean\"),\n            F.stddev(col).alias(\"std\"),\n            F.min(col).alias(\"min\"),\n            F.max(col).alias(\"max\"),\n            F.expr(f\"percentile_approx({col}, 0.5)\").alias(\"median\"),\n            (F.count(F.when(F.col(col).isNull(), 1)) / F.count(\"*\")).alias(\"null_rate\"),\n        ).collect()[0]\n        \n        stats[col] = {\n            \"mean\": col_stats[\"mean\"],\n            \"std\": col_stats[\"std\"],\n            \"min\": col_stats[\"min\"],\n            \"max\": col_stats[\"max\"],\n            \"median\": col_stats[\"median\"],\n            \"null_rate\": col_stats[\"null_rate\"],\n        }\n    \n    return stats\n\ndef detect_drift(reference_stats, current_stats, threshold=0.2):\n    \"\"\"\n    DÃ©tecter le drift en comparant les statistiques.\n    \n    MÃ©thode simple : alerte si la moyenne change de plus de threshold%.\n    \"\"\"\n    alerts = []\n    \n    for col in reference_stats:\n        ref_mean = reference_stats[col][\"mean\"]\n        cur_mean = current_stats[col][\"mean\"]\n        \n        if ref_mean != 0:\n            pct_change = abs(cur_mean - ref_mean) / abs(ref_mean)\n            \n            if pct_change &gt; threshold:\n                alerts.append({\n                    \"column\": col,\n                    \"reference_mean\": ref_mean,\n                    \"current_mean\": cur_mean,\n                    \"pct_change\": pct_change * 100,\n                })\n    \n    return alerts\n\n# Exemple d'utilisation\nfeature_cols = [\"total_transactions\", \"total_spent\", \"avg_transaction_amount\"]\n\n# Simuler des donnÃ©es de rÃ©fÃ©rence et actuelles\nreference_stats = compute_feature_stats(customer_features, feature_cols)\n\n# Simuler un drift (en production, ce serait les nouvelles donnÃ©es)\ndrifted_data = customer_features.withColumn(\n    \"total_spent\", F.col(\"total_spent\") * 1.5  # +50% drift\n)\ncurrent_stats = compute_feature_stats(drifted_data, feature_cols)\n\n# DÃ©tecter le drift\nalerts = detect_drift(reference_stats, current_stats, threshold=0.2)\n\nprint(\"ğŸ“Š Monitoring Results:\")\nif alerts:\n    print(\"ğŸš¨ DRIFT DETECTED:\")\n    for alert in alerts:\n        print(f\"   - {alert['column']}: {alert['pct_change']:.1f}% change\")\n        print(f\"     (reference: {alert['reference_mean']:.2f} â†’ current: {alert['current_mean']:.2f})\")\nelse:\n    print(\"âœ… No significant drift detected\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#mlflow-pour-data-engineers",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#mlflow-pour-data-engineers",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "8. MLflow pour Data Engineers",
    "text": "8. MLflow pour Data Engineers\nEn tant que Data Engineer, tu nâ€™as pas besoin de tout connaÃ®tre de MLflow. Voici ce qui te concerne.\n\n8.1 Ce quâ€™un DE doit savoir\n\n\n\n\n\n\n\n\nComposant MLflow\nResponsabilitÃ© DE\nResponsabilitÃ© DS/MLE\n\n\n\n\nTracking\nLogger les datasets utilisÃ©s\nLogger les mÃ©triques, paramÃ¨tres\n\n\nProjects\nâ€”\nPackager le code\n\n\nModels\nâ€”\nSauvegarder les modÃ¨les\n\n\nRegistry\nSavoir quel modÃ¨le est en Production\nPromouvoir les modÃ¨les\n\n\n\n\n\n8.2 Logger les Datasets avec MLflow\nimport mlflow\n\n# Dans ton pipeline de feature engineering\ndef log_dataset_to_mlflow(df, dataset_name, run_id=None):\n    \"\"\"\n    Logger les mÃ©tadonnÃ©es du dataset pour traÃ§abilitÃ©.\n    \"\"\"\n    with mlflow.start_run(run_id=run_id):\n        # Logger les infos du dataset\n        mlflow.log_param(f\"{dataset_name}_rows\", df.count())\n        mlflow.log_param(f\"{dataset_name}_cols\", len(df.columns))\n        mlflow.log_param(f\"{dataset_name}_columns\", \",\".join(df.columns))\n        \n        # Logger les statistiques\n        stats = df.describe().toPandas()\n        stats.to_csv(f\"/tmp/{dataset_name}_stats.csv\")\n        mlflow.log_artifact(f\"/tmp/{dataset_name}_stats.csv\")\n        \n        # Logger le chemin du dataset\n        mlflow.log_param(f\"{dataset_name}_path\", f\"s3://data/features/{dataset_name}\")\n\n\n8.3 RÃ©cupÃ©rer le modÃ¨le en Production\nimport mlflow\n\n# Pour le batch scoring, rÃ©cupÃ©rer le modÃ¨le en Production\nmodel_name = \"churn_model\"\n\n# MÃ©thode 1 : par stage\nmodel = mlflow.pyfunc.load_model(f\"models:/{model_name}/Production\")\n\n# MÃ©thode 2 : par version\nmodel = mlflow.pyfunc.load_model(f\"models:/{model_name}/3\")\n\n# Scorer\npredictions = model.predict(features_df.toPandas())\n\n\n8.4 MLflow avec Spark\nimport mlflow.spark\n\n# Logger un modÃ¨le Spark MLlib\nmlflow.spark.log_model(spark_model, \"model\")\n\n# Charger pour batch scoring\nloaded_model = mlflow.spark.load_model(\"models:/my_spark_model/Production\")\n\n# Scorer directement sur un DataFrame Spark\npredictions_df = loaded_model.transform(features_df)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#exercices-pratiques",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#exercices-pratiques",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "9. Exercices Pratiques",
    "text": "9. Exercices Pratiques\n\nExercice 1 : Feature Pipeline avec Window Functions\nCrÃ©er un pipeline de features pour un modÃ¨le de dÃ©tection de fraude avec : - Montant moyen des 5 derniÃ¨res transactions - Nombre de transactions dans lâ€™heure prÃ©cÃ©dente - Ã‰cart par rapport au montant moyen habituel\n\n\n\nExercice 2 : Setup Feast Local\n\nInstaller Feast\nCrÃ©er un projet avec les features customer\nMatÃ©rialiser les features\nRÃ©cupÃ©rer des features historiques\n\n\n\n\nExercice 3 : Training Dataset Point-in-Time\nCrÃ©er un training dataset pour prÃ©dire le churn avec : - Events : clients qui ont churnÃ© (label=1) ou non (label=0) - Features : rÃ©cupÃ©rÃ©es 7 jours AVANT lâ€™event - Validation : vÃ©rifier quâ€™il nâ€™y a pas de leakage\n\n\n\nExercice 4 : Data Validation Pipeline\nImplÃ©menter un pipeline de validation avec : - VÃ©rification des nulls - VÃ©rification des plages de valeurs - DÃ©tection dâ€™outliers - IntÃ©gration Airflow\n\n\n\nExercice 5 : Data Drift Detection\n\nCrÃ©er un dataset de rÃ©fÃ©rence\nSimuler un drift sur certaines features\nImplÃ©menter la dÃ©tection automatique\nGÃ©nÃ©rer une alerte si drift &gt; 20%",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#mini-projet-ml-data-platform",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#mini-projet-ml-data-platform",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "10. Mini-Projet : ML Data Platform",
    "text": "10. Mini-Projet : ML Data Platform\n\nObjectif\nConstruire une plateforme data complÃ¨te pour alimenter un modÃ¨le de prÃ©diction de churn.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      MINI-PROJET : ML DATA PLATFORM                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   Raw Data (CSV)                                                            â”‚\nâ”‚        â”‚                                                                    â”‚\nâ”‚        â–¼                                                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ Data Ingestion  â”‚  Bronze Layer (Delta)                                 â”‚\nâ”‚   â”‚   (Spark)       â”‚                                                       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ Data Validation â”‚  Great Expectations                                   â”‚\nâ”‚   â”‚                 â”‚                                                       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ Feature Pipelineâ”‚  Silver Layer (Features)                              â”‚\nâ”‚   â”‚   (Spark)       â”‚                                                       â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\nâ”‚   â”‚  Feature Store  â”‚â”€â”€â”€â”€â–¶â”‚ Training Datasetâ”‚  Point-in-time correct       â”‚\nâ”‚   â”‚    (Feast)      â”‚     â”‚   Generator     â”‚                               â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\nâ”‚            â”‚                                                                â”‚\nâ”‚            â–¼                                                                â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\nâ”‚   â”‚ Data Monitoring â”‚  Drift detection                                      â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLivrables\n\nData Ingestion : Script Spark pour ingÃ©rer les CSV â†’ Delta\nValidation : Suite Great Expectations pour les donnÃ©es brutes\nFeature Pipeline : Job Spark complet avec toutes les features\nFeature Store : Configuration Feast + matÃ©rialisation\nTraining Dataset : GÃ©nÃ©rateur avec point-in-time join\nMonitoring : Script de dÃ©tection de drift\nOrchestration : DAG Airflow qui orchestre le tout\n\n\n\nDonnÃ©es\nUtiliser les transactions crÃ©Ã©es dans ce notebook + gÃ©nÃ©rer des events de churn.\n\n\nCritÃ¨res de succÃ¨s\n\nPipeline end-to-end fonctionnel\nPas de data leakage dans le training dataset\nValidation automatique des donnÃ©es\nDrift detection opÃ©rationnel\nDocumentation claire",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#ressources",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#ressources",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nDocumentation\n\nFeast Documentation â€” Feature Store\nGreat Expectations â€” Data Validation\nEvidently AI â€” ML Monitoring\nMLflow â€” ML Lifecycle\n\n\n\nArticles\n\nGoogleâ€™s ML Technical Debt Paper\nFeature Store: The Missing Data Layer\nMLOps Maturity Model\n\n\n\nOutils\n\nFeast â€” Feature Store open-source\nTecton â€” Feature Store managed\nDVC â€” Data versioning",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "notebooks/advanced/31_data_engineering_for_ml.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/31_data_engineering_for_ml.html#prochaine-Ã©tape",
    "title": "ğŸ¤– Data Engineering for ML",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 32_data_mesh_contracts â€” Data Mesh & Data Contracts\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant lâ€™infrastructure data pour le ML.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ¤– ML & Analytics",
      "31 Â· Data Engineering pour le ML"
    ]
  },
  {
    "objectID": "index.html#pourquoi-choisir-ce-bootcamp",
    "href": "index.html#pourquoi-choisir-ce-bootcamp",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ¯ Pourquoi choisir ce Bootcamp ?",
    "text": "ğŸ¯ Pourquoi choisir ce Bootcamp ?\nCe programme a Ã©tÃ© conÃ§u par des Data Engineers expÃ©rimentÃ©s, en se concentrant sur les exigences rÃ©elles du marchÃ©.\nIl vous accompagne de maniÃ¨re progressive vers un niveau professionnel â€“ Senior Ready.\n\nâœ” Ce que vous allez maÃ®triser\n\nOutils industriels : Spark 3.x, Kafka, Flink, Docker, Kubernetes, dbt, Lakehouse (Delta/Iceberg).\nArchitectures modernes : MÃ©daillon (Bronze/Silver/Gold), Kappa, Lambda, Data Lakehouse, Data Mesh.\nCompÃ©tences dâ€™ingÃ©nierie : optimisation, gouvernance, orchestration, CI/CD, cloud.\nIndustrialisation : crÃ©ation de pipelines batch et streaming totalement opÃ©rationnels.\nLeadership technique : RFC, Design Reviews, ADR, pensÃ©e architecturale."
  },
  {
    "objectID": "index.html#Ã -qui-sadresse-ce-bootcamp",
    "href": "index.html#Ã -qui-sadresse-ce-bootcamp",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ‘¥ Ã€ qui sâ€™adresse ce Bootcamp ?",
    "text": "ğŸ‘¥ Ã€ qui sâ€™adresse ce Bootcamp ?\n\nÃ‰tudiants / DÃ©veloppeurs voulant entrer dans le monde du Big Data.\nAnalystes BI souhaitant Ã©voluer vers le Data Engineering moderne.\nProfessionnels expÃ©rimentÃ©s voulant structurer une plateforme Data complÃ¨te.\nArchitectes & Managers cherchant Ã  comprendre les technos Data modernes."
  },
  {
    "objectID": "index.html#votre-boÃ®te-Ã -outils-data-engineering",
    "href": "index.html#votre-boÃ®te-Ã -outils-data-engineering",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ› ï¸ Votre BoÃ®te Ã  Outils Data Engineering",
    "text": "ğŸ› ï¸ Votre BoÃ®te Ã  Outils Data Engineering\n\n\n\n\n\n\n\nDomaine\nOutils & Technologies\n\n\n\n\nBig Data Processing\nPySpark, Spark SQL, Scala, Polars, Dask\n\n\nLakehouse\nDelta Lake, Apache Iceberg, Hudi\n\n\nStreaming\nKafka, Spark Streaming, Flink, Debezium CDC\n\n\nDevOps / Infra\nDocker, Kubernetes, Helm, ArgoCD, Spark Operator\n\n\nMonitoring\nPrometheus, Grafana, Alertmanager\n\n\nCloud\nAWS S3, GCP GCS, Azure Blob, IAM, MinIO\n\n\nQualitÃ© & ETL\ndbt, Great Expectations, Data Contracts\n\n\nMLOps\nMLflow, Feast (Feature Store), Model Monitoring\n\n\nGovernance\nDataHub, Unity Catalog, RLS/CLS, GDPR"
  },
  {
    "objectID": "index.html#conseils-pour-rÃ©ussir",
    "href": "index.html#conseils-pour-rÃ©ussir",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ’¡ Conseils pour RÃ©ussir",
    "text": "ğŸ’¡ Conseils pour RÃ©ussir\n\nSuivez lâ€™ordre : Les modules sont conÃ§us pour Ãªtre suivis sÃ©quentiellement.\nPratiquez : Faites tous les exercices et projets intÃ©grateurs.\nExpÃ©rimentez : Modifiez le code, cassez des choses, apprenez des erreurs.\nDocumentez : Prenez des notes, crÃ©ez votre propre documentation.\nConstruisez votre portfolio : Les projets intÃ©grateurs sont prÃ©sentables en entretien."
  },
  {
    "objectID": "index.html#bonne-montÃ©e-en-compÃ©tence",
    "href": "index.html#bonne-montÃ©e-en-compÃ©tence",
    "title": "Bootcamp Data Engineering â€“ From Zero to Hero",
    "section": "ğŸ‰ Bonne montÃ©e en compÃ©tence !",
    "text": "ğŸ‰ Bonne montÃ©e en compÃ©tence !\nVous Ãªtes maintenant prÃªt Ã  progresser Ã©tape par Ã©tape jusquâ€™au niveau Senior Data Engineer."
  },
  {
    "objectID": "resources/links.html",
    "href": "resources/links.html",
    "title": "ğŸ”— Liens Utiles",
    "section": "",
    "text": "Outil\nDescription\nLien\n\n\n\n\nVS Code\nÃ‰diteur de code\ncode.visualstudio.com\n\n\nPyCharm\nIDE Python\njetbrains.com/pycharm\n\n\nDataGrip\nIDE SQL\njetbrains.com/datagrip\n\n\nDBeaver\nClient SQL gratuit\ndbeaver.io\n\n\nPostman\nTest dâ€™API\npostman.com\n\n\n\n\n\n\n\n\n\nOutil\nDescription\nLien\n\n\n\n\niTerm2\nTerminal macOS\niterm2.com\n\n\nOh My Zsh\nFramework Zsh\nohmyz.sh\n\n\nStarship\nPrompt moderne\nstarship.rs\n\n\ntmux\nMultiplexeur terminal\ngithub.com/tmux\n\n\n\n\n\n\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nGitHub\nHÃ©bergement Git\ngithub.com\n\n\nGitLab\nAlternative GitHub\ngitlab.com\n\n\nGitKraken\nClient Git visuel\ngitkraken.com\n\n\nSourcetree\nClient Git Atlassian\nsourcetreeapp.com"
  },
  {
    "objectID": "resources/links.html#outils-essentiels",
    "href": "resources/links.html#outils-essentiels",
    "title": "ğŸ”— Liens Utiles",
    "section": "",
    "text": "Outil\nDescription\nLien\n\n\n\n\nVS Code\nÃ‰diteur de code\ncode.visualstudio.com\n\n\nPyCharm\nIDE Python\njetbrains.com/pycharm\n\n\nDataGrip\nIDE SQL\njetbrains.com/datagrip\n\n\nDBeaver\nClient SQL gratuit\ndbeaver.io\n\n\nPostman\nTest dâ€™API\npostman.com\n\n\n\n\n\n\n\n\n\nOutil\nDescription\nLien\n\n\n\n\niTerm2\nTerminal macOS\niterm2.com\n\n\nOh My Zsh\nFramework Zsh\nohmyz.sh\n\n\nStarship\nPrompt moderne\nstarship.rs\n\n\ntmux\nMultiplexeur terminal\ngithub.com/tmux\n\n\n\n\n\n\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nGitHub\nHÃ©bergement Git\ngithub.com\n\n\nGitLab\nAlternative GitHub\ngitlab.com\n\n\nGitKraken\nClient Git visuel\ngitkraken.com\n\n\nSourcetree\nClient Git Atlassian\nsourcetreeapp.com"
  },
  {
    "objectID": "resources/links.html#datasets-pour-pratiquer",
    "href": "resources/links.html#datasets-pour-pratiquer",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ“Š Datasets pour Pratiquer",
    "text": "ğŸ“Š Datasets pour Pratiquer\n\nDatasets Kaggle\n\n\n\nDataset\nDescription\nLien\n\n\n\n\nOlist E-commerce\n100K commandes BrÃ©sil\nkaggle.com/olistbr\n\n\nNYC Taxi\nTrajets taxis NYC\nkaggle.com/nyc-taxi\n\n\nSpotify\nDonnÃ©es musicales\nkaggle.com/spotify\n\n\nCOVID-19\nDonnÃ©es pandÃ©mie\nkaggle.com/covid19\n\n\n\n\n\nDatasets Open Data\n\n\n\nSource\nDescription\nLien\n\n\n\n\ndata.gouv.fr\nOpen Data France\ndata.gouv.fr\n\n\nNYC Open Data\nDonnÃ©es NYC\nopendata.cityofnewyork.us\n\n\nWorld Bank\nDonnÃ©es mondiales\ndata.worldbank.org\n\n\nAwesome Public Datasets\nListe curatÃ©e\ngithub.com/awesomedata"
  },
  {
    "objectID": "resources/links.html#cloud-infrastructure",
    "href": "resources/links.html#cloud-infrastructure",
    "title": "ğŸ”— Liens Utiles",
    "section": "â˜ï¸ Cloud & Infrastructure",
    "text": "â˜ï¸ Cloud & Infrastructure\n\nCloud Providers\n\n\n\nProvider\nConsole\nDocumentation\n\n\n\n\nAWS\nconsole.aws.amazon.com\ndocs.aws.amazon.com\n\n\nGoogle Cloud\nconsole.cloud.google.com\ncloud.google.com/docs\n\n\nAzure\nportal.azure.com\ndocs.microsoft.com\n\n\n\n\n\nData Platforms\n\n\n\nPlateforme\nDescription\nLien\n\n\n\n\nDatabricks\nLakehouse platform\ndatabricks.com\n\n\nSnowflake\nData Cloud\nsnowflake.com\n\n\nConfluent Cloud\nKafka managed\nconfluent.cloud\n\n\ndbt Cloud\ndbt managed\ngetdbt.com\n\n\n\n\n\nContainers & Orchestration\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nDocker Hub\nRegistry dâ€™images\nhub.docker.com\n\n\nArtifact Hub\nCharts Helm\nartifacthub.io\n\n\nOperatorHub\nOperators K8s\noperatorhub.io"
  },
  {
    "objectID": "resources/links.html#apprentissage",
    "href": "resources/links.html#apprentissage",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ“š Apprentissage",
    "text": "ğŸ“š Apprentissage\n\nCours en ligne\n\n\n\nPlateforme\nFocus\nLien\n\n\n\n\nCoursera\nCertifications cloud\ncoursera.org\n\n\nDataCamp\nData skills\ndatacamp.com\n\n\nUdemy\nCours variÃ©s\nudemy.com\n\n\nPluralsight\nTech skills\npluralsight.com\n\n\ndbt Learn\ndbt gratuit\ncourses.getdbt.com\n\n\n\n\n\nYouTube Channels\n\n\n\nChannel\nFocus\nLien\n\n\n\n\nDatabricks\nSpark, Delta Lake\nyoutube.com/@Databricks\n\n\nConfluent\nKafka\nyoutube.com/@Confluent\n\n\nData Engineering\nTutoriels\nyoutube.com/@DataEngineering\n\n\nSeattle Data Guy\nCarriÃ¨re DE\nyoutube.com/@SeattleDataGuy\n\n\n\n\n\nLivres (liens dâ€™achat)\n\n\n\nLivre\nAuteur\nLien\n\n\n\n\nDesigning Data-Intensive Applications\nKleppmann\nOâ€™Reilly\n\n\nFundamentals of Data Engineering\nReis & Housley\nOâ€™Reilly\n\n\nLearning Spark, 2nd Ed\nDamji et al.\nOâ€™Reilly"
  },
  {
    "objectID": "resources/links.html#playgrounds-labs",
    "href": "resources/links.html#playgrounds-labs",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ§ª Playgrounds & Labs",
    "text": "ğŸ§ª Playgrounds & Labs\n\nSQL\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nSQLFiddle\nSQL en ligne\nsqlfiddle.com\n\n\nDB Fiddle\nMulti-DB\ndb-fiddle.com\n\n\nMode SQL Tutorial\nApprendre SQL\nmode.com/sql-tutorial\n\n\nLeetCode\nChallenges SQL\nleetcode.com\n\n\n\n\n\nPython & Spark\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nGoogle Colab\nNotebooks gratuits\ncolab.research.google.com\n\n\nDatabricks Community\nSpark gratuit\ncommunity.cloud.databricks.com\n\n\nReplit\nIDE en ligne\nreplit.com\n\n\nGitpod\nDev environments\ngitpod.io\n\n\n\n\n\nKubernetes\n\n\n\nOutil\nDescription\nLien\n\n\n\n\nKillercoda\nLabs K8s\nkillercoda.com\n\n\nPlay with K8s\nK8s gratuit\nlabs.play-with-k8s.com\n\n\nKatacoda\nTutoriels interactifs\nkatacoda.com"
  },
  {
    "objectID": "resources/links.html#newsletters-blogs",
    "href": "resources/links.html#newsletters-blogs",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ“° Newsletters & Blogs",
    "text": "ğŸ“° Newsletters & Blogs\n\nNewsletters\n\n\n\nNewsletter\nFrÃ©quence\nLien\n\n\n\n\nData Engineering Weekly\nHebdo\ndataengineeringweekly.com\n\n\nSeattle Data Guy\nHebdo\nseattledataguy.substack.com\n\n\nData Council\nMensuel\ndatacouncil.ai\n\n\nLast Week in AWS\nHebdo\nlastweekinaws.com\n\n\n\n\n\nBlogs\n\n\n\nBlog\nFocus\nLien\n\n\n\n\nDatabricks Blog\nSpark, Lakehouse\ndatabricks.com/blog\n\n\nConfluent Blog\nKafka, Streaming\nconfluent.io/blog\n\n\ndbt Blog\nAnalytics Engineering\ngetdbt.com/blog\n\n\nNetflix Tech Blog\nScale\nnetflixtechblog.com\n\n\nUber Engineering\nData at scale\neng.uber.com"
  },
  {
    "objectID": "resources/links.html#communautÃ©s",
    "href": "resources/links.html#communautÃ©s",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ‘¥ CommunautÃ©s",
    "text": "ğŸ‘¥ CommunautÃ©s\n\nForums & Discord\n\n\n\nCommunautÃ©\nPlateforme\nLien\n\n\n\n\nr/dataengineering\nReddit\nreddit.com/r/dataengineering\n\n\ndbt Community\nSlack\ncommunity.getdbt.com\n\n\nData Talks Club\nSlack\ndatatalks.club\n\n\nApache Slack\nSlack\nthe-asf.slack.com\n\n\nMLOps Community\nSlack\nmlops.community\n\n\n\n\n\nMeetups & ConfÃ©rences\n\n\n\nEvent\nType\nLien\n\n\n\n\nData Council\nConfÃ©rence\ndatacouncil.ai\n\n\nSpark Summit\nConfÃ©rence\ndatabricks.com/dataaisummit\n\n\nKafka Summit\nConfÃ©rence\nkafka-summit.org\n\n\ndbt Coalesce\nConfÃ©rence\ncoalesce.getdbt.com"
  },
  {
    "objectID": "resources/links.html#prÃ©paration-entretiens",
    "href": "resources/links.html#prÃ©paration-entretiens",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ¯ PrÃ©paration Entretiens",
    "text": "ğŸ¯ PrÃ©paration Entretiens\n\nCoding Practice\n\n\n\nSite\nType\nLien\n\n\n\n\nLeetCode\nAlgorithmes + SQL\nleetcode.com\n\n\nHackerRank\nChallenges\nhackerrank.com\n\n\nStrataScratch\nSQL rÃ©el\nstratascratch.com\n\n\nDataLemur\nSQL pour DE\ndatalemur.com\n\n\n\n\n\nSystem Design\n\n\n\nRessource\nDescription\nLien\n\n\n\n\nSystem Design Primer\nGuide complet\ngithub.com/donnemartin\n\n\nGrokking System Design\nCours\neducative.io\n\n\nByteByteGo\nNewsletter\nbytebytego.com"
  },
  {
    "objectID": "resources/links.html#bookmarks-recommandÃ©s",
    "href": "resources/links.html#bookmarks-recommandÃ©s",
    "title": "ğŸ”— Liens Utiles",
    "section": "ğŸ”– Bookmarks RecommandÃ©s",
    "text": "ğŸ”– Bookmarks RecommandÃ©s\nAjoutez ces liens Ã  vos favoris :\nğŸ“ Data Engineering/\nâ”œâ”€â”€ ğŸ“ Documentation/\nâ”‚   â”œâ”€â”€ Spark Docs\nâ”‚   â”œâ”€â”€ Kafka Docs\nâ”‚   â”œâ”€â”€ dbt Docs\nâ”‚   â””â”€â”€ K8s Docs\nâ”œâ”€â”€ ğŸ“ Outils/\nâ”‚   â”œâ”€â”€ GitHub\nâ”‚   â”œâ”€â”€ Docker Hub\nâ”‚   â””â”€â”€ Artifact Hub\nâ”œâ”€â”€ ğŸ“ Apprentissage/\nâ”‚   â”œâ”€â”€ dbt Learn\nâ”‚   â”œâ”€â”€ Databricks Academy\nâ”‚   â””â”€â”€ Confluent Developer\nâ””â”€â”€ ğŸ“ CommunautÃ©s/\n    â”œâ”€â”€ r/dataengineering\n    â””â”€â”€ dbt Community\n\nğŸ”™ Retour Ã  lâ€™accueil"
  },
  {
    "objectID": "resources/documentation.html",
    "href": "resources/documentation.html",
    "title": "ğŸ“– Documentation",
    "section": "",
    "text": "Technologie\nDocumentation\nNotes\n\n\n\n\nPython\ndocs.python.org\nRÃ©fÃ©rence complÃ¨te\n\n\nPandas\npandas.pydata.org\nManipulation de donnÃ©es\n\n\nPolars\npola.rs\nAlternative haute performance\n\n\nNumPy\nnumpy.org\nCalcul scientifique\n\n\n\n\n\n\n\n\n\n\nRessource\nLien\nDescription\n\n\n\n\nDocumentation officielle\nspark.apache.org/docs\nGuide complet\n\n\nPySpark API\nspark.apache.org/docs/latest/api/python\nRÃ©fÃ©rence API Python\n\n\nSpark SQL Guide\nspark.apache.org/docs/latest/sql-programming-guide\nGuide SQL\n\n\nStructured Streaming\nspark.apache.org/docs/latest/structured-streaming\nStreaming guide\n\n\n\nLivres recommandÃ©s :\n\nLearning Spark, 2nd Edition â€” Oâ€™Reilly\nSpark: The Definitive Guide â€” Oâ€™Reilly\n\n\n\n\n\n\n\n\nFormat\nDocumentation\nGitHub\n\n\n\n\nDelta Lake\ndocs.delta.io\ndelta-io/delta\n\n\nApache Iceberg\niceberg.apache.org\napache/iceberg\n\n\nApache Hudi\nhudi.apache.org\napache/hudi\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nNotes\n\n\n\n\nApache Kafka\nkafka.apache.org/documentation\nMessage broker\n\n\nConfluent\ndocs.confluent.io\nKafka enterprise\n\n\nApache Flink\nflink.apache.org/docs\nStream processing\n\n\nDebezium\ndebezium.io/documentation\nCDC\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nCheatsheet\n\n\n\n\nDocker\ndocs.docker.com\nDocker Cheatsheet\n\n\nKubernetes\nkubernetes.io/docs\nkubectl Cheatsheet\n\n\nHelm\nhelm.sh/docs\nPackage manager K8s\n\n\nArgoCD\nargo-cd.readthedocs.io\nGitOps\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nNotes\n\n\n\n\nApache Airflow\nairflow.apache.org/docs\nOrchestration\n\n\ndbt\ndocs.getdbt.com\nTransformation\n\n\nGreat Expectations\ndocs.greatexpectations.io\nData Quality\n\n\nDagster\ndocs.dagster.io\nAlternative moderne\n\n\n\n\n\n\n\n\n\n\nProvider\nDocumentation Data\nServices clÃ©s\n\n\n\n\nAWS\ndocs.aws.amazon.com\nS3, EMR, Glue, Redshift\n\n\nGCP\ncloud.google.com/docs\nGCS, Dataproc, BigQuery\n\n\nAzure\ndocs.microsoft.com/azure\nBlob, Synapse, Databricks\n\n\n\n\n\n\n\n\n\n\nOutil\nDocumentation\nUsage\n\n\n\n\nPrometheus\nprometheus.io/docs\nMÃ©triques\n\n\nGrafana\ngrafana.com/docs\nDashboards\n\n\nDataHub\ndatahubproject.io/docs\nData Catalog"
  },
  {
    "objectID": "resources/documentation.html#documentation-officielle-par-technologie",
    "href": "resources/documentation.html#documentation-officielle-par-technologie",
    "title": "ğŸ“– Documentation",
    "section": "",
    "text": "Technologie\nDocumentation\nNotes\n\n\n\n\nPython\ndocs.python.org\nRÃ©fÃ©rence complÃ¨te\n\n\nPandas\npandas.pydata.org\nManipulation de donnÃ©es\n\n\nPolars\npola.rs\nAlternative haute performance\n\n\nNumPy\nnumpy.org\nCalcul scientifique\n\n\n\n\n\n\n\n\n\n\nRessource\nLien\nDescription\n\n\n\n\nDocumentation officielle\nspark.apache.org/docs\nGuide complet\n\n\nPySpark API\nspark.apache.org/docs/latest/api/python\nRÃ©fÃ©rence API Python\n\n\nSpark SQL Guide\nspark.apache.org/docs/latest/sql-programming-guide\nGuide SQL\n\n\nStructured Streaming\nspark.apache.org/docs/latest/structured-streaming\nStreaming guide\n\n\n\nLivres recommandÃ©s :\n\nLearning Spark, 2nd Edition â€” Oâ€™Reilly\nSpark: The Definitive Guide â€” Oâ€™Reilly\n\n\n\n\n\n\n\n\nFormat\nDocumentation\nGitHub\n\n\n\n\nDelta Lake\ndocs.delta.io\ndelta-io/delta\n\n\nApache Iceberg\niceberg.apache.org\napache/iceberg\n\n\nApache Hudi\nhudi.apache.org\napache/hudi\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nNotes\n\n\n\n\nApache Kafka\nkafka.apache.org/documentation\nMessage broker\n\n\nConfluent\ndocs.confluent.io\nKafka enterprise\n\n\nApache Flink\nflink.apache.org/docs\nStream processing\n\n\nDebezium\ndebezium.io/documentation\nCDC\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nCheatsheet\n\n\n\n\nDocker\ndocs.docker.com\nDocker Cheatsheet\n\n\nKubernetes\nkubernetes.io/docs\nkubectl Cheatsheet\n\n\nHelm\nhelm.sh/docs\nPackage manager K8s\n\n\nArgoCD\nargo-cd.readthedocs.io\nGitOps\n\n\n\n\n\n\n\n\n\n\nTechnologie\nDocumentation\nNotes\n\n\n\n\nApache Airflow\nairflow.apache.org/docs\nOrchestration\n\n\ndbt\ndocs.getdbt.com\nTransformation\n\n\nGreat Expectations\ndocs.greatexpectations.io\nData Quality\n\n\nDagster\ndocs.dagster.io\nAlternative moderne\n\n\n\n\n\n\n\n\n\n\nProvider\nDocumentation Data\nServices clÃ©s\n\n\n\n\nAWS\ndocs.aws.amazon.com\nS3, EMR, Glue, Redshift\n\n\nGCP\ncloud.google.com/docs\nGCS, Dataproc, BigQuery\n\n\nAzure\ndocs.microsoft.com/azure\nBlob, Synapse, Databricks\n\n\n\n\n\n\n\n\n\n\nOutil\nDocumentation\nUsage\n\n\n\n\nPrometheus\nprometheus.io/docs\nMÃ©triques\n\n\nGrafana\ngrafana.com/docs\nDashboards\n\n\nDataHub\ndatahubproject.io/docs\nData Catalog"
  },
  {
    "objectID": "resources/documentation.html#cours-formations-gratuits",
    "href": "resources/documentation.html#cours-formations-gratuits",
    "title": "ğŸ“– Documentation",
    "section": "ğŸ“ Cours & Formations Gratuits",
    "text": "ğŸ“ Cours & Formations Gratuits\n\nPlateformes\n\n\n\nPlateforme\nCours recommandÃ©s\n\n\n\n\nCoursera\nData Engineering on Google Cloud\n\n\nDataCamp\nData Engineer Track\n\n\nUdemy\nSpark, Airflow, Kafka courses\n\n\nYouTube\nDatabricks, Confluent channels\n\n\n\n\n\nTutoriels officiels\n\ndbt Learn â€” Gratuit, certifiant\nKafka Tutorials â€” Confluent Developer\nDatabricks Academy â€” Spark & Delta Lake"
  },
  {
    "objectID": "resources/documentation.html#blogs-articles",
    "href": "resources/documentation.html#blogs-articles",
    "title": "ğŸ“– Documentation",
    "section": "ğŸ“ Blogs & Articles",
    "text": "ğŸ“ Blogs & Articles\n\nBlogs techniques\n\n\n\nBlog\nFocus\n\n\n\n\nDatabricks Blog\nSpark, Delta Lake\n\n\nConfluent Blog\nKafka, Streaming\n\n\ndbt Blog\nAnalytics Engineering\n\n\nTowards Data Science\nData Engineering articles\n\n\nData Engineering Weekly\nNewsletter\n\n\n\n\n\nArticles fondamentaux\n\nThe Modern Data Stack\nMedallion Architecture\nData Mesh Principles\nLakehouse Architecture"
  },
  {
    "objectID": "resources/documentation.html#outils-utiles",
    "href": "resources/documentation.html#outils-utiles",
    "title": "ğŸ“– Documentation",
    "section": "ğŸ› ï¸ Outils Utiles",
    "text": "ğŸ› ï¸ Outils Utiles\n\nIDEs & Ã‰diteurs\n\n\n\nOutil\nUsage\nLien\n\n\n\n\nVS Code\nDÃ©veloppement gÃ©nÃ©ral\ncode.visualstudio.com\n\n\nPyCharm\nPython IDE\njetbrains.com/pycharm\n\n\nDataGrip\nSQL IDE\njetbrains.com/datagrip\n\n\nDBeaver\nSQL gratuit\ndbeaver.io\n\n\n\n\n\nExtensions VS Code recommandÃ©es\n- Python\n- Jupyter\n- Docker\n- Kubernetes\n- YAML\n- SQL Tools\n- GitLens"
  },
  {
    "objectID": "resources/documentation.html#livres-recommandÃ©s",
    "href": "resources/documentation.html#livres-recommandÃ©s",
    "title": "ğŸ“– Documentation",
    "section": "ğŸ“– Livres RecommandÃ©s",
    "text": "ğŸ“– Livres RecommandÃ©s\n\nEssentiels\n\n\n\n\n\n\n\n\nLivre\nAuteur\nSujet\n\n\n\n\nDesigning Data-Intensive Applications\nMartin Kleppmann\nArchitecture\n\n\nFundamentals of Data Engineering\nJoe Reis & Matt Housley\nVue dâ€™ensemble\n\n\nData Pipelines Pocket Reference\nJames Densmore\nPipelines\n\n\nLearning Spark, 2nd Ed\nDamji et al.\nSpark\n\n\n\n\n\nAvancÃ©s\n\n\n\nLivre\nAuteur\nSujet\n\n\n\n\nStreaming Systems\nAkidau et al.\nStreaming\n\n\nBuilding Microservices\nSam Newman\nArchitecture\n\n\nThe Data Warehouse Toolkit\nRalph Kimball\nModÃ©lisation"
  },
  {
    "objectID": "resources/documentation.html#communautÃ©s",
    "href": "resources/documentation.html#communautÃ©s",
    "title": "ğŸ“– Documentation",
    "section": "ğŸ”— CommunautÃ©s",
    "text": "ğŸ”— CommunautÃ©s\n\n\n\nCommunautÃ©\nPlateforme\n\n\n\n\nData Engineering\nr/dataengineering\n\n\ndbt Community\ncommunity.getdbt.com\n\n\nApache Slack\nthe-asf.slack.com\n\n\nData Talks Club\ndatatalks.club\n\n\n\n\nğŸ”™ Retour Ã  lâ€™accueil"
  },
  {
    "objectID": "resources/faq.html",
    "href": "resources/faq.html",
    "title": "â“ FAQ",
    "section": "",
    "text": "Un Data Engineer conÃ§oit, construit et maintient les infrastructures et pipelines de donnÃ©es qui permettent aux entreprises de collecter, stocker, transformer et analyser leurs donnÃ©es Ã  grande Ã©chelle.\nData Engineer = Architecte + DÃ©veloppeur + DevOps pour la Data\n\n\n\n\n\n\n\n\n\n\n\nRÃ´le\nFocus\nOutils\n\n\n\n\nData Engineer\nInfrastructure & Pipelines\nSpark, Kafka, Airflow, SQL\n\n\nData Scientist\nModÃ¨les & PrÃ©dictions\nPython, ML, Statistics\n\n\nData Analyst\nInsights & Reporting\nSQL, BI Tools, Excel\n\n\n\n\n\n\n\n\n\nNiveau\nDurÃ©e estimÃ©e\nRythme\n\n\n\n\nDÃ©butant\n6-8 semaines\n10-15h/semaine\n\n\nIntermÃ©diaire\n8-10 semaines\n10-15h/semaine\n\n\nAvancÃ©\n8-10 semaines\n10-15h/semaine\n\n\nTotal\n22-28 semaines\n~6 mois"
  },
  {
    "objectID": "resources/faq.html#questions-gÃ©nÃ©rales",
    "href": "resources/faq.html#questions-gÃ©nÃ©rales",
    "title": "â“ FAQ",
    "section": "",
    "text": "Un Data Engineer conÃ§oit, construit et maintient les infrastructures et pipelines de donnÃ©es qui permettent aux entreprises de collecter, stocker, transformer et analyser leurs donnÃ©es Ã  grande Ã©chelle.\nData Engineer = Architecte + DÃ©veloppeur + DevOps pour la Data\n\n\n\n\n\n\n\n\n\n\n\nRÃ´le\nFocus\nOutils\n\n\n\n\nData Engineer\nInfrastructure & Pipelines\nSpark, Kafka, Airflow, SQL\n\n\nData Scientist\nModÃ¨les & PrÃ©dictions\nPython, ML, Statistics\n\n\nData Analyst\nInsights & Reporting\nSQL, BI Tools, Excel\n\n\n\n\n\n\n\n\n\nNiveau\nDurÃ©e estimÃ©e\nRythme\n\n\n\n\nDÃ©butant\n6-8 semaines\n10-15h/semaine\n\n\nIntermÃ©diaire\n8-10 semaines\n10-15h/semaine\n\n\nAvancÃ©\n8-10 semaines\n10-15h/semaine\n\n\nTotal\n22-28 semaines\n~6 mois"
  },
  {
    "objectID": "resources/faq.html#questions-techniques",
    "href": "resources/faq.html#questions-techniques",
    "title": "â“ FAQ",
    "section": "ğŸ’» Questions Techniques",
    "text": "ğŸ’» Questions Techniques\n\nAi-je besoin dâ€™un ordinateur puissant ?\nConfiguration minimale :\n\nRAM : 8 Go (16 Go recommandÃ©)\nCPU : 4 cores\nDisque : 50 Go libres\nOS : macOS, Linux, ou Windows + WSL2\n\n\n\nPuis-je suivre le bootcamp sur Windows ?\nOui, avec WSL2 (Windows Subsystem for Linux). Câ€™est mÃªme recommandÃ© car la plupart des outils Data sont conÃ§us pour Linux.\n# Installer WSL2\nwsl --install\n\n# Puis suivre les instructions Ubuntu\n\n\nDocker ou Kubernetes local ?\nPour le bootcamp :\n\n\n\nOutil\nUsage\nQuand\n\n\n\n\nDocker Desktop\nContainers simples\nModules 14-16\n\n\nminikube\nCluster K8s local\nModules 19-22\n\n\nkind\nAlternative lÃ©gÃ¨re\nSi minikube est trop lourd\n\n\n\n\n\nQuelle version de Python utiliser ?\nPython 3.11 est recommandÃ©. Câ€™est la version la plus stable et compatible avec PySpark 3.5.\npython3 --version\n# Python 3.11.x âœ“"
  },
  {
    "objectID": "resources/faq.html#questions-sur-spark",
    "href": "resources/faq.html#questions-sur-spark",
    "title": "â“ FAQ",
    "section": "âš¡ Questions sur Spark",
    "text": "âš¡ Questions sur Spark\n\nPySpark ou Scala ?\n\n\n\n\n\n\n\n\nLangage\nAvantages\nRecommandÃ© pour\n\n\n\n\nPySpark\nFacile, Ã©cosystÃ¨me Python\n90% des cas, Data Scientists\n\n\nScala\nPerformance, API native\nOptimisation avancÃ©e, core development\n\n\n\nLe bootcamp utilise PySpark pour les niveaux 1-2, et introduit Scala au niveau 3.\n\n\nSpark local vs cluster ?\nPour apprendre, Spark local suffit :\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName(\"Bootcamp\") \\\n    .getOrCreate()\nEn production, vous utiliserez un cluster (Kubernetes, EMR, Dataproc).\n\n\nDelta Lake ou Iceberg ?\n\n\n\nFormat\nAvantages\nÃ‰cosystÃ¨me\n\n\n\n\nDelta Lake\nMature, Databricks-backed\nSpark-centric\n\n\nIceberg\nOpen, multi-engine\nTrino, Flink, Spark\n\n\nHudi\nCDC optimisÃ©\nKafka, Flink\n\n\n\nLe bootcamp couvre Delta Lake en prioritÃ© (plus rÃ©pandu), avec une introduction Ã  Iceberg."
  },
  {
    "objectID": "resources/faq.html#questions-sur-les-pipelines",
    "href": "resources/faq.html#questions-sur-les-pipelines",
    "title": "â“ FAQ",
    "section": "ğŸ”„ Questions sur les Pipelines",
    "text": "ğŸ”„ Questions sur les Pipelines\n\nAirflow ou Dagster ?\n\n\n\nOutil\nAvantages\nInconvÃ©nients\n\n\n\n\nAirflow\nStandard industrie, mature\nComplexe, legacy\n\n\nDagster\nModerne, testable\nMoins adoptÃ©\n\n\nPrefect\nSimple, cloud-native\nMoins de features\n\n\n\nLe bootcamp enseigne Airflow car câ€™est le standard de lâ€™industrie (90%+ des offres dâ€™emploi).\n\n\nBatch ou Streaming ?\n\n\n\nType\nLatence\nComplexitÃ©\nCas dâ€™usage\n\n\n\n\nBatch\nMinutes-Heures\nSimple\nReporting, analytics\n\n\nStreaming\nSecondes\nComplexe\nReal-time, alerting\n\n\n\nCommencez par le batch, puis Ã©voluez vers le streaming quand nÃ©cessaire.\n\n\nCâ€™est quoi le Lakehouse ?\nLe Lakehouse combine les avantages du Data Lake (stockage pas cher, donnÃ©es brutes) et du Data Warehouse (performance, ACID, SQL).\nData Lake + Data Warehouse = Lakehouse\n\nTechnologies : Delta Lake, Iceberg, Hudi\nArchitecture : Bronze â†’ Silver â†’ Gold (Medallion)"
  },
  {
    "objectID": "resources/faq.html#questions-sur-la-carriÃ¨re",
    "href": "resources/faq.html#questions-sur-la-carriÃ¨re",
    "title": "â“ FAQ",
    "section": "ğŸ¯ Questions sur la CarriÃ¨re",
    "text": "ğŸ¯ Questions sur la CarriÃ¨re\n\nQuel salaire peut-on espÃ©rer ?\n\n\n\nNiveau\nFrance\nUSA\nRemote\n\n\n\n\nJunior (0-2 ans)\n40-50Kâ‚¬\n$80-100K\n$60-90K\n\n\nConfirmÃ© (2-5 ans)\n50-70Kâ‚¬\n$100-140K\n$90-120K\n\n\nSenior (5+ ans)\n70-90Kâ‚¬\n$140-200K\n$120-180K\n\n\nStaff/Principal\n90K+â‚¬\n$200K+\n$150K+\n\n\n\n\n\nQuelles certifications sont utiles ?\n\n\n\nCertification\nValeur\nCoÃ»t\n\n\n\n\nAWS Data Analytics\nâ­â­â­â­\n~$300\n\n\nGCP Data Engineer\nâ­â­â­â­\n~$200\n\n\nDatabricks Certified\nâ­â­â­â­â­\n~$200\n\n\ndbt Certification\nâ­â­â­\nGratuit\n\n\n\n\n\nComment prÃ©parer les entretiens ?\n\nThÃ©orie : SQL, Python, Spark, architecture\nPratique : Projets GitHub, contributions open-source\nSystem Design : Savoir concevoir un pipeline end-to-end\nCoding : LeetCode (SQL, Python) â€” niveau Medium\n\nLes projets intÃ©grateurs du bootcamp sont conÃ§us pour Ãªtre prÃ©sentÃ©s en entretien."
  },
  {
    "objectID": "resources/faq.html#problÃ¨mes-courants",
    "href": "resources/faq.html#problÃ¨mes-courants",
    "title": "â“ FAQ",
    "section": "ğŸ†˜ ProblÃ¨mes Courants",
    "text": "ğŸ†˜ ProblÃ¨mes Courants\n\nSpark : â€œJava gateway process exitedâ€\n# VÃ©rifier Java\njava -version\n\n# Installer Java 11\n# macOS\nbrew install openjdk@11\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11)\n\n# Ubuntu\nsudo apt install openjdk-11-jdk\n\n\nDocker : â€œpermission deniedâ€\nsudo usermod -aG docker $USER\nnewgrp docker\n# Puis logout/login\n\n\nKafka : â€œbroker not availableâ€\n# VÃ©rifier que Zookeeper est dÃ©marrÃ© avant Kafka\ndocker-compose logs zookeeper\ndocker-compose logs kafka\n\n# RedÃ©marrer\ndocker-compose down\ndocker-compose up -d\n\n\nAirflow : â€œNo module named â€˜xyzâ€™â€\n# Installer dans le mÃªme environnement qu'Airflow\npip install xyz\n\n# Ou avec contraintes Airflow\npip install xyz --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.8.0/constraints-3.11.txt\""
  },
  {
    "objectID": "resources/faq.html#support",
    "href": "resources/faq.html#support",
    "title": "â“ FAQ",
    "section": "ğŸ“ Support",
    "text": "ğŸ“ Support\n\nComment poser une question ?\n\nGitHub Issues : Pour les bugs ou suggestions\nDiscussions : Pour les questions gÃ©nÃ©rales\nCommentaires Giscus : En bas de chaque page\n\n\n\nJe suis bloquÃ©, que faire ?\n\nRelire le module prÃ©cÃ©dent\nChercher lâ€™erreur sur Google/StackOverflow\nConsulter la documentation officielle\nPoser une question sur GitHub Discussions\n\n\nğŸ”™ Retour Ã  lâ€™accueil"
  },
  {
    "objectID": "resources/setup.html",
    "href": "resources/setup.html",
    "title": "ğŸ”§ Setup Environnement",
    "section": "",
    "text": "Avant de commencer, assurez-vous dâ€™avoir :\n\n\n\nOutil\nVersion minimale\nVÃ©rification\n\n\n\n\nOS\nmacOS 12+, Ubuntu 20.04+, Windows 10+ (WSL2)\n-\n\n\nRAM\n8 Go minimum (16 Go recommandÃ©)\n-\n\n\nDisque\n50 Go dâ€™espace libre\n-"
  },
  {
    "objectID": "resources/setup.html#prÃ©requis",
    "href": "resources/setup.html#prÃ©requis",
    "title": "ğŸ”§ Setup Environnement",
    "section": "",
    "text": "Avant de commencer, assurez-vous dâ€™avoir :\n\n\n\nOutil\nVersion minimale\nVÃ©rification\n\n\n\n\nOS\nmacOS 12+, Ubuntu 20.04+, Windows 10+ (WSL2)\n-\n\n\nRAM\n8 Go minimum (16 Go recommandÃ©)\n-\n\n\nDisque\n50 Go dâ€™espace libre\n-"
  },
  {
    "objectID": "resources/setup.html#python",
    "href": "resources/setup.html#python",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ 1. Python",
    "text": "ğŸ 1. Python\n\nInstallation\n\nmacOSUbuntu/DebianWindows (WSL2)\n\n\n# Avec Homebrew\nbrew install python@3.11\n\n# VÃ©rifier\npython3 --version\n\n\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3-pip\n\n# VÃ©rifier\npython3 --version\n\n\n# Dans WSL2\nsudo apt update\nsudo apt install python3.11 python3.11-venv python3-pip\n\n\n\n\n\nEnvironnement virtuel\n# CrÃ©er un venv pour le bootcamp\npython3 -m venv ~/bootcamp-env\n\n# Activer\nsource ~/bootcamp-env/bin/activate  # Linux/macOS\n# ou\n.\\bootcamp-env\\Scripts\\activate     # Windows\n\n# Installer les packages de base\npip install --upgrade pip\npip install jupyter pandas numpy pyspark polars"
  },
  {
    "objectID": "resources/setup.html#docker",
    "href": "resources/setup.html#docker",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ³ 2. Docker",
    "text": "ğŸ³ 2. Docker\n\nInstallation\n\nmacOSUbuntuWindows\n\n\n# TÃ©lÃ©charger Docker Desktop\n# https://www.docker.com/products/docker-desktop\n\n# Ou avec Homebrew\nbrew install --cask docker\n\n\n# Installer Docker Engine\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n\n# Ajouter l'utilisateur au groupe docker\nsudo usermod -aG docker $USER\nnewgrp docker\n\n# VÃ©rifier\ndocker --version\ndocker run hello-world\n\n\n# Installer Docker Desktop avec WSL2 backend\n# https://www.docker.com/products/docker-desktop\n\n\n\n\n\nVÃ©rification\ndocker --version\ndocker-compose --version\ndocker run hello-world"
  },
  {
    "objectID": "resources/setup.html#kubernetes-kubectl-minikube",
    "href": "resources/setup.html#kubernetes-kubectl-minikube",
    "title": "ğŸ”§ Setup Environnement",
    "section": "â˜¸ï¸ 3. Kubernetes (kubectl + minikube)",
    "text": "â˜¸ï¸ 3. Kubernetes (kubectl + minikube)\n\nkubectl\n\nmacOSUbuntu\n\n\nbrew install kubectl\n\n\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl\n\n\n\n\n\nminikube (cluster local)\n\nmacOSUbuntu\n\n\nbrew install minikube\nminikube start --memory=4096 --cpus=2\n\n\ncurl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\nminikube start --memory=4096 --cpus=2\n\n\n\n\n\nVÃ©rification\nkubectl version --client\nminikube status\nkubectl get nodes"
  },
  {
    "objectID": "resources/setup.html#apache-spark",
    "href": "resources/setup.html#apache-spark",
    "title": "ğŸ”§ Setup Environnement",
    "section": "âš¡ 4. Apache Spark",
    "text": "âš¡ 4. Apache Spark\n\nInstallation locale (optionnel)\n# Avec pip (PySpark inclut Spark)\npip install pyspark==3.5.0\n\n# VÃ©rifier\npyspark --version\n\n\nAvec Docker (recommandÃ©)\n# Image Bitnami Spark\ndocker pull bitnami/spark:3.5\n\n# Lancer Spark standalone\ndocker run -it --rm \\\n  -p 8080:8080 \\\n  -p 7077:7077 \\\n  bitnami/spark:3.5"
  },
  {
    "objectID": "resources/setup.html#apache-kafka",
    "href": "resources/setup.html#apache-kafka",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ“¨ 5. Apache Kafka",
    "text": "ğŸ“¨ 5. Apache Kafka\n\nAvec Docker Compose\nCrÃ©ez un fichier docker-compose-kafka.yml :\nversion: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n    ports:\n      - \"2181:2181\"\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\ndocker-compose -f docker-compose-kafka.yml up -d"
  },
  {
    "objectID": "resources/setup.html#bases-de-donnÃ©es",
    "href": "resources/setup.html#bases-de-donnÃ©es",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ—„ï¸ 6. Bases de donnÃ©es",
    "text": "ğŸ—„ï¸ 6. Bases de donnÃ©es\n\nPostgreSQL\n# Avec Docker\ndocker run -d \\\n  --name postgres \\\n  -e POSTGRES_PASSWORD=bootcamp \\\n  -p 5432:5432 \\\n  postgres:15\n\n\nMongoDB\n# Avec Docker\ndocker run -d \\\n  --name mongodb \\\n  -p 27017:27017 \\\n  mongo:7\n\n\nMinIO (S3 local)\n# Avec Docker\ndocker run -d \\\n  --name minio \\\n  -p 9000:9000 \\\n  -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\""
  },
  {
    "objectID": "resources/setup.html#outils-additionnels",
    "href": "resources/setup.html#outils-additionnels",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ› ï¸ 7. Outils additionnels",
    "text": "ğŸ› ï¸ 7. Outils additionnels\n\ndbt\npip install dbt-core dbt-postgres\ndbt --version\n\n\nGreat Expectations\npip install great-expectations\ngreat_expectations --version\n\n\nAirflow\npip install apache-airflow==2.8.0\nairflow version"
  },
  {
    "objectID": "resources/setup.html#structure-du-projet",
    "href": "resources/setup.html#structure-du-projet",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ“ 8. Structure du projet",
    "text": "ğŸ“ 8. Structure du projet\nCrÃ©ez cette structure pour suivre le bootcamp :\n~/data-engineering-bootcamp/\nâ”œâ”€â”€ .venv/                  # Environnement virtuel\nâ”œâ”€â”€ data/                   # Datasets\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”œâ”€â”€ processed/\nâ”‚   â””â”€â”€ output/\nâ”œâ”€â”€ notebooks/              # Vos notebooks\nâ”œâ”€â”€ spark_jobs/             # Scripts Spark\nâ”œâ”€â”€ dbt_project/            # Projet dbt\nâ”œâ”€â”€ docker/                 # Dockerfiles\nâ”œâ”€â”€ k8s/                    # Manifests Kubernetes\nâ””â”€â”€ airflow/                # DAGs Airflow\n    â””â”€â”€ dags/\nmkdir -p ~/data-engineering-bootcamp/{data/{raw,processed,output},notebooks,spark_jobs,dbt_project,docker,k8s,airflow/dags}\ncd ~/data-engineering-bootcamp\npython3 -m venv .venv\nsource .venv/bin/activate"
  },
  {
    "objectID": "resources/setup.html#checklist-de-vÃ©rification",
    "href": "resources/setup.html#checklist-de-vÃ©rification",
    "title": "ğŸ”§ Setup Environnement",
    "section": "âœ… Checklist de vÃ©rification",
    "text": "âœ… Checklist de vÃ©rification\nExÃ©cutez ces commandes pour vÃ©rifier votre setup :\n# Python\npython3 --version          # âœ“ 3.10+\n\n# Docker\ndocker --version           # âœ“ 24+\ndocker-compose --version   # âœ“ 2.0+\n\n# Kubernetes\nkubectl version --client   # âœ“ 1.28+\nminikube version          # âœ“ 1.32+\n\n# Spark\npyspark --version         # âœ“ 3.5+\n\n# Autres\ngit --version             # âœ“ 2.0+"
  },
  {
    "objectID": "resources/setup.html#problÃ¨mes-courants",
    "href": "resources/setup.html#problÃ¨mes-courants",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸ†˜ ProblÃ¨mes courants",
    "text": "ğŸ†˜ ProblÃ¨mes courants\n\nDocker : permission denied\nsudo usermod -aG docker $USER\nnewgrp docker\n\n\nSpark : Java not found\n# macOS\nbrew install openjdk@11\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11)\n\n# Ubuntu\nsudo apt install openjdk-11-jdk\nexport JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n\n\nminikube : not enough memory\nminikube delete\nminikube start --memory=4096 --cpus=2 --driver=docker"
  },
  {
    "objectID": "resources/setup.html#prÃªt",
    "href": "resources/setup.html#prÃªt",
    "title": "ğŸ”§ Setup Environnement",
    "section": "ğŸš€ PrÃªt !",
    "text": "ğŸš€ PrÃªt !\nUne fois tout installÃ©, vous pouvez commencer le bootcamp :\nğŸ“˜ Commencer le Module 01"
  },
  {
    "objectID": "curriculum.html",
    "href": "curriculum.html",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "",
    "text": "Ce bootcamp est structurÃ© en 3 niveaux progressifs, conÃ§us pour vous emmener de zÃ©ro Ã  un niveau Senior Data Engineer.\nDurÃ©e totale estimÃ©e : \nâ”œâ”€â”€ ğŸŸ¦ Niveau 1 (DÃ©butant)      \nâ”œâ”€â”€ ğŸŸ© Niveau 2 (IntermÃ©diaire) \nâ””â”€â”€ ğŸŸ¥ Niveau 3 (AvancÃ©)"
  },
  {
    "objectID": "curriculum.html#parcours-pÃ©dagogique",
    "href": "curriculum.html#parcours-pÃ©dagogique",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "",
    "text": "Ce bootcamp est structurÃ© en 3 niveaux progressifs, conÃ§us pour vous emmener de zÃ©ro Ã  un niveau Senior Data Engineer.\nDurÃ©e totale estimÃ©e : \nâ”œâ”€â”€ ğŸŸ¦ Niveau 1 (DÃ©butant)      \nâ”œâ”€â”€ ğŸŸ© Niveau 2 (IntermÃ©diaire) \nâ””â”€â”€ ğŸŸ¥ Niveau 3 (AvancÃ©)"
  },
  {
    "objectID": "curriculum.html#niveau-1-dÃ©butant-fondations",
    "href": "curriculum.html#niveau-1-dÃ©butant-fondations",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸŸ¦ Niveau 1 : DÃ©butant â€” Fondations",
    "text": "ğŸŸ¦ Niveau 1 : DÃ©butant â€” Fondations\nObjectif : Construire des bases solides en Python, SQL, et systÃ¨mes distribuÃ©s.\n\n\n\n\n\n\n\n\n\n#\nModule\nDurÃ©e\nCompÃ©tences\n\n\n\n\n01\nIntroduction au Data Engineering\n2h\nRÃ´le du DE, Ã©cosystÃ¨me, architectures\n\n\n02\nLinux & Bash\n4h\nCommandes, scripting, cron\n\n\n03\nGit & Versioning\n3h\nBranches, merge, workflows\n\n\n04\nPython Fondamental\n6h\nSyntaxe, structures, fichiers\n\n\n05\nPython Data Processing\n6h\nPOO, dÃ©corateurs, gÃ©nÃ©rateurs\n\n\n06\nIntroduction BDD Relationnelles\n3h\nConcepts, modÃ©lisation\n\n\n07\nSQL pour Data Engineers\n8h\nRequÃªtes, jointures, optimisation\n\n\n08\nIntroduction Big Data\n4h\nHadoop, HDFS, MapReduce\n\n\n09\nMongoDB\n4h\nNoSQL, CRUD, agrÃ©gations\n\n\n10\nElasticsearch\n4h\nRecherche, indexation\n\n\n11\nIntroduction PySpark\n6h\nRDD, DataFrame, transformations\n\n\n12\nOrchestration de Pipelines\n4h\nConcepts, scheduling\n\n\n13\nBonus : FastAPI\n4h\nAPI REST pour Data Engineers\n\n\n\nğŸ“¦ Projet DÃ©butant : Pipeline CSV â†’ Nettoyage â†’ AgrÃ©gation â†’ Export"
  },
  {
    "objectID": "curriculum.html#niveau-2-intermÃ©diaire-industrialisation",
    "href": "curriculum.html#niveau-2-intermÃ©diaire-industrialisation",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸŸ© Niveau 2 : IntermÃ©diaire â€” Industrialisation",
    "text": "ğŸŸ© Niveau 2 : IntermÃ©diaire â€” Industrialisation\nObjectif : MaÃ®triser les technologies dâ€™entreprise : Docker, Kubernetes, Lakehouse, Streaming.\n\n\n\n#\nModule\nDurÃ©e\nCompÃ©tences\n\n\n\n\n14\nDocker pour Data Engineers\n6h\nImages, containers, Compose\n\n\n15\nKubernetes Fondamentaux\n6h\nPods, Deployments, Services\n\n\n16\nK8s pour Data Workloads\n4h\nStatefulSets, volumes\n\n\n17\nCloud Object Storage\n4h\nS3, GCS, MinIO, IAM\n\n\n18\nPolars\n4h\nHigh-performance DataFrames\n\n\n19\nHigh Performance Python\n4h\nProfiling, optimisation\n\n\n20\nPySpark AvancÃ©\n8h\nPartitioning, broadcast, UDF\n\n\n21\nSpark SQL Deep Dive\n6h\nCatalyst, plans dâ€™exÃ©cution\n\n\n22\nSpark on Kubernetes\n6h\nSpark Operator, scaling\n\n\n23\nTable Formats (Delta, Iceberg)\n8h\nACID, Time Travel, MERGE\n\n\n24\nKafka & Streaming\n6h\nProducers, Consumers, Topics\n\n\n25\nSpark Structured Streaming\n6h\nWatermarks, Windows\n\n\n26\nOrchestration Airflow\n6h\nDAGs, Operators, XCom\n\n\n27\ndbt & Data Quality\n6h\nModels, Tests, Great Expectations\n\n\n28\nCI/CD pour Data Pipelines\n4h\nGitHub Actions, dÃ©ploiement\n\n\n\nğŸ“¦ Projet IntermÃ©diaire : Pipeline E-commerce Olist (Kafka â†’ Spark â†’ Delta â†’ dbt)"
  },
  {
    "objectID": "curriculum.html#niveau-3-avancÃ©-architecture-leadership",
    "href": "curriculum.html#niveau-3-avancÃ©-architecture-leadership",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸŸ¥ Niveau 3 : AvancÃ© â€” Architecture & Leadership",
    "text": "ğŸŸ¥ Niveau 3 : AvancÃ© â€” Architecture & Leadership\nObjectif : Atteindre le niveau Senior Data Engineer / Architecte Data.\n\n\n\n\n\n\n\n\n\n#\nModule\nDurÃ©e\nCompÃ©tences\n\n\n\n\n31\nKubernetes Deep Dive\n8h\nPrometheus, Grafana, Helm, ArgoCD\n\n\n32\nMessaging Systems\n6h\nKafka avancÃ©, RabbitMQ, Pulsar, Debezium\n\n\n33\nSpark & Scala Internals\n8h\nCatalyst, Tungsten, AQE, optimisation\n\n\n34\nApache Flink\n8h\nExactly-once, State, Flink SQL\n\n\n35\nMLOps pour Data Engineers\n6h\nFeature Stores, MLflow\n\n\n36\nData Governance & Security\n6h\nIAM, RLS, Data Contracts, GDPR\n\n\n37\nArchitectural Trade-Offs\n4h\nFinOps, ADR, Capacity Planning\n\n\n38\nTechnical Leadership\n4h\nRFC, Design Reviews, Mentoring\n\n\n39\nData Platform Design\n6h\nData Mesh, Lakehouse patterns\n\n\n\nğŸ“¦ Projet AvancÃ© : Plateforme Data Multi-Tenant (K8s + Kafka + Flink + MLOps + Governance)"
  },
  {
    "objectID": "curriculum.html#compÃ©tences-par-niveau",
    "href": "curriculum.html#compÃ©tences-par-niveau",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸ† CompÃ©tences par Niveau",
    "text": "ğŸ† CompÃ©tences par Niveau\n                    DÃ©butant    IntermÃ©diaire    AvancÃ©\n                    â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€\nPython & SQL           â–ˆâ–ˆâ–‘â–‘â–‘        â–ˆâ–ˆâ–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nPySpark                â–ˆâ–‘â–‘â–‘â–‘        â–ˆâ–ˆâ–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nDocker & K8s           â–‘â–‘â–‘â–‘â–‘        â–ˆâ–ˆâ–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nLakehouse              â–‘â–‘â–‘â–‘â–‘        â–ˆâ–ˆâ–ˆâ–ˆâ–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nStreaming              â–‘â–‘â–‘â–‘â–‘        â–ˆâ–ˆâ–ˆâ–‘â–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nArchitecture           â–‘â–‘â–‘â–‘â–‘        â–ˆâ–ˆâ–‘â–‘â–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\nLeadership             â–‘â–‘â–‘â–‘â–‘        â–‘â–‘â–‘â–‘â–‘           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ"
  },
  {
    "objectID": "curriculum.html#progression-recommandÃ©e",
    "href": "curriculum.html#progression-recommandÃ©e",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸ“ˆ Progression RecommandÃ©e",
    "text": "ğŸ“ˆ Progression RecommandÃ©e\n\nSuivez lâ€™ordre des modules â€” ils sont conÃ§us pour Ãªtre sÃ©quentiels\nFaites tous les exercices â€” la pratique est essentielle\nComplÃ©tez les projets intÃ©grateurs â€” ils valident vos compÃ©tences\nPrenez des notes â€” crÃ©ez votre propre documentation\nExpÃ©rimentez â€” modifiez le code, cassez des choses, apprenez"
  },
  {
    "objectID": "curriculum.html#certification",
    "href": "curriculum.html#certification",
    "title": "ğŸ“Š Curriculum Complet",
    "section": "ğŸ“ Certification",
    "text": "ğŸ“ Certification\nÃ€ la fin de chaque niveau, vous serez capable de :\n\n\n\n\n\n\n\nNiveau\nVous pouvezâ€¦\n\n\n\n\nğŸŸ¦ DÃ©butant\nCrÃ©er des pipelines batch simples, manipuler des donnÃ©es avec Python/SQL/Spark\n\n\nğŸŸ© IntermÃ©diaire\nDÃ©ployer des jobs Spark sur K8s, construire un Lakehouse, orchestrer avec Airflow\n\n\nğŸŸ¥ AvancÃ©\nArchitecturer une plateforme Data complÃ¨te, optimiser les performances, lead une Ã©quipe\n\n\n\n\nğŸš€ Commencer le Bootcamp"
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas plonger dans les entrailles de Kubernetes. Tu dÃ©couvriras comment fonctionne rÃ©ellement un cluster K8s, comment packager tes dÃ©ploiements avec Helm, automatiser le dÃ©ploiement continu avec ArgoCD, et mettre en place un monitoring professionnel avec Prometheus et Grafana â€” des compÃ©tences indispensables pour un Data Engineer Senior !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#prÃ©requis",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#prÃ©requis",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi les modules 14_docker_for_data_engineers et 15_kubernetes_fundamentals\n\n\nâœ… Requis\nAvoir suivi le module 16_k8s_for_data_workloads\n\n\nâœ… Requis\nMaÃ®triser kubectl, Pods, Deployments, Services, ConfigMaps, Secrets\n\n\nâœ… Requis\nConnaissances solides en YAML\n\n\nğŸ’¡ RecommandÃ©\nUn cluster K8s accessible (Minikube, kind, ou cloud)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#objectifs-du-module",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#objectifs-du-module",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture interne de Kubernetes (etcd, Scheduler, Controllers)\nSauvegarder et restaurer etcd\nConfigurer le scheduling avancÃ© (affinity, taints, tolerations, priority classes)\nPackager et dÃ©ployer des applications avec Helm\nMettre en place un workflow GitOps avec ArgoCD\nDÃ©ployer et configurer Prometheus et Grafana pour monitorer tes pipelines data\nCrÃ©er des dashboards et des alertes personnalisÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#rappel-ce-quon-a-vu-vs-ce-quon-va-approfondir",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#rappel-ce-quon-a-vu-vs-ce-quon-va-approfondir",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "Rappel : Ce quâ€™on a vu vs Ce quâ€™on va approfondir",
    "text": "Rappel : Ce quâ€™on a vu vs Ce quâ€™on va approfondir\n\n\n\n\n\n\n\n\nModule\nCe quâ€™on a couvert\nCe module approfondit\n\n\n\n\nM15 - Fundamentals\nArchitecture simplifiÃ©e, Pods, Deployments, Services\nArchitecture interne complÃ¨te, etcd, Scheduler\n\n\nM16 - Data Workloads\nSpark/Airflow sur K8s, HPA, Helm basics\nHelm avancÃ©, GitOps, Monitoring pro\n\n\n\n\nSchÃ©ma : Du Fundamentals au Deep Dive\nM15 Fundamentals          M16 Data Workloads         M27 Deep Dive (ce module)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ â€¢ Pods          â”‚       â”‚ â€¢ Spark on K8s  â”‚       â”‚ â€¢ etcd internals        â”‚\nâ”‚ â€¢ Deployments   â”‚  â”€â”€â”€â–¶ â”‚ â€¢ Airflow on K8sâ”‚  â”€â”€â”€â–¶ â”‚ â€¢ Scheduler avancÃ©      â”‚\nâ”‚ â€¢ Services      â”‚       â”‚ â€¢ HPA           â”‚       â”‚ â€¢ Helm charts custom    â”‚\nâ”‚ â€¢ kubectl       â”‚       â”‚ â€¢ Helm intro    â”‚       â”‚ â€¢ ArgoCD / GitOps       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ â€¢ Prometheus / Grafana  â”‚\n                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Ce module est orientÃ© â€œcomprendre et opÃ©rerâ€ â€” tu vas apprendre ce qui se passe sous le capot pour mieux diagnostiquer, optimiser et industrialiser tes dÃ©ploiements data.\n\n\nâ„¹ï¸ Le savais-tu ?\nGoogle exÃ©cute des milliards de containers par semaine sur son systÃ¨me interne Borg, lâ€™ancÃªtre de Kubernetes.\nLe cluster Kubernetes le plus grand documentÃ© publiquement compte plus de 15 000 nodes et gÃ¨re des centaines de milliers de pods simultanÃ©ment.\netcd, le cerveau de Kubernetes, utilise lâ€™algorithme de consensus Raft â€” le mÃªme algorithme utilisÃ© par des systÃ¨mes critiques comme CockroachDB et Consul.\nğŸ“– Borg: The Predecessor to Kubernetes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#architecture-interne-de-kubernetes",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#architecture-interne-de-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "1. Architecture Interne de Kubernetes",
    "text": "1. Architecture Interne de Kubernetes\nDans M15, on a vu lâ€™architecture simplifiÃ©e. Maintenant, plongeons dans tous les composants du Control Plane.\n\nVue complÃ¨te du Control Plane\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            CONTROL PLANE                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚             â”‚    â”‚                     â”‚    â”‚                      â”‚    â”‚\nâ”‚  â”‚    etcd     â”‚â—€â”€â”€â–¶â”‚    API Server       â”‚â—€â”€â”€â–¶â”‚  Controller Manager  â”‚    â”‚\nâ”‚  â”‚  (database) â”‚    â”‚  (point d'entrÃ©e)   â”‚    â”‚  (boucles de contrÃ´le)â”‚   â”‚\nâ”‚  â”‚             â”‚    â”‚                     â”‚    â”‚                      â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                â”‚                                            â”‚\nâ”‚                                â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚                                â”‚              â”‚                      â”‚      â”‚\nâ”‚                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚     Scheduler        â”‚      â”‚\nâ”‚                                               â”‚  (placement pods)    â”‚      â”‚\nâ”‚                                               â”‚                      â”‚      â”‚\nâ”‚                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚                    Admission Controllers                             â”‚   â”‚\nâ”‚  â”‚  (validation, mutation, policies)                                    â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                    â”‚\n                                    â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            WORKER NODES                                     â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\nâ”‚  â”‚  Node 1         â”‚  â”‚  Node 2         â”‚  â”‚  Node 3         â”‚             â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚\nâ”‚  â”‚  â”‚  kubelet  â”‚  â”‚  â”‚  â”‚  kubelet  â”‚  â”‚  â”‚  â”‚  kubelet  â”‚  â”‚             â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚             â”‚\nâ”‚  â”‚  â”‚kube-proxy â”‚  â”‚  â”‚  â”‚kube-proxy â”‚  â”‚  â”‚  â”‚kube-proxy â”‚  â”‚             â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚             â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nComposants du Control Plane\n\n\n\n\n\n\n\n\nComposant\nRÃ´le\nAnalogie\n\n\n\n\netcd\nBase de donnÃ©es clÃ©-valeur distribuÃ©e, stocke tout lâ€™Ã©tat du cluster\nLe disque dur du cerveau\n\n\nAPI Server\nPoint dâ€™entrÃ©e unique, valide et persiste les objets\nLa rÃ©ception de lâ€™entreprise\n\n\nController Manager\nExÃ©cute les boucles de contrÃ´le (Deployment, ReplicaSetâ€¦)\nLes managers qui vÃ©rifient que tout tourne\n\n\nScheduler\nAssigne les pods aux nodes\nLe RH qui place les employÃ©s\n\n\nAdmission Controllers\nInterceptent les requÃªtes pour valider/muter\nLes vigiles Ã  lâ€™entrÃ©e\n\n\n\n\n\nFlux dâ€™une requÃªte kubectl\nQuand tu fais kubectl apply -f deployment.yaml, voici ce qui se passe :\nkubectl apply â”€â”€â”€â–¶ API Server â”€â”€â”€â–¶ Admission Controllers â”€â”€â”€â–¶ etcd (persist)\n                                                                    â”‚\n                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                        â–¼\n              Controller Manager (voit le nouveau Deployment)\n                        â”‚\n                        â–¼\n              CrÃ©e un ReplicaSet â”€â”€â”€â–¶ API Server â”€â”€â”€â–¶ etcd\n                        â”‚\n                        â–¼\n              ReplicaSet Controller crÃ©e des Pods â”€â”€â”€â–¶ etcd\n                        â”‚\n                        â–¼\n              Scheduler assigne les Pods aux Nodes â”€â”€â”€â–¶ etcd\n                        â”‚\n                        â–¼\n              kubelet (sur chaque node) dÃ©marre les containers",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#etcd-deep-dive",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#etcd-deep-dive",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "2. etcd Deep Dive",
    "text": "2. etcd Deep Dive\n\netcd est une base de donnÃ©es clÃ©-valeur distribuÃ©e, fortement consistante, qui stocke tout lâ€™Ã©tat de ton cluster Kubernetes.\n\n\nPourquoi etcd est critique\n\n\n\n\n\n\n\nAspect\nImpact\n\n\n\n\nTout est dedans\nPods, Services, Secrets, ConfigMaps, Ã©tat des Deploymentsâ€¦\n\n\nSingle source of truth\nSi etcd meurt sans backup = cluster perdu\n\n\nPerformance\nLatence etcd = latence de tout le cluster\n\n\n\n\n\nAlgorithme de consensus Raft\netcd utilise Raft pour garantir la consistance entre plusieurs instances :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CLUSTER etcd (3 nodes)                   â”‚\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\nâ”‚   â”‚ etcd-1  â”‚       â”‚ etcd-2  â”‚       â”‚ etcd-3  â”‚          â”‚\nâ”‚   â”‚ LEADER  â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚FOLLOWER â”‚â—€â”€â”€â”€â”€â”€â–¶â”‚FOLLOWER â”‚          â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\nâ”‚        â”‚                                                    â”‚\nâ”‚        â”‚  Toutes les Ã©critures passent par le Leader       â”‚\nâ”‚        â”‚  puis sont rÃ©pliquÃ©es aux Followers                â”‚\nâ”‚        â–¼                                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\nâ”‚   â”‚ Quorum = majoritÃ© (2/3 nodes OK = OK)   â”‚              â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nRÃ¨gles de dimensionnement etcd\n\n\n\nNombre de nodes\nTolÃ©rance aux pannes\nRecommandation\n\n\n\n\n1\n0 (aucune)\nDev/test uniquement\n\n\n3\n1 node\nProduction standard\n\n\n5\n2 nodes\nProduction critique\n\n\n7\n3 nodes\nTrÃ¨s rare, overhead important\n\n\n\n\nâš ï¸ Toujours un nombre impair pour Ã©viter le split-brain !\n\n\n\nCommandes etcd essentielles\n# VÃ©rifier la santÃ© du cluster etcd\netcdctl endpoint health --cluster\n\n# Lister les membres du cluster\netcdctl member list\n\n# Voir le leader actuel\netcdctl endpoint status --cluster -w table\n\n# Obtenir une clÃ© (exemple: voir les namespaces)\netcdctl get /registry/namespaces --prefix --keys-only\n\n\nBackup et Restore etcd\nCâ€™est LA compÃ©tence critique pour un cluster de production.\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# BACKUP etcd\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Variables (adapter selon ton cluster)\nETCD_ENDPOINTS=\"https://127.0.0.1:2379\"\nETCD_CACERT=\"/etc/kubernetes/pki/etcd/ca.crt\"\nETCD_CERT=\"/etc/kubernetes/pki/etcd/server.crt\"\nETCD_KEY=\"/etc/kubernetes/pki/etcd/server.key\"\n\n# CrÃ©er un snapshot\netcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d).db \\\n  --endpoints=$ETCD_ENDPOINTS \\\n  --cacert=$ETCD_CACERT \\\n  --cert=$ETCD_CERT \\\n  --key=$ETCD_KEY\n\n# VÃ©rifier le snapshot\netcdctl snapshot status /backup/etcd-snapshot-20240115.db -w table\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# RESTORE etcd (âš ï¸ OpÃ©ration critique !)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# 1. ArrÃªter le kubelet et etcd\nsystemctl stop kubelet\nsystemctl stop etcd\n\n# 2. Sauvegarder l'ancien data-dir\nmv /var/lib/etcd /var/lib/etcd.backup\n\n# 3. Restaurer depuis le snapshot\netcdctl snapshot restore /backup/etcd-snapshot-20240115.db \\\n  --data-dir=/var/lib/etcd \\\n  --name=master-1 \\\n  --initial-cluster=master-1=https://192.168.1.10:2380 \\\n  --initial-advertise-peer-urls=https://192.168.1.10:2380\n\n# 4. RedÃ©marrer les services\nsystemctl start etcd\nsystemctl start kubelet\n\n# 5. VÃ©rifier\nkubectl get nodes\nkubectl get pods -A\n\n\nBonnes pratiques etcd\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nBackups automatiques (toutes les heures)\nLimiter la perte de donnÃ©es\n\n\nStocker les backups hors du cluster\nSi le cluster meurt, les backups survivent\n\n\nTester les restore rÃ©guliÃ¨rement\nUn backup non testÃ© = pas de backup\n\n\nMonitorer les mÃ©triques etcd\nLatence, espace disque, leader elections\n\n\nSSD obligatoire\netcd est trÃ¨s sensible aux I/O disque",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#scheduler-avancÃ©",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#scheduler-avancÃ©",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "3. Scheduler AvancÃ©",
    "text": "3. Scheduler AvancÃ©\nLe kube-scheduler dÃ©cide sur quel node placer chaque pod. En niveau avancÃ©, tu dois savoir influencer ces dÃ©cisions.\n\nComment le Scheduler fonctionne\nPod Ã  scheduler\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PHASE 1 : FILTERING                â”‚\nâ”‚  Ã‰liminer les nodes qui ne conviennent pas      â”‚\nâ”‚  â€¢ Ressources insuffisantes (CPU, RAM)          â”‚\nâ”‚  â€¢ Taints non tolÃ©rÃ©es                          â”‚\nâ”‚  â€¢ NodeSelector non matchÃ©                      â”‚\nâ”‚  â€¢ Affinity rules non respectÃ©es                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PHASE 2 : SCORING                  â”‚\nâ”‚  Classer les nodes restants par score           â”‚\nâ”‚  â€¢ Spread (rÃ©partir les pods)                   â”‚\nâ”‚  â€¢ Resource balancing                           â”‚\nâ”‚  â€¢ Preferred affinity                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n      â”‚\n      â–¼\nNode avec le meilleur score sÃ©lectionnÃ©\n\n\nNode Selector (basique)\nLe plus simple : matcher un label sur le node.\n# Labelliser un node\n# kubectl label nodes worker-1 disk=ssd\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: spark-driver\nspec:\n  nodeSelector:\n    disk: ssd          # Ce pod ira uniquement sur les nodes avec disk=ssd\n  containers:\n  - name: spark\n    image: apache/spark:3.5.0\n\n\nNode Affinity (avancÃ©)\nPlus de contrÃ´le que nodeSelector : rÃ¨gles required vs preferred.\napiVersion: v1\nkind: Pod\nmetadata:\n  name: spark-executor\nspec:\n  affinity:\n    nodeAffinity:\n      # OBLIGATOIRE : le pod ne sera PAS schedulÃ© si non respectÃ©\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: node-type\n            operator: In\n            values:\n            - compute\n            - highmem\n      \n      # PRÃ‰FÃ‰RÃ‰ : le scheduler essaie, mais schedule quand mÃªme si impossible\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 80    # Poids de 1 Ã  100\n        preference:\n          matchExpressions:\n          - key: zone\n            operator: In\n            values:\n            - eu-west-1a\n  containers:\n  - name: spark\n    image: apache/spark:3.5.0\n\n\nPod Affinity / Anti-Affinity\nPlacer des pods ensemble ou sÃ©parÃ©ment.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spark-executors\nspec:\n  replicas: 5\n  selector:\n    matchLabels:\n      app: spark-executor\n  template:\n    metadata:\n      labels:\n        app: spark-executor\n    spec:\n      affinity:\n        # Pod Affinity : placer AVEC les autres spark-executors\n        podAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: spark-driver    # Proche du driver\n              topologyKey: kubernetes.io/hostname\n        \n        # Pod Anti-Affinity : NE PAS placer sur le mÃªme node qu'un autre executor\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app: spark-executor\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: executor\n        image: apache/spark:3.5.0\n\n\nTaints et Tolerations\nTaints = â€œje repousse certains podsâ€ (sur le node) Tolerations = â€œje tolÃ¨re cette taintâ€ (sur le pod)\n# Ajouter une taint Ã  un node\nkubectl taint nodes gpu-node-1 gpu=true:NoSchedule\n\n# Effets possibles :\n# - NoSchedule : nouveaux pods sans toleration rejetÃ©s\n# - PreferNoSchedule : Ã©viter si possible\n# - NoExecute : pods existants sans toleration Ã©vincÃ©s\n# Pod qui tolÃ¨re la taint GPU\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ml-training\nspec:\n  tolerations:\n  - key: \"gpu\"\n    operator: \"Equal\"\n    value: \"true\"\n    effect: \"NoSchedule\"\n  containers:\n  - name: training\n    image: pytorch/pytorch:2.0.0-cuda11.7\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n\n\nPriority Classes\nDÃ©finir des prioritÃ©s entre pods â€” les plus prioritaires peuvent prÃ©empter les autres.\n# CrÃ©er une PriorityClass\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: data-pipeline-critical\nvalue: 1000000           # Plus c'est haut, plus c'est prioritaire\nglobalDefault: false\ndescription: \"Pour les pipelines data critiques\"\npreemptionPolicy: PreemptLowerPriority  # Peut Ã©vincer des pods moins prioritaires\n---\napiVersion: scheduling.k8s.io/v1\nkind: PriorityClass\nmetadata:\n  name: data-pipeline-batch\nvalue: 100000\nglobalDefault: false\ndescription: \"Pour les jobs batch non critiques\"\npreemptionPolicy: Never   # Ne prÃ©empte jamais\n# Utiliser la PriorityClass\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-critical\nspec:\n  template:\n    spec:\n      priorityClassName: data-pipeline-critical\n      containers:\n      - name: etl\n        image: my-etl:1.0\n      restartPolicy: OnFailure\n\n\nCas dâ€™usage Data Engineering\n\n\n\nScÃ©nario\nSolution\n\n\n\n\nSpark executors sur nodes avec SSD\nnodeSelector: disk=ssd\n\n\nRÃ©partir Kafka brokers sur diffÃ©rents nodes\nPod Anti-Affinity\n\n\nRÃ©server des nodes GPU pour le ML\nTaints + Tolerations\n\n\nETL critique doit toujours tourner\nPriorityClass Ã©levÃ©e\n\n\nSpark driver proche des executors\nPod Affinity",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#helm-le-package-manager-kubernetes",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#helm-le-package-manager-kubernetes",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "4. Helm â€” Le Package Manager Kubernetes",
    "text": "4. Helm â€” Le Package Manager Kubernetes\n\nHelm est le gestionnaire de packages pour Kubernetes. Il te permet de packager, versionner et dÃ©ployer des applications complexes en une seule commande.\n\n\nPourquoi Helm ?\n\n\n\n\n\n\n\nSans Helm\nAvec Helm\n\n\n\n\n10+ fichiers YAML Ã  maintenir\n1 chart versionnÃ©\n\n\nCopier/coller pour chaque environnement\nVariables par environnement\n\n\nPas de rollback facile\nhelm rollback en 1 commande\n\n\nDifficile de partager\nCharts publics sur Artifact Hub\n\n\n\n\n\nConcepts Helm\n\n\n\nConcept\nDescription\n\n\n\n\nChart\nPackage Helm (dossier avec templates + values)\n\n\nRelease\nInstance dÃ©ployÃ©e dâ€™un chart\n\n\nRepository\nServeur qui hÃ©berge des charts\n\n\nValues\nVariables de configuration\n\n\n\n\n\nStructure dâ€™un Chart\nmy-data-pipeline/\nâ”œâ”€â”€ Chart.yaml           # MÃ©tadonnÃ©es du chart\nâ”œâ”€â”€ values.yaml          # Valeurs par dÃ©faut\nâ”œâ”€â”€ values-dev.yaml      # Override pour dev\nâ”œâ”€â”€ values-prod.yaml     # Override pour prod\nâ”œâ”€â”€ templates/           # Templates Kubernetes\nâ”‚   â”œâ”€â”€ _helpers.tpl     # Fonctions rÃ©utilisables\nâ”‚   â”œâ”€â”€ deployment.yaml\nâ”‚   â”œâ”€â”€ service.yaml\nâ”‚   â”œâ”€â”€ configmap.yaml\nâ”‚   â”œâ”€â”€ secret.yaml\nâ”‚   â”œâ”€â”€ cronjob.yaml\nâ”‚   â””â”€â”€ NOTES.txt        # Message post-install\nâ””â”€â”€ charts/              # DÃ©pendances (sub-charts)\n\n\nCrÃ©er un Chart pour un Pipeline ETL\n# CrÃ©er la structure de base\nhelm create etl-pipeline\ncd etl-pipeline\nChart.yaml\napiVersion: v2\nname: etl-pipeline\ndescription: Pipeline ETL pour Data Engineering\ntype: application\nversion: 1.0.0        # Version du chart\nappVersion: \"2.1.0\"   # Version de l'application\nmaintainers:\n  - name: Data Team\n    email: data@company.com\ndependencies:\n  - name: postgresql\n    version: \"12.x.x\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n    condition: postgresql.enabled\nvalues.yaml\n# Configuration globale\nreplicaCount: 1\n\nimage:\n  repository: my-registry/etl-pipeline\n  tag: \"2.1.0\"\n  pullPolicy: IfNotPresent\n\n# Configuration ETL\netl:\n  schedule: \"0 2 * * *\"    # Tous les jours Ã  2h\n  sourceBucket: \"raw-data\"\n  destBucket: \"processed-data\"\n  batchSize: 10000\n\n# Configuration base de donnÃ©es\ndatabase:\n  host: \"postgres\"\n  port: 5432\n  name: \"analytics\"\n  user: \"etl_user\"\n  # Le password vient d'un secret existant\n  existingSecret: \"db-credentials\"\n  secretKey: \"password\"\n\n# Ressources\nresources:\n  requests:\n    cpu: \"500m\"\n    memory: \"512Mi\"\n  limits:\n    cpu: \"2000m\"\n    memory: \"2Gi\"\n\n# PostgreSQL subchart\npostgresql:\n  enabled: true\n  auth:\n    database: analytics\n    username: etl_user\ntemplates/cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: {{ include \"etl-pipeline.fullname\" . }}\n  labels:\n    {{- include \"etl-pipeline.labels\" . | nindent 4 }}\nspec:\n  schedule: {{ .Values.etl.schedule | quote }}\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            {{- include \"etl-pipeline.selectorLabels\" . | nindent 12 }}\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: etl\n            image: \"{{ .Values.image.repository }}:{{ .Values.image.tag }}\"\n            imagePullPolicy: {{ .Values.image.pullPolicy }}\n            env:\n            - name: SOURCE_BUCKET\n              value: {{ .Values.etl.sourceBucket | quote }}\n            - name: DEST_BUCKET\n              value: {{ .Values.etl.destBucket | quote }}\n            - name: BATCH_SIZE\n              value: {{ .Values.etl.batchSize | quote }}\n            - name: DB_HOST\n              value: {{ .Values.database.host | quote }}\n            - name: DB_PORT\n              value: {{ .Values.database.port | quote }}\n            - name: DB_NAME\n              value: {{ .Values.database.name | quote }}\n            - name: DB_USER\n              value: {{ .Values.database.user | quote }}\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: {{ .Values.database.existingSecret }}\n                  key: {{ .Values.database.secretKey }}\n            resources:\n              {{- toYaml .Values.resources | nindent 14 }}\n**templates/_helpers.tpl**\n{{/*\nNom complet de l'application\n*/}}\n{{- define \"etl-pipeline.fullname\" -}}\n{{- if .Values.fullnameOverride }}\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}\n{{- else }}\n{{- $name := default .Chart.Name .Values.nameOverride }}\n{{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }}\n{{- end }}\n{{- end }}\n\n{{/*\nLabels communs\n*/}}\n{{- define \"etl-pipeline.labels\" -}}\nhelm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version }}\napp.kubernetes.io/name: {{ .Chart.Name }}\napp.kubernetes.io/instance: {{ .Release.Name }}\napp.kubernetes.io/version: {{ .Chart.AppVersion | quote }}\napp.kubernetes.io/managed-by: {{ .Release.Service }}\n{{- end }}\n\n{{/*\nSelector labels\n*/}}\n{{- define \"etl-pipeline.selectorLabels\" -}}\napp.kubernetes.io/name: {{ .Chart.Name }}\napp.kubernetes.io/instance: {{ .Release.Name }}\n{{- end }}\n\n\nCommandes Helm essentielles\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# GESTION DES REPOSITORIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Ajouter un repo\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n\n# Mettre Ã  jour les repos\nhelm repo update\n\n# Chercher un chart\nhelm search repo kafka\nhelm search hub airflow    # Cherche sur Artifact Hub\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# INSTALLATION ET DÃ‰PLOIEMENT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Installer un chart depuis un repo\nhelm install my-kafka bitnami/kafka -n data --create-namespace\n\n# Installer avec des values custom\nhelm install my-etl ./etl-pipeline -f values-prod.yaml -n production\n\n# Dry-run : voir ce qui serait gÃ©nÃ©rÃ© sans installer\nhelm install my-etl ./etl-pipeline --dry-run --debug\n\n# Template : gÃ©nÃ©rer les manifests YAML\nhelm template my-etl ./etl-pipeline -f values-prod.yaml &gt; manifests.yaml\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MISE Ã€ JOUR ET ROLLBACK\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Mettre Ã  jour une release\nhelm upgrade my-etl ./etl-pipeline -f values-prod.yaml -n production\n\n# Install ou upgrade (idempotent)\nhelm upgrade --install my-etl ./etl-pipeline -f values-prod.yaml -n production\n\n# Voir l'historique des releases\nhelm history my-etl -n production\n\n# Rollback Ã  une version prÃ©cÃ©dente\nhelm rollback my-etl 2 -n production\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# INSPECTION ET DEBUG\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Lister les releases\nhelm list -A\n\n# Voir les values d'une release\nhelm get values my-etl -n production\n\n# Voir tous les manifests dÃ©ployÃ©s\nhelm get manifest my-etl -n production\n\n# Voir les notes post-install\nhelm get notes my-etl -n production\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# SUPPRESSION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# DÃ©sinstaller une release\nhelm uninstall my-etl -n production\n\n# Garder l'historique (permet rollback aprÃ¨s uninstall)\nhelm uninstall my-etl -n production --keep-history\n\n\nHelm vs Kustomize\n\n\n\nAspect\nHelm\nKustomize\n\n\n\n\nApproche\nTemplating\nPatching/Overlay\n\n\nComplexitÃ©\nPlus riche\nPlus simple\n\n\nDÃ©pendances\nSub-charts\nNon natif\n\n\nRollback\nIntÃ©grÃ©\nVia kubectl\n\n\nCas dâ€™usage\nApps complexes\nCustomisations simples\n\n\n\n\nğŸ’¡ En pratique, Helm est le standard pour les charts communautaires et les applications complexes. Kustomize est souvent utilisÃ© en complÃ©ment pour des overlays simples.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#gitops-avec-argocd",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#gitops-avec-argocd",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "5. GitOps avec ArgoCD",
    "text": "5. GitOps avec ArgoCD\n\nGitOps est une pratique oÃ¹ Git est la source de vÃ©ritÃ© pour lâ€™Ã©tat de ton infrastructure. ArgoCD surveille ton repo Git et synchronise automatiquement ton cluster Kubernetes.\n\n\nPourquoi GitOps ?\n\n\n\nApproche Traditionnelle\nGitOps\n\n\n\n\nkubectl apply manuel\nGit push â†’ dÃ©ploiement auto\n\n\nQui a dÃ©ployÃ© quoi ?\nHistorique Git complet\n\n\nRollback complexe\ngit revert\n\n\nÃ‰tat du cluster inconnu\nÃ‰tat = ce qui est dans Git\n\n\n\n\n\nSchÃ©ma GitOps avec ArgoCD\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          WORKFLOW GITOPS                                â”‚\nâ”‚                                                                         â”‚\nâ”‚   DÃ©veloppeur                                                           â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â”‚ 1. git push (manifests/charts)                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚   Git Repo      â”‚                                                  â”‚\nâ”‚   â”‚ (GitHub/GitLab) â”‚                                                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ”‚            â”‚                                                            â”‚\nâ”‚            â”‚ 2. ArgoCD dÃ©tecte le changement                           â”‚\nâ”‚            â–¼                                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚     ArgoCD      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚         Cluster Kubernetes          â”‚  â”‚\nâ”‚   â”‚  (dans le K8s)  â”‚ 3. Sync â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”         â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚  â”‚ Pod â”‚         â”‚  â”‚\nâ”‚            â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜         â”‚  â”‚\nâ”‚            â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚            â”‚ 4. Compare Ã©tat rÃ©el vs Ã©tat Git                          â”‚\nâ”‚            â”‚    (et corrige les drifts)                                â”‚\nâ”‚            â–¼                                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\nâ”‚   â”‚  UI ArgoCD      â”‚  Visualisation, alertes, rollback                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nInstallation dâ€™ArgoCD\n# CrÃ©er le namespace\nkubectl create namespace argocd\n\n# Installer ArgoCD\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Attendre que tous les pods soient prÃªts\nkubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s\n\n# RÃ©cupÃ©rer le mot de passe admin initial\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n\n# Exposer l'UI (pour dev local)\nkubectl port-forward svc/argocd-server -n argocd 8080:443\n\n# AccÃ©der Ã  https://localhost:8080\n# User: admin, Password: (celui rÃ©cupÃ©rÃ© ci-dessus)\n\n\nInstaller le CLI ArgoCD\n# Linux\ncurl -sSL -o argocd https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64\nchmod +x argocd\nsudo mv argocd /usr/local/bin/\n\n# macOS\nbrew install argocd\n\n# Se connecter\nargocd login localhost:8080 --username admin --password &lt;password&gt; --insecure\n\n\nCrÃ©er une Application ArgoCD\nUne Application ArgoCD pointe vers un repo Git et un path contenant les manifests.\n# argocd-app-etl.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: etl-pipeline\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  \n  # Source : oÃ¹ sont les manifests\n  source:\n    repoURL: https://github.com/myorg/data-platform.git\n    targetRevision: main           # Branche Ã  suivre\n    path: kubernetes/etl-pipeline  # Dossier contenant les manifests ou chart Helm\n    \n    # Si c'est un chart Helm\n    helm:\n      valueFiles:\n        - values-prod.yaml\n  \n  # Destination : oÃ¹ dÃ©ployer\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  \n  # Politique de synchronisation\n  syncPolicy:\n    automated:\n      prune: true        # Supprimer les ressources qui ne sont plus dans Git\n      selfHeal: true     # Corriger les drifts (si quelqu'un modifie manuellement)\n      allowEmpty: false\n    syncOptions:\n      - CreateNamespace=true\n      - PrunePropagationPolicy=foreground\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n# CrÃ©er l'application\nkubectl apply -f argocd-app-etl.yaml\n\n# Ou via CLI\nargocd app create etl-pipeline \\\n  --repo https://github.com/myorg/data-platform.git \\\n  --path kubernetes/etl-pipeline \\\n  --dest-server https://kubernetes.default.svc \\\n  --dest-namespace production \\\n  --sync-policy automated \\\n  --auto-prune \\\n  --self-heal\n\n\nApplicationSets\nDÃ©ployer la mÃªme application dans plusieurs environnements ou clusters.\napiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: etl-pipeline-envs\n  namespace: argocd\nspec:\n  generators:\n  - list:\n      elements:\n      - env: dev\n        namespace: dev\n        valuesFile: values-dev.yaml\n      - env: staging\n        namespace: staging\n        valuesFile: values-staging.yaml\n      - env: prod\n        namespace: production\n        valuesFile: values-prod.yaml\n  \n  template:\n    metadata:\n      name: 'etl-pipeline-{{env}}'\n    spec:\n      project: default\n      source:\n        repoURL: https://github.com/myorg/data-platform.git\n        targetRevision: main\n        path: kubernetes/etl-pipeline\n        helm:\n          valueFiles:\n            - '{{valuesFile}}'\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: '{{namespace}}'\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n\n\nCommandes ArgoCD essentielles\n# Lister les applications\nargocd app list\n\n# Voir le statut dÃ©taillÃ©\nargocd app get etl-pipeline\n\n# Synchroniser manuellement\nargocd app sync etl-pipeline\n\n# Voir l'historique\nargocd app history etl-pipeline\n\n# Rollback\nargocd app rollback etl-pipeline &lt;revision&gt;\n\n# Voir les diffÃ©rences (ce qui va changer)\nargocd app diff etl-pipeline\n\n# Supprimer une application (et ses ressources)\nargocd app delete etl-pipeline --cascade\n\n\nBonnes pratiques GitOps\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nUn repo dÃ©diÃ© pour les manifests K8s\nSÃ©paration code applicatif / config infra\n\n\nBranches par environnement ou values files\nPromotion claire dev â†’ staging â†’ prod\n\n\nPR obligatoires pour main/prod\nReview avant dÃ©ploiement\n\n\nSecrets chiffrÃ©s (Sealed Secrets, SOPS)\nNe jamais commiter de secrets en clair\n\n\nNotifications (Slack, Teams)\nSavoir quand un sync Ã©choue",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#monitoring-avec-prometheus-et-grafana",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#monitoring-avec-prometheus-et-grafana",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "6. Monitoring avec Prometheus et Grafana",
    "text": "6. Monitoring avec Prometheus et Grafana\n\nPrometheus collecte et stocke les mÃ©triques. Grafana les visualise. Ensemble, ils forment le stack de monitoring standard de Kubernetes.\n\n\nArchitecture du Stack Monitoring\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        STACK MONITORING K8S                                â”‚\nâ”‚                                                                            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                         PROMETHEUS                                   â”‚  â”‚\nâ”‚  â”‚                                                                      â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”‚\nâ”‚  â”‚   â”‚   Scraper    â”‚â”€â”€â”€â”€ pull metrics â”€â”€â”€â–¶â”‚   Time Series DB  â”‚       â”‚  â”‚\nâ”‚  â”‚   â”‚  (discovery) â”‚                      â”‚   (local storage) â”‚       â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  â”‚\nâ”‚  â”‚                                                   â”‚                  â”‚  â”‚\nâ”‚  â”‚                                                   â”‚ PromQL queries   â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â–¼                  â”‚  â”‚\nâ”‚  â”‚   â”‚ AlertManager â”‚â—€â”€â”€â”€â”€ alerting rules â”€â”€â”€ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚\nâ”‚  â”‚   â”‚ (Slack/PD)   â”‚                         â”‚   Rules   â”‚            â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                    â”‚                                       â”‚\nâ”‚                                    â”‚ PromQL                                â”‚\nâ”‚                                    â–¼                                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                          GRAFANA                                     â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚  â”‚\nâ”‚  â”‚   â”‚ Dashboard 1 â”‚  â”‚ Dashboard 2 â”‚  â”‚ Dashboard 3 â”‚                 â”‚  â”‚\nâ”‚  â”‚   â”‚ K8s Cluster â”‚  â”‚ Data Pipelinesâ”‚ â”‚    Spark   â”‚                 â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                     SOURCES DE MÃ‰TRIQUES                             â”‚  â”‚\nâ”‚  â”‚                                                                      â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚\nâ”‚  â”‚   â”‚ kubelet  â”‚  â”‚ node-    â”‚  â”‚ kube-    â”‚  â”‚ App      â”‚           â”‚  â”‚\nâ”‚  â”‚   â”‚ /metrics â”‚  â”‚ exporter â”‚  â”‚ state    â”‚  â”‚ metrics  â”‚           â”‚  â”‚\nâ”‚  â”‚   â”‚          â”‚  â”‚          â”‚  â”‚ metrics  â”‚  â”‚ (custom) â”‚           â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nInstallation avec kube-prometheus-stack\nLe moyen le plus simple dâ€™installer tout le stack :\n# Ajouter le repo Helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\n# CrÃ©er le namespace\nkubectl create namespace monitoring\n\n# Installer le stack complet\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --set grafana.adminPassword=admin123 \\\n  --set prometheus.prometheusSpec.retention=15d \\\n  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi\n\n# VÃ©rifier l'installation\nkubectl get pods -n monitoring\n\n# AccÃ©der Ã  Grafana\nkubectl port-forward svc/prometheus-grafana -n monitoring 3000:80\n# http://localhost:3000 (admin / admin123)\n\n# AccÃ©der Ã  Prometheus\nkubectl port-forward svc/prometheus-kube-prometheus-prometheus -n monitoring 9090:9090\n# http://localhost:9090\n\n\nPrometheus : Concepts ClÃ©s\n\n\n\nConcept\nDescription\n\n\n\n\nMetric\nDonnÃ©e numÃ©rique avec timestamp (ex: cpu_usage)\n\n\nLabel\nClÃ©-valeur pour filtrer (ex: pod=\"etl-123\")\n\n\nScrape\nPrometheus tire les mÃ©triques des targets\n\n\nTarget\nEndpoint exposant des mÃ©triques (/metrics)\n\n\nPromQL\nLangage de requÃªte Prometheus\n\n\n\n\n\nTypes de mÃ©triques\n\n\n\nType\nDescription\nExemple\n\n\n\n\nCounter\nValeur croissante uniquement\nhttp_requests_total\n\n\nGauge\nValeur qui monte et descend\nmemory_usage_bytes\n\n\nHistogram\nDistribution de valeurs\nrequest_duration_seconds\n\n\nSummary\nQuantiles prÃ©-calculÃ©s\nrequest_latency_seconds\n\n\n\n\n\nPromQL : RequÃªtes Essentielles\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# REQUÃŠTES DE BASE\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# CPU utilisÃ© par namespace\nsum(rate(container_cpu_usage_seconds_total{namespace=\"production\"}[5m])) by (pod)\n\n# MÃ©moire utilisÃ©e par pod\ncontainer_memory_usage_bytes{namespace=\"production\", container!=\"\"}\n\n# Pods en Ã©tat non-Running\nkube_pod_status_phase{phase!=\"Running\", phase!=\"Succeeded\"}\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# MÃ‰TRIQUES DATA ENGINEERING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Jobs CronJob Ã©chouÃ©s dans les derniÃ¨res 24h\nkube_job_status_failed{namespace=\"data-pipeline\"} &gt; 0\n\n# DurÃ©e moyenne des jobs ETL\navg(kube_job_status_completion_time - kube_job_status_start_time) by (job_name)\n\n# Taux de redÃ©marrage des pods (signe de problÃ¨me)\nrate(kube_pod_container_status_restarts_total{namespace=\"production\"}[1h]) &gt; 0\n\n# Utilisation PVC (espace disque)\n(kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100\n\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# FONCTIONS UTILES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# rate() : taux de changement par seconde (pour counters)\nrate(http_requests_total[5m])\n\n# increase() : augmentation sur une pÃ©riode (pour counters)\nincrease(etl_rows_processed_total[1h])\n\n# sum() by () : agrÃ©gation\nsum(rate(cpu_usage[5m])) by (namespace)\n\n# topk() : top N\ntopk(5, sum(rate(container_cpu_usage_seconds_total[5m])) by (pod))\n\n# histogram_quantile() : percentiles\nhistogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\n\n\nServiceMonitor : Scraper des Applications Custom\nPour que Prometheus scrape automatiquement ton application :\n# 1. Ton application expose /metrics\napiVersion: v1\nkind: Service\nmetadata:\n  name: etl-pipeline\n  namespace: production\n  labels:\n    app: etl-pipeline\nspec:\n  ports:\n  - name: metrics\n    port: 8080\n    targetPort: 8080\n  selector:\n    app: etl-pipeline\n---\n# 2. ServiceMonitor pour Prometheus\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: etl-pipeline\n  namespace: monitoring\n  labels:\n    release: prometheus    # Important : matcher le label du Prometheus\nspec:\n  selector:\n    matchLabels:\n      app: etl-pipeline\n  namespaceSelector:\n    matchNames:\n    - production\n  endpoints:\n  - port: metrics\n    interval: 30s\n    path: /metrics\n\n\nAlerting avec PrometheusRule\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: data-pipeline-alerts\n  namespace: monitoring\n  labels:\n    release: prometheus\nspec:\n  groups:\n  - name: data-pipeline\n    rules:\n    # Alerte : Job ETL Ã©chouÃ©\n    - alert: ETLJobFailed\n      expr: kube_job_status_failed{namespace=\"production\", job_name=~\"etl-.*\"} &gt; 0\n      for: 1m\n      labels:\n        severity: critical\n        team: data\n      annotations:\n        summary: \"ETL Job {{ $labels.job_name }} a Ã©chouÃ©\"\n        description: \"Le job {{ $labels.job_name }} dans {{ $labels.namespace }} est en Ã©chec depuis plus d'1 minute.\"\n    \n    # Alerte : Pod restart trop frÃ©quent\n    - alert: PodRestartingTooMuch\n      expr: rate(kube_pod_container_status_restarts_total{namespace=\"production\"}[15m]) &gt; 0.1\n      for: 5m\n      labels:\n        severity: warning\n        team: data\n      annotations:\n        summary: \"Pod {{ $labels.pod }} redÃ©marre trop souvent\"\n        description: \"Le pod {{ $labels.pod }} a redÃ©marrÃ© plus de 3 fois en 15 minutes.\"\n    \n    # Alerte : Espace disque PVC &gt; 80%\n    - alert: PVCAlmostFull\n      expr: (kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes) * 100 &gt; 80\n      for: 10m\n      labels:\n        severity: warning\n        team: data\n      annotations:\n        summary: \"PVC {{ $labels.persistentvolumeclaim }} presque plein\"\n        description: \"Le PVC {{ $labels.persistentvolumeclaim }} est utilisÃ© Ã  {{ $value }}%.\"\n\n\nGrafana : Dashboard pour Data Pipelines\nVoici les panels essentiels pour un dashboard Data Engineering :\n\n\n\n\n\n\n\n\nPanel\nPromQL\nType\n\n\n\n\nJobs actifs\nkube_job_status_active{namespace=\"production\"}\nStat\n\n\nJobs Ã©chouÃ©s (24h)\nsum(increase(kube_job_status_failed{namespace=\"production\"}[24h]))\nStat\n\n\nCPU par pod\nsum(rate(container_cpu_usage_seconds_total{namespace=\"production\"}[5m])) by (pod)\nTime series\n\n\nMÃ©moire par pod\ncontainer_memory_usage_bytes{namespace=\"production\"}\nTime series\n\n\nDurÃ©e des jobs\nkube_job_status_completion_time - kube_job_status_start_time\nHistogram\n\n\nPods non-Ready\nkube_pod_status_ready{condition=\"false\", namespace=\"production\"}\nTable\n\n\n\n\nğŸ’¡ Dashboards prÃ©-faits : Import ID 315 pour le dashboard â€œKubernetes cluster monitoringâ€ et 13770 pour â€œKubernetes All-in-one Cluster Monitoringâ€.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#exercices-pratiques",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#exercices-pratiques",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "7. Exercices Pratiques",
    "text": "7. Exercices Pratiques\n\nExercice 1 : Backup etcd\nObjectif : CrÃ©er un CronJob qui sauvegarde etcd toutes les heures.\nInstructions : 1. CrÃ©er un CronJob qui exÃ©cute etcdctl snapshot save 2. Stocker le backup dans un PVC 3. Garder les 24 derniers backups (rotation)\n\n\nğŸ’¡ Indice\n\nTu auras besoin : - Dâ€™un PVC pour stocker les backups - Des certificats etcd montÃ©s dans le pod - Dâ€™un script shell pour la rotation\n\n\n\n\nExercice 2 : Scheduling Spark\nObjectif : Configurer le scheduling dâ€™un job Spark avec les contraintes suivantes :\n\nLe driver doit tourner sur un node avec le label role=driver\nLes executors doivent Ãªtre rÃ©partis sur des nodes diffÃ©rents (anti-affinity)\nLes executors doivent prÃ©fÃ©rer les nodes avec disk=ssd\n\nInstructions : 1. Ã‰crire le manifest YAML du driver avec nodeSelector 2. Ã‰crire le manifest des executors avec podAntiAffinity et nodeAffinity\n\n\n\nExercice 3 : Chart Helm Kafka\nObjectif : CrÃ©er un chart Helm pour dÃ©ployer un cluster Kafka simple.\nLe chart doit inclure : - Un StatefulSet Kafka (3 replicas) - Un Service headless - Un ConfigMap pour la config Kafka - Des values pour : replicas, resources, retention.ms\n\n\n\nExercice 4 : GitOps Pipeline\nObjectif : Mettre en place un workflow GitOps complet.\nInstructions : 1. CrÃ©er un repo Git avec la structure : data-platform/    â”œâ”€â”€ apps/    â”‚   â””â”€â”€ etl-pipeline/    â”‚       â”œâ”€â”€ Chart.yaml    â”‚       â”œâ”€â”€ values.yaml    â”‚       â”œâ”€â”€ values-dev.yaml    â”‚       â””â”€â”€ values-prod.yaml    â””â”€â”€ argocd/        â””â”€â”€ applications.yaml 2. CrÃ©er un ApplicationSet ArgoCD qui dÃ©ploie en dev et prod 3. Tester le workflow : modifier les values â†’ git push â†’ observer le sync\n\n\n\nExercice 5 : Dashboard Grafana\nObjectif : CrÃ©er un dashboard Grafana pour monitorer un pipeline ETL.\nPanels requis : 1. Nombre de jobs ETL en cours 2. Taux de succÃ¨s/Ã©chec sur 24h 3. DurÃ©e moyenne des jobs (gauge) 4. Top 5 des jobs les plus lents 5. Alerte visuelle si un job Ã©choue\nBonus : Exporter le dashboard en JSON et le versionner dans Git.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#mini-projet-pipeline-data-avec-gitops-monitoring",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#mini-projet-pipeline-data-avec-gitops-monitoring",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "8. Mini-Projet : Pipeline Data avec GitOps & Monitoring",
    "text": "8. Mini-Projet : Pipeline Data avec GitOps & Monitoring\n\nObjectif\nMettre en place un pipeline ETL complet dÃ©ployÃ© via GitOps avec monitoring professionnel.\n\n\nArchitecture cible\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          MINI-PROJET                                    â”‚\nâ”‚                                                                         â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚\nâ”‚   â”‚   Git Repo   â”‚                                                     â”‚\nâ”‚   â”‚ (manifests)  â”‚                                                     â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                     â”‚\nâ”‚          â”‚                                                              â”‚\nâ”‚          â–¼                                                              â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚   ArgoCD     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚        Namespace: data-pipeline      â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                                      â”‚   â”‚\nâ”‚                            â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚\nâ”‚                            â”‚  â”‚  PostgreSQL â”‚  â”‚   CronJob   â”‚    â”‚   â”‚\nâ”‚                            â”‚  â”‚  (Helm)     â”‚  â”‚    ETL      â”‚    â”‚   â”‚\nâ”‚                            â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚\nâ”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                          â”‚                             â”‚\nâ”‚                                          â”‚ mÃ©triques                   â”‚\nâ”‚                                          â–¼                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚                  Namespace: monitoring                        â”‚    â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚\nâ”‚   â”‚   â”‚ Prometheus â”‚â”€â”€â–¶â”‚  Grafana   â”‚   â”‚  ServiceMonitor    â”‚   â”‚    â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ (dashboard)â”‚   â”‚  PrometheusRule    â”‚   â”‚    â”‚\nâ”‚   â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nInstructions\nÃ‰tape 1 : PrÃ©parer le repo Git\nStructure requise :\nk8s-data-platform/\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ apps/\nâ”‚   â”œâ”€â”€ etl-pipeline/           # Chart Helm custom\nâ”‚   â”‚   â”œâ”€â”€ Chart.yaml\nâ”‚   â”‚   â”œâ”€â”€ values.yaml\nâ”‚   â”‚   â””â”€â”€ templates/\nâ”‚   â”‚       â”œâ”€â”€ namespace.yaml\nâ”‚   â”‚       â”œâ”€â”€ configmap.yaml\nâ”‚   â”‚       â”œâ”€â”€ secret.yaml\nâ”‚   â”‚       â””â”€â”€ cronjob.yaml\nâ”‚   â””â”€â”€ monitoring/\nâ”‚       â”œâ”€â”€ servicemonitor.yaml\nâ”‚       â””â”€â”€ prometheusrule.yaml\nâ””â”€â”€ argocd/\n    â””â”€â”€ applications.yaml       # ApplicationSet\nÃ‰tape 2 : CrÃ©er le Chart ETL\n\nCronJob qui sâ€™exÃ©cute toutes les heures\nSe connecte Ã  PostgreSQL (dependency Helm)\nExpose des mÃ©triques sur /metrics\n\nÃ‰tape 3 : Configurer ArgoCD\n\nCrÃ©er lâ€™Application ArgoCD\nActiver le sync automatique\n\nÃ‰tape 4 : Monitoring\n\nServiceMonitor pour scraper les mÃ©triques ETL\nPrometheusRule avec alerte si job Ã©choue\nDashboard Grafana\n\nÃ‰tape 5 : Tester le workflow\n\nModifier le schedule dans values.yaml\nGit commit & push\nObserver ArgoCD synchroniser\nVÃ©rifier dans Grafana\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. apps/etl-pipeline/Chart.yaml\napiVersion: v2\nname: etl-pipeline\ndescription: Pipeline ETL avec monitoring\ntype: application\nversion: 1.0.0\nappVersion: \"1.0.0\"\ndependencies:\n  - name: postgresql\n    version: \"12.x.x\"\n    repository: \"https://charts.bitnami.com/bitnami\"\n2. apps/etl-pipeline/values.yaml\nnamespace: data-pipeline\n\netl:\n  image: python:3.11-slim\n  schedule: \"0 * * * *\"\n  resources:\n    requests:\n      cpu: \"100m\"\n      memory: \"256Mi\"\n    limits:\n      cpu: \"500m\"\n      memory: \"512Mi\"\n\ndatabase:\n  host: \"postgresql\"\n  port: \"5432\"\n  name: \"analytics\"\n  user: \"etl_user\"\n\npostgresql:\n  enabled: true\n  auth:\n    username: etl_user\n    password: etl_password\n    database: analytics\n  primary:\n    persistence:\n      size: 5Gi\n3. apps/etl-pipeline/templates/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: {{ .Values.namespace }}\n  labels:\n    app.kubernetes.io/name: {{ .Chart.Name }}\n4. apps/etl-pipeline/templates/configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\n  namespace: {{ .Values.namespace }}\ndata:\n  DB_HOST: {{ .Values.database.host | quote }}\n  DB_PORT: {{ .Values.database.port | quote }}\n  DB_NAME: {{ .Values.database.name | quote }}\n  DB_USER: {{ .Values.database.user | quote }}\n5. apps/etl-pipeline/templates/secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: etl-secret\n  namespace: {{ .Values.namespace }}\ntype: Opaque\nstringData:\n  DB_PASSWORD: {{ .Values.postgresql.auth.password | quote }}\n6. apps/etl-pipeline/templates/cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-job\n  namespace: {{ .Values.namespace }}\n  labels:\n    app: etl-pipeline\nspec:\n  schedule: {{ .Values.etl.schedule | quote }}\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 5\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: etl-job\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: etl\n            image: {{ .Values.etl.image }}\n            command: [\"python\", \"-c\"]\n            args:\n            - |\n              import os\n              import time\n              print(\"ğŸš€ Starting ETL job...\")\n              print(f\"DB_HOST: {os.environ.get('DB_HOST')}\")\n              print(f\"DB_NAME: {os.environ.get('DB_NAME')}\")\n              # Simulate ETL work\n              time.sleep(10)\n              print(\"âœ… ETL completed successfully!\")\n            envFrom:\n            - configMapRef:\n                name: etl-config\n            env:\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: etl-secret\n                  key: DB_PASSWORD\n            resources:\n              {{- toYaml .Values.etl.resources | nindent 14 }}\n7. apps/monitoring/servicemonitor.yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: etl-pipeline-monitor\n  namespace: monitoring\n  labels:\n    release: prometheus\nspec:\n  selector:\n    matchLabels:\n      app: etl-pipeline\n  namespaceSelector:\n    matchNames:\n    - data-pipeline\n  endpoints:\n  - port: metrics\n    interval: 30s\n8. apps/monitoring/prometheusrule.yaml\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: etl-alerts\n  namespace: monitoring\n  labels:\n    release: prometheus\nspec:\n  groups:\n  - name: etl-pipeline\n    rules:\n    - alert: ETLJobFailed\n      expr: kube_job_status_failed{namespace=\"data-pipeline\", job_name=~\"etl-.*\"} &gt; 0\n      for: 1m\n      labels:\n        severity: critical\n      annotations:\n        summary: \"ETL Job failed\"\n        description: \"Job {{ $labels.job_name }} has failed.\"\n    \n    - alert: ETLJobTooLong\n      expr: time() - kube_job_status_start_time{namespace=\"data-pipeline\", job_name=~\"etl-.*\"} &gt; 3600\n      for: 5m\n      labels:\n        severity: warning\n      annotations:\n        summary: \"ETL Job running too long\"\n        description: \"Job {{ $labels.job_name }} is running for more than 1 hour.\"\n9. argocd/applications.yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: etl-pipeline\n  namespace: argocd\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/YOUR_ORG/k8s-data-platform.git\n    targetRevision: main\n    path: apps/etl-pipeline\n    helm:\n      valueFiles:\n        - values.yaml\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: data-pipeline\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n---\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: etl-monitoring\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/YOUR_ORG/k8s-data-platform.git\n    targetRevision: main\n    path: apps/monitoring\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: monitoring\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n10. Commandes de dÃ©ploiement\n# 1. S'assurer qu'ArgoCD est installÃ©\nkubectl get pods -n argocd\n\n# 2. S'assurer que le monitoring stack est installÃ©\nkubectl get pods -n monitoring\n\n# 3. Appliquer les applications ArgoCD\nkubectl apply -f argocd/applications.yaml\n\n# 4. VÃ©rifier le sync\nargocd app list\nargocd app get etl-pipeline\n\n# 5. Tester manuellement le job\nkubectl create job --from=cronjob/etl-job test-etl -n data-pipeline\n\n# 6. Voir les logs\nkubectl logs -f job/test-etl -n data-pipeline\n\n# 7. AccÃ©der Ã  Grafana\nkubectl port-forward svc/prometheus-grafana -n monitoring 3000:80",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#ressources-pour-aller-plus-loin",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nKubernetes Docs - Scheduling â€” Scheduling avancÃ©\netcd Docs â€” Documentation etcd\nHelm Docs â€” Documentation Helm\nArgoCD Docs â€” Documentation ArgoCD\nPrometheus Docs â€” Documentation Prometheus\nGrafana Docs â€” Documentation Grafana\n\n\n\nğŸ“ Certifications\n\nCKA - Certified Kubernetes Administrator â€” Certification officielle CNCF\nCKAD - Certified Kubernetes Application Developer â€” Pour les dÃ©veloppeurs\n\n\n\nğŸ® Pratique\n\nKillercoda â€” Labs Kubernetes interactifs\nPlay with Kubernetes â€” Cluster K8s gratuit\nArtifact Hub â€” Catalogue de charts Helm\n\n\n\nğŸ“– Livres\n\nKubernetes Patterns â€” Bilgin Ibryam & Roland HuÃŸ\nGitOps and Kubernetes â€” Billy Yuen et al.\nPrometheus: Up & Running â€” Brian Brazil\n\n\n\nğŸ”§ Outils\n\nk9s â€” Terminal UI pour Kubernetes\nLens â€” IDE Kubernetes\nHelm Dashboard â€” UI pour Helm\nArgo CD Autopilot â€” Bootstrap GitOps",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/27_kubernetes_deep_dive.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/27_kubernetes_deep_dive.html#prochaine-Ã©tape",
    "title": "â˜¸ï¸ Kubernetes Deep Dive pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Kubernetes en profondeur, passons Ã  lâ€™orchestration avancÃ©e des pipelines data !\nğŸ‘‰ Module suivant : 28_advanced_orchestration â€” Orchestration avancÃ©e\nTu vas apprendre : - Airflow sur Kubernetes (KubernetesExecutor) - TaskFlow API - Comparatif Airflow vs Dagster vs Prefect - OpenLineage pour le data lineage\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Kubernetes Deep Dive pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "27 Â· Kubernetes Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "",
    "text": "Bienvenue dans ce module final oÃ¹ tu vas dÃ©velopper les soft skills et la pensÃ©e stratÃ©gique qui distinguent un Data Engineer Senior dâ€™un Staff/Principal Engineer. Ce nâ€™est plus seulement â€œcomment coderâ€ mais â€œcomment dÃ©cider, communiquer et influencerâ€.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#prÃ©requis",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#prÃ©requis",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nTous les modules techniques (M01-M34)\n\n\nâœ… Requis\nExpÃ©rience en Ã©quipe data\n\n\nğŸ’¡ RecommandÃ©\nAvoir gÃ©rÃ© des projets ou mentorÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#objectifs-du-module",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#objectifs-du-module",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nNaviguer les trade-offs techniques avec confiance\nCommuniquer efficacement avec diffÃ©rents stakeholders\nRÃ©diger une documentation technique de qualitÃ©\nEstimer des projets data rÃ©alistement\nGÃ©rer les incidents et post-mortems\nComprendre les career paths en Data Engineering",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#lart-des-trade-offs",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#lart-des-trade-offs",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "1. Lâ€™Art des Trade-offs",
    "text": "1. Lâ€™Art des Trade-offs\n\n1.1 Pourquoi les Trade-offs sont InÃ©vitables\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    LA RÃ‰ALITÃ‰ DE L'ENGINEERING                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   \"Il n'y a pas de solution parfaite, seulement des trade-offs.\"           â”‚\nâ”‚                                                                             â”‚\nâ”‚   Chaque dÃ©cision technique implique :                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Gagner quelque chose                                                    â”‚\nâ”‚   â€¢ Perdre quelque chose d'autre                                            â”‚\nâ”‚   â€¢ Accepter des risques                                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   Le rÃ´le du Senior/Staff Engineer :                                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   âŒ Trouver la solution parfaite (n'existe pas)                            â”‚\nâ”‚   âœ… Identifier les trade-offs                                              â”‚\nâ”‚   âœ… Choisir consciemment ce qu'on sacrifie                                 â”‚\nâ”‚   âœ… Communiquer ces choix clairement                                       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Les Trade-offs Fondamentaux\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    TRADE-OFF #1 : COST vs PERFORMANCE                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Performance                                                               â”‚\nâ”‚       â–²                                                                     â”‚\nâ”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\nâ”‚       â”‚        â•±                   â”‚                                        â”‚\nâ”‚       â”‚       â•±   Zone optimale    â”‚                                        â”‚\nâ”‚       â”‚      â•±    (sweet spot)     â”‚                                        â”‚\nâ”‚       â”‚     â•±                      â”‚                                        â”‚\nâ”‚       â”‚â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚\nâ”‚       â”‚   â•±                                                                 â”‚\nâ”‚       â”‚  â•±  Diminishing returns                                            â”‚\nâ”‚       â”‚ â•±   (plus de $ = peu de gain)                                      â”‚\nâ”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Cost                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   QUESTIONS Ã€ POSER :                                                       â”‚\nâ”‚   â€¢ Quel est le coÃ»t d'une seconde de latence en plus ?                    â”‚\nâ”‚   â€¢ Le business justifie-t-il 2x le budget pour 20% de perf ?              â”‚\nâ”‚   â€¢ Peut-on optimiser le code avant d'ajouter des resources ?              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.3 CAP Theorem : Consistency vs Availability\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CAP THEOREM                                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   Dans un systÃ¨me distribuÃ©, tu ne peux avoir que 2 sur 3 :                â”‚\nâ”‚                                                                             â”‚\nâ”‚                         Consistency                                         â”‚\nâ”‚                             â–²                                               â”‚\nâ”‚                            â•± â•²                                              â”‚\nâ”‚                           â•±   â•²                                             â”‚\nâ”‚                          â•±     â•²                                            â”‚\nâ”‚                         â•±  CA   â•²                                           â”‚\nâ”‚                        â•± (RDBMS) â•²                                          â”‚\nâ”‚                       â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²                                         â”‚\nâ”‚                      â•±             â•²                                        â”‚\nâ”‚                     â•±    CP    AP   â•²                                       â”‚\nâ”‚                    â•±  (HBase) (Cass) â•²                                      â”‚\nâ”‚                   â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼                                   â”‚\nâ”‚             Partition                 Availability                          â”‚\nâ”‚             Tolerance                                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   EN PRATIQUE (systÃ¨mes distribuÃ©s = P obligatoire) :                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   CP (Consistency + Partition Tolerance)                                   â”‚\nâ”‚   â€¢ DonnÃ©es toujours cohÃ©rentes                                            â”‚\nâ”‚   â€¢ Peut refuser des requÃªtes si partition                                 â”‚\nâ”‚   â€¢ Ex: HBase, MongoDB (strong consistency), Zookeeper                     â”‚\nâ”‚   â€¢ Use case: Transactions financiÃ¨res, inventory                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   AP (Availability + Partition Tolerance)                                  â”‚\nâ”‚   â€¢ Toujours disponible                                                    â”‚\nâ”‚   â€¢ DonnÃ©es peuvent Ãªtre temporairement incohÃ©rentes                       â”‚\nâ”‚   â€¢ Ex: Cassandra, DynamoDB, CouchDB                                       â”‚\nâ”‚   â€¢ Use case: Social feeds, analytics, caching                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.4 Latency vs Throughput\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    LATENCY vs THROUGHPUT                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   LATENCY = Temps pour traiter UNE requÃªte                                 â”‚\nâ”‚   THROUGHPUT = Nombre de requÃªtes par seconde                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   OPTIMISER LATENCY :              OPTIMISER THROUGHPUT :           â”‚  â”‚\nâ”‚   â”‚   â€¢ Moins de batch size            â€¢ Plus de batch size             â”‚  â”‚\nâ”‚   â”‚   â€¢ Plus de parallÃ©lisme           â€¢ Moins de parallÃ©lisme          â”‚  â”‚\nâ”‚   â”‚   â€¢ Caching agressif               â€¢ Traitement en bulk             â”‚  â”‚\nâ”‚   â”‚   â€¢ Moins de network hops          â€¢ Compression                    â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â”‚   Use case:                        Use case:                        â”‚  â”‚\nâ”‚   â”‚   API real-time, gaming            ETL batch, data migration        â”‚  â”‚\nâ”‚   â”‚                                                                     â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                                                             â”‚\nâ”‚   ğŸ’¡ EXEMPLE SPARK :                                                        â”‚\nâ”‚   â€¢ Streaming micro-batch 100ms = Low latency, lower throughput            â”‚\nâ”‚   â€¢ Batch job hourly = High latency, maximum throughput                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.5 Flexibility vs Simplicity\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    FLEXIBILITY vs SIMPLICITY                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   FLEXIBLE (ex: Spark sur K8s)          SIMPLE (ex: BigQuery)              â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Customisation totale               âœ… Rapide Ã  dÃ©marrer               â”‚\nâ”‚   âœ… Multi-cloud possible               âœ… Peu d'ops                        â”‚\nâ”‚   âœ… Pas de vendor lock-in              âœ… Ã‰quipe peut se focus sur data   â”‚\nâ”‚                                                                             â”‚\nâ”‚   âŒ ComplexitÃ© ops Ã©levÃ©e              âŒ Vendor lock-in                   â”‚\nâ”‚   âŒ Expertise requise                  âŒ Moins de contrÃ´le                â”‚\nâ”‚   âŒ Time-to-market plus long           âŒ CoÃ»ts peuvent exploser           â”‚\nâ”‚                                                                             â”‚\nâ”‚   QUAND CHOISIR QUOI :                                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   Startup, MVP, petite Ã©quipe     â†’     Simple (managed services)          â”‚\nâ”‚   Scale-up, Ã©quipe expÃ©rimentÃ©e   â†’     Flexible (mais progressif)         â”‚\nâ”‚   Enterprise, compliance stricte  â†’     DÃ©pend du contexte                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.6 Build vs Buy vs Open Source\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    BUILD vs BUY vs OPEN SOURCE                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\nâ”‚   â”‚     BUILD     â”‚      BUY      â”‚  OPEN SOURCE  â”‚                        â”‚\nâ”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚\nâ”‚   â”‚ Custom code   â”‚ Snowflake     â”‚ Spark         â”‚                        â”‚\nâ”‚   â”‚ Internal tool â”‚ Databricks    â”‚ Airflow       â”‚                        â”‚\nâ”‚   â”‚               â”‚ Fivetran      â”‚ Kafka         â”‚                        â”‚\nâ”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚\nâ”‚   â”‚ âœ… 100% fit   â”‚ âœ… Fast start â”‚ âœ… No license â”‚                        â”‚\nâ”‚   â”‚ âœ… Full controlâ”‚ âœ… Support   â”‚ âœ… Community  â”‚                        â”‚\nâ”‚   â”‚ âŒ Time/cost  â”‚ âŒ $$$        â”‚ âŒ Ops burden â”‚                        â”‚\nâ”‚   â”‚ âŒ Maintenanceâ”‚ âŒ Lock-in    â”‚ âŒ No SLA     â”‚                        â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   FRAMEWORK DE DÃ‰CISION :                                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. Est-ce un avantage compÃ©titif ?                                       â”‚\nâ”‚      OUI â†’ Consider BUILD                                                  â”‚\nâ”‚      NON â†’ BUY ou OPEN SOURCE                                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   2. Budget disponible ?                                                    â”‚\nâ”‚      Ã‰levÃ© + pas d'ops â†’ BUY                                               â”‚\nâ”‚      LimitÃ© + Ã©quipe ops â†’ OPEN SOURCE                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   3. Time-to-market critique ?                                             â”‚\nâ”‚      OUI â†’ BUY                                                             â”‚\nâ”‚      NON â†’ Ã‰valuer toutes les options                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# Outil : Trade-off Decision Matrix\n\ndef evaluate_tradeoffs(options: dict, criteria: dict) -&gt; None:\n    \"\"\"\n    Ã‰value des options selon des critÃ¨res avec trade-offs explicites.\n    \n    options: {\"Option A\": {\"cost\": 3, \"perf\": 5, ...}, ...}\n    criteria: {\"cost\": {\"weight\": 4, \"direction\": \"lower\"}, ...}\n    \"\"\"\n    print(\"â•\" * 70)\n    print(\"TRADE-OFF ANALYSIS\")\n    print(\"â•\" * 70)\n    \n    # Header\n    header = f\"{'Criterion':&lt;20} {'Weight':&lt;8}\"\n    for opt in options:\n        header += f\" {opt:&lt;12}\"\n    print(header)\n    print(\"-\" * 70)\n    \n    scores = {opt: 0 for opt in options}\n    \n    for crit, config in criteria.items():\n        weight = config['weight']\n        direction = config.get('direction', 'higher')  # higher is better by default\n        \n        row = f\"{crit:&lt;20} {weight:&lt;8}\"\n        for opt, values in options.items():\n            val = values.get(crit, 0)\n            # Adjust score based on direction\n            adjusted = val if direction == 'higher' else (6 - val)\n            scores[opt] += adjusted * weight\n            row += f\" {val:&lt;12}\"\n        print(row)\n    \n    print(\"-\" * 70)\n    \n    # Weighted scores\n    score_row = f\"{'WEIGHTED SCORE':&lt;20} {'':&lt;8}\"\n    for opt in options:\n        score_row += f\" {scores[opt]:&lt;12}\"\n    print(score_row)\n    \n    # Winner\n    winner = max(scores, key=scores.get)\n    print(f\"\\nâœ… Recommended: {winner} (score: {scores[winner]})\")\n    \n    # Trade-offs summary\n    print(f\"\\nâš ï¸  TRADE-OFFS TO ACCEPT:\")\n    for crit, config in criteria.items():\n        winner_val = options[winner].get(crit, 0)\n        best_val = max(options[opt].get(crit, 0) for opt in options)\n        if winner_val &lt; best_val and config.get('direction', 'higher') == 'higher':\n            best_opt = [opt for opt in options if options[opt].get(crit, 0) == best_val][0]\n            print(f\"   â€¢ {crit}: {winner} ({winner_val}) &lt; {best_opt} ({best_val})\")\n\n# Exemple : Choisir une solution de Data Warehouse\noptions = {\n    \"BigQuery\": {\"cost\": 2, \"performance\": 5, \"flexibility\": 2, \"ops_burden\": 5, \"learning_curve\": 4},\n    \"Snowflake\": {\"cost\": 2, \"performance\": 5, \"flexibility\": 3, \"ops_burden\": 5, \"learning_curve\": 4},\n    \"Spark+Delta\": {\"cost\": 4, \"performance\": 4, \"flexibility\": 5, \"ops_burden\": 2, \"learning_curve\": 2},\n    \"Redshift\": {\"cost\": 3, \"performance\": 4, \"flexibility\": 2, \"ops_burden\": 4, \"learning_curve\": 4},\n}\n\ncriteria = {\n    \"cost\": {\"weight\": 3, \"direction\": \"higher\"},  # higher = cheaper\n    \"performance\": {\"weight\": 4, \"direction\": \"higher\"},\n    \"flexibility\": {\"weight\": 2, \"direction\": \"higher\"},\n    \"ops_burden\": {\"weight\": 4, \"direction\": \"higher\"},  # higher = less burden\n    \"learning_curve\": {\"weight\": 2, \"direction\": \"higher\"},  # higher = easier\n}\n\nevaluate_tradeoffs(options, criteria)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#communication-avec-les-stakeholders",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#communication-avec-les-stakeholders",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "2. Communication avec les Stakeholders",
    "text": "2. Communication avec les Stakeholders\n\n2.1 Adapter son Message\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ADAPTER LE MESSAGE AU PUBLIC                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   EXECUTIVES (CEO, CFO, VP)                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚\nâ”‚   Ce qu'ils veulent :          Comment communiquer :                       â”‚\nâ”‚   â€¢ Impact business            â€¢ 1 slide max                               â”‚\nâ”‚   â€¢ ROI, coÃ»ts                 â€¢ Chiffres business (â‚¬, %, users)           â”‚\nâ”‚   â€¢ Timeline                   â€¢ Recommandation claire                     â”‚\nâ”‚   â€¢ Risques majeurs            â€¢ Pas de jargon technique                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   PRODUCT MANAGERS                                                          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚\nâ”‚   Ce qu'ils veulent :          Comment communiquer :                       â”‚\nâ”‚   â€¢ Feasibility                â€¢ Options avec trade-offs                   â”‚\nâ”‚   â€¢ Timeline rÃ©aliste          â€¢ Ce qui est possible vs impossible         â”‚\nâ”‚   â€¢ DÃ©pendances                â€¢ User impact                               â”‚\nâ”‚   â€¢ FlexibilitÃ©                â€¢ Alternatives si deadline serrÃ©e          â”‚\nâ”‚                                                                             â”‚\nâ”‚   AUTRES ENGINEERS                                                          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚\nâ”‚   Ce qu'ils veulent :          Comment communiquer :                       â”‚\nâ”‚   â€¢ DÃ©tails techniques         â€¢ Architecture diagrams                     â”‚\nâ”‚   â€¢ Pourquoi ce choix          â€¢ Code examples                             â”‚\nâ”‚   â€¢ Comment Ã§a marche          â€¢ Documentation dÃ©taillÃ©e                   â”‚\nâ”‚   â€¢ Impact sur leur travail    â€¢ ADRs                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 Le Framework â€œBottom Line Up Frontâ€ (BLUF)\n\n\nVoir le code\n# Exemple de communication BLUF\n\nbad_communication = \"\"\"\nâŒ MAUVAIS EXEMPLE (enterrer le lead) :\n\n\"Alors on a analysÃ© plusieurs options pour le data warehouse. \nOn a regardÃ© Snowflake qui a des bonnes performances mais coÃ»te cher.\nOn a aussi regardÃ© BigQuery qui est bien intÃ©grÃ© avec GCP.\nEt puis il y a Redshift mais on n'est pas sur AWS.\nAprÃ¨s avoir comparÃ© les coÃ»ts, les performances, la facilitÃ© d'utilisation...\n[10 minutes plus tard]\n...donc on recommande Snowflake.\"\n\nProblÃ¨me : Le destinataire ne sait pas oÃ¹ on va pendant 10 minutes.\n\"\"\"\n\ngood_communication = \"\"\"\nâœ… BON EXEMPLE (BLUF) :\n\n\"Je recommande Snowflake pour notre data warehouse. Budget : $50K/an.\n\nPourquoi Snowflake :\nâ€¢ Meilleur rapport coÃ»t/performance pour notre volume (10TB)\nâ€¢ Ã‰quipe dÃ©jÃ  formÃ©e sur SQL, courbe d'apprentissage minimale\nâ€¢ Auto-scaling = pas de gestion d'infrastructure\n\nAlternatives considÃ©rÃ©es :\nâ€¢ BigQuery : similaire mais on n'est pas sur GCP\nâ€¢ Spark+Delta : moins cher mais 3 mois de setup, besoin d'ops\n\nTrade-off acceptÃ© : vendor lock-in Snowflake\n\nNext step : Validation budget avec Finance cette semaine.\"\n\nPourquoi c'est mieux :\nâ€¢ Conclusion en premier\nâ€¢ Chiffres concrets\nâ€¢ Alternatives mentionnÃ©es\nâ€¢ Trade-off explicite\nâ€¢ Action claire\n\"\"\"\n\nprint(bad_communication)\nprint(\"=\" * 70)\nprint(good_communication)\n\n\n\n\n2.3 Lâ€™Art de Dire Non (Constructivement)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DIRE NON (SANS DIRE NON)                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   âŒ \"Non, c'est pas possible.\"                                            â”‚\nâ”‚   âŒ \"Ã‡a va prendre trop de temps.\"                                        â”‚\nâ”‚   âŒ \"Le systÃ¨me ne peut pas faire Ã§a.\"                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… \"Oui, ET voici ce que Ã§a implique...\"                                 â”‚\nâ”‚   âœ… \"Oui, MAIS il faudra choisir entre X et Y.\"                           â”‚\nâ”‚   âœ… \"Voici 3 options avec leurs trade-offs...\"                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚\nâ”‚                                                                             â”‚\nâ”‚   EXEMPLE :                                                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   PM: \"On a besoin du dashboard real-time pour lundi.\"                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   âŒ \"Impossible, on n'a pas le temps.\"                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… \"Pour lundi, je vois 3 options :                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚      Option A : Dashboard real-time complet                                â”‚\nâ”‚      â†’ 3 semaines, besoin de 2 engineers                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚      Option B : Dashboard avec refresh toutes les 5 minutes                â”‚\nâ”‚      â†’ Possible pour lundi, 90% des besoins couverts                       â”‚\nâ”‚                                                                             â”‚\nâ”‚      Option C : MÃ©triques clÃ©s seulement, reste en V2                      â”‚\nâ”‚      â†’ Lundi OK, on itÃ¨re ensuite                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚      Je recommande Option B. Qu'en penses-tu ?\"                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.4 GÃ©rer les Conflits Techniques\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RÃ‰SOUDRE LES DÃ‰SACCORDS TECHNIQUES                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Ã‰TAPE 1 : Comprendre l'autre position                                    â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚\nâ”‚   â€¢ \"Aide-moi Ã  comprendre pourquoi tu prÃ©fÃ¨res X ?\"                       â”‚\nâ”‚   â€¢ \"Quelles sont tes concerns avec mon approche ?\"                        â”‚\nâ”‚   â€¢ Ã‰couter vraiment, pas juste attendre son tour                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   Ã‰TAPE 2 : Trouver le terrain commun                                      â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚\nâ”‚   â€¢ \"On est d'accord que l'objectif est Y, correct ?\"                      â”‚\nâ”‚   â€¢ \"On veut tous les deux Ã©viter Z.\"                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   Ã‰TAPE 3 : Proposer un test                                               â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\nâ”‚   â€¢ \"Et si on faisait un PoC de 2 jours pour comparer ?\"                   â”‚\nâ”‚   â€¢ \"DÃ©finissons les critÃ¨res de succÃ¨s ensemble.\"                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   Ã‰TAPE 4 : Disagree and commit                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚\nâ”‚   â€¢ Si pas de consensus : escalader ou time-box la dÃ©cision                â”‚\nâ”‚   â€¢ Une fois dÃ©cidÃ© : supporter la dÃ©cision Ã  100%                         â”‚\nâ”‚   â€¢ \"Je n'Ã©tais pas d'accord mais je m'engage Ã  faire rÃ©ussir.\"            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#documentation-technique",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#documentation-technique",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "3. Documentation Technique",
    "text": "3. Documentation Technique\n\n3.1 Types de Documentation\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PYRAMID OF DOCUMENTATION                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\nâ”‚                         â”‚  Vision   â”‚  â† Pourquoi on fait Ã§a               â”‚\nâ”‚                         â”‚  (ADRs)   â”‚    (rarement mis Ã  jour)             â”‚\nâ”‚                        â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€                                     â”‚\nâ”‚                       â•±               â•²                                    â”‚\nâ”‚                      â•±   Architecture  â•²  â† Comment c'est structurÃ©        â”‚\nâ”‚                     â•±    (Diagrams)     â•²   (mis Ã  jour trimestriel)       â”‚\nâ”‚                    â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€                                 â”‚\nâ”‚                   â•±                       â•²                                â”‚\nâ”‚                  â•±     How-to Guides       â•²  â† Comment faire X            â”‚\nâ”‚                 â•±      (Runbooks)           â•²   (mis Ã  jour rÃ©gulier)      â”‚\nâ”‚                â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€                             â”‚\nâ”‚               â•±                               â•²                            â”‚\nâ”‚              â•±         API / Code Docs         â•²  â† DÃ©tails techniques    â”‚\nâ”‚             â•±          (Docstrings)             â•²   (auto-gÃ©nÃ©rÃ© si poss.) â”‚\nâ”‚            â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€                         â”‚\nâ”‚                                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 Template README Projet Data\n\n\nVoir le code\n# Template README pour projet Data Engineering\n\nreadme_template = \"\"\"\n# ğŸ“Š [Nom du Projet]\n\n&gt; [Une phrase qui explique ce que fait le projet]\n\n## ğŸ¯ Overview\n\n[2-3 phrases sur le problÃ¨me rÃ©solu et la solution]\n\n### Key Features\n- âœ… [Feature 1]\n- âœ… [Feature 2]\n- âœ… [Feature 3]\n\n## ğŸ—ï¸ Architecture\n\n```\n[Diagram ASCII simple ou lien vers image]\n\nSource â†’ Kafka â†’ Spark â†’ Delta Lake â†’ Dashboard\n```\n\n**Stack technique :**\n- Ingestion : [Kafka, Fivetran, etc.]\n- Processing : [Spark, dbt, etc.]\n- Storage : [S3 + Delta Lake, Snowflake, etc.]\n- Orchestration : [Airflow, Dagster, etc.]\n\n## ğŸš€ Quick Start\n\n### Prerequisites\n- Python 3.9+\n- Docker\n- [Autres dÃ©pendances]\n\n### Installation\n```bash\ngit clone [repo]\ncd [project]\nmake setup  # ou pip install -r requirements.txt\n```\n\n### Run locally\n```bash\nmake run  # ou docker-compose up\n```\n\n## ğŸ“ Project Structure\n\n```\nâ”œâ”€â”€ src/                 # Code source\nâ”‚   â”œâ”€â”€ ingestion/       # Jobs d'ingestion\nâ”‚   â”œâ”€â”€ transformations/ # Transformations Spark/dbt\nâ”‚   â””â”€â”€ serving/         # APIs, exports\nâ”œâ”€â”€ dags/                # Airflow DAGs\nâ”œâ”€â”€ tests/               # Tests\nâ”œâ”€â”€ docs/                # Documentation\nâ”‚   â””â”€â”€ architecture/    # ADRs\nâ”œâ”€â”€ docker-compose.yaml\nâ””â”€â”€ Makefile\n```\n\n## ğŸ”§ Configuration\n\n| Variable | Description | Default |\n|----------|-------------|----------|\n| `DATABASE_URL` | Connection string | `localhost` |\n| `KAFKA_BROKERS` | Kafka brokers | `localhost:9092` |\n\n## ğŸ“Š Data Model\n\n[Lien vers documentation du data model ou schÃ©ma simplifiÃ©]\n\n## ğŸ§ª Testing\n\n```bash\nmake test        # Unit tests\nmake test-e2e    # End-to-end tests\n```\n\n## ğŸš¢ Deployment\n\n```bash\nmake deploy-staging\nmake deploy-prod\n```\n\n## ğŸ“ˆ Monitoring\n\n- Dashboards : [lien Grafana]\n- Alerts : [lien PagerDuty/Slack]\n- Logs : [lien Datadog/CloudWatch]\n\n## ğŸ†˜ Troubleshooting\n\n### Common issues\n\n**Problem:** [Description]\n```\nSolution: [Commande ou explication]\n```\n\n## ğŸ‘¥ Team\n\n- Owner : [Ã‰quipe/Personne]\n- Slack : #[channel]\n- On-call : [rotation]\n\n## ğŸ“š Related Documentation\n\n- [ADR-001: Choix de Kafka](docs/architecture/decisions/001-kafka.md)\n- [Runbook: Incident Pipeline](docs/runbooks/pipeline-incident.md)\n\"\"\"\n\nprint(readme_template)\n\n\n\n\n3.3 Template Runbook\n\n\nVoir le code\n# Template Runbook pour incident/opÃ©ration\n\nrunbook_template = \"\"\"\n# ğŸ”§ Runbook: [Nom de l'incident/opÃ©ration]\n\n## ğŸ“‹ Overview\n\n| PropriÃ©tÃ© | Valeur |\n|-----------|--------|\n| Severity | P1 / P2 / P3 |\n| Time to resolve | ~XX minutes |\n| Last updated | YYYY-MM-DD |\n| Owner | @team-data |\n\n## ğŸš¨ SymptÃ´mes\n\nComment identifier ce problÃ¨me :\n- [ ] Alerte \"[Nom de l'alerte]\" dÃ©clenchÃ©e\n- [ ] Dashboard montre [symptÃ´me visible]\n- [ ] Users reportent [comportement]\n\n## ğŸ” Diagnostic\n\n### Ã‰tape 1 : VÃ©rifier [X]\n```bash\n[Commande de diagnostic]\n```\n\n**Si output contient `ERROR`** â†’ Aller Ã  \"RÃ©solution A\"\n**Si output normal** â†’ Continuer Ã  Ã‰tape 2\n\n### Ã‰tape 2 : VÃ©rifier [Y]\n```bash\n[Commande de diagnostic]\n```\n\n## ğŸ› ï¸ RÃ©solution\n\n### RÃ©solution A : [Cas 1]\n\n1. [Action 1]\n```bash\n[Commande]\n```\n\n2. [Action 2]\n```bash\n[Commande]\n```\n\n3. VÃ©rifier que c'est rÃ©solu :\n```bash\n[Commande de vÃ©rification]\n```\n\n### RÃ©solution B : [Cas 2]\n\n[...]\n\n## âš ï¸ Rollback (si nÃ©cessaire)\n\n```bash\n[Commandes de rollback]\n```\n\n## ğŸ“ Escalation\n\nSi non rÃ©solu aprÃ¨s 30 minutes :\n1. Ping @senior-engineer sur Slack #data-incidents\n2. Si P1 : Appeler on-call via PagerDuty\n\n## ğŸ“ Post-Incident\n\n- [ ] Mettre Ã  jour le ticket avec la rÃ©solution\n- [ ] Si nouveau cas : mettre Ã  jour ce runbook\n- [ ] Si P1/P2 : scheduler post-mortem\n\n## ğŸ“š RÃ©fÃ©rences\n\n- [Lien dashboard monitoring]\n- [Lien logs]\n- [Doc architecture systÃ¨me]\n\"\"\"\n\nprint(runbook_template)\n\n\n\n\n3.4 Diagrammes dâ€™Architecture (C4 Model)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    C4 MODEL : 4 NIVEAUX DE ZOOM                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   LEVEL 1 : CONTEXT                                                         â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚\nâ”‚   â€¢ Vue trÃ¨s haut niveau                                                   â”‚\nâ”‚   â€¢ SystÃ¨me + users + systÃ¨mes externes                                    â”‚\nâ”‚   â€¢ Pour : Executives, PM, nouveaux membres                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚   â”‚  Users  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Data Platformâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ BI Toolsâ”‚                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\nâ”‚                              â”‚                                              â”‚\nâ”‚                              â–¼                                              â”‚\nâ”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚\nâ”‚                       â”‚ Source DBs  â”‚                                      â”‚\nâ”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   LEVEL 2 : CONTAINER                                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\nâ”‚   â€¢ Zoom sur le systÃ¨me                                                    â”‚\nâ”‚   â€¢ Applications, databases, file systems                                  â”‚\nâ”‚   â€¢ Pour : Tech leads, architects                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚                      DATA PLATFORM                                 â”‚   â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚   â”‚\nâ”‚   â”‚   â”‚ Airflow â”‚   â”‚  Spark  â”‚   â”‚  Delta  â”‚   â”‚  dbt    â”‚          â”‚   â”‚\nâ”‚   â”‚   â”‚  (Orch) â”‚â”€â”€â–¶â”‚(Process)â”‚â”€â”€â–¶â”‚  Lake   â”‚â”€â”€â–¶â”‚ (Trans) â”‚          â”‚   â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚                                                                             â”‚\nâ”‚   LEVEL 3 : COMPONENT                                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\nâ”‚   â€¢ Zoom sur un container                                                  â”‚\nâ”‚   â€¢ Composants internes                                                    â”‚\nâ”‚   â€¢ Pour : Developers                                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   LEVEL 4 : CODE                                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ Classes, fonctions                                                     â”‚\nâ”‚   â€¢ Rarement nÃ©cessaire (le code est la doc)                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#estimation-de-projets-data",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#estimation-de-projets-data",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "4. Estimation de Projets Data",
    "text": "4. Estimation de Projets Data\n\n4.1 Pourquoi les Estimations sont Difficiles\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    LE CONE D'INCERTITUDE                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   Incertitude                                                               â”‚\nâ”‚       â–²                                                                     â”‚\nâ”‚   4x  â”‚â•²                                                                   â”‚\nâ”‚       â”‚ â•²                                                                  â”‚\nâ”‚   2x  â”‚  â•²                                                                 â”‚\nâ”‚       â”‚   â•²                                                                â”‚\nâ”‚   1x  â”‚â”€â”€â”€â”€â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚\nâ”‚       â”‚     â•²                                                              â”‚\nâ”‚  0.5x â”‚      â•²                                                             â”‚\nâ”‚       â”‚       â•²__________________________________________                  â”‚\nâ”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Temps           â”‚\nâ”‚        IdÃ©e    Design   Build    Test    Deploy                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   Au dÃ©but : estimation peut Ãªtre 4x trop haute ou trop basse              â”‚\nâ”‚   Plus on avance : plus l'estimation devient prÃ©cise                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   ğŸ’¡ CONSÃ‰QUENCE : Donner des RANGES, pas des chiffres prÃ©cis              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n4.2 Techniques dâ€™Estimation\n\n\nVoir le code\n# Techniques d'estimation\n\nestimation_techniques = \"\"\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# TECHNIQUES D'ESTIMATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n## 1. THREE-POINT ESTIMATION (PERT)\n\nFormule : (Optimiste + 4Ã—Probable + Pessimiste) / 6\n\nExemple : Construire un pipeline ETL\n- Optimiste (tout va bien) : 3 jours\n- Probable (cas normal) : 5 jours  \n- Pessimiste (problÃ¨mes) : 12 jours\n\nEstimation = (3 + 4Ã—5 + 12) / 6 = 5.8 jours\nCommuniquer : \"5-6 jours, jusqu'Ã  12 si complications\"\n\n## 2. T-SHIRT SIZING\n\n| Taille | Effort | Exemple Data Engineering |\n|--------|--------|-------------------------|\n| XS | &lt; 1 jour | Fix config, add column |\n| S | 1-3 jours | Nouveau job simple |\n| M | 1-2 semaines | Pipeline complet |\n| L | 2-4 semaines | Nouveau data source |\n| XL | 1-2 mois | Nouvelle architecture |\n\n## 3. STORY POINTS (Relative)\n\nComparer Ã  une tÃ¢che de rÃ©fÃ©rence :\n- RÃ©fÃ©rence (8 points) : Pipeline Kafka â†’ Delta existant\n- Nouvelle tÃ¢che : \"2x plus complexe\" â†’ 16 points\n\n## 4. DÃ‰COUPAGE EN PHASES\n\nGrande estimation â†’ Somme de petites estimations\n\nProjet \"Real-time Dashboard\" :\nâ”œâ”€â”€ Phase 1: Setup Kafka (M) : 1-2 semaines\nâ”œâ”€â”€ Phase 2: Pipeline Spark (M) : 1-2 semaines\nâ”œâ”€â”€ Phase 3: ClickHouse (S) : 3-5 jours\nâ”œâ”€â”€ Phase 4: Grafana (S) : 2-3 jours\nâ”œâ”€â”€ Phase 5: Tests & Polish (M) : 1 semaine\nâ””â”€â”€ Total : 5-8 semaines\n\n## 5. RÃˆGLES D'OR\n\nâ€¢ Multiplier par 2 si technologie nouvelle\nâ€¢ Multiplier par 1.5 si intÃ©gration avec systÃ¨me legacy\nâ€¢ Ajouter 20% pour tests et documentation\nâ€¢ Ajouter 20% pour imprÃ©vus (meetings, bugs, etc.)\n\"\"\"\n\nprint(estimation_techniques)\n\n\n\n\nVoir le code\n# Calculateur d'estimation PERT\n\ndef estimate_pert(optimistic: float, most_likely: float, pessimistic: float) -&gt; dict:\n    \"\"\"\n    Calcule une estimation PERT avec Ã©cart-type.\n    \"\"\"\n    # PERT estimate\n    expected = (optimistic + 4 * most_likely + pessimistic) / 6\n    \n    # Standard deviation\n    std_dev = (pessimistic - optimistic) / 6\n    \n    # Confidence intervals\n    ci_68 = (expected - std_dev, expected + std_dev)  # 68% confidence\n    ci_95 = (expected - 2*std_dev, expected + 2*std_dev)  # 95% confidence\n    \n    return {\n        \"expected\": round(expected, 1),\n        \"std_dev\": round(std_dev, 1),\n        \"ci_68\": (round(ci_68[0], 1), round(ci_68[1], 1)),\n        \"ci_95\": (round(ci_95[0], 1), round(ci_95[1], 1)),\n    }\n\ndef estimate_project(tasks: list) -&gt; None:\n    \"\"\"\n    Estime un projet composÃ© de plusieurs tÃ¢ches.\n    \n    tasks: [(\"name\", optimistic, likely, pessimistic), ...]\n    \"\"\"\n    print(\"â•\" * 70)\n    print(\"PROJECT ESTIMATION\")\n    print(\"â•\" * 70)\n    print(f\"{'Task':&lt;30} {'O':&gt;6} {'M':&gt;6} {'P':&gt;6} {'Est':&gt;8} {'Range':&gt;12}\")\n    print(\"-\" * 70)\n    \n    total_expected = 0\n    total_variance = 0\n    \n    for name, o, m, p in tasks:\n        result = estimate_pert(o, m, p)\n        total_expected += result['expected']\n        total_variance += result['std_dev'] ** 2\n        \n        range_str = f\"{result['ci_68'][0]}-{result['ci_68'][1]}\"\n        print(f\"{name:&lt;30} {o:&gt;6} {m:&gt;6} {p:&gt;6} {result['expected']:&gt;8} {range_str:&gt;12}\")\n    \n    total_std = total_variance ** 0.5\n    \n    print(\"-\" * 70)\n    print(f\"{'TOTAL':&lt;30} {'':&lt;20} {total_expected:&gt;8} {total_expected-total_std:.1f}-{total_expected+total_std:.1f}\")\n    print(f\"\\nğŸ“Š Summary:\")\n    print(f\"   Expected: {total_expected:.1f} days\")\n    print(f\"   68% confidence: {total_expected-total_std:.1f} - {total_expected+total_std:.1f} days\")\n    print(f\"   95% confidence: {total_expected-2*total_std:.1f} - {total_expected+2*total_std:.1f} days\")\n    print(f\"\\nğŸ’¬ Communicate: \\\"{int(total_expected-total_std)}-{int(total_expected+total_std)} days, likely around {int(total_expected)} days\\\"\")\n\n# Exemple : Projet Real-Time Analytics\ntasks = [\n    (\"Setup Kafka cluster\", 3, 5, 10),\n    (\"Spark Streaming pipeline\", 5, 8, 15),\n    (\"ClickHouse setup + schema\", 2, 4, 7),\n    (\"Kafka â†’ ClickHouse ingestion\", 2, 3, 6),\n    (\"Grafana dashboards\", 2, 3, 5),\n    (\"Testing & documentation\", 3, 5, 8),\n]\n\nestimate_project(tasks)\n\n\n\n\n4.3 Communiquer les Estimations\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    COMMUNIQUER LES ESTIMATIONS                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   âŒ Ã€ Ã‰VITER                                                               â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ \"Ã‡a prendra 2 semaines.\" (trop prÃ©cis)                                 â”‚\nâ”‚   â€¢ \"Je sais pas, c'est compliquÃ©.\" (pas utile)                            â”‚\nâ”‚   â€¢ \"ASAP\" comme rÃ©ponse Ã  \"Quand ?\" (pas une estimation)                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… BONNES PRATIQUES                                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\nâ”‚   â€¢ Donner une RANGE : \"2-4 semaines\"                                      â”‚\nâ”‚   â€¢ PrÃ©ciser les HYPOTHÃˆSES : \"Si l'API est stable...\"                     â”‚\nâ”‚   â€¢ Identifier les RISQUES : \"Peut aller Ã  6 semaines si...\"               â”‚\nâ”‚   â€¢ Proposer des MILESTONES : \"V1 en 2 semaines, V2 en 4\"                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   TEMPLATE :                                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚\nâ”‚   \"Je estime [X-Y semaines], plus probablement autour de [Z].              â”‚\nâ”‚    Ceci suppose que [hypothÃ¨se 1] et [hypothÃ¨se 2].                        â”‚\nâ”‚    Risque principal : [risque] qui pourrait ajouter [temps].               â”‚\nâ”‚    Je propose un checkpoint Ã  [date] pour revalider l'estimation.\"         â”‚\nâ”‚                                                                             â”‚\nâ”‚   GÃ‰RER LE \"C'EST TROP LONG\"                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\nâ”‚   1. \"Qu'est-ce qui est vraiment nÃ©cessaire pour [date] ?\"                 â”‚\nâ”‚   2. \"Voici ce qu'on peut livrer en [temps disponible]...\"                 â”‚\nâ”‚   3. \"Pour aller plus vite, il faudrait [plus de resources/moins de scope]\"â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#incident-management",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#incident-management",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "5. Incident Management",
    "text": "5. Incident Management\n\n5.1 Niveaux de SÃ©vÃ©ritÃ©\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    SEVERITY LEVELS                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   P1 - CRITICAL                                                             â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ Service complÃ¨tement down                                              â”‚\nâ”‚   â€¢ Perte de donnÃ©es                                                        â”‚\nâ”‚   â€¢ Impact financier majeur                                                â”‚\nâ”‚   â†’ Response: ImmÃ©diat, all-hands, war room                                â”‚\nâ”‚   â†’ SLA: Acknowledge &lt; 15 min, Resolve &lt; 4h                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   P2 - HIGH                                                                 â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚\nâ”‚   â€¢ Service dÃ©gradÃ© significativement                                      â”‚\nâ”‚   â€¢ Feature majeure broken                                                 â”‚\nâ”‚   â€¢ Workaround existe mais pÃ©nible                                         â”‚\nâ”‚   â†’ Response: Urgent, pendant heures ouvrÃ©es                               â”‚\nâ”‚   â†’ SLA: Acknowledge &lt; 1h, Resolve &lt; 24h                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   P3 - MEDIUM                                                               â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚\nâ”‚   â€¢ Feature mineure broken                                                 â”‚\nâ”‚   â€¢ Performance dÃ©gradÃ©e                                                   â”‚\nâ”‚   â€¢ Workaround facile existe                                               â”‚\nâ”‚   â†’ Response: Next business day                                            â”‚\nâ”‚   â†’ SLA: Resolve &lt; 1 week                                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   P4 - LOW                                                                  â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                 â”‚\nâ”‚   â€¢ CosmÃ©tique, nice-to-fix                                                â”‚\nâ”‚   â€¢ Pas d'impact utilisateur                                               â”‚\nâ”‚   â†’ Response: Backlog, quand on a le temps                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.2 Process de Gestion dâ€™Incident\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    INCIDENT LIFECYCLE                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. DETECT              2. TRIAGE              3. RESPOND                  â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚\nâ”‚   â€¢ Alert triggered      â€¢ Assign severity      â€¢ Incident Commander       â”‚\nâ”‚   â€¢ User report          â€¢ Identify owner       â€¢ Communication lead       â”‚\nâ”‚   â€¢ Monitoring           â€¢ Create ticket        â€¢ Technical responders     â”‚\nâ”‚                                                                             â”‚\nâ”‚   4. RESOLVE             5. COMMUNICATE         6. POST-MORTEM             â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚\nâ”‚   â€¢ Fix applied          â€¢ Status updates       â€¢ Blameless review         â”‚\nâ”‚   â€¢ Verified             â€¢ Stakeholder comms    â€¢ Timeline                 â”‚\nâ”‚   â€¢ Monitoring OK        â€¢ User notification    â€¢ Action items             â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃ”LES CLÃ‰S :                                                              â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ Incident Commander (IC) : Coordonne, prend les dÃ©cisions               â”‚\nâ”‚   â€¢ Technical Lead : RÃ©sout le problÃ¨me technique                          â”‚\nâ”‚   â€¢ Communication Lead : Updates stakeholders, status page                 â”‚\nâ”‚   â€¢ Scribe : Documente la timeline pour post-mortem                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.3 Post-Mortem Template\n\n\nVoir le code\n# Template Post-Mortem (Blameless)\n\npostmortem_template = \"\"\"\n# ğŸ“‹ Post-Mortem: [Titre de l'incident]\n\n**Date de l'incident:** YYYY-MM-DD\n**DurÃ©e:** X heures Y minutes\n**Severity:** P1/P2/P3\n**Auteur:** @name\n**Status:** Draft / Review / Final\n\n---\n\n## ğŸ“Š Summary\n\n[2-3 phrases : quoi, impact, durÃ©e]\n\n## ğŸ“ˆ Impact\n\n| MÃ©trique | Valeur |\n|----------|--------|\n| Users impactÃ©s | X |\n| DurÃ©e downtime | X min |\n| DonnÃ©es perdues | X rows |\n| Revenue impact | $X |\n\n## ğŸ• Timeline (UTC)\n\n| Time | Event |\n|------|-------|\n| 14:00 | DÃ©ploiement de la version X.Y.Z |\n| 14:15 | PremiÃ¨re alerte : latency spike |\n| 14:20 | IC assignÃ© : @name |\n| 14:25 | Root cause identifiÃ© |\n| 14:35 | Rollback initiÃ© |\n| 14:45 | Service restaurÃ© |\n| 15:00 | Monitoring confirmÃ© stable |\n\n## ğŸ” Root Cause\n\n[Explication technique dÃ©taillÃ©e de la cause]\n\n## ğŸ› ï¸ Resolution\n\n[Comment le problÃ¨me a Ã©tÃ© rÃ©solu]\n\n## âœ… What Went Well\n\n- DÃ©tection rapide grÃ¢ce Ã  [monitoring]\n- Communication efficace entre Ã©quipes\n- Rollback process a fonctionnÃ©\n\n## âŒ What Went Wrong\n\n- [Point 1]\n- [Point 2]\n\n## ğŸ¯ Action Items\n\n| Action | Owner | Priority | Due Date | Status |\n|--------|-------|----------|----------|--------|\n| Ajouter test pour ce cas | @dev1 | P1 | YYYY-MM-DD | TODO |\n| AmÃ©liorer alerte | @dev2 | P2 | YYYY-MM-DD | TODO |\n| Documenter dans runbook | @dev3 | P3 | YYYY-MM-DD | TODO |\n\n## ğŸ“š Lessons Learned\n\n1. [LeÃ§on 1]\n2. [LeÃ§on 2]\n\n---\n\n## âš ï¸ REMINDER: Blameless Culture\n\nCe post-mortem vise Ã  amÃ©liorer nos systÃ¨mes, pas Ã  blÃ¢mer des individus.\nLes erreurs humaines sont des symptÃ´mes de problÃ¨mes systÃ©miques.\n\"\"\"\n\nprint(postmortem_template)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#career-growth-en-data-engineering",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#career-growth-en-data-engineering",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "6. Career Growth en Data Engineering",
    "text": "6. Career Growth en Data Engineering\n\n6.1 Career Ladder\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DATA ENGINEERING CAREER LADDER                           â”‚\nâ”‚                                                                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   MANAGEMENT TRACK              â”‚              IC TRACK                     â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚              â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚\nâ”‚                                 â”‚                                           â”‚\nâ”‚   VP of Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Distinguished Eng     â”‚\nâ”‚        â”‚                        â”‚                            â”‚              â”‚\nâ”‚   Director of DE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Principal Engineer    â”‚\nâ”‚        â”‚                        â”‚                            â”‚              â”‚\nâ”‚   Engineering Manager â”€â”€â”€â”€â”€â”€â”€   â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Staff Engineer       â”‚\nâ”‚        â”‚                        â”‚                            â”‚              â”‚\nâ”‚   Tech Lead â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Senior Engineer     â”‚\nâ”‚                                 â”‚                            â”‚              â”‚\nâ”‚                                 â”‚                      Mid Engineer         â”‚\nâ”‚                                 â”‚                            â”‚              â”‚\nâ”‚                                 â”‚                     Junior Engineer       â”‚\nâ”‚                                 â”‚                                           â”‚\nâ”‚   Focus: People, Process        â”‚        Focus: Technical Excellence       â”‚\nâ”‚   Leverage: Through others      â”‚        Leverage: Through expertise       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.2 CompÃ©tences par Niveau\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    COMPÃ‰TENCES PAR NIVEAU                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   JUNIOR (0-2 ans)                                                          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚\nâ”‚   â€¢ ExÃ©cute des tÃ¢ches bien dÃ©finies                                       â”‚\nâ”‚   â€¢ Apprend les outils et patterns                                         â”‚\nâ”‚   â€¢ Demande de l'aide quand bloquÃ©                                         â”‚\nâ”‚   â€¢ Code review : reÃ§oit feedback                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   MID (2-5 ans)                                                             â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ Autonome sur des features complÃ¨tes                                    â”‚\nâ”‚   â€¢ Participe au design                                                    â”‚\nâ”‚   â€¢ Commence Ã  mentorer les juniors                                        â”‚\nâ”‚   â€¢ Code review : donne et reÃ§oit feedback                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   SENIOR (5-8 ans)                                                          â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚\nâ”‚   â€¢ Lead le design de systÃ¨mes                                             â”‚\nâ”‚   â€¢ RÃ©sout les problÃ¨mes ambigus                                           â”‚\nâ”‚   â€¢ Mentore activement                                                     â”‚\nâ”‚   â€¢ Influence au niveau Ã©quipe                                             â”‚\nâ”‚   â€¢ Communique avec stakeholders non-tech                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   STAFF (8+ ans)                                                            â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚\nâ”‚   â€¢ DÃ©finit la direction technique                                         â”‚\nâ”‚   â€¢ RÃ©sout les problÃ¨mes cross-Ã©quipes                                     â”‚\nâ”‚   â€¢ Influence au niveau organisation                                       â”‚\nâ”‚   â€¢ Ã‰tablit les standards et best practices                                â”‚\nâ”‚   â€¢ Sponsorise et dÃ©veloppe les seniors                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   PRINCIPAL (10+ ans)                                                       â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\nâ”‚   â€¢ Vision technique long terme                                            â”‚\nâ”‚   â€¢ Influence l'industrie (talks, papers, OSS)                             â”‚\nâ”‚   â€¢ RÃ©sout les problÃ¨mes \"impossibles\"                                     â”‚\nâ”‚   â€¢ Trusted advisor pour executives                                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.3 Comment Progresser\n\n\nVoir le code\n# Conseils de progression de carriÃ¨re\n\ncareer_tips = \"\"\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# COMMENT PROGRESSER EN DATA ENGINEERING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n## 1. JUNIOR â†’ MID\n\nâœ… Ã€ FAIRE :\nâ€¢ MaÃ®triser les fondamentaux (SQL, Python, un outil ETL)\nâ€¢ Livrer des projets de A Ã  Z (pas juste des tickets)\nâ€¢ Comprendre le \"pourquoi\" business, pas juste le \"comment\" technique\nâ€¢ Documenter son travail\nâ€¢ Demander du feedback rÃ©guliÃ¨rement\n\nâŒ Ã€ Ã‰VITER :\nâ€¢ Rester dans sa zone de confort\nâ€¢ Ne jamais demander d'aide (ou toujours demander)\nâ€¢ Ignorer les soft skills\n\n## 2. MID â†’ SENIOR\n\nâœ… Ã€ FAIRE :\nâ€¢ Prendre ownership de systÃ¨mes complets\nâ€¢ Proposer des amÃ©liorations (pas juste exÃ©cuter)\nâ€¢ Mentorer au moins 1 junior\nâ€¢ Ã‰crire des ADRs et documentation d'architecture\nâ€¢ DÃ©velopper expertise dans 1-2 domaines profonds\nâ€¢ PrÃ©senter en interne (tech talks)\n\nâŒ Ã€ Ã‰VITER :\nâ€¢ ÃŠtre le \"hÃ©ros\" qui fait tout seul\nâ€¢ Ignorer les aspects non-techniques (communication, politique)\nâ€¢ Ne pas partager ses connaissances\n\n## 3. SENIOR â†’ STAFF\n\nâœ… Ã€ FAIRE :\nâ€¢ RÃ©soudre des problÃ¨mes cross-Ã©quipes\nâ€¢ Influencer sans autoritÃ© directe\nâ€¢ DÃ©finir des standards adoptÃ©s par l'org\nâ€¢ Sponsoriser des projets (pas juste contribuer)\nâ€¢ DÃ©velopper une vision technique\nâ€¢ ÃŠtre la personne \"go-to\" pour un domaine\n\nâŒ Ã€ Ã‰VITER :\nâ€¢ Continuer Ã  coder 100% du temps\nâ€¢ Ignorer l'impact organisationnel\nâ€¢ Ne pas dÃ©velopper les autres\n\n## 4. CONSEILS GÃ‰NÃ‰RAUX\n\nğŸ“ˆ VisibilitÃ© :\nâ€¢ Documenter et partager son travail\nâ€¢ PrÃ©senter aux stakeholders\nâ€¢ Contribuer Ã  l'open source\nâ€¢ Ã‰crire des blog posts (internes ou externes)\n\nğŸ¤ RÃ©seau :\nâ€¢ ConnaÃ®tre les gens en dehors de son Ã©quipe\nâ€¢ Avoir des sponsors/mentors seniors\nâ€¢ Aider les autres (rÃ©ciprocitÃ©)\n\nğŸ“Š Impact :\nâ€¢ Mesurer et communiquer son impact\nâ€¢ Lier son travail aux objectifs business\nâ€¢ Garder un \"brag document\" Ã  jour\n\"\"\"\n\nprint(career_tips)\n\n\n\n\n6.4 Le â€œBrag Documentâ€\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    BRAG DOCUMENT (Self-Promotion Doc)                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Document personnel oÃ¹ tu notes tes accomplissements.                     â”‚\nâ”‚   Ã€ mettre Ã  jour rÃ©guliÃ¨rement (idÃ©alement chaque semaine).               â”‚\nâ”‚   Utile pour : performance reviews, promotions, CV, interviews.            â”‚\nâ”‚                                                                             â”‚\nâ”‚   STRUCTURE :                                                               â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   ## Q1 2024                                                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   ### Projets                                                               â”‚\nâ”‚   â€¢ [Projet] : [Description 1 ligne]                                       â”‚\nâ”‚     Impact : [MÃ©trique quantifiable]                                       â”‚\nâ”‚     Mon rÃ´le : [Ce que j'ai fait spÃ©cifiquement]                           â”‚\nâ”‚                                                                             â”‚\nâ”‚   ### AmÃ©liorations                                                         â”‚\nâ”‚   â€¢ RÃ©duit le temps de pipeline de 4h Ã  45min                              â”‚\nâ”‚   â€¢ Ã‰conomisÃ© $X/mois en optimisant le cluster                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   ### Collaboration                                                         â”‚\nâ”‚   â€¢ MentorÃ© [Nom] sur [Sujet]                                              â”‚\nâ”‚   â€¢ PrÃ©sentÃ© [Sujet] Ã  l'Ã©quipe                                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   ### Feedback reÃ§u                                                         â”‚\nâ”‚   â€¢ \"[Citation positive d'un collÃ¨gue/manager]\"                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   TIPS :                                                                    â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€                                                                   â”‚\nâ”‚   â€¢ Quantifier quand possible (%, $, temps)                                â”‚\nâ”‚   â€¢ Noter le contexte (sinon on oublie)                                    â”‚\nâ”‚   â€¢ Inclure les \"petites\" victoires aussi                                  â”‚\nâ”‚   â€¢ Mettre Ã  jour PENDANT le trimestre, pas aprÃ¨s                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#exercices-pratiques",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#exercices-pratiques",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "7. Exercices Pratiques",
    "text": "7. Exercices Pratiques\n\nExercice 1 : Trade-off Analysis\nTon Ã©quipe doit choisir entre : - Option A : Snowflake (managed, $$$, facile) - Option B : Spark + Delta sur K8s (flexible, complexe, moins cher)\nContexte : Startup sÃ©rie B, Ã©quipe de 5 DE, budget $100K/an data infra.\nQuestions : 1. Identifie 5 critÃ¨res de dÃ©cision avec leurs poids 2. Score chaque option 3. Quel trade-off acceptes-tu avec ton choix ? 4. Comment communiquerais-tu cette dÃ©cision au CTO ?\n\n\n\nExercice 2 : Communication BLUF\nRÃ©Ã©cris ce message en format BLUF :\n\nâ€œJâ€™ai regardÃ© plusieurs solutions pour le problÃ¨me de performance du dashboard. Dâ€™abord jâ€™ai essayÃ© dâ€™optimiser les requÃªtes SQL mais Ã§a nâ€™a pas suffi. Ensuite jâ€™ai regardÃ© lâ€™indexation et Ã§a aide un peu. Jâ€™ai aussi Ã©valuÃ© ClickHouse comme alternative Ã  PostgreSQL. Finalement, je pense quâ€™on devrait migrer vers ClickHouse. Ã‡a va prendre environ 3 semaines.â€\n\n\n\n\nExercice 3 : Estimation\nEstime ce projet avec la mÃ©thode PERT :\nâ€œConstruire un pipeline dâ€™ingestion depuis une API REST vers Delta Lakeâ€\nDÃ©compose en tÃ¢ches, estime chaque tÃ¢che (O/M/P), calcule le total.\n\n\n\nExercice 4 : Post-Mortem\nRÃ©dige un post-mortem pour cet incident :\n\nLe pipeline de donnÃ©es quotidien a Ã©chouÃ© pendant 3 jours sans que personne ne sâ€™en rende compte. Les dashboards affichaient des donnÃ©es obsolÃ¨tes. Cause : le job Airflow Ã©tait en retry infini, pas dâ€™alerte configurÃ©e.\n\n\n\n\nExercice 5 : Career Planning\n\nOÃ¹ te situes-tu dans la career ladder ?\nQuelles sont tes 3 principales compÃ©tences Ã  dÃ©velopper ?\nCrÃ©e ton â€œbrag documentâ€ pour le dernier trimestre\nIdentifie 1 action concrÃ¨te pour chaque compÃ©tence Ã  dÃ©velopper",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#ressources",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#ressources",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nLivres\n\nThe Staff Engineerâ€™s Path â€” Tanya Reilly\nAn Elegant Puzzle: Systems of Engineering Management â€” Will Larson\nDesigning Data-Intensive Applications â€” Martin Kleppmann\nThe Managerâ€™s Path â€” Camille Fournier\n\n\n\nArticles\n\nStaff Engineer Archetypes\nBrag Documents â€” Julia Evans\nOn Being a Senior Engineer\nHow to Do a Post-Mortem\n\n\n\nPodcasts\n\nThe Data Engineering Podcast\nSoftware Engineering Daily",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/35_leadership_tradeoffs.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/35_leadership_tradeoffs.html#prochaine-Ã©tape",
    "title": "ğŸ¯ Leadership & Trade-offs in Data Engineering",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 36_capstone_project â€” Projet Final IntÃ©grateur\nTu vas mettre en pratique TOUT ce que tu as appris dans un projet complet !\n\nğŸ‰ FÃ©licitations ! Tu as maintenant les compÃ©tences techniques ET les soft skills pour devenir un Data Engineer Senior/Staff.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ‘” Leadership",
      "35 Â· Leadership & Trade-offs"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  concevoir des architectures data et Ã  prendre des dÃ©cisions techniques Ã©clairÃ©es. Câ€™est le passage du â€œje sais coderâ€ Ã  â€œje sais architecturerâ€.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#prÃ©requis",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#prÃ©requis",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nTous les modules prÃ©cÃ©dents (M01-M33)\n\n\nâœ… Requis\nExpÃ©rience sur des projets data rÃ©els\n\n\nğŸ’¡ RecommandÃ©\nAvoir Ã©tÃ© confrontÃ© Ã  des choix dâ€™architecture",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#objectifs-du-module",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#objectifs-du-module",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nConnaÃ®tre les patterns dâ€™architecture data majeurs\nAppliquer un framework de dÃ©cision structurÃ©\nRÃ©diger des Architecture Decision Records (ADR)\nFaire du capacity planning rÃ©aliste\nÃ‰viter les anti-patterns classiques\nAdapter lâ€™architecture au contexte (startup vs enterprise)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#introduction-lart-de-larchitecture-data",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#introduction-lart-de-larchitecture-data",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "1. Introduction : Lâ€™Art de lâ€™Architecture Data",
    "text": "1. Introduction : Lâ€™Art de lâ€™Architecture Data\n\n1.1 Quâ€™est-ce quâ€™un Architecte Data ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Ã‰VOLUTION DU DATA ENGINEER                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   Junior               Senior               Staff/Principal                 â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   \"Comment faire\"      \"Quoi faire\"         \"Pourquoi faire\"               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ ExÃ©cute les        â€¢ ConÃ§oit les        â€¢ DÃ©finit la vision            â”‚\nâ”‚     tÃ¢ches               pipelines          â€¢ Prend les dÃ©cisions          â”‚\nâ”‚   â€¢ Suit les           â€¢ Choisit les          d'architecture               â”‚\nâ”‚     patterns             outils             â€¢ Anticipe les                 â”‚\nâ”‚   â€¢ Apprend            â€¢ RÃ©sout les           problÃ¨mes futurs             â”‚\nâ”‚                          problÃ¨mes          â€¢ Influence l'org              â”‚\nâ”‚                          complexes                                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   Focus: Code          Focus: SystÃ¨me       Focus: StratÃ©gie               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Les Questions Fondamentales\nAvant tout choix technique, pose-toi ces questions :\n\n\n\n\n\n\n\nQuestion\nPourquoi câ€™est important\n\n\n\n\nQuel problÃ¨me rÃ©sout-on ?\nÃ‰viter le solutionnisme technologique\n\n\nQuelles sont les contraintes ?\nBudget, Ã©quipe, deadline, legacy\n\n\nQuelle est lâ€™Ã©chelle ?\n1 GB vs 1 PB = architectures trÃ¨s diffÃ©rentes\n\n\nQuelle latence est acceptable ?\nBatch vs Streaming\n\n\nQui va maintenir ?\nComplexitÃ© vs compÃ©tences Ã©quipe\n\n\nComment Ã§a va Ã©voluer ?\nAnticipation vs over-engineering\n\n\n\n\n\n1.3 Le Principe de la Solution la Plus Simple\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    COMPLEXITÃ‰ vs VALEUR                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   Valeur                                                                    â”‚\nâ”‚   Business    â”‚                      â”Œâ”€â”€â”€â”€ Zone Danger                     â”‚\nâ”‚       â–²       â”‚                     â•±      (over-engineering)              â”‚\nâ”‚       â”‚       â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±                                       â”‚\nâ”‚       â”‚       â”‚        â•±                                                   â”‚\nâ”‚       â”‚       â”‚       â•±   Zone Optimale                                    â”‚\nâ”‚       â”‚       â”‚      â•±    (bon trade-off)                                  â”‚\nâ”‚       â”‚       â”‚     â•±                                                      â”‚\nâ”‚       â”‚       â”‚â”€â”€â”€â”€â•±                                                       â”‚\nâ”‚       â”‚       â”‚   â•±  Zone Simple                                           â”‚\nâ”‚       â”‚       â”‚  â•±   (MVP, quick wins)                                     â”‚\nâ”‚       â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶                 â”‚\nâ”‚                              ComplexitÃ© Technique                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃˆGLE : Commence simple, complexifie seulement quand NÃ‰CESSAIRE         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#patterns-darchitecture-data",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#patterns-darchitecture-data",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "2. Patterns dâ€™Architecture Data",
    "text": "2. Patterns dâ€™Architecture Data\n\n2.1 Lambda vs Kappa vs Delta Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    LAMBDA ARCHITECTURE                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚\nâ”‚                         â”‚   Batch Layer    â”‚                               â”‚\nâ”‚                    â”Œâ”€â”€â”€â–¶â”‚   (Spark Batch)  â”‚â”€â”€â”€â”                           â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚  Source â”‚â”€â”€â”€â”€â”€â”¤                           â”œâ”€â”€â”€â–¶â”‚  Serving Layer  â”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                    â””â”€â”€â”€â–¶â”‚   Speed Layer    â”‚â”€â”€â”€â”˜                           â”‚\nâ”‚                         â”‚   (Streaming)    â”‚                               â”‚\nâ”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Accurate (batch) + Fast (streaming)                                   â”‚\nâ”‚   âŒ 2 codebases Ã  maintenir                                               â”‚\nâ”‚   âŒ ComplexitÃ© opÃ©rationnelle                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    KAPPA ARCHITECTURE                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\nâ”‚   â”‚  Source â”‚â”€â”€â”€â”€â–¶â”‚   Stream Layer   â”‚â”€â”€â”€â”€â–¶â”‚  Serving Layer  â”‚            â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   (Kafka + Flink)â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\nâ”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Une seule codebase                                                    â”‚\nâ”‚   âœ… SimplicitÃ© opÃ©rationnelle                                             â”‚\nâ”‚   âŒ Reprocessing = rejouer tout le stream                                 â”‚\nâ”‚   âŒ Pas idÃ©al pour ML training (besoin de batch)                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DELTA ARCHITECTURE (Lakehouse)                           â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\nâ”‚   â”‚  Source â”‚â”€â”€â”€â”€â–¶â”‚   Unified Layer  â”‚â”€â”€â”€â”€â–¶â”‚  Serving Layer  â”‚            â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚   (Delta Lake)   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚\nâ”‚                   â”‚                  â”‚                                     â”‚\nâ”‚                   â”‚  Batch + Stream  â”‚                                     â”‚\nâ”‚                   â”‚  Same code/table â”‚                                     â”‚\nâ”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Une seule codebase                                                    â”‚\nâ”‚   âœ… Time travel pour reprocessing                                         â”‚\nâ”‚   âœ… ACID transactions                                                     â”‚\nâ”‚   âœ… Batch et streaming sur mÃªmes tables                                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 Matrice de DÃ©cision : Lambda vs Kappa vs Delta\n\n\n\n\n\n\n\n\n\nCritÃ¨re\nLambda\nKappa\nDelta\n\n\n\n\nComplexitÃ© ops\nğŸ”´ Haute\nğŸŸ¢ Basse\nğŸŸ¢ Basse\n\n\nReprocessing\nğŸŸ¢ Facile\nğŸ”´ Difficile\nğŸŸ¢ Facile\n\n\nLatence\nğŸŸ¡ Mixte\nğŸŸ¢ Temps rÃ©el\nğŸŸ¡ Near real-time\n\n\nML Training\nğŸŸ¢ Bon\nğŸ”´ Difficile\nğŸŸ¢ Bon\n\n\nCoÃ»t\nğŸ”´ Ã‰levÃ©\nğŸŸ¢ ModÃ©rÃ©\nğŸŸ¢ ModÃ©rÃ©\n\n\nQuand choisir\nLegacy, besoins spÃ©cifiques\nPure streaming\nRecommandÃ© par dÃ©faut\n\n\n\n\n\n2.3 Batch vs Streaming vs Hybrid\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    QUAND UTILISER QUOI ?                                    â”‚\nâ”‚                                                                             â”‚\nâ”‚   BATCH                          STREAMING                  HYBRID         â”‚\nâ”‚   â”€â”€â”€â”€â”€                          â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€         â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Reports quotidiens           â€¢ Fraud detection          â€¢ La plupart   â”‚\nâ”‚   â€¢ ML training                  â€¢ Real-time reco             des cas      â”‚\nâ”‚   â€¢ Data warehouse refresh       â€¢ Alerting                               â”‚\nâ”‚   â€¢ Backfill historique          â€¢ IoT monitoring          Batch pour      â”‚\nâ”‚                                  â€¢ Live dashboards          historique,    â”‚\nâ”‚   Latence: heures               Latence: secondes          Streaming pour  â”‚\nâ”‚                                                              temps rÃ©el    â”‚\nâ”‚                                                                             â”‚\nâ”‚   ğŸ’¡ RÃˆGLE : Si \"temps rÃ©el\" n'est pas un VRAI besoin business,           â”‚\nâ”‚              commence par BATCH. C'est plus simple et moins cher.          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.4 Questions pour Choisir Batch vs Streaming\n\n\n\n\n\n\n\n\nQuestion\nSi OUI â†’\nSi NON â†’\n\n\n\n\nLe business perd de lâ€™argent si latence &gt; 1 min ?\nStreaming\nBatch\n\n\nLes donnÃ©es arrivent en continu 24/7 ?\nStreaming\nBatch\n\n\nLâ€™Ã©quipe a de lâ€™expÃ©rience streaming ?\nStreaming OK\nBatch dâ€™abord\n\n\nBudget ops illimitÃ© ?\nStreaming OK\nBatch moins cher\n\n\nUse case = alerting/fraud/monitoring ?\nStreaming\nBatch souvent OK\n\n\n\n\n\n2.5 Data Lake vs Data Warehouse vs Lakehouse\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DATA LAKE vs WAREHOUSE vs LAKEHOUSE                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   DATA LAKE                DATA WAREHOUSE           LAKEHOUSE               â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\nâ”‚   â”‚   S3/GCS    â”‚         â”‚  Snowflake  â”‚         â”‚ Delta Lake  â”‚          â”‚\nâ”‚   â”‚   (files)   â”‚         â”‚  Redshift   â”‚         â”‚  Iceberg    â”‚          â”‚\nâ”‚   â”‚             â”‚         â”‚  BigQuery   â”‚         â”‚             â”‚          â”‚\nâ”‚   â”‚ Raw + Semi  â”‚         â”‚  Structured â”‚         â”‚   Unified   â”‚          â”‚\nâ”‚   â”‚ structured  â”‚         â”‚    only     â”‚         â”‚             â”‚          â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Cheap storage        âœ… Fast queries          âœ… Best of both          â”‚\nâ”‚   âœ… Flexible             âœ… ACID                  âœ… ACID                  â”‚\nâ”‚   âœ… All data types       âœ… Governance            âœ… Open formats          â”‚\nâ”‚   âŒ No ACID              âŒ Expensive             âœ… ML + Analytics        â”‚\nâ”‚   âŒ Query perf           âŒ Vendor lock-in        âœ… Streaming             â”‚\nâ”‚   âŒ Data swamp risk      âŒ ETL required                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   ğŸ“… 2010s                ğŸ“… 2000s-now             ğŸ“… 2020s+                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.6 Recommandation Moderne\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RECOMMANDATION 2024+                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   DÃ‰FAUT : Lakehouse (Delta/Iceberg) + Compute sÃ©parÃ© (Spark/Trino)        â”‚\nâ”‚                                                                             â”‚\nâ”‚   EXCEPTIONS :                                                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Ã‰quipe 100% SQL, pas de ML â†’ Cloud DW (Snowflake, BigQuery)            â”‚\nâ”‚   â€¢ Budget serrÃ©, petit volume â†’ DuckDB + Parquet                          â”‚\nâ”‚   â€¢ Enterprise legacy â†’ Conserver DW existant, ajouter Lake                â”‚\nâ”‚   â€¢ Real-time OLAP â†’ ClickHouse/Druid en complÃ©ment                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.7 Patterns de Data Modeling\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    MEDALLION ARCHITECTURE                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚   â”‚   BRONZE    â”‚â”€â”€â”€â”€â–¶â”‚   SILVER    â”‚â”€â”€â”€â”€â–¶â”‚    GOLD     â”‚                  â”‚\nâ”‚   â”‚   (Raw)     â”‚     â”‚  (Cleaned)  â”‚     â”‚ (Aggregated)â”‚                  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Ingestion brute    â€¢ DÃ©duplication     â€¢ Business metrics              â”‚\nâ”‚   â€¢ Append-only        â€¢ Validation        â€¢ Dimensional models            â”‚\nâ”‚   â€¢ Schemaless OK      â€¢ Enrichissement    â€¢ Ready for BI                  â”‚\nâ”‚                        â€¢ Schema enforced                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DIMENSIONAL MODELING (Kimball)                           â”‚\nâ”‚                                                                             â”‚\nâ”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚\nâ”‚                        â”‚   dim_date    â”‚                                   â”‚\nâ”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\nâ”‚   â”‚ dim_customer  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  dim_product  â”‚               â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\nâ”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                     â”‚\nâ”‚                        â”‚ fact_sales  â”‚                                     â”‚\nâ”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… OptimisÃ© pour BI/reporting                                            â”‚\nâ”‚   âœ… Facile Ã  comprendre                                                   â”‚\nâ”‚   âŒ Redondance (dÃ©normalisation)                                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DATA VAULT                                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\nâ”‚   â”‚   HUB   â”‚â”€â”€â”€â”€â–¶â”‚  LINK   â”‚â—€â”€â”€â”€â”€â”‚   HUB   â”‚                              â”‚\nâ”‚   â”‚(Customer)â”‚     â”‚ (Order) â”‚     â”‚(Product)â”‚                              â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚\nâ”‚        â”‚                               â”‚                                    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                              â”‚\nâ”‚   â”‚SATELLITEâ”‚                     â”‚SATELLITEâ”‚                              â”‚\nâ”‚   â”‚(attrs)  â”‚                     â”‚(attrs)  â”‚                              â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   âœ… Historisation complÃ¨te                                                â”‚\nâ”‚   âœ… FlexibilitÃ© (ajout sources facile)                                    â”‚\nâ”‚   âŒ Complexe Ã  implÃ©menter                                                â”‚\nâ”‚   âŒ Queries plus complexes                                                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.8 Quand Utiliser Quel Modeling ?\n\n\n\n\n\n\n\n\nPattern\nQuand lâ€™utiliser\nQuand Ã©viter\n\n\n\n\nMedallion\nLakehouse, ETL moderne, ML\nPetit projet sans layers\n\n\nDimensional (Kimball)\nBI/Reporting, DW classique\nData science, ML features\n\n\nData Vault\nEnterprise, audit strict, multi-sources\nStartup, MVP, Ã©quipe petite\n\n\nOne Big Table (OBT)\nAnalytics simple, ML features\nBeaucoup de dimensions",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#framework-de-dÃ©cision",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#framework-de-dÃ©cision",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "3. Framework de DÃ©cision",
    "text": "3. Framework de DÃ©cision\n\n3.1 Le Process de DÃ©cision\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    FRAMEWORK DE DÃ‰CISION ARCHITECTURE                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. DEFINE          2. DISCOVER         3. DECIDE          4. DOCUMENT    â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚ Quel     â”‚      â”‚ Quelles  â”‚       â”‚ Ã‰valuer  â”‚       â”‚ ADR      â”‚    â”‚\nâ”‚   â”‚ problÃ¨me â”‚â”€â”€â”€â”€â”€â–¶â”‚ options  â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ trade-   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ (Ã©crit)  â”‚    â”‚\nâ”‚   â”‚ ?        â”‚      â”‚ ?        â”‚       â”‚ offs     â”‚       â”‚          â”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                                                             â”‚\nâ”‚   â€¢ Requirements     â€¢ 3-5 options      â€¢ Matrice          â€¢ Contexte      â”‚\nâ”‚   â€¢ Constraints      â€¢ Recherche        â€¢ PoC si doute     â€¢ DÃ©cision      â”‚\nâ”‚   â€¢ Success criteria â€¢ Benchmarks       â€¢ Consensus        â€¢ ConsÃ©quences  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 Template de Requirements\n\n\nVoir le code\n# Template de Requirements pour une dÃ©cision d'architecture\n\nrequirements_template = \"\"\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# REQUIREMENTS DOCUMENT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n## 1. Contexte\n- Projet : [Nom du projet]\n- Date : [Date]\n- Auteur : [Ton nom]\n- Stakeholders : [Qui est impactÃ©]\n\n## 2. ProblÃ¨me Ã  rÃ©soudre\n[Description claire du problÃ¨me en 2-3 phrases]\n\n## 3. Functional Requirements (FR)\n- FR1 : [Ce que le systÃ¨me DOIT faire]\n- FR2 : [Ce que le systÃ¨me DOIT faire]\n- FR3 : [...]\n\n## 4. Non-Functional Requirements (NFR)\n\n### Performance\n- Latence max : [X secondes/minutes]\n- Throughput : [X events/sec ou X GB/jour]\n- Concurrent users : [X]\n\n### ScalabilitÃ©\n- Volume actuel : [X GB]\n- Volume dans 1 an : [X GB]\n- Volume dans 3 ans : [X GB]\n\n### DisponibilitÃ©\n- SLA cible : [99.9% ?]\n- Downtime acceptable : [X heures/mois]\n- RTO (Recovery Time) : [X heures]\n- RPO (Data Loss) : [X minutes]\n\n### SÃ©curitÃ©\n- DonnÃ©es sensibles : [Oui/Non]\n- Compliance : [GDPR, HIPAA, SOC2...]\n- Encryption : [At rest, in transit]\n\n## 5. Contraintes\n- Budget : [X â‚¬/mois]\n- Timeline : [Date de livraison]\n- Ã‰quipe : [X personnes, compÃ©tences]\n- Legacy : [SystÃ¨mes existants Ã  intÃ©grer]\n- Vendor : [Restrictions cloud/vendor]\n\n## 6. CritÃ¨res de succÃ¨s\n- [ ] [CritÃ¨re mesurable 1]\n- [ ] [CritÃ¨re mesurable 2]\n- [ ] [CritÃ¨re mesurable 3]\n\"\"\"\n\nprint(requirements_template)\n\n\n\n\n3.3 Matrice de DÃ©cision\n\n\nVoir le code\nimport pandas as pd\n\n# Exemple : Choisir un orchestrateur\n\ncriteria = {\n    'CritÃ¨re': [\n        'FacilitÃ© d\\'apprentissage',\n        'ScalabilitÃ©',\n        'CommunautÃ©/Support',\n        'CoÃ»t opÃ©rationnel',\n        'IntÃ©gration cloud',\n        'FonctionnalitÃ©s avancÃ©es'\n    ],\n    'Poids': [3, 4, 3, 4, 3, 2],  # Importance 1-5\n    'Airflow': [3, 5, 5, 3, 4, 5],  # Score 1-5\n    'Prefect': [4, 4, 3, 4, 3, 4],\n    'Dagster': [3, 4, 3, 4, 3, 5],\n    'Step Functions': [4, 5, 4, 5, 5, 3]\n}\n\ndf = pd.DataFrame(criteria)\n\n# Calcul des scores pondÃ©rÃ©s\nfor option in ['Airflow', 'Prefect', 'Dagster', 'Step Functions']:\n    df[f'{option}_weighted'] = df['Poids'] * df[option]\n\n# Totaux\ntotals = {\n    'Option': ['Airflow', 'Prefect', 'Dagster', 'Step Functions'],\n    'Score Total': [\n        df['Airflow_weighted'].sum(),\n        df['Prefect_weighted'].sum(),\n        df['Dagster_weighted'].sum(),\n        df['Step Functions_weighted'].sum()\n    ]\n}\n\nprint(\"ğŸ“Š MATRICE DE DÃ‰CISION : Choix d'Orchestrateur\")\nprint(\"=\"*60)\nprint(df[['CritÃ¨re', 'Poids', 'Airflow', 'Prefect', 'Dagster', 'Step Functions']].to_string(index=False))\nprint(\"\\nğŸ“ˆ SCORES TOTAUX (pondÃ©rÃ©s) :\")\nprint(\"-\"*40)\nfor opt, score in zip(totals['Option'], totals['Score Total']):\n    print(f\"  {opt:20} : {score}\")\nprint(\"\\nâœ… Recommandation : \" + totals['Option'][totals['Score Total'].index(max(totals['Score Total']))])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#architecture-decision-records-adr",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#architecture-decision-records-adr",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "4. Architecture Decision Records (ADR)",
    "text": "4. Architecture Decision Records (ADR)\n\n4.1 Pourquoi Documenter les DÃ©cisions ?\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    PROBLÃˆME CLASSIQUE                                       â”‚\nâ”‚                                                                             â”‚\nâ”‚   Nouveau dÃ©veloppeur : \"Pourquoi on utilise Kafka et pas RabbitMQ ?\"      â”‚\nâ”‚                                                                             â”‚\nâ”‚   Ã‰quipe : \"Euh... c'Ã©tait lÃ  quand je suis arrivÃ©.\"                       â”‚\nâ”‚            \"Je crois que c'est Jean qui a dÃ©cidÃ©, mais il est parti.\"      â”‚\nâ”‚            \"On a toujours fait comme Ã§a.\"                                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃ‰SULTAT :                                                                â”‚\nâ”‚   â€¢ DÃ©cisions remises en question sans contexte                            â”‚\nâ”‚   â€¢ MÃªmes erreurs rÃ©pÃ©tÃ©es                                                 â”‚\nâ”‚   â€¢ Perte de temps Ã  rediscuter                                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   SOLUTION : Architecture Decision Records (ADR)                            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n4.2 Format ADR\n\n\nVoir le code\n# Template ADR (Architecture Decision Record)\n\nadr_template = \"\"\"\n# ADR-001 : [Titre de la dÃ©cision]\n\n## Status\n[Proposed | Accepted | Deprecated | Superseded by ADR-XXX]\n\n## Date\n2024-XX-XX\n\n## Context\n[Quel est le problÃ¨me ? Pourquoi cette dÃ©cision est nÃ©cessaire ?]\n\n## Decision Drivers\n- [Driver 1 : ex. \"Besoin de traiter 10K events/sec\"]\n- [Driver 2 : ex. \"Ã‰quipe familiÃ¨re avec Python\"]\n- [Driver 3 : ex. \"Budget limitÃ© Ã  Xâ‚¬/mois\"]\n\n## Considered Options\n1. [Option A]\n2. [Option B]\n3. [Option C]\n\n## Decision\n[Quelle option a Ã©tÃ© choisie et POURQUOI]\n\n## Consequences\n\n### Positives\n- [ConsÃ©quence positive 1]\n- [ConsÃ©quence positive 2]\n\n### Negatives\n- [ConsÃ©quence nÃ©gative 1]\n- [Trade-off acceptÃ©]\n\n### Risks\n- [Risque identifiÃ© et mitigation]\n\n## Related\n- [Lien vers ADR connexes]\n- [Lien vers documentation]\n\"\"\"\n\nprint(adr_template)\n\n\n\n\nVoir le code\n# Exemple concret d'ADR\n\nadr_example = \"\"\"\n# ADR-003 : Choix de Delta Lake comme Table Format\n\n## Status\nAccepted\n\n## Date\n2024-03-15\n\n## Context\nNotre Data Lake sur S3 souffre de plusieurs problÃ¨mes :\n- Pas de transactions ACID (corruptions lors d'Ã©checs de jobs)\n- Pas de schema enforcement (colonnes manquantes non dÃ©tectÃ©es)\n- Updates/deletes impossibles (GDPR compliance)\n- Pas de time travel pour debug\n\nNous devons choisir un table format pour rÃ©soudre ces problÃ¨mes.\n\n## Decision Drivers\n- Besoin d'ACID pour Ã©viter les corruptions\n- Support GDPR (delete user data)\n- Ã‰quipe utilise dÃ©jÃ  Spark 3.x\n- Pas de budget pour Databricks\n- PrÃ©fÃ©rence pour l'open source\n\n## Considered Options\n1. **Delta Lake (OSS)** - Format Databricks, maintenant open source\n2. **Apache Iceberg** - Format Netflix, trÃ¨s actif\n3. **Apache Hudi** - Format Uber, orientÃ© CDC\n4. **Rester sur Parquet brut** - Status quo\n\n## Decision\nNous choisissons **Delta Lake (Open Source)** car :\n\n1. Meilleure intÃ©gration Spark (notre stack principale)\n2. Documentation extensive et communautÃ© active\n3. Migration facile depuis Parquet (CONVERT TO DELTA)\n4. FonctionnalitÃ©s matures (MERGE, time travel, schema evolution)\n5. Pas de dÃ©pendance Ã  Databricks (version OSS suffit)\n\nIceberg Ã©tait un close second, mais l'Ã©cosystÃ¨me Spark+Delta\nest plus mature en 2024.\n\n## Consequences\n\n### Positives\n- ACID transactions â†’ plus de corruptions\n- Time travel â†’ debug et audit facilitÃ©s\n- Schema enforcement â†’ erreurs dÃ©tectÃ©es tÃ´t\n- MERGE â†’ GDPR compliance possible\n\n### Negatives\n- LÃ©gÃ¨re courbe d'apprentissage Ã©quipe\n- Vacuum Ã  scheduler rÃ©guliÃ¨rement\n- Pas de support Trino natif (workaround existe)\n\n### Risks\n- Databricks pourrait rÃ©duire investissement OSS\n  â†’ Mitigation : Iceberg comme plan B, migration possible\n\n## Related\n- ADR-001 : Choix de S3 comme storage\n- ADR-002 : Choix de Spark comme compute engine\n\"\"\"\n\nprint(adr_example)\n\n\n\n\n4.3 Organisation des ADRs\ndocs/\nâ””â”€â”€ architecture/\n    â””â”€â”€ decisions/\n        â”œâ”€â”€ README.md           # Index des ADRs\n        â”œâ”€â”€ 0001-use-s3-storage.md\n        â”œâ”€â”€ 0002-choose-spark.md\n        â”œâ”€â”€ 0003-delta-lake.md\n        â”œâ”€â”€ 0004-airflow-orchestration.md\n        â””â”€â”€ template.md         # Template pour nouvelles ADRs\n\n\n4.4 Bonnes Pratiques ADR\n\n\n\nâœ… DO\nâŒ DONâ€™T\n\n\n\n\nÃ‰crire au moment de la dÃ©cision\nÃ‰crire des mois aprÃ¨s\n\n\nInclure le contexte complet\nSupposer que le lecteur sait\n\n\nDocumenter les options rejetÃ©es\nNe documenter que le choix final\n\n\nLister les trade-offs acceptÃ©s\nPrÃ©tendre que câ€™est parfait\n\n\nMettre Ã  jour le status\nLaisser des ADRs obsolÃ¨tes actifs",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#sizing-capacity-planning",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#sizing-capacity-planning",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "5. Sizing & Capacity Planning",
    "text": "5. Sizing & Capacity Planning\n\n5.1 Estimer le Storage\n\n\nVoir le code\n# Calculator de Storage\n\ndef estimate_storage(\n    events_per_day: int,\n    avg_event_size_bytes: int,\n    retention_days: int,\n    compression_ratio: float = 0.1,  # Parquet/Delta = ~10x compression\n    replication_factor: int = 1,\n    growth_rate_monthly: float = 0.1  # 10% growth/month\n) -&gt; dict:\n    \"\"\"\n    Estime le storage nÃ©cessaire pour un Data Lake.\n    \"\"\"\n    # Calcul de base\n    raw_daily_gb = (events_per_day * avg_event_size_bytes) / (1024**3)\n    compressed_daily_gb = raw_daily_gb * compression_ratio\n    \n    # Avec retention\n    total_gb = compressed_daily_gb * retention_days * replication_factor\n    \n    # Projection 1 an avec croissance\n    monthly_growth = 1 + growth_rate_monthly\n    total_1year_gb = total_gb * (monthly_growth ** 12)\n    \n    return {\n        \"daily_raw_gb\": round(raw_daily_gb, 2),\n        \"daily_compressed_gb\": round(compressed_daily_gb, 2),\n        \"total_current_gb\": round(total_gb, 2),\n        \"total_1year_gb\": round(total_1year_gb, 2),\n        \"monthly_cost_s3_standard\": round(total_gb * 0.023, 2),  # $0.023/GB\n        \"monthly_cost_1year\": round(total_1year_gb * 0.023, 2),\n    }\n\n# Exemple : E-commerce\nresult = estimate_storage(\n    events_per_day=10_000_000,      # 10M events/jour\n    avg_event_size_bytes=500,        # 500 bytes/event (JSON)\n    retention_days=365,              # 1 an de rÃ©tention\n    compression_ratio=0.1,           # Parquet compresse ~10x\n    growth_rate_monthly=0.15         # 15% croissance/mois\n)\n\nprint(\"ğŸ“Š ESTIMATION STORAGE\")\nprint(\"=\"*50)\nprint(f\"  Daily raw data      : {result['daily_raw_gb']} GB\")\nprint(f\"  Daily compressed    : {result['daily_compressed_gb']} GB\")\nprint(f\"  Total current       : {result['total_current_gb']} GB ({result['total_current_gb']/1024:.1f} TB)\")\nprint(f\"  Total in 1 year     : {result['total_1year_gb']} GB ({result['total_1year_gb']/1024:.1f} TB)\")\nprint(f\"\\nğŸ’° COÃ›T S3 (Standard)\")\nprint(f\"  Current monthly     : ${result['monthly_cost_s3_standard']}\")\nprint(f\"  In 1 year monthly   : ${result['monthly_cost_1year']}\")\n\n\n\n\n5.2 Estimer le Compute\n\n\nVoir le code\n# Calculator de Compute Spark\n\ndef estimate_spark_cluster(\n    data_to_process_gb: float,\n    processing_time_target_hours: float,\n    complexity_factor: float = 1.0,  # 1.0 = simple ETL, 2.0 = joins complexes, 3.0 = ML\n    memory_per_gb_data: float = 2.0,  # 2GB RAM pour 1GB data (rule of thumb)\n) -&gt; dict:\n    \"\"\"\n    Estime la taille d'un cluster Spark.\n    \"\"\"\n    # MÃ©moire totale nÃ©cessaire\n    total_memory_gb = data_to_process_gb * memory_per_gb_data * complexity_factor\n    \n    # Throughput cible\n    throughput_gb_per_hour = data_to_process_gb / processing_time_target_hours\n    \n    # Sizing nodes (ex: r5.2xlarge = 64GB RAM, 8 cores)\n    node_memory_gb = 64\n    node_cores = 8\n    num_nodes = max(1, int(total_memory_gb / (node_memory_gb * 0.75)))  # 75% usable\n    \n    # CoÃ»t (r5.2xlarge on-demand = ~$0.50/hour)\n    hourly_cost = num_nodes * 0.50\n    job_cost = hourly_cost * processing_time_target_hours\n    \n    return {\n        \"total_memory_needed_gb\": round(total_memory_gb, 0),\n        \"throughput_gb_per_hour\": round(throughput_gb_per_hour, 1),\n        \"recommended_nodes\": num_nodes,\n        \"total_cores\": num_nodes * node_cores,\n        \"total_memory_gb\": num_nodes * node_memory_gb,\n        \"hourly_cost\": round(hourly_cost, 2),\n        \"job_cost\": round(job_cost, 2),\n    }\n\n# Exemple : Job ETL quotidien\nresult = estimate_spark_cluster(\n    data_to_process_gb=500,           # 500 GB Ã  traiter\n    processing_time_target_hours=2,    # En 2 heures max\n    complexity_factor=1.5,             # ETL avec quelques joins\n)\n\nprint(\"ğŸ–¥ï¸ ESTIMATION CLUSTER SPARK\")\nprint(\"=\"*50)\nprint(f\"  Memory needed       : {result['total_memory_needed_gb']} GB\")\nprint(f\"  Throughput target   : {result['throughput_gb_per_hour']} GB/hour\")\nprint(f\"\\nğŸ“Š CLUSTER SIZING (r5.2xlarge)\")\nprint(f\"  Nodes recommended   : {result['recommended_nodes']}\")\nprint(f\"  Total cores         : {result['total_cores']}\")\nprint(f\"  Total memory        : {result['total_memory_gb']} GB\")\nprint(f\"\\nğŸ’° COÃ›T\")\nprint(f\"  Hourly              : ${result['hourly_cost']}\")\nprint(f\"  Per job             : ${result['job_cost']}\")\nprint(f\"  Monthly (1 job/day) : ${result['job_cost'] * 30:.0f}\")\n\n\n\n\n5.3 RÃ¨gles de Capacity Planning\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RÃˆGLES DE SIZING                                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   STORAGE                                                                   â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€                                                                   â”‚\nâ”‚   â€¢ PrÃ©voir 2-3x le volume actuel pour la croissance                       â”‚\nâ”‚   â€¢ Parquet/Delta = 5-10x compression vs JSON/CSV                          â”‚\nâ”‚   â€¢ Ajouter 20% pour les metadata, logs, temp files                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   SPARK MEMORY                                                              â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                              â”‚\nâ”‚   â€¢ 2-4 GB RAM par 1 GB de donnÃ©es Ã  traiter                               â”‚\nâ”‚   â€¢ Joins/aggregations complexes = x2 ou x3                                â”‚\nâ”‚   â€¢ Broadcast joins si une table &lt; 10 MB                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   SPARK PARALLELISM                                                         â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚\nâ”‚   â€¢ 2-4 partitions par core                                                â”‚\nâ”‚   â€¢ Partition size idÃ©ale = 128 MB - 256 MB                                â”‚\nâ”‚   â€¢ Trop de partitions = overhead scheduling                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   KAFKA                                                                     â”‚\nâ”‚   â”€â”€â”€â”€â”€                                                                     â”‚\nâ”‚   â€¢ Partitions = throughput / 10 MB/s (par partition)                      â”‚\nâ”‚   â€¢ Replication factor = 3 (production)                                    â”‚\nâ”‚   â€¢ Retention = jours nÃ©cessaires pour replay                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   CLICKHOUSE                                                                â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚\nâ”‚   â€¢ 1 core = ~100-500 MB/s scan rate                                       â”‚\nâ”‚   â€¢ RAM = 10-20% du dataset frÃ©quemment queryÃ©                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#anti-patterns-Ã -Ã©viter",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#anti-patterns-Ã -Ã©viter",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "6. Anti-Patterns Ã  Ã‰viter",
    "text": "6. Anti-Patterns Ã  Ã‰viter\n\n6.1 Resume-Driven Development\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    âŒ RESUME-DRIVEN DEVELOPMENT                             â”‚\nâ”‚                                                                             â”‚\nâ”‚   \"On devrait utiliser Kubernetes + Kafka + Flink + Delta Lake +           â”‚\nâ”‚    GraphQL + Terraform + ArgoCD pour notre dashboard interne               â”‚\nâ”‚    qui a 10 utilisateurs.\"                                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   SYMPTÃ”MES :                                                               â”‚\nâ”‚   â€¢ Choisir une techno parce que \"Ã§a fait bien sur le CV\"                  â”‚\nâ”‚   â€¢ Over-engineering pour des use cases simples                            â”‚\nâ”‚   â€¢ Stack incomprÃ©hensible par l'Ã©quipe                                    â”‚\nâ”‚   â€¢ CoÃ»ts d'infrastructure disproportionnÃ©s                                â”‚\nâ”‚                                                                             â”‚\nâ”‚   SOLUTION :                                                                â”‚\nâ”‚   â€¢ \"Boring technology\" d'abord                                            â”‚\nâ”‚   â€¢ Justifier CHAQUE ajout de complexitÃ©                                   â”‚\nâ”‚   â€¢ Demander : \"Est-ce que PostgreSQL suffirait ?\"                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.2 Cargo Cult Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    âŒ CARGO CULT ARCHITECTURE                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   \"Netflix utilise X, donc on devrait utiliser X.\"                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   SYMPTÃ”MES :                                                               â”‚\nâ”‚   â€¢ Copier l'architecture des FAANG sans comprendre le contexte            â”‚\nâ”‚   â€¢ Ignorer les diffÃ©rences d'Ã©chelle (10 users vs 100M users)             â”‚\nâ”‚   â€¢ Ignorer les diffÃ©rences de budget et d'Ã©quipe                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   RÃ‰ALITÃ‰ :                                                                 â”‚\nâ”‚   â€¢ Netflix a des milliers d'ingÃ©nieurs                                    â”‚\nâ”‚   â€¢ Leur Ã©chelle justifie la complexitÃ©                                    â”‚\nâ”‚   â€¢ Ce qui marche pour eux ne marchera pas pour toi                        â”‚\nâ”‚                                                                             â”‚\nâ”‚   SOLUTION :                                                                â”‚\nâ”‚   â€¢ Comprendre POURQUOI ils ont fait ce choix                              â”‚\nâ”‚   â€¢ Ã‰valuer si tes contraintes sont similaires                             â”‚\nâ”‚   â€¢ Adapter, ne pas copier aveuglÃ©ment                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.3 Autres Anti-Patterns\n\n\n\n\n\n\n\n\nAnti-Pattern\nDescription\nSolution\n\n\n\n\nPremature Optimization\nOptimiser avant de mesurer\nMesurer dâ€™abord, optimiser ensuite\n\n\nGolden Hammer\nâ€œSpark pour tout !â€\nChoisir lâ€™outil adaptÃ© au problÃ¨me\n\n\nNot Invented Here\nRefuser les solutions existantes\nÃ‰valuer buy vs build objectivement\n\n\nAnalysis Paralysis\nTrop dâ€™analyse, pas dâ€™action\nTime-box les dÃ©cisions, PoC rapides\n\n\nShiny Object Syndrome\nAdopter chaque nouvelle techno\nAttendre la maturitÃ©, Ã©valuer les risques\n\n\nDistributed Monolith\nMicroservices trop couplÃ©s\nSi tout doit Ãªtre dÃ©ployÃ© ensemble = monolith",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#case-studies",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#case-studies",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "7. Case Studies",
    "text": "7. Case Studies\n\n7.1 Startup vs Enterprise\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    STARTUP (&lt; 50 personnes, &lt; $1M ARR)                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   CONTRAINTES :                          ARCHITECTURE RECOMMANDÃ‰E :         â”‚\nâ”‚   â€¢ Budget limitÃ©                        â€¢ Managed services (BigQuery,      â”‚\nâ”‚   â€¢ Ã‰quipe petite (1-3 DE)                 Snowflake, Fivetran)             â”‚\nâ”‚   â€¢ Time-to-market critique              â€¢ Pas de Kubernetes                â”‚\nâ”‚   â€¢ Besoins qui changent vite            â€¢ PostgreSQL &gt; Data Lake           â”‚\nâ”‚                                          â€¢ dbt Cloud &gt; dbt Core             â”‚\nâ”‚   RÃˆGLE : Minimiser l'ops,               â€¢ Ã‰viter le self-hosted            â”‚\nâ”‚           maximiser la vÃ©locitÃ©                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    SCALE-UP (50-500 personnes, $10M+ ARR)                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   CONTRAINTES :                          ARCHITECTURE RECOMMANDÃ‰E :         â”‚\nâ”‚   â€¢ Volume croissant rapidement          â€¢ Lakehouse (Delta/Iceberg)        â”‚\nâ”‚   â€¢ Ã‰quipe data 5-20 personnes           â€¢ Spark + Kubernetes               â”‚\nâ”‚   â€¢ Besoin de gouvernance                â€¢ Airflow managed ou K8s           â”‚\nâ”‚   â€¢ Multi-Ã©quipes consomment data        â€¢ Data Catalog (DataHub)           â”‚\nâ”‚                                          â€¢ Data Contracts                   â”‚\nâ”‚   RÃˆGLE : Investir dans la plateforme,                                      â”‚\nâ”‚           mais rester pragmatique                                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    ENTERPRISE (500+ personnes)                              â”‚\nâ”‚                                                                             â”‚\nâ”‚   CONTRAINTES :                          ARCHITECTURE RECOMMANDÃ‰E :         â”‚\nâ”‚   â€¢ Legacy systems Ã  intÃ©grer            â€¢ Data Mesh                        â”‚\nâ”‚   â€¢ Compliance stricte (GDPR, SOX)       â€¢ Platform team dÃ©diÃ©              â”‚\nâ”‚   â€¢ Ã‰quipes data 50+ personnes           â€¢ Self-serve analytics             â”‚\nâ”‚   â€¢ Multi-cloud possible                 â€¢ Strong governance                â”‚\nâ”‚   â€¢ Budgets importants                   â€¢ Lineage & cataloging             â”‚\nâ”‚                                          â€¢ Hybrid cloud                     â”‚\nâ”‚   RÃˆGLE : Gouvernance + autonomie                                           â”‚\nâ”‚           des domaines                                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n7.2 Case Study : E-Commerce\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CASE STUDY : E-COMMERCE PLATFORM                         â”‚\nâ”‚                                                                             â”‚\nâ”‚   CONTEXTE :                                                                â”‚\nâ”‚   â€¢ 1M commandes/jour                                                       â”‚\nâ”‚   â€¢ 50M products                                                            â”‚\nâ”‚   â€¢ Besoin : Real-time inventory, recommendations, fraud detection         â”‚\nâ”‚                                                                             â”‚\nâ”‚   ARCHITECTURE :                                                            â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚   â”‚   Sources   â”‚â”€â”€â”€â”€â–¶â”‚  Kafka  â”‚â”€â”€â”€â”€â–¶â”‚   Flink     â”‚â”€â”€â”¬â”€â”€â–¶ Redis (reco)  â”‚\nâ”‚   â”‚ (orders,    â”‚     â”‚         â”‚     â”‚ (streaming) â”‚  â”‚                   â”‚\nâ”‚   â”‚  clicks,    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â–¶ Alerts (fraud)â”‚\nâ”‚   â”‚  inventory) â”‚          â”‚                                               â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Spark     â”‚â”€â”€â”€â”€â–¶â”‚ Delta Lake  â”‚ â”‚\nâ”‚                                       â”‚   (batch)   â”‚     â”‚ (Lakehouse) â”‚ â”‚\nâ”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                                                  â”‚        â”‚\nâ”‚                                                           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚                                                           â”‚  ClickHouse â”‚ â”‚\nâ”‚                                                           â”‚ (dashboards)â”‚ â”‚\nâ”‚                                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                                                            â”‚\nâ”‚   DÃ‰CISIONS CLÃ‰S :                                                         â”‚\nâ”‚   â€¢ Kafka : source of truth pour events                                   â”‚\nâ”‚   â€¢ Flink : real-time pour fraud (&lt; 1s latency)                           â”‚\nâ”‚   â€¢ Spark : batch pour ML training, heavy transformations                 â”‚\nâ”‚   â€¢ ClickHouse : OLAP pour dashboards (vs query Delta direct)             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n7.3 Case Study : FinTech\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CASE STUDY : FINTECH (Trading Platform)                  â”‚\nâ”‚                                                                             â”‚\nâ”‚   CONTEXTE :                                                                â”‚\nâ”‚   â€¢ Compliance stricte (audit, GDPR)                                        â”‚\nâ”‚   â€¢ Low latency critique (&lt; 100ms)                                          â”‚\nâ”‚   â€¢ Data retention 7 ans                                                    â”‚\nâ”‚   â€¢ Exactitude 100% requise                                                 â”‚\nâ”‚                                                                             â”‚\nâ”‚   DÃ‰CISIONS SPÃ‰CIFIQUES :                                                   â”‚\nâ”‚                                                                             â”‚\nâ”‚   1. IMMUTABILITÃ‰                                                           â”‚\nâ”‚      â€¢ Append-only tables (jamais update/delete)                           â”‚\nâ”‚      â€¢ Event sourcing pattern                                               â”‚\nâ”‚      â€¢ Full audit trail                                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   2. EXACTITUDE                                                             â”‚\nâ”‚      â€¢ Decimal types (jamais float pour l'argent !)                        â”‚\nâ”‚      â€¢ Idempotency sur tous les pipelines                                  â”‚\nâ”‚      â€¢ Reconciliation batch quotidienne                                     â”‚\nâ”‚                                                                             â”‚\nâ”‚   3. RETENTION                                                              â”‚\nâ”‚      â€¢ Hot storage : 30 jours (SSD)                                        â”‚\nâ”‚      â€¢ Warm storage : 1 an (HDD)                                           â”‚\nâ”‚      â€¢ Cold storage : 7 ans (S3 Glacier)                                   â”‚\nâ”‚      â€¢ Lifecycle policies automatisÃ©es                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   4. COMPLIANCE                                                             â”‚\nâ”‚      â€¢ Data Catalog obligatoire                                            â”‚\nâ”‚      â€¢ PII tagging automatique                                              â”‚\nâ”‚      â€¢ Access logs + alerts                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#checklist-architecture-review",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#checklist-architecture-review",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "8. Checklist Architecture Review",
    "text": "8. Checklist Architecture Review\nUtilise cette checklist pour valider une architecture avant de lâ€™implÃ©menter :\n\n\nVoir le code\n# Checklist Architecture Review\n\nchecklist = \"\"\"\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ARCHITECTURE REVIEW CHECKLIST\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n## 1. PROBLEM DEFINITION\n- [ ] Le problÃ¨me business est clairement dÃ©fini\n- [ ] Les requirements sont documentÃ©s\n- [ ] Les success criteria sont mesurables\n- [ ] Les contraintes sont identifiÃ©es (budget, temps, Ã©quipe)\n\n## 2. SOLUTION FIT\n- [ ] La solution rÃ©sout le problÃ¨me (pas plus, pas moins)\n- [ ] Les alternatives ont Ã©tÃ© considÃ©rÃ©es\n- [ ] Une solution plus simple a Ã©tÃ© Ã©valuÃ©e\n- [ ] L'ADR est rÃ©digÃ©\n\n## 3. SCALABILITÃ‰\n- [ ] Volume actuel supportÃ©\n- [ ] Croissance 1 an anticipÃ©e\n- [ ] Croissance 3 ans anticipÃ©e\n- [ ] Bottlenecks identifiÃ©s et plan de mitigation\n\n## 4. RELIABILITY\n- [ ] SLA dÃ©fini et rÃ©aliste\n- [ ] Single points of failure identifiÃ©s\n- [ ] Disaster recovery plan\n- [ ] Monitoring et alerting prÃ©vu\n\n## 5. OPÃ‰RABILITÃ‰\n- [ ] L'Ã©quipe a les compÃ©tences nÃ©cessaires\n- [ ] Documentation prÃ©vue\n- [ ] Runbooks pour incidents\n- [ ] On-call rotation dÃ©finie\n\n## 6. COÃ›TS\n- [ ] CoÃ»ts estimÃ©s (storage, compute, licenses)\n- [ ] Budget approuvÃ©\n- [ ] Optimisations identifiÃ©es (spot instances, reserved, etc.)\n- [ ] Alertes coÃ»ts configurÃ©es\n\n## 7. SÃ‰CURITÃ‰\n- [ ] DonnÃ©es sensibles identifiÃ©es\n- [ ] Encryption at rest et in transit\n- [ ] Access control (RBAC)\n- [ ] Audit logging\n\n## 8. Ã‰VOLUTIVITÃ‰\n- [ ] Architecture modulaire\n- [ ] Pas de vendor lock-in critique\n- [ ] Migration possible si besoin\n- [ ] Backward compatibility considÃ©rÃ©e\n\n## VALIDATION FINALE\n- [ ] Review par un pair\n- [ ] Stakeholders informÃ©s\n- [ ] Go/No-Go decision\n\"\"\"\n\nprint(checklist)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#exercices-pratiques",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#exercices-pratiques",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "9. Exercices Pratiques",
    "text": "9. Exercices Pratiques\n\nExercice 1 : Analyse de Requirements\nUne startup de livraison de repas te demande de concevoir leur data platform : - 50K commandes/jour, croissance 20%/mois - Besoin de dashboards temps rÃ©el pour les opÃ©rations - Ã‰quipe : 2 data engineers (expÃ©rience Python/SQL) - Budget : $5K/mois max - Timeline : 2 mois pour le MVP\nQuestions : 1. Quels sont les requirements fonctionnels et non-fonctionnels ? 2. Quelle architecture recommandes-tu ? Pourquoi ? 3. Quels outils spÃ©cifiques ? 4. Quels sont les risques et mitigations ?\n\n\n\nExercice 2 : ADR Writing\nRÃ©dige un ADR complet pour le choix suivant : &gt; â€œUtiliser Airflow plutÃ´t que Prefect pour lâ€™orchestrationâ€\nInclus : context, options considÃ©rÃ©es, dÃ©cision, consÃ©quences.\n\n\n\nExercice 3 : Capacity Planning\nUne plateforme IoT gÃ©nÃ¨re : - 10,000 devices - Chaque device envoie 1 event/seconde - Chaque event = 200 bytes - Retention : 90 jours\nCalcule : 1. Volume quotidien (raw et compressÃ©) 2. Volume total avec retention 3. CoÃ»t S3 mensuel estimÃ© 4. Sizing Kafka (partitions recommandÃ©es)\n\n\n\nExercice 4 : Anti-Pattern Detection\nAnalyse cette proposition dâ€™architecture et identifie les anti-patterns :\n\nâ€œPour notre app interne avec 50 utilisateurs, je propose : - Kubernetes cluster avec 10 nodes - Kafka avec 100 partitions - Flink pour le streaming - Cassandra pour le stockage - Elasticsearch pour le search - Custom ML pipeline from scratchâ€\n\n\n\n\nExercice 5 : Case Study Analysis\nCompare les architectures data de : 1. Uber (https://eng.uber.com/) 2. Airbnb (https://medium.com/airbnb-engineering) 3. Spotify (https://engineering.atspotify.com/)\nPour chaque : - Quels sont leurs principaux dÃ©fis data ? - Quelles solutions ont-ils adoptÃ©es ? - Quâ€™est-ce qui est applicable Ã  une entreprise plus petite ?",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#ressources",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#ressources",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nArticles & Blogs\n\nDesigning Data-Intensive Applications â€” Martin Kleppmann (THE book)\nArchitecture of Open Source Applications\nHigh Scalability Blog\nNetflix Tech Blog\n\n\n\nADR Resources\n\nADR GitHub\nDocumenting Architecture Decisions\n\n\n\nArchitecture Patterns\n\nLambda Architecture\nKappa Architecture\nData Mesh Principles",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/34_architecture_patterns_decisions.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/34_architecture_patterns_decisions.html#prochaine-Ã©tape",
    "title": "ğŸ›ï¸ Data Architecture Patterns & Decision Making",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 35_leadership_tradeoffs â€” Leadership & Trade-offs in Data Engineering\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises maintenant les patterns dâ€™architecture et la prise de dÃ©cision.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "ğŸ›ï¸ Architecture & Governance",
      "34 Â· Patterns & DÃ©cisions d'Architecture"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html",
    "href": "notebooks/advanced/29_distributed_messaging.html",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module avancÃ© oÃ¹ tu vas maÃ®triser les systÃ¨mes de messaging distribuÃ©s. Tu apprendras les fonctionnalitÃ©s avancÃ©es de Kafka, les alternatives comme RabbitMQ et Pulsar, et le Change Data Capture avec Debezium â€” des compÃ©tences essentielles pour construire des architectures data temps rÃ©el !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#prÃ©requis",
    "href": "notebooks/advanced/29_distributed_messaging.html#prÃ©requis",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 24_kafka_streaming (Kafka, Spark SSS, Watermarks)\n\n\nâœ… Requis\nMaÃ®triser topics, partitions, offsets, consumer groups\n\n\nâœ… Requis\nConnaÃ®tre kafka-python et confluent-kafka (producers/consumers)\n\n\nâœ… Requis\nMaÃ®triser Spark Structured Streaming (readStream, writeStream, foreachBatch)\n\n\nâœ… Requis\nConnaissances en Docker et Kubernetes (M14-M16, M27)\n\n\nâœ… Requis\nBases de donnÃ©es relationnelles et SQL (pour Debezium CDC)\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec des pipelines streaming en production",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#objectifs-du-module",
    "href": "notebooks/advanced/29_distributed_messaging.html#objectifs-du-module",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nConfigurer les fonctionnalitÃ©s avancÃ©es de Kafka (Quotas, Tiered Storage, Transactions)\nComprendre et choisir entre Kafka, RabbitMQ et Pulsar\nImplÃ©menter le Change Data Capture complet avec Debezium\nConcevoir des architectures de messaging robustes\nGÃ©rer les patterns avancÃ©s : exactly-once, dead letter queues, event sourcing",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#rappel-ce-quon-a-vu-en-m24-vs-ce-quon-approfondit-ici",
    "href": "notebooks/advanced/29_distributed_messaging.html#rappel-ce-quon-a-vu-en-m24-vs-ce-quon-approfondit-ici",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "Rappel : Ce quâ€™on a vu en M24 vs Ce quâ€™on approfondit ici",
    "text": "Rappel : Ce quâ€™on a vu en M24 vs Ce quâ€™on approfondit ici\n\n\n\n\n\n\n\nModule M24 (Intermediate)\nCe module M29 (Advanced)\n\n\n\n\nArchitecture Lambda vs Kappa\nâ€”\n\n\nTopics, Partitions, Offsets, Consumer Groups\nTiered Storage, Quotas, Monitoring\n\n\nProducers / Consumers (kafka-python, confluent-kafka)\nTransactions Kafka, Exactly-Once (EOS)\n\n\nSchema Registry basics (Avro)\nSchema Registry avancÃ© (compatibilitÃ©, Ã©volution)\n\n\nSpark Structured Streaming complet\nâ€”\n\n\nWindowing, Watermarks, foreachBatch\nâ€”\n\n\nFaust (aperÃ§u)\nâ€”\n\n\nDebezium (mentionnÃ©)\nDebezium CDC en profondeur\n\n\nâ€”\nRabbitMQ (alternative queue-based)\n\n\nâ€”\nApache Pulsar (alternative multi-tenant)\n\n\nâ€”\nPatterns : DLQ, Saga, Event Sourcing, CQRS\n\n\n\n\nSchÃ©ma : Ã‰cosystÃ¨me Messaging\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    DISTRIBUTED MESSAGING LANDSCAPE                          â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚\nâ”‚   â”‚     KAFKA       â”‚   â”‚   RABBITMQ      â”‚   â”‚     PULSAR      â”‚          â”‚\nâ”‚   â”‚  Log-based      â”‚   â”‚  Queue-based    â”‚   â”‚  Multi-tenant   â”‚          â”‚\nâ”‚   â”‚  High throughputâ”‚   â”‚  Flexible routingâ”‚  â”‚  Geo-replicationâ”‚          â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚\nâ”‚            â”‚                    â”‚                     â”‚                     â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                 â”‚                                           â”‚\nâ”‚                                 â–¼                                           â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                        DEBEZIUM (CDC)                                â”‚  â”‚\nâ”‚   â”‚   Capture changes from databases â†’ Stream to messaging systems      â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ„¹ï¸ Le savais-tu ?\nKafka traite plus de 7 trillions de messages par jour chez LinkedIn, son crÃ©ateur.\nRabbitMQ a Ã©tÃ© crÃ©Ã© en 2007 et implÃ©mente le protocole AMQP, un standard ouvert pour le messaging.\nApache Pulsar a Ã©tÃ© dÃ©veloppÃ© par Yahoo! pour gÃ©rer leurs 100 milliards de messages quotidiens avec une architecture multi-tenant native.\nDebezium (du latin â€œfrom the beginningâ€) capture chaque changement depuis le dÃ©but du log de la base de donnÃ©es â€” câ€™est la base du Change Data Capture.\nğŸ“– Kafka at LinkedIn",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#kafka-avancÃ©",
    "href": "notebooks/advanced/29_distributed_messaging.html#kafka-avancÃ©",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "1. Kafka AvancÃ©",
    "text": "1. Kafka AvancÃ©\nCette section couvre les fonctionnalitÃ©s avancÃ©es de Kafka pour la production Ã  grande Ã©chelle. Tu connais dÃ©jÃ  les bases (M24), on passe directement aux sujets avancÃ©s.\n\n1.1 Quotas et Throttling\nLes quotas permettent de limiter les ressources consommÃ©es par les clients pour Ã©viter quâ€™un client ne monopolise le cluster.\n\nTypes de quotas\n\n\n\n\n\n\n\n\nQuota\nDescription\nUnitÃ©\n\n\n\n\nproducer_byte_rate\nDÃ©bit max en Ã©criture\nbytes/sec\n\n\nconsumer_byte_rate\nDÃ©bit max en lecture\nbytes/sec\n\n\nrequest_percentage\n% CPU du broker\n%\n\n\ncontroller_mutation_rate\nTaux de mutations (create/delete)\nmutations/sec\n\n\n\n\n\nConfigurer les quotas\n# Quota par user\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --alter --add-config 'producer_byte_rate=1048576,consumer_byte_rate=2097152' \\\n  --entity-type users --entity-name data-pipeline-user\n\n# Quota par client-id\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --alter --add-config 'producer_byte_rate=5242880' \\\n  --entity-type clients --entity-name etl-producer\n\n# Quota par user + client-id (plus spÃ©cifique)\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --alter --add-config 'producer_byte_rate=10485760' \\\n  --entity-type users --entity-name spark-user \\\n  --entity-type clients --entity-name spark-producer\n\n# Quota par dÃ©faut pour tous les users\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --alter --add-config 'producer_byte_rate=1048576' \\\n  --entity-type users --entity-default\n\n# Voir les quotas configurÃ©s\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --describe --entity-type users --entity-name data-pipeline-user\n\n\nQuotas dans le code Python\nfrom confluent_kafka import Producer\n\n# Le client doit spÃ©cifier son client.id pour Ãªtre identifiÃ© par les quotas\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'client.id': 'etl-producer',  # Identifiant pour les quotas\n    'acks': 'all',\n})\n\n# Si le quota est dÃ©passÃ©, Kafka throttle automatiquement le client\n# Le producer recevra des dÃ©lais dans les rÃ©ponses\n\n\n\n1.2 Tiered Storage (KIP-405)\nLe Tiered Storage permet de stocker les donnÃ©es anciennes sur un stockage moins cher (S3, GCS, Azure Blob) tout en gardant les donnÃ©es rÃ©centes sur disque local.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         TIERED STORAGE                                      â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                    LOCAL TIER (Hot Data)                             â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                           â”‚  â”‚\nâ”‚   â”‚   â”‚Seg 5â”‚ â”‚Seg 6â”‚ â”‚Seg 7â”‚ â”‚Seg 8â”‚ â”‚Seg 9â”‚  â† DonnÃ©es rÃ©centes      â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    (SSD local, rapide)    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚                                    â”‚                                        â”‚\nâ”‚                        Offload automatique                                  â”‚\nâ”‚                                    â–¼                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚                   REMOTE TIER (Cold Data)                            â”‚  â”‚\nâ”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”                                   â”‚  â”‚\nâ”‚   â”‚   â”‚Seg 1â”‚ â”‚Seg 2â”‚ â”‚Seg 3â”‚ â”‚Seg 4â”‚  â† DonnÃ©es anciennes             â”‚  â”‚\nâ”‚   â”‚   â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜    (S3/GCS/Azure, Ã©conomique)    â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nConfiguration Tiered Storage\n# server.properties (broker)\n\n# Activer le tiered storage\nremote.log.storage.system.enable=true\n\n# Plugin de stockage (exemple S3)\nremote.log.storage.manager.class.name=org.apache.kafka.tiered.storage.s3.S3RemoteStorageManager\nremote.log.storage.manager.class.path=/opt/kafka/plugins/tiered-storage-s3.jar\n\n# Configuration S3\nremote.log.storage.s3.bucket=my-kafka-tiered-storage\nremote.log.storage.s3.region=eu-west-1\n\n# RÃ©tention locale (donnÃ©es chaudes)\nlocal.retention.ms=86400000  # 1 jour en local\n\n# RÃ©tention totale (incluant remote)\nretention.ms=2592000000  # 30 jours au total\n# Activer le tiered storage sur un topic existant\nkafka-configs.sh --bootstrap-server localhost:9092 \\\n  --alter --entity-type topics --entity-name events \\\n  --add-config 'remote.storage.enable=true,local.retention.ms=86400000,retention.ms=2592000000'\n\n\nAvantages du Tiered Storage\n\n\n\nAvantage\nDescription\n\n\n\n\nCoÃ»t rÃ©duit\nStockage S3 ~10x moins cher que SSD\n\n\nRÃ©tention illimitÃ©e\nGarder des annÃ©es de donnÃ©es\n\n\nCluster plus petit\nMoins de disque local nÃ©cessaire\n\n\nReplay facilitÃ©\nRelire des donnÃ©es anciennes pour reprocessing\n\n\n\n\n\n\n1.3 Transactions et Exactly-Once Semantics (EOS)\nEn M24, tu as vu les garanties de livraison (at-most-once, at-least-once, exactly-once). Ici, on va implÃ©menter exactly-once avec les transactions Kafka.\n\nRappel des garanties\n\n\n\n\n\n\n\n\nNiveau\nDescription\nRisque\n\n\n\n\nAt-most-once\nFire & forget\nPerte de messages\n\n\nAt-least-once\nRetry jusquâ€™Ã  ACK\nDoublons possibles\n\n\nExactly-once\nTransactions + idempotence\nAucun (mais plus complexe)\n\n\n\n\n\nProducer Idempotent (pas de doublons)\nLâ€™idempotence garantit quâ€™un message nâ€™est Ã©crit quâ€™une seule fois mÃªme en cas de retry rÃ©seau.\nfrom confluent_kafka import Producer\n\n# Producer idempotent â€” PAS de duplicatas mÃªme avec retries\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'enable.idempotence': True,  # â† Active l'idempotence\n    'acks': 'all',               # Requis pour idempotence\n    'retries': 2147483647,       # Retries infinis (best practice)\n    'max.in.flight.requests.per.connection': 5,  # Max 5 avec idempotence\n})\n\n# Comment Ã§a marche ?\n# 1. Le producer assigne un Producer ID (PID) et un Sequence Number Ã  chaque message\n# 2. Le broker dÃ©tecte les doublons en comparant (PID, Sequence)\n# 3. Si un retry envoie le mÃªme message, le broker le reconnaÃ®t et ignore le doublon\n\n\nTransactions complÃ¨tes (multi-topics atomique)\nLes transactions permettent dâ€™Ã©crire sur plusieurs topics/partitions de maniÃ¨re atomique : tout rÃ©ussit ou tout Ã©choue.\nfrom confluent_kafka import Producer, KafkaException\n\n# Producer transactionnel\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'transactional.id': 'etl-pipeline-001',  # ID unique et STABLE\n    'enable.idempotence': True,              # Implicitement activÃ©\n    'acks': 'all',\n})\n\n# Initialiser les transactions (une seule fois au dÃ©marrage)\nproducer.init_transactions()\n\ntry:\n    # DÃ©marrer une transaction\n    producer.begin_transaction()\n    \n    # Ã‰crire sur PLUSIEURS topics (atomique)\n    producer.produce('orders-processed', key='order-1', value='{\"status\": \"done\"}')\n    producer.produce('audit-log', key='order-1', value='{\"action\": \"order_processed\"}')\n    producer.produce('metrics', key='counter', value='{\"orders_processed\": 1}')\n    \n    # Commit la transaction â€” TOUT ou RIEN\n    producer.commit_transaction()\n    print(\"âœ… Transaction committed successfully\")\n    \nexcept KafkaException as e:\n    # Abort en cas d'erreur â€” aucun message n'est visible\n    producer.abort_transaction()\n    print(f\"âŒ Transaction aborted: {e}\")\n\n\nPattern Read-Process-Write (Exactly-Once complet)\nLe pattern le plus puissant : lire, traiter, Ã©crire, et commiter les offsets dans la mÃªme transaction.\nfrom confluent_kafka import Consumer, Producer, KafkaException\n\n# Consumer avec isolation transactionnelle\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'etl-group',\n    'isolation.level': 'read_committed',  # â† Ne lit que les messages committÃ©s\n    'enable.auto.commit': False,          # â† Commit manuel dans la transaction\n})\n\nproducer = Producer({\n    'bootstrap.servers': 'localhost:9092',\n    'transactional.id': 'etl-processor-001',\n    'enable.idempotence': True,\n})\n\nproducer.init_transactions()\nconsumer.subscribe(['raw-events'])\n\nwhile True:\n    msg = consumer.poll(1.0)\n    if msg is None:\n        continue\n    if msg.error():\n        continue\n    \n    try:\n        # 1. DÃ©marrer la transaction\n        producer.begin_transaction()\n        \n        # 2. Traiter le message\n        processed = process_message(msg.value())\n        \n        # 3. Ã‰crire le rÃ©sultat\n        producer.produce('processed-events', value=processed)\n        \n        # 4. Commit les offsets DANS la transaction\n        producer.send_offsets_to_transaction(\n            consumer.position(consumer.assignment()),\n            consumer.consumer_group_metadata()\n        )\n        \n        # 5. Commit atomique : Ã©criture + offset ensemble\n        producer.commit_transaction()\n        \n    except Exception as e:\n        producer.abort_transaction()\n        print(f\"Transaction failed: {e}\")\n\n\n\n1.4 Schema Registry AvancÃ©\nEn M24, tu as vu les bases du Schema Registry avec Avro. Approfondissons les modes de compatibilitÃ© et lâ€™Ã©volution de schÃ©mas.\n\nModes de compatibilitÃ© dÃ©taillÃ©s\n\n\n\n\n\n\n\n\n\nMode\nNouveau consumer lit ancien\nAncien consumer lit nouveau\nChangements autorisÃ©s\n\n\n\n\nBACKWARD\nâœ… Oui\nâŒ Non\nAjouter champs optionnels, supprimer champs\n\n\nFORWARD\nâŒ Non\nâœ… Oui\nAjouter champs, supprimer champs optionnels\n\n\nFULL\nâœ… Oui\nâœ… Oui\nAjouter/supprimer champs optionnels uniquement\n\n\nNONE\nâ€”\nâ€”\nTout (âš ï¸ dangereux en production)\n\n\n\n# Configurer la compatibilitÃ© globale\ncurl -X PUT http://localhost:8081/config \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"compatibility\": \"BACKWARD\"}'\n\n# Configurer par sujet (override global)\ncurl -X PUT http://localhost:8081/config/orders-value \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"compatibility\": \"FULL\"}'\n\n# Tester la compatibilitÃ© AVANT de publier un nouveau schÃ©ma\ncurl -X POST http://localhost:8081/compatibility/subjects/orders-value/versions/latest \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"schema\": \"{...}\"}'\n# RÃ©ponse: {\"is_compatible\": true} ou {\"is_compatible\": false}\n\n\nÃ‰volution de schÃ©ma â€” Exemple pratique\n# Version 1 du schÃ©ma\nschema_v1 = '''\n{\n  \"type\": \"record\",\n  \"name\": \"Order\",\n  \"fields\": [\n    {\"name\": \"order_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"}\n  ]\n}\n'''\n\n# Version 2 â€” Ajouter un champ optionnel (BACKWARD compatible)\nschema_v2 = '''\n{\n  \"type\": \"record\",\n  \"name\": \"Order\",\n  \"fields\": [\n    {\"name\": \"order_id\", \"type\": \"string\"},\n    {\"name\": \"amount\", \"type\": \"double\"},\n    {\"name\": \"currency\", \"type\": \"string\", \"default\": \"EUR\"}\n  ]\n}\n'''\n# âœ… Les nouveaux consumers peuvent lire les anciens messages (currency = \"EUR\" par dÃ©faut)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#rabbitmq-lalternative-queue-based",
    "href": "notebooks/advanced/29_distributed_messaging.html#rabbitmq-lalternative-queue-based",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "2. RabbitMQ â€” Lâ€™Alternative Queue-Based",
    "text": "2. RabbitMQ â€” Lâ€™Alternative Queue-Based\n\nRabbitMQ est un message broker traditionnel implÃ©mentant le protocole AMQP. Contrairement Ã  Kafka (log-based), RabbitMQ est queue-based avec un routage flexible.\n\n\nKafka vs RabbitMQ â€” Comparaison dÃ©taillÃ©e\n\n\n\n\n\n\n\n\nAspect\nKafka\nRabbitMQ\n\n\n\n\nModÃ¨le\nLog distribuÃ© (append-only)\nMessage queue (FIFO)\n\n\nPersistance\nToujours sur disque\nOptionnelle (mÃ©moire ou disque)\n\n\nOrdre\nGaranti par partition\nGaranti par queue\n\n\nReplay\nâœ… Natif (offsets)\nâŒ Messages supprimÃ©s aprÃ¨s ACK\n\n\nRoutage\nTopics + Partitions\nExchanges (fanout, direct, topic, headers)\n\n\nThroughput\nTrÃ¨s Ã©levÃ© (millions/sec)\nÃ‰levÃ© (dizaines de milliers/sec)\n\n\nLatence\nMillisecondes\nSub-milliseconde\n\n\nUse case principal\nEvent streaming, analytics\nTask queues, RPC, notifications\n\n\n\n\n\nArchitecture RabbitMQ\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         RABBITMQ ARCHITECTURE                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   Producer â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚\nâ”‚                    â”‚                                                        â”‚\nâ”‚                    â–¼                                                        â”‚\nâ”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚\nâ”‚            â”‚   EXCHANGE    â”‚  â† Routing logic (type: fanout/direct/topic)  â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚\nâ”‚                    â”‚ Bindings (routing rules)                               â”‚\nâ”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\nâ”‚        â”‚           â”‚           â”‚                                            â”‚\nâ”‚        â–¼           â–¼           â–¼                                            â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\nâ”‚   â”‚ Queue 1 â”‚ â”‚ Queue 2 â”‚ â”‚ Queue 3 â”‚  â† Messages stockÃ©s ici              â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                       â”‚\nâ”‚        â”‚           â”‚           â”‚                                            â”‚\nâ”‚        â–¼           â–¼           â–¼                                            â”‚\nâ”‚   Consumer 1   Consumer 2   Consumer 3                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nTypes dâ€™Exchanges\n\n\n\n\n\n\n\n\nExchange\nRoutage\nUse case\n\n\n\n\nDirect\nrouting_key exact match\nLogs par niveau (errorâ†’error_queue)\n\n\nFanout\nBroadcast Ã  toutes les queues liÃ©es\nNotifications, cache invalidation\n\n\nTopic\nPattern matching (*.error, logs.#)\nLogs multi-critÃ¨res (app.module.level)\n\n\nHeaders\nMatch sur headers du message\nRoutage complexe multi-attributs\n\n\n\n\n\nInstallation\n# Docker\ndocker run -d --name rabbitmq \\\n  -p 5672:5672 \\\n  -p 15672:15672 \\\n  rabbitmq:3-management\n\n# Management UI: http://localhost:15672 (guest/guest)\n\n\nProducer Python (pika)\nimport pika\nimport json\n\n# Connexion\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\n\n# DÃ©clarer un exchange de type topic\nchannel.exchange_declare(exchange='data_events', exchange_type='topic', durable=True)\n\n# Publier un message\nmessage = {'event_type': 'order_created', 'order_id': 'ORD-001', 'amount': 99.99}\n\nchannel.basic_publish(\n    exchange='data_events',\n    routing_key='orders.created',\n    body=json.dumps(message),\n    properties=pika.BasicProperties(delivery_mode=2, content_type='application/json')\n)\n\nconnection.close()\n\n\nConsumer Python\nimport pika\nimport json\n\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\n\nchannel.queue_declare(queue='order_processor', durable=True)\nchannel.queue_bind(exchange='data_events', queue='order_processor', routing_key='orders.*')\n\ndef callback(ch, method, properties, body):\n    message = json.loads(body)\n    print(f\"Received: {message}\")\n    ch.basic_ack(delivery_tag=method.delivery_tag)\n\nchannel.basic_qos(prefetch_count=1)\nchannel.basic_consume(queue='order_processor', on_message_callback=callback)\nchannel.start_consuming()\n\n\nQuand utiliser RabbitMQ vs Kafka ?\n\n\n\nâœ… RabbitMQ\nâœ… Kafka\n\n\n\n\nTask queues (jobs async)\nEvent streaming temps rÃ©el\n\n\nRPC (request/reply)\nLog aggregation\n\n\nRoutage complexe (exchanges)\nReplay de donnÃ©es historiques\n\n\nFaible latence critique (&lt;1ms)\nTrÃ¨s haut throughput (millions/sec)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#apache-pulsar-le-challenger-multi-tenant",
    "href": "notebooks/advanced/29_distributed_messaging.html#apache-pulsar-le-challenger-multi-tenant",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "3. Apache Pulsar â€” Le Challenger Multi-Tenant",
    "text": "3. Apache Pulsar â€” Le Challenger Multi-Tenant\n\nApache Pulsar combine les avantages de Kafka (log-based) et RabbitMQ (queuing) avec une architecture cloud-native.\n\n\nKafka vs Pulsar\n\n\n\n\n\n\n\n\nAspect\nKafka\nPulsar\n\n\n\n\nArchitecture\nBrokers = Storage + Compute\nSÃ©paration Brokers / BookKeeper\n\n\nMulti-tenancy\nLimitÃ©\nNatif (tenants, namespaces)\n\n\nGeo-replication\nMirrorMaker (externe)\nNatif et synchrone\n\n\nQueuing\nNon natif\nNatif (shared subscriptions)\n\n\n\n\n\nTypes de Subscriptions Pulsar\n\n\n\nType\nDescription\nÃ‰quivalent\n\n\n\n\nExclusive\n1 seul consumer\nKafka standard\n\n\nFailover\nFailover automatique\nâ€”\n\n\nShared\nLoad balanced (round-robin)\nRabbitMQ\n\n\nKey_Shared\nOrdre par clÃ©\nKafka partitions\n\n\n\n\n\nProducer/Consumer Python\nimport pulsar\nimport json\n\nclient = pulsar.Client('pulsar://localhost:6650')\n\n# Producer\nproducer = client.create_producer('persistent://public/default/orders')\nproducer.send(json.dumps({'order_id': 'ORD-001'}).encode('utf-8'))\n\n# Consumer (shared = load balanced)\nconsumer = client.subscribe(\n    'persistent://public/default/orders',\n    subscription_name='order-processor',\n    consumer_type=pulsar.ConsumerType.Shared\n)\n\nmsg = consumer.receive()\nprint(json.loads(msg.data()))\nconsumer.acknowledge(msg)\nclient.close()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#debezium-change-data-capture-en-profondeur",
    "href": "notebooks/advanced/29_distributed_messaging.html#debezium-change-data-capture-en-profondeur",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "4. Debezium â€” Change Data Capture en Profondeur",
    "text": "4. Debezium â€” Change Data Capture en Profondeur\n\nDebezium est une plateforme open-source de Change Data Capture (CDC) qui capture les changements dans les bases de donnÃ©es et les streame vers Kafka.\n\nEn M24, Debezium Ã©tait mentionnÃ© dans le quiz. Ici, on lâ€™implÃ©mente en profondeur.\n\nPourquoi le CDC ?\n\n\n\nApproche traditionnelle\nCDC avec Debezium\n\n\n\n\nBatch ETL (SELECT * toutes les heures)\nStreaming temps rÃ©el\n\n\nQuery la DB source (charge CPU/IO)\nLit le transaction log (lÃ©ger)\n\n\nDÃ©tection des DELETEs difficile\nCapture TOUS les changements\n\n\nLatence Ã©levÃ©e (heures)\nLatence sub-seconde\n\n\n\n\n\nArchitecture Debezium\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         DEBEZIUM ARCHITECTURE                               â”‚\nâ”‚                                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                           â”‚\nâ”‚   â”‚  PostgreSQL â”‚     Transaction Log (WAL)                                 â”‚\nâ”‚   â”‚   (source)  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚                                        â”‚\nâ”‚                                    â–¼                                        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚    MySQL    â”‚â”€â”€â”€â–¶â”‚   KAFKA CONNECT     â”‚â”€â”€â”€â–¶â”‚   KAFKA TOPICS      â”‚    â”‚\nâ”‚   â”‚   (source)  â”‚    â”‚  + Debezium         â”‚    â”‚  (change events)    â”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    Connectors       â”‚    â”‚                     â”‚    â”‚\nâ”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â€¢ dbserver.schema  â”‚    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚    .table           â”‚    â”‚\nâ”‚   â”‚   MongoDB   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\nâ”‚   â”‚   (source)  â”‚                                         â”‚                â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                         â–¼                â”‚\nâ”‚                                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚                                                â”‚    Consumers        â”‚     â”‚\nâ”‚                                                â”‚  â€¢ Data Warehouse   â”‚     â”‚\nâ”‚                                                â”‚  â€¢ Elasticsearch    â”‚     â”‚\nâ”‚                                                â”‚  â€¢ Microservices    â”‚     â”‚\nâ”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nConnecteurs Debezium\n\n\n\nBase de donnÃ©es\nMÃ©thode de capture\nMaturitÃ©\n\n\n\n\nPostgreSQL\nLogical replication (pgoutput)\nâ­â­â­â­â­\n\n\nMySQL/MariaDB\nBinary log (binlog)\nâ­â­â­â­â­\n\n\nMongoDB\nOplog / Change Streams\nâ­â­â­â­â­\n\n\nSQL Server\nCDC tables\nâ­â­â­â­\n\n\nOracle\nLogMiner\nâ­â­â­â­\n\n\n\n\n\nDocker Compose complet\nversion: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.5.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  kafka:\n    image: confluentinc/cp-kafka:7.5.0\n    depends_on: [zookeeper]\n    ports: [\"9092:9092\"]\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n\n  postgres:\n    image: postgres:15\n    ports: [\"5432:5432\"]\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_DB: source_db\n    command: [\"postgres\", \"-c\", \"wal_level=logical\"]  # CRUCIAL\n\n  connect:\n    image: debezium/connect:2.5\n    depends_on: [kafka, postgres]\n    ports: [\"8083:8083\"]\n    environment:\n      BOOTSTRAP_SERVERS: kafka:29092\n      GROUP_ID: debezium-connect\n      CONFIG_STORAGE_TOPIC: connect_configs\n      OFFSET_STORAGE_TOPIC: connect_offsets\n      STATUS_STORAGE_TOPIC: connect_statuses\n\n\nEnregistrer le connecteur\ncurl -X POST http://localhost:8083/connectors \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"postgres-connector\",\n    \"config\": {\n      \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n      \"database.hostname\": \"postgres\",\n      \"database.port\": \"5432\",\n      \"database.user\": \"postgres\",\n      \"database.password\": \"postgres\",\n      \"database.dbname\": \"source_db\",\n      \"topic.prefix\": \"cdc\",\n      \"table.include.list\": \"public.orders\",\n      \"plugin.name\": \"pgoutput\",\n      \"slot.name\": \"debezium_slot\",\n      \"transforms\": \"unwrap\",\n      \"transforms.unwrap.type\": \"io.debezium.transforms.ExtractNewRecordState\",\n      \"transforms.unwrap.drop.tombstones\": \"false\",\n      \"transforms.unwrap.delete.handling.mode\": \"rewrite\",\n      \"transforms.unwrap.add.fields\": \"op,source.ts_ms\"\n    }\n  }'\n\n\nFormat des messages Debezium\n\n\n\n\n\n\n\nChamp\nDescription\n\n\n\n\nbefore\nÃ‰tat AVANT le changement (null pour INSERT)\n\n\nafter\nÃ‰tat APRÃˆS le changement (null pour DELETE)\n\n\nop\nOpÃ©ration : c=create, u=update, d=delete, r=read (snapshot)\n\n\nsource\nMÃ©tadonnÃ©es (table, transaction ID, LSN)\n\n\n\n\n\nConsumer Python CDC\nfrom confluent_kafka import Consumer\nimport json\n\nconsumer = Consumer({\n    'bootstrap.servers': 'localhost:9092',\n    'group.id': 'cdc-processor',\n    'auto.offset.reset': 'earliest',\n})\nconsumer.subscribe(['cdc.public.orders'])\n\nwhile True:\n    msg = consumer.poll(1.0)\n    if msg is None:\n        continue\n    \n    event = json.loads(msg.value())\n    op = event.get('__op')\n    data = {k: v for k, v in event.items() if not k.startswith('__')}\n    \n    if op in ('c', 'r'):  # INSERT ou SNAPSHOT\n        print(f\"INSERT: {data}\")\n    elif op == 'u':  # UPDATE\n        print(f\"UPDATE: {data}\")\n    elif op == 'd':  # DELETE\n        print(f\"DELETE: id={data.get('id')}\")\n\n\nOutbox Pattern\nLe Outbox Pattern garantit la cohÃ©rence entre les mises Ã  jour DB et lâ€™envoi dâ€™Ã©vÃ©nements.\n-- Table outbox\nCREATE TABLE outbox (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    aggregate_type VARCHAR(255) NOT NULL,\n    aggregate_id VARCHAR(255) NOT NULL,\n    event_type VARCHAR(255) NOT NULL,\n    payload JSONB NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Dans une TRANSACTION\nBEGIN;\n    UPDATE orders SET status = 'shipped' WHERE id = 1;\n    INSERT INTO outbox (aggregate_type, aggregate_id, event_type, payload)\n    VALUES ('Order', '1', 'OrderShipped', '{\"order_id\": 1}');\nCOMMIT;\n-- Debezium capture l'INSERT dans outbox â†’ Kafka\n\n\nUse Cases CDC\n\n\n\n\n\n\n\nUse Case\nDescription\n\n\n\n\nSync Data Warehouse\nRÃ©plication temps rÃ©el vers Snowflake, BigQuery\n\n\nCache invalidation\nInvalider Redis quand la DB change\n\n\nSearch indexing\nSync vers Elasticsearch\n\n\nMicroservices events\nOutbox pattern\n\n\nAudit log\nCompliance, RGPD",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#patterns-de-messaging-distribuÃ©",
    "href": "notebooks/advanced/29_distributed_messaging.html#patterns-de-messaging-distribuÃ©",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "5. Patterns de Messaging DistribuÃ©",
    "text": "5. Patterns de Messaging DistribuÃ©\n\n5.1 Dead Letter Queue (DLQ)\nMAX_RETRIES = 3\n\nwhile True:\n    msg = consumer.poll(1.0)\n    headers = dict(msg.headers() or [])\n    retry_count = int(headers.get('retry_count', b'0'))\n    \n    try:\n        process_message(msg.value())\n        consumer.commit(msg)\n    except Exception as e:\n        if retry_count &gt;= MAX_RETRIES:\n            # Envoyer vers DLQ\n            producer.produce('orders-dlq', key=msg.key(), value=msg.value(),\n                           headers=[('error', str(e))])\n        else:\n            # Retry\n            producer.produce('orders-retry', key=msg.key(), value=msg.value(),\n                           headers=[('retry_count', str(retry_count + 1))])\n        consumer.commit(msg)\n\n\n5.2 Saga Pattern\nCreate Order â”€â”€â–¶ Reserve Stock â”€â”€â–¶ Process Payment â”€â”€â–¶ Ship Order\n      â”‚               â”‚                  â”‚\n      â–¼               â–¼                  â–¼\nCancel Order â—€â”€â”€ Release Stock â—€â”€â”€ Refund Payment  (compensation)\n\n\n5.3 Event Sourcing\nevents = [\n    {'type': 'OrderCreated', 'order_id': 1, 'amount': 100},\n    {'type': 'PaymentReceived', 'order_id': 1},\n    {'type': 'OrderShipped', 'order_id': 1},\n]\n\ndef rebuild_state(events):\n    state = {}\n    for e in events:\n        if e['type'] == 'OrderCreated':\n            state = {'id': e['order_id'], 'status': 'created'}\n        elif e['type'] == 'PaymentReceived':\n            state['status'] = 'paid'\n        elif e['type'] == 'OrderShipped':\n            state['status'] = 'shipped'\n    return state\n\n\n5.4 CQRS\nCommands â”€â”€â–¶ Events (Kafka) â”€â”€â–¶ Projector â”€â”€â–¶ Read Model (optimized)\n   â”‚                                              â”‚\n   â–¼                                              â–¼\nWrite DB                                      Query API\n(PostgreSQL)                                  (Elasticsearch)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#exercices-pratiques",
    "href": "notebooks/advanced/29_distributed_messaging.html#exercices-pratiques",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "6. Exercices Pratiques",
    "text": "6. Exercices Pratiques\n\nExercice 1 : Kafka Transactions\nImplÃ©menter un producer transactionnel qui Ã©crit sur 2 topics atomiquement.\n\n\nExercice 2 : RabbitMQ Task Queue\nCrÃ©er une task queue avec prioritÃ©s et DLQ.\n\n\nExercice 3 : Pipeline CDC Complet\nDÃ©ployer PostgreSQL + Kafka + Debezium et sync vers un data warehouse.\n\n\nExercice 4 : Comparatif Performance\nComparer Kafka vs RabbitMQ (100K messages, throughput, latence).\n\n\nExercice 5 : Outbox Pattern\nImplÃ©menter le pattern Outbox avec Debezium.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#ressources",
    "href": "notebooks/advanced/29_distributed_messaging.html#ressources",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nApache Kafka Docs\nRabbitMQ Docs\nApache Pulsar Docs\nDebezium Docs\nKafka: The Definitive Guide â€” Neha Narkhede\nDesigning Data-Intensive Applications â€” Martin Kleppmann",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/advanced/29_distributed_messaging.html#prochaine-Ã©tape",
    "href": "notebooks/advanced/29_distributed_messaging.html#prochaine-Ã©tape",
    "title": "ğŸ“¨ Distributed Messaging pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nğŸ‘‰ Module suivant : 30_spark_scala_deep_dive â€” Spark & Scala Deep Dive\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Distributed Messaging.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¥ Niveau 3 : AvancÃ©",
      "â˜¸ï¸ Infrastructure AvancÃ©e",
      "29 Â· Messaging DistribuÃ© (Kafka, Pulsar)"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prÃ©requis",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nAvoir suivi le module 07_sql_for_data_engineers",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#objectifs-du-module",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nComprendre ce quâ€™est le Big Data et ses caractÃ©ristiques (5V)\nExpliquer pourquoi le traitement distribuÃ© est nÃ©cessaire\nDÃ©crire lâ€™architecture Hadoop (HDFS, MapReduce, YARN)\nComprendre le modÃ¨le MapReduce\nExpliquer pourquoi Spark a remplacÃ© MapReduce\nDiffÃ©rencier Data Lake et Data Lakehouse\nComprendre les architectures Lambda et Kappa\nConnaÃ®tre le concept de Data Mesh\nComprendre lâ€™architecture Medallion (Bronze/Silver/Gold)\nComprendre le NoSQL et ses diffÃ©rents types\nSavoir quand utiliser SQL vs NoSQL\nConnaÃ®tre le thÃ©orÃ¨me CAP\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique viendra avec MongoDB (module suivant) et PySpark !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#cest-quoi-le-big-data",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "1. Câ€™est quoi le Big Data ?",
    "text": "1. Câ€™est quoi le Big Data ?\n\nDÃ©finition\nLe Big Data dÃ©signe des ensembles de donnÃ©es tellement volumineux et complexes quâ€™ils ne peuvent pas Ãªtre traitÃ©s par des outils traditionnels (Excel, bases SQL classiques, un seul serveur).\n\n\nOrigine\nLe terme a Ã©mergÃ© dans les annÃ©es 2000 avec lâ€™explosion :\n\nDâ€™Internet et des rÃ©seaux sociaux\nDes smartphones\nDes capteurs IoT\nDes transactions en ligne\n\n\n\nOrdres de grandeur\n1 Ko  (Kilooctet)   = 1 page de texte\n1 Mo  (MÃ©gaoctet)   = 1 photo HD\n1 Go  (Gigaoctet)   = 1 film HD\n1 To  (TÃ©raoctet)   = 1 000 films HD\n1 Po  (PÃ©taoctet)   = 1 000 To = 1 million de Go\n1 Eo  (Exaoctet)    = 1 000 Po\n1 Zo  (Zettaoctet)  = 1 000 Eo\n\n\nExemples concrets\n\n\n\nEntreprise\nVolume de donnÃ©es\n\n\n\n\nFacebook\n~4 Po gÃ©nÃ©rÃ©s par jour\n\n\nGoogle\n~20 Po traitÃ©s par jour\n\n\nNetflix\n~60 Po de vidÃ©os stockÃ©es\n\n\nCERN (LHC)\n~1 Po par seconde pendant les expÃ©riences",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-5v-du-big-data",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "2. Les 5V du Big Data",
    "text": "2. Les 5V du Big Data\nLes caractÃ©ristiques du Big Data sont souvent rÃ©sumÃ©es par les 5V :\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   VOLUME    â”‚\n                    â”‚  (quantitÃ©) â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â”‚                  â”‚                  â”‚\n        â–¼                  â–¼                  â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   VELOCITY    â”‚  â”‚   VARIETY     â”‚  â”‚   VERACITY    â”‚\nâ”‚   (vitesse)   â”‚  â”‚  (diversitÃ©)  â”‚  â”‚  (fiabilitÃ©)  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                           â”‚\n                           â–¼\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚    VALUE    â”‚\n                    â”‚  (valeur)   â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVolume â€” La quantitÃ© massive de donnÃ©es\n\n\n\n\n\n\n\nDÃ©fi\nSolution\n\n\n\n\nImpossible de stocker sur un seul disque\nStockage distribuÃ© (HDFS, S3)\n\n\nImpossible de charger en RAM\nTraitement par partitions\n\n\n\n\n\n\nVelocity â€” La vitesse de gÃ©nÃ©ration et traitement\n\n\n\nType\nExemple\nLatence\n\n\n\n\nBatch\nRapport mensuel\nHeures\n\n\nNear real-time\nDashboard\nMinutes\n\n\nReal-time / Streaming\nDÃ©tection de fraude\nMillisecondes\n\n\n\n\n\n\nVariety â€” La diversitÃ© des formats\n\nğŸ’¡ Rappel : Tu as dÃ©jÃ  vu Ã§a dans le module 06 !\n\n\n\n\n\n\n\n\n\nType\nFormat\nExemple\n\n\n\n\nStructurÃ©\nTables, colonnes fixes\nSQL, CSV\n\n\nSemi-structurÃ©\nSchÃ©ma flexible\nJSON, XML (MongoDB, Elasticsearch)\n\n\nNon-structurÃ©\nPas de schÃ©ma\nImages, vidÃ©os, texte libre\n\n\n\n\n\n\nVeracity â€” La fiabilitÃ© des donnÃ©es\n\n\n\nProblÃ¨me\nImpact\n\n\n\n\nDonnÃ©es manquantes\nRÃ©sultats biaisÃ©s\n\n\nDoublons\nComptages faux\n\n\nErreurs de saisie\nMauvaises dÃ©cisions\n\n\nDonnÃ©es obsolÃ¨tes\nAnalyses non pertinentes\n\n\n\n\nğŸ’¡ Câ€™est lÃ  que le Data Engineer intervient : nettoyer, valider, transformer !\n\n\n\n\nValue â€” La valeur extraite\nLes donnÃ©es nâ€™ont de valeur que si on peut en tirer des insights : - PrÃ©dictions (ML) - Dashboards - Alertes - Optimisation business.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-le-traitement-distribuÃ©",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "3. Pourquoi le traitement distribuÃ© ?",
    "text": "3. Pourquoi le traitement distribuÃ© ?\n\nLimites dâ€™une machine unique\nImaginons que tu dois traiter 10 To de logs :\n\n\n\nRessource\nLimite typique\nProblÃ¨me\n\n\n\n\nRAM\n64-256 Go\n10 To ne tient pas en mÃ©moire\n\n\nCPU\n8-64 cÅ“urs\nTraitement sÃ©quentiel = trop lent\n\n\nDisque\n500 Mo/s lecture\n10 To = 5+ heures juste pour lire\n\n\nRÃ©seau\nGoulot dâ€™Ã©tranglement\nTransfÃ©rer 10 To = des heures\n\n\n\n\n\n\nScale-Up vs Scale-Out\nSCALE-UP (vertical)              SCALE-OUT (horizontal)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”\n   â”‚             â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚   MEGA      â”‚               â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚ â”‚ S â”‚\n   â”‚  SERVEUR    â”‚      vs       â”‚ 1 â”‚ â”‚ 2 â”‚ â”‚ 3 â”‚ â”‚ 4 â”‚\n   â”‚   ğŸ’ªğŸ’ªğŸ’ª    â”‚               â”‚   â”‚ â”‚   â”‚ â”‚   â”‚ â”‚   â”‚\n   â”‚             â”‚               â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               \n                                 Cluster de serveurs\n   + Plus de RAM                 + Moins cher (commodity)\n   + Plus de CPU                 + ScalabilitÃ© infinie\n   - TrÃ¨s cher $$$               + TolÃ©rance aux pannes\n   - Limite physique             - Plus complexe\n\n\nLe Big Data utilise le Scale-Out !\nAu lieu dâ€™une machine surpuissante, on utilise un cluster de machines ordinaires.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#concepts-clÃ©s-du-traitement-distribuÃ©",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "4. Concepts clÃ©s du traitement distribuÃ©",
    "text": "4. Concepts clÃ©s du traitement distribuÃ©\n\nParallÃ©lisme vs Distribution\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nParallÃ©lisme\nPlusieurs tÃ¢ches en mÃªme temps sur une machine\nMulti-threading\n\n\nDistribution\nTÃ¢ches rÃ©parties sur plusieurs machines\nCluster Hadoop/Spark\n\n\n\n\n\n\nData Locality â€” â€œAmener le code aux donnÃ©esâ€\nâŒ MAUVAIS : DÃ©placer les donnÃ©es vers le code\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â•â•10 Toâ•â•â–º    â”‚   Serveur 2    â”‚\nâ”‚   (donnÃ©es)    â”‚   rÃ©seau       â”‚    (calcul)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   lent !       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… BON : DÃ©placer le code vers les donnÃ©es\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Serveur 1    â”‚  â—„â•â•codeâ•â•     â”‚   Serveur 2    â”‚\nâ”‚ donnÃ©es+calcul â”‚   (petit)      â”‚    (master)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Câ€™est le principe fondamental de Hadoop et Spark !\n\n\n\n\nFault Tolerance â€” TolÃ©rance aux pannes\nDans un cluster de 1000 machines, des pannes arrivent tous les jours !\n\n\n\n\n\n\n\nStratÃ©gie\nDescription\n\n\n\n\nRÃ©plication\nCopier les donnÃ©es sur plusieurs nÅ“uds (HDFS : 3 copies)\n\n\nCheckpointing\nSauvegarder lâ€™Ã©tat intermÃ©diaire\n\n\nLineage\nRecalculer les donnÃ©es perdues (Spark RDD)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#lÃ©cosystÃ¨me-hadoop",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "5. Lâ€™Ã©cosystÃ¨me Hadoop",
    "text": "5. Lâ€™Ã©cosystÃ¨me Hadoop\nHadoop est un framework open-source crÃ©Ã© par Yahoo (2006), inspirÃ© des papiers de Google (GFS, MapReduce).\n\nğŸ—ï¸ Architecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    Ã‰COSYSTÃˆME HADOOP                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚   â”‚  Hive   â”‚  â”‚   Pig   â”‚  â”‚  HBase  â”‚  â”‚  Sqoop  â”‚  ...  â”‚\nâ”‚   â”‚  (SQL)  â”‚  â”‚(scripts)â”‚  â”‚ (NoSQL) â”‚  â”‚ (import)â”‚       â”‚\nâ”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚\nâ”‚        â”‚            â”‚            â”‚            â”‚             â”‚\nâ”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚  MapReduce  â”‚  â—„â”€â”€ Traitement         â”‚\nâ”‚                    â”‚  (calcul)   â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    YARN     â”‚  â—„â”€â”€ Ressources         â”‚\nâ”‚                    â”‚ (scheduler) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                           â”‚                                 â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                         â”‚\nâ”‚                    â”‚    HDFS     â”‚  â—„â”€â”€ Stockage           â”‚\nâ”‚                    â”‚  (fichiers) â”‚                         â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nHDFS â€” Hadoop Distributed File System\nSystÃ¨me de fichiers distribuÃ© qui stocke les donnÃ©es sur plusieurs machines.\nFichier original : data.csv (300 Mo)\n                      â”‚\n        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n        â–¼             â–¼             â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ Block 1 â”‚   â”‚ Block 2 â”‚   â”‚ Block 3 â”‚   (128 Mo chacun)\n   â”‚ 128 Mo  â”‚   â”‚ 128 Mo  â”‚   â”‚  44 Mo  â”‚\n   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n        â”‚             â”‚             â”‚\n   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x   RÃ©pliquÃ© 3x\n\n\n\nCaractÃ©ristique\nValeur par dÃ©faut\n\n\n\n\nTaille de bloc\n128 Mo\n\n\nFacteur de rÃ©plication\n3\n\n\nType dâ€™accÃ¨s\nWrite once, read many\n\n\n\n\n\n\nYARN â€” Yet Another Resource Negotiator\nGestionnaire de ressources du cluster :\n\nAlloue CPU/RAM aux applications\nGÃ¨re la file dâ€™attente des jobs\nSurveille lâ€™Ã©tat des nÅ“uds.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#mapreduce-le-modÃ¨le-de-traitement",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "6. MapReduce â€” Le modÃ¨le de traitement",
    "text": "6. MapReduce â€” Le modÃ¨le de traitement\nMapReduce est un modÃ¨le de programmation pour traiter de grandes quantitÃ©s de donnÃ©es en parallÃ¨le.\n\nLes 3 Ã©tapes\n\n\n\nÃ‰tape\nAction\nParallÃ©lisme\n\n\n\n\nMap\nTransformer chaque Ã©lÃ©ment\nâœ… ParallÃ¨le\n\n\nShuffle\nRegrouper par clÃ©\nâš ï¸ RÃ©seau\n\n\nReduce\nAgrÃ©ger les valeurs\nâœ… ParallÃ¨le\n\n\n\n\n\n\nExemple : Word Count\nCompter les occurrences de chaque mot dans un texte.\nENTRÃ‰E : \"hello world hello\"\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                              MAP\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  \"hello world hello\"   \n         â”‚\n         â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  (hello, 1)  (world, 1)  (hello, 1)  â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            SHUFFLE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  Regrouper par clÃ© (mot) :\n  \n  hello â†’ [1, 1]\n  world â†’ [1]\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n                            REDUCE\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n  hello â†’ sum([1, 1]) â†’ 2\n  world â†’ sum([1])    â†’ 1\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nSORTIE : { \"hello\": 2, \"world\": 1 }\n\n\n\nMapReduce sur un cluster\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚   MASTER    â”‚\n                         â”‚  (Driver)   â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â”‚                    â”‚                    â”‚\n           â–¼                    â–¼                    â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚   NODE 1    â”‚      â”‚   NODE 2    â”‚      â”‚   NODE 3    â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚      â”‚ DonnÃ©es:    â”‚\n    â”‚ \"hello\"     â”‚      â”‚ \"world\"     â”‚      â”‚ \"hello\"     â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ MAP:        â”‚      â”‚ MAP:        â”‚      â”‚ MAP:        â”‚\n    â”‚ (hello, 1)  â”‚      â”‚ (world, 1)  â”‚      â”‚ (hello, 1)  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n           â”‚                    â”‚                    â”‚\n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n                         SHUFFLE (rÃ©seau)\n                                â”‚\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n           â–¼                                         â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚  REDUCER 1  â”‚                           â”‚  REDUCER 2  â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ hello: [1,1]â”‚                           â”‚ world: [1]  â”‚\n    â”‚ â†’ hello: 2  â”‚                           â”‚ â†’ world: 1  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Chaque nÅ“ud traite ses donnÃ©es localement (data locality) !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#limites-de-mapreduce",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "7. Limites de MapReduce",
    "text": "7. Limites de MapReduce\n\nProblÃ¨mes de MapReduce\n\n\n\n\n\n\n\n\nProblÃ¨me\nCause\nImpact\n\n\n\n\nLent\nÃ‰crit sur disque entre chaque Ã©tape\nI/O intensif\n\n\nVerbose\nCode Java complexe\nProductivitÃ© basse\n\n\nBatch only\nPas de streaming\nPas de temps rÃ©el\n\n\nPas de cache\nRelit les donnÃ©es Ã  chaque job\nItÃ©rations lentes (ML)\n\n\n\n\n\nLe problÃ¨me du disque\nMapReduce : DISQUE â†’ Map â†’ DISQUE â†’ Shuffle â†’ DISQUE â†’ Reduce â†’ DISQUE\n                 â†‘           â†‘              â†‘              â†‘\n                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              LENT ! (I/O disque)\n\nSpark :     DISQUE â†’ Map â†’ MÃ‰MOIRE â†’ Shuffle â†’ MÃ‰MOIRE â†’ Reduce\n                            â†‘                    â†‘\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              RAPIDE ! (in-memory)\n\n\n\nExemple : Algorithme itÃ©ratif (ML)\nPour un algorithme qui fait 10 itÃ©rations sur les mÃªmes donnÃ©es :\n\n\n\nFramework\nComportement\nTemps\n\n\n\n\nMapReduce\nRelit les donnÃ©es du disque 10 fois\nğŸ˜“\n\n\nSpark\nGarde les donnÃ©es en mÃ©moire, itÃ¨re 10 fois\nâš¡",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#spark-lÃ©volution",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "8. Spark â€” Lâ€™Ã©volution",
    "text": "8. Spark â€” Lâ€™Ã©volution\nApache Spark (2014) a Ã©tÃ© crÃ©Ã© pour rÃ©soudre les limitations de MapReduce.\n\nComparaison Hadoop MapReduce vs Spark\n\n\n\nCritÃ¨re\nHadoop MapReduce\nApache Spark\n\n\n\n\nVitesse\nLent (disque)\n100x plus rapide (mÃ©moire)\n\n\nFacilitÃ©\nJava verbeux\nPython, Scala, SQL\n\n\nTraitement\nBatch only\nBatch + Streaming\n\n\nItÃ©rations\nLent (relit le disque)\nRapide (cache en RAM)\n\n\nÃ‰cosystÃ¨me\nHive, Pig, etc.\nSpark SQL, MLlib, GraphX\n\n\nStockage\nHDFS\nHDFS, S3, Cassandra, etc.\n\n\n\n\n\n\nArchitecture Spark\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      APACHE SPARK                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚   â”‚ Spark SQL â”‚ â”‚ Streaming â”‚ â”‚   MLlib   â”‚ â”‚  GraphX   â”‚  â”‚\nâ”‚   â”‚  (SQL)    â”‚ â”‚ (temps    â”‚ â”‚   (ML)    â”‚ â”‚ (graphes) â”‚  â”‚\nâ”‚   â”‚           â”‚ â”‚   rÃ©el)   â”‚ â”‚           â”‚ â”‚           â”‚  â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚         â”‚             â”‚             â”‚             â”‚        â”‚\nâ”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”‚\nâ”‚                       â”‚ Spark Core  â”‚                      â”‚\nâ”‚                       â”‚   (RDD)     â”‚                      â”‚\nâ”‚                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â”‚\nâ”‚                              â”‚                              â”‚\nâ”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚         â–¼                    â–¼                    â–¼        â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚   â”‚   YARN   â”‚        â”‚   Mesos  â”‚        â”‚Standaloneâ”‚    â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Spark peut fonctionner sur YARN (cluster Hadoop existant) ou en mode standalone.\n\n\n\n\nConcepts Spark Ã  venir\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nRDD\nResilient Distributed Dataset â€” collection distribuÃ©e\n\n\nDataFrame\nComme un tableau avec colonnes (similaire Ã  Pandas)\n\n\nTransformation\nOpÃ©ration lazy (map, filter, groupBy)\n\n\nAction\nDÃ©clenche le calcul (collect, count, show)\n\n\nLazy Evaluation\nRien ne sâ€™exÃ©cute tant quâ€™une action nâ€™est pas appelÃ©e\n\n\nPartition\nMorceau de donnÃ©es traitÃ© par un worker\n\n\n\n\n\n\nCe que tu vas apprendre avec PySpark\n\nCrÃ©er et manipuler des DataFrames distribuÃ©s\nÃ‰crire des transformations SQL-like\nLire/Ã©crire des fichiers (CSV, Parquet, JSON)\nOptimiser les performances\nConstruire des pipelines de donnÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-stockage-modernes",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-stockage-modernes",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "9. Architectures de Stockage Modernes",
    "text": "9. Architectures de Stockage Modernes\nMaintenant que tu connais le Big Data et le traitement distribuÃ©, voyons comment organiser et stocker ces donnÃ©es.\n\nğŸ’¡ Rappel : Tu as vu Data Warehouse et Data Mart dans le module 06. Ici, on complÃ¨te avec Data Lake et Lakehouse !\n\n\n\n9.1 Data Lake â€” Le lac de donnÃ©es\nUn Data Lake est un systÃ¨me de stockage qui contient une grande quantitÃ© de donnÃ©es brutes dans leur format natif.\n\nDÃ©finition\n\nâ€œUn Data Lake est un rÃ©fÃ©rentiel centralisÃ© qui permet de stocker toutes les donnÃ©es structurÃ©es et non structurÃ©es Ã  nâ€™importe quelle Ã©chelle.â€\n\n\n\nCaractÃ©ristiques\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nSchema-on-read\nPas de schÃ©ma dÃ©fini Ã  lâ€™Ã©criture, appliquÃ© Ã  la lecture\n\n\nTous formats\nStructurÃ©, semi-structurÃ©, non-structurÃ©\n\n\nStockage brut\nDonnÃ©es dans leur format original\n\n\nScalabilitÃ©\nPÃ©taoctets de donnÃ©es\n\n\nCoÃ»t faible\nStockage object (S3, ADLS, GCS)\n\n\n\n\n\nExemples de Data Lakes\n\n\n\nProduit\nProvider\nStockage\n\n\n\n\nAmazon S3\nAWS\nObject storage\n\n\nAzure Data Lake Storage (ADLS)\nAzure\nObject storage\n\n\nGoogle Cloud Storage (GCS)\nGCP\nObject storage\n\n\nHDFS\nOn-premise\nDistributed filesystem\n\n\n\n\n\nLe piÃ¨ge : Data Swamp (MarÃ©cage de donnÃ©es)\nâŒ DATA SWAMP â€” Ce qu'il faut Ã©viter :\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       DATA LAKE                                 â”‚\nâ”‚                                                                 â”‚\nâ”‚   ğŸ“ old_data/                  ğŸ“ test_final_v2_FINAL/         â”‚\nâ”‚   ğŸ“ backup_2019/               ğŸ“ john_analysis/               â”‚\nâ”‚   ğŸ“„ data.csv                   ğŸ“„ data_new.csv                 â”‚\nâ”‚   ğŸ“„ data_copy.csv              ğŸ“„ ???.parquet                  â”‚\nâ”‚   ğŸ“ temp/                      ğŸ“ DO_NOT_DELETE/               â”‚\nâ”‚                                                                 â”‚\nâ”‚   ğŸ˜± Personne ne sait ce que contiennent ces fichiers !        â”‚\nâ”‚   ğŸ˜± Pas de documentation                                       â”‚\nâ”‚   ğŸ˜± DonnÃ©es dupliquÃ©es, obsolÃ¨tes, invalides                   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Sans gouvernance, un Data Lake devient un Data Swamp inutilisable !\n\n\n\n\n\n9.2 Data Lakehouse â€” Le meilleur des deux mondes\nUn Data Lakehouse combine les avantages du Data Lake (flexibilitÃ©, coÃ»t) et du Data Warehouse (performance, ACID).\n\nDÃ©finition\n\nâ€œUn Lakehouse est une nouvelle architecture qui combine les meilleurs Ã©lÃ©ments des Data Lakes et des Data Warehouses.â€\n\n\n\nTechnologies clÃ©s\n\n\n\nTechnologie\nDescription\nCrÃ©ateur\n\n\n\n\nDelta Lake\nFormat de table ACID sur Data Lake\nDatabricks\n\n\nApache Iceberg\nFormat de table open source\nNetflix\n\n\nApache Hudi\nFormat de table pour upserts\nUber\n\n\n\n\n\nCe que le Lakehouse apporte\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      DATA LAKEHOUSE                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚      Transactions ACID (comme un Data Warehouse)                â”‚\nâ”‚      Schema enforcement & evolution                             â”‚\nâ”‚      Time Travel (historique des versions)                      â”‚\nâ”‚      Stockage sur object storage (comme un Data Lake)           â”‚\nâ”‚      Support batch ET streaming                                 â”‚\nâ”‚      Format ouvert (Parquet + mÃ©tadonnÃ©es)                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n9.3 Comparaison : Warehouse vs Lake vs Lakehouse\n\n\n\nCritÃ¨re\nData Warehouse\nData Lake\nData Lakehouse\n\n\n\n\nDonnÃ©es\nStructurÃ©es\nToutes\nToutes\n\n\nSchÃ©ma\nSchema-on-write\nSchema-on-read\nSchema-on-write\n\n\nACID\nâœ… Oui\nâŒ Non\nâœ… Oui\n\n\nCoÃ»t\nğŸ’°ğŸ’°ğŸ’° Ã‰levÃ©\nğŸ’° Faible\nğŸ’° Faible\n\n\nPerformance\nâš¡ Haute\nâš ï¸ Variable\nâš¡ Haute\n\n\nUse cases\nBI, Reporting\nML, Data Science\nBI + ML\n\n\nTime Travel\nâš ï¸ LimitÃ©\nâŒ Non\nâœ… Oui\n\n\nExemples\nSnowflake, Redshift\nS3, ADLS\nDelta Lake, Iceberg\n\n\n\nÃ‰volution historique :\n\n    1990s-2000s           2010s              2020s+\n         â”‚                  â”‚                  â”‚\n         â–¼                  â–¼                  â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚    DATA     â”‚    â”‚    DATA     â”‚    â”‚    DATA     â”‚\n  â”‚  WAREHOUSE  â”‚ â†’  â”‚    LAKE     â”‚ â†’  â”‚  LAKEHOUSE  â”‚\n  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚\n  â”‚ StructurÃ©   â”‚    â”‚ Tout format â”‚    â”‚ Best of     â”‚\n  â”‚ CoÃ»teux     â”‚    â”‚ Pas d'ACID  â”‚    â”‚ both worlds â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ”® Ã€ venir : Tu apprendras Delta Lake et Iceberg en dÃ©tail dans le module intermÃ©diaire !\n\n\n\n\n9.4 Medallion Architecture â€” Bronze, Silver, Gold\nLa Medallion Architecture (ou architecture en mÃ©daillons) est un pattern dâ€™organisation des donnÃ©es dans un Data Lakehouse. Elle structure les donnÃ©es en 3 couches de qualitÃ© croissante.\n\nğŸ’¡ Cette architecture est trÃ¨s populaire avec Delta Lake et Databricks, mais sâ€™applique Ã  tout Lakehouse.\n\n\n\nVue dâ€™ensemble\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     MEDALLION ARCHITECTURE                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚   SOURCES              BRONZE            SILVER            GOLD         â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€         â”‚\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚   API   â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  Raw    â”‚       â”‚ Cleaned â”‚       â”‚ Curated â”‚    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  Data   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚  Data   â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚  Data   â”‚    â”‚\nâ”‚  â”‚   DB    â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  ğŸ¥‰     â”‚       â”‚  ğŸ¥ˆ     â”‚       â”‚  ğŸ¥‡     â”‚    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚ BRONZE  â”‚       â”‚ SILVER  â”‚       â”‚  GOLD   â”‚    â”‚\nâ”‚  â”‚  Files  â”‚â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚         â”‚       â”‚         â”‚       â”‚         â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚        â”‚\nâ”‚  â”‚ Streams â”‚â”€â”€â”€â”€â”€â”€â”€â–º        ...                               â–¼        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚                                                        â”‚ Dashboard â”‚   â”‚\nâ”‚                                                        â”‚ ML Models â”‚   â”‚\nâ”‚                                                        â”‚ Reports   â”‚   â”‚\nâ”‚                                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nCouche Bronze â€” DonnÃ©es brutes\nLa couche Bronze contient les donnÃ©es exactement comme elles arrivent des sources.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nDonnÃ©es brutes, non transformÃ©es\n\n\nFormat\nCopie exacte des sources (JSON, CSV, logsâ€¦)\n\n\nQualitÃ©\nAucun nettoyage, peut contenir des erreurs\n\n\nBut\nHistorique complet, traÃ§abilitÃ©, replay\n\n\nRÃ©tention\nLongue durÃ©e (mois/annÃ©es)\n\n\n\nğŸ¥‰ BRONZE â€” Exemple de donnÃ©es brutes\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ {                                                              â”‚\nâ”‚   \"event_id\": \"abc123\",                                       â”‚\nâ”‚   \"timestamp\": \"2024-01-15T14:30:00Z\",                        â”‚\nâ”‚   \"user_id\": \"usr_456\",                                       â”‚\nâ”‚   \"action\": \"purchase\",                                       â”‚\nâ”‚   \"amount\": \"99.99\",      â† String au lieu de number         â”‚\nâ”‚   \"product\": null,         â† Valeur manquante                 â”‚\nâ”‚   \"_ingested_at\": \"2024-01-15T14:30:05Z\"  â† MÃ©tadonnÃ©e ajoutÃ©eâ”‚\nâ”‚ }                                                              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ¥ˆ Couche Silver â€” DonnÃ©es nettoyÃ©es\nLa couche Silver contient les donnÃ©es nettoyÃ©es, validÃ©es et conformÃ©es.\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nDonnÃ©es nettoyÃ©es et validÃ©es\n\n\nTransformations\nTypes corrigÃ©s, doublons supprimÃ©s, nulls gÃ©rÃ©s\n\n\nQualitÃ©\nDonnÃ©es fiables, schÃ©ma appliquÃ©\n\n\nBut\nSource de vÃ©ritÃ© pour les analyses\n\n\nStructure\nTables normalisÃ©es ou semi-normalisÃ©es\n\n\n\nğŸ¥ˆ SILVER â€” DonnÃ©es nettoyÃ©es\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Table: silver_events                                          â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚ â”‚ event_id  â”‚     timestamp      â”‚ user_id  â”‚ action  â”‚amountâ”‚â”‚\nâ”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤â”‚\nâ”‚ â”‚ abc123    â”‚ 2024-01-15 14:30:00â”‚ usr_456  â”‚purchase â”‚ 99.99â”‚â”‚\nâ”‚ â”‚ def789    â”‚ 2024-01-15 14:31:00â”‚ usr_789  â”‚ view    â”‚  0.00â”‚â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚                                                                â”‚\nâ”‚ âœ… Types corrects (DECIMAL pour amount)                       â”‚\nâ”‚ âœ… Pas de doublons                                            â”‚\nâ”‚ âœ… Valeurs nulles remplacÃ©es par dÃ©fauts                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ¥‡ Couche Gold â€” DonnÃ©es agrÃ©gÃ©es pour le business\nLa couche Gold contient les donnÃ©es agrÃ©gÃ©es et optimisÃ©es pour des cas dâ€™usage mÃ©tier spÃ©cifiques.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nContenu\nAgrÃ©gations, KPIs, mÃ©triques business\n\n\nStructure\nTables dÃ©normalisÃ©es (Star Schema)\n\n\nQualitÃ©\nDonnÃ©es prÃªtes pour dashboards et ML\n\n\nBut\nConsommation directe par les analystes\n\n\nPerformance\nOptimisÃ©es pour les requÃªtes\n\n\n\nğŸ¥‡ GOLD â€” DonnÃ©es agrÃ©gÃ©es\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Table: gold_daily_sales                                       â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚\nâ”‚ â”‚    date    â”‚   region    â”‚ category  â”‚total_salesâ”‚num_ordersâ”‚â”‚\nâ”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   France    â”‚Electronicsâ”‚ 125,430.50â”‚    1,245â”‚â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   Germany   â”‚Electronicsâ”‚  98,200.00â”‚      987â”‚â”‚\nâ”‚ â”‚ 2024-01-15 â”‚   France    â”‚  Fashion  â”‚  45,670.00â”‚      567â”‚â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\nâ”‚                                                                â”‚\nâ”‚ â†’ PrÃªt pour Power BI, Tableau, Looker                         â”‚\nâ”‚ â†’ RequÃªtes ultra-rapides                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nRÃ©sumÃ© des 3 couches\n\n\n\n\n\n\n\n\n\n\nCouche\nQualitÃ©\nTransformations\nUtilisateurs\nFormat typique\n\n\n\n\nğŸ¥‰ Bronze\nBrute\nAucune\nData Engineers\nJSON, CSV, Parquet brut\n\n\nğŸ¥ˆ Silver\nNettoyÃ©e\nCleaning, validation, typing\nData Engineers, Analysts\nDelta Lake, Iceberg\n\n\nğŸ¥‡ Gold\nBusiness-ready\nAgrÃ©gations, dÃ©normalisation\nAnalysts, Dashboard, ML\nTables Star Schema\n\n\n\n\n\n\nAvantages de Medallion Architecture\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nTraÃ§abilitÃ©\nOn peut toujours revenir aux donnÃ©es brutes (Bronze)\n\n\nQualitÃ© progressive\nChaque couche amÃ©liore la qualitÃ©\n\n\nRÃ©utilisabilitÃ©\nSilver sert de source commune Ã  plusieurs tables Gold\n\n\nDebugging\nFacile de trouver oÃ¹ une erreur sâ€™est introduite\n\n\nPerformance\nGold optimisÃ© pour les requÃªtes analytiques\n\n\n\n\n\n\nFlux de donnÃ©es typique\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         PIPELINE ETL/ELT                                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                          â”‚\nâ”‚   SOURCE          BRONZE              SILVER              GOLD           â”‚\nâ”‚      â”‚               â”‚                   â”‚                  â”‚            â”‚\nâ”‚      â”‚   Ingestion   â”‚    Cleaning       â”‚   Aggregation    â”‚            â”‚\nâ”‚      â”‚   (raw copy)  â”‚    Validation     â”‚   Denormalizationâ”‚            â”‚\nâ”‚      â”‚               â”‚    Deduplication  â”‚   Business rules â”‚            â”‚\nâ”‚      â–¼               â–¼                   â–¼                  â–¼            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚  API  â”‚â”€â”€â”€â”€â”€â–ºâ”‚ bronze_â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ silver_  â”‚â”€â”€â”€â”€â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚  â”‚ logs  â”‚      â”‚ events â”‚         â”‚ events   â”‚   â”Œâ”€â”€â–ºâ”‚ daily_   â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ metrics  â”‚      â”‚\nâ”‚                                         â”‚        â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚  DB   â”‚â”€â”€â”€â”€â”€â–ºâ”‚ bronze_â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ silver_  â”‚â”€â”€â”€â”¼â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚  â”‚ users â”‚      â”‚ users  â”‚         â”‚ users    â”‚   â”‚   â”‚ user_    â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚ segments â”‚      â”‚\nâ”‚                                                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚                                                   â””â”€â”€â–ºâ”‚ gold_    â”‚      â”‚\nâ”‚                                                       â”‚ funnel   â”‚      â”‚\nâ”‚                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ”® Ã€ venir : Tu implÃ©menteras cette architecture avec Delta Lake dans le module intermÃ©diaire !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-traitement-lambda-kappa",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#architectures-de-traitement-lambda-kappa",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "10. Architectures de Traitement â€” Lambda & Kappa",
    "text": "10. Architectures de Traitement â€” Lambda & Kappa\nComment organiser le traitement des donnÃ©es ? Deux grandes architectures dominent.\n\n\n10.1 Lambda Architecture â€” Batch + Streaming\nLâ€™architecture Lambda (Nathan Marz, 2011) combine traitement batch et streaming pour avoir le meilleur des deux.\n\nStructure\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚            DATA SOURCES             â”‚\n                         â”‚     (Events, Logs, Transactions)    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚                                             â”‚\n                    â–¼                                             â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚    BATCH LAYER      â”‚                      â”‚    SPEED LAYER      â”‚\n         â”‚   (Traitement lot)  â”‚                      â”‚   (Temps rÃ©el)      â”‚\n         â”‚                     â”‚                      â”‚                     â”‚\n         â”‚  â€¢ Hadoop/Spark     â”‚                      â”‚  â€¢ Kafka Streams    â”‚\n         â”‚  â€¢ DonnÃ©es complÃ¨tesâ”‚                      â”‚  â€¢ Spark Streaming  â”‚\n         â”‚  â€¢ Haute prÃ©cision  â”‚                      â”‚  â€¢ Flink            â”‚\n         â”‚  â€¢ Latence: heures  â”‚                      â”‚  â€¢ Latence: secondesâ”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                    â”‚                                             â”‚\n                    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n                    â”‚     â”‚     SERVING LAYER       â”‚             â”‚\n                    â””â”€â”€â”€â”€â–ºâ”‚  (Couche de service)    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                          â”‚                         â”‚\n                          â”‚  â€¢ Combine les deux     â”‚\n                          â”‚  â€¢ RequÃªtes rapides     â”‚\n                          â”‚  â€¢ Vue unifiÃ©e          â”‚\n                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                      â”‚\n                                      â–¼\n                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                          â”‚      APPLICATIONS       â”‚\n                          â”‚   Dashboards, APIs, ML  â”‚\n                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLes 3 couches\n\n\n\n\n\n\n\n\nCouche\nRÃ´le\nCaractÃ©ristiques\n\n\n\n\nBatch Layer\nTraitement complet, prÃ©cis\nLatence haute, donnÃ©es historiques\n\n\nSpeed Layer\nTraitement temps rÃ©el\nLatence basse, donnÃ©es rÃ©centes\n\n\nServing Layer\nCombine et sert les donnÃ©es\nRequÃªtes rapides, vue unifiÃ©e\n\n\n\n\n\nAvantages / InconvÃ©nients\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nRobuste (double traitement)\nComplexe (2 systÃ¨mes Ã  maintenir)\n\n\nDonnÃ©es historiques complÃ¨tes\nCode dupliquÃ© (batch + streaming)\n\n\nFaible latence possible\nCoÃ»t Ã©levÃ© (2 pipelines)\n\n\n\n\n\n\n\n10.2 Kappa Architecture â€” Streaming-first\nLâ€™architecture Kappa (Jay Kreps, 2014) simplifie Lambda en utilisant uniquement du streaming.\n\nStructure\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚            DATA SOURCES             â”‚\n                         â”‚     (Events, Logs, Transactions)    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚       MESSAGE QUEUE / LOG           â”‚\n                         â”‚           (Apache Kafka)            â”‚\n                         â”‚                                     â”‚\n                         â”‚   â€¢ Stocke TOUS les Ã©vÃ©nements      â”‚\n                         â”‚   â€¢ RÃ©tention longue (jours/mois)   â”‚\n                         â”‚   â€¢ Replay possible                 â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚        STREAM PROCESSING            â”‚\n                         â”‚    (Kafka Streams, Flink, Spark)    â”‚\n                         â”‚                                     â”‚\n                         â”‚   â€¢ Une seule codebase              â”‚\n                         â”‚   â€¢ Traite events en temps rÃ©el     â”‚\n                         â”‚   â€¢ Reprocessing = replay du log    â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                           â”‚\n                                           â–¼\n                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                         â”‚         SERVING LAYER               â”‚\n                         â”‚   (Base de donnÃ©es, Cache, API)     â”‚\n                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLe concept clÃ© : Replay\nBesoin de recalculer les donnÃ©es ?\n\nLambda : Relance le batch job (heures)\nKappa  : Rejoue le log Kafka (mÃªme vitesse que le streaming)\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Kafka Log (rÃ©tention 30 jours)                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚ E1 â”‚ E2 â”‚ E3 â”‚ E4 â”‚ E5 â”‚ E6 â”‚ E7 â”‚ E8 â”‚ E9 â”‚E10 â”‚ ...   â”‚\nâ”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜        â”‚\nâ”‚                              â–²                               â”‚\nâ”‚                              â”‚                               â”‚\nâ”‚                    Consumer peut \"remonter\"                  â”‚\nâ”‚                    et rejouer les Ã©vÃ©nements                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nAvantages / InconvÃ©nients\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nSimple (1 seul systÃ¨me)\nStockage Kafka coÃ»teux (rÃ©tention longue)\n\n\nUne seule codebase\nPas idÃ©al pour analytics trÃ¨s complexes\n\n\nReprocessing facile\nNÃ©cessite Kafka bien configurÃ©\n\n\n\n\n\n\n\n10.3 Lambda vs Kappa â€” Quand utiliser quoi ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nLambda\nKappa\n\n\n\n\nComplexitÃ©\nHaute (2 systÃ¨mes)\nMoyenne (1 systÃ¨me)\n\n\nMaintenance\n2 codebases\n1 codebase\n\n\nReprocessing\nBatch job (lent mais fiable)\nReplay Kafka (rapide)\n\n\nUse case idÃ©al\nAnalytics historiques + temps rÃ©el\nEvent-driven, streaming-first\n\n\nCoÃ»t\nÃ‰levÃ©\nMoyen\n\n\n\n\nRecommandations\n\n\n\nSituation\nArchitecture recommandÃ©e\n\n\n\n\nBesoin de rapports historiques prÃ©cis + temps rÃ©el\nLambda\n\n\nApplication principalement event-driven\nKappa\n\n\nÃ‰quipe petite, budget limitÃ©\nKappa\n\n\nDonnÃ©es complexes nÃ©cessitant du batch lourd\nLambda\n\n\nStartup, MVP, itÃ©ration rapide\nKappa\n\n\n\n\nğŸ’¡ En pratique : Beaucoup dâ€™entreprises utilisent des architectures hybrides adaptÃ©es Ã  leurs besoins spÃ©cifiques.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#data-mesh-lapproche-dÃ©centralisÃ©e",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#data-mesh-lapproche-dÃ©centralisÃ©e",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "11. Data Mesh â€” Lâ€™approche dÃ©centralisÃ©e",
    "text": "11. Data Mesh â€” Lâ€™approche dÃ©centralisÃ©e\nLe Data Mesh est une approche organisationnelle (pas technique) pour gÃ©rer les donnÃ©es Ã  grande Ã©chelle.\n\nOrigine du problÃ¨me\nArchitecture centralisÃ©e traditionnelle :\n\n    Domain A        Domain B        Domain C\n        â”‚               â”‚               â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                        â”‚\n                        â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚   DATA TEAM         â”‚\n              â”‚   CENTRALISÃ‰E       â”‚  â† Goulot d'Ã©tranglement !\n              â”‚                     â”‚\n              â”‚ â€¢ Pipeline A        â”‚\n              â”‚ â€¢ Pipeline B        â”‚\n              â”‚ â€¢ Pipeline C        â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                        â”‚\n                        â–¼\n              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n              â”‚   DATA WAREHOUSE    â”‚\n              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ProblÃ¨mes :\n  â€¢ L'Ã©quipe data devient un goulot d'Ã©tranglement\n  â€¢ Elle ne comprend pas tous les domaines mÃ©tiers\n  â€¢ Temps de livraison trÃ¨s long\n\n\n\nLes 4 principes du Data Mesh\n\n\n\n\n\n\n\nPrincipe\nDescription\n\n\n\n\n1. Domain Ownership\nChaque domaine mÃ©tier est responsable de ses donnÃ©es\n\n\n2. Data as a Product\nLes donnÃ©es sont traitÃ©es comme des produits avec SLA, qualitÃ©, documentation\n\n\n3. Self-serve Platform\nPlateforme en libre-service pour crÃ©er des data products\n\n\n4. Federated Governance\nGouvernance fÃ©dÃ©rÃ©e avec des standards communs\n\n\n\n\n\n\nArchitecture Data Mesh\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         DATA MESH                                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚   DOMAIN: Sales   â”‚  â”‚  DOMAIN: Product  â”‚  â”‚ DOMAIN: Customer  â”‚   â”‚\nâ”‚  â”‚                   â”‚  â”‚                   â”‚  â”‚                   â”‚   â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚\nâ”‚  â”‚  â”‚ Data Productâ”‚  â”‚  â”‚  â”‚ Data Productâ”‚  â”‚  â”‚  â”‚ Data Productâ”‚  â”‚   â”‚\nâ”‚  â”‚  â”‚ \"Orders\"    â”‚  â”‚  â”‚  â”‚ \"Catalog\"   â”‚  â”‚  â”‚  â”‚ \"Profiles\"  â”‚  â”‚   â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚\nâ”‚  â”‚                   â”‚  â”‚                   â”‚  â”‚                   â”‚   â”‚\nâ”‚  â”‚  Owner: Sales Teamâ”‚  â”‚  Owner: Product   â”‚  â”‚  Owner: CRM Team  â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚            â”‚                      â”‚                      â”‚             â”‚\nâ”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚\nâ”‚                                   â”‚                                     â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                    â”‚    SELF-SERVE PLATFORM      â”‚                     â”‚\nâ”‚                    â”‚  (Infrastructure commune)   â”‚                     â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                   â”‚                                     â”‚\nâ”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                    â”‚   FEDERATED GOVERNANCE      â”‚                     â”‚\nâ”‚                    â”‚  (Standards, SÃ©curitÃ©, ...)â”‚                     â”‚\nâ”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nData Mesh vs Architecture CentralisÃ©e\n\n\n\n\n\n\n\n\nAspect\nCentralisÃ©e\nData Mesh\n\n\n\n\nOwnership\nÃ‰quipe data centrale\nÃ‰quipes domaines\n\n\nScalabilitÃ©\nLimitÃ©e par lâ€™Ã©quipe data\nScale avec lâ€™organisation\n\n\nConnaissance mÃ©tier\nFaible\nForte (experts domaine)\n\n\nTime-to-market\nLent\nRapide\n\n\nComplexitÃ©\nTechnique\nOrganisationnelle\n\n\n\n\nâš ï¸ Attention : Data Mesh nâ€™est pas pour tout le monde. Il convient aux grandes organisations avec de nombreux domaines mÃ©tiers distincts.\n\n\nğŸ”® Pour approfondir : Le Data Mesh est un concept avancÃ© que tu exploreras au niveau expert du bootcamp !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#pourquoi-nosql",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "Pourquoi NoSQL ?",
    "text": "Pourquoi NoSQL ?\n\nLes limites des bases relationnelles face au Big Data\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚              PROBLÃˆMES DES BASES SQL CLASSIQUES                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚     SchÃ©ma rigide â†’ Difficile de changer la structure           â”‚\nâ”‚     Scale-up only â†’ Un seul serveur (coÃ»teux)                   â”‚\nâ”‚     Jointures â†’ Lentes sur des milliards de lignes              â”‚\nâ”‚     ACID strict â†’ Latence Ã©levÃ©e                                â”‚\nâ”‚     DonnÃ©es variÃ©es â†’ JSON, graphes mal gÃ©rÃ©s                   â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nLa solution NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    AVANTAGES NoSQL                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚     SchÃ©ma flexible â†’ S'adapte aux donnÃ©es                      â”‚\nâ”‚     Scale-out natif â†’ Ajout de serveurs facile                  â”‚\nâ”‚     Pas de jointures â†’ DonnÃ©es dÃ©normalisÃ©es, rapides           â”‚\nâ”‚     Haute disponibilitÃ© â†’ TolÃ©rance aux pannes                  â”‚\nâ”‚     ModÃ¨les variÃ©s â†’ Document, clÃ©-valeur, graphe...            â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ NoSQL = â€œNot Only SQLâ€ (pas seulement SQL), pas â€œNo SQLâ€ !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#les-4-types-de-bases-nosql",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "Les 4 types de bases NoSQL",
    "text": "Les 4 types de bases NoSQL\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         TYPES DE BASES NoSQL                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     ğŸ“„ DOCUMENT     â”‚   ğŸ”‘ CLÃ‰-VALEUR    â”‚    ğŸ“Š COLONNES      â”‚  ğŸ”— GRAPHE â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”‚  {                  â”‚   key1 â†’ value1     â”‚  Row1: col1, col2   â”‚   (A)â”€â”€â†’(B)â”‚\nâ”‚    \"nom\": \"Alice\", â”‚   key2 â†’ value2     â”‚  Row2: col1, col3   â”‚    â”‚       â”‚\nâ”‚    \"age\": 30        â”‚   key3 â†’ value3     â”‚  Row3: col2, col4   â”‚    â†“       â”‚\nâ”‚  }                  â”‚                     â”‚                     â”‚   (C)     â”‚\nâ”‚                     â”‚                     â”‚                     â”‚           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  MongoDB            â”‚  Redis              â”‚  Cassandra          â”‚ Neo4j     â”‚\nâ”‚  Couchbase          â”‚  Memcached          â”‚  HBase              â”‚ Amazon    â”‚\nâ”‚  Firestore          â”‚  DynamoDB           â”‚  ScyllaDB           â”‚ Neptune   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1. Bases Document (MongoDB, Couchbase)\nStockent des documents JSON/BSON avec un schÃ©ma flexible.\n// Document MongoDB\n{\n  \"_id\": \"user123\",\n  \"nom\": \"Alice Dupont\",\n  \"email\": \"alice@example.com\",\n  \"adresses\": [\n    {\"type\": \"domicile\", \"ville\": \"Paris\"},\n    {\"type\": \"travail\", \"ville\": \"Lyon\"}\n  ]\n}\n\n\n\nAvantages\nInconvÃ©nients\n\n\n\n\nSchÃ©ma flexible\nPas de jointures natives\n\n\nDocuments imbriquÃ©s\nDonnÃ©es dupliquÃ©es\n\n\nRequÃªtes riches\nTransactions limitÃ©es\n\n\n\nCas dâ€™usage : Catalogues produits, profils utilisateurs, CMS, logs\n\n\n\n2. Bases ClÃ©-Valeur (Redis, Memcached)\nStockage ultra-simple : une clÃ© â†’ une valeur.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      CLÃ‰        â”‚          VALEUR            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  user:123       â”‚  {\"nom\": \"Alice\", ...}    â”‚\nâ”‚  session:abc    â”‚  {\"user_id\": 123, ...}    â”‚\nâ”‚  cache:page:42  â”‚  \"&lt;html&gt;...&lt;/html&gt;\"       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : Cache, sessions, compteurs, files dâ€™attente\n\n\n\n3. Bases Colonnes (Cassandra, HBase)\nOptimisÃ©es pour les Ã©critures massives et les requÃªtes analytiques.\nCas dâ€™usage : IoT, time-series, logs, analytics\n\n\n\n4. Bases Graphe (Neo4j, Amazon Neptune)\nOptimisÃ©es pour les relations complexes entre entitÃ©s.\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚  Alice  â”‚\n         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n              â”‚ KNOWS\n              â–¼\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       WORKS_AT       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚   Bob   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  TechCorp   â”‚\n         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : RÃ©seaux sociaux, recommandations, fraude, connaissances",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#le-thÃ©orÃ¨me-cap",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "Le thÃ©orÃ¨me CAP",
    "text": "Le thÃ©orÃ¨me CAP\nLe thÃ©orÃ¨me CAP (Eric Brewer, 2000) est fondamental pour comprendre les choix de design des bases distribuÃ©es.\n                         C\n                    Consistency\n                    (CohÃ©rence)\n                        /\\\n                       /  \\\n                      /    \\\n                     /  CA  \\\n                    /________\\\n                   /\\        /\\\n                  /  \\      /  \\\n                 / CP \\    / AP \\\n                /______\\  /______\\\n               A                    P\n          Availability          Partition\n         (DisponibilitÃ©)        Tolerance\n\nLes 3 propriÃ©tÃ©s\n\n\n\nPropriÃ©tÃ©\nSignification\n\n\n\n\nConsistency\nTous les nÅ“uds voient les mÃªmes donnÃ©es\n\n\nAvailability\nLe systÃ¨me rÃ©pond toujours\n\n\nPartition Tolerance\nFonctionne malgrÃ© des pannes rÃ©seau\n\n\n\n\n\nLe thÃ©orÃ¨me dit :\n\nEn cas de partition rÃ©seau, tu dois choisir entre CohÃ©rence et DisponibilitÃ©.\n\nTu ne peux avoir que 2 sur 3 !\n\n\n\nClassification des bases selon CAP\n\n\n\n\n\n\n\n\n\nType\nChoix CAP\nBases\nComportement\n\n\n\n\nCP\nCohÃ©rence + Partition\nMongoDB, HBase, Redis\nPeut refuser des requÃªtes si partition\n\n\nAP\nDisponibilitÃ© + Partition\nCassandra, DynamoDB, CouchDB\nRÃ©pond toujours, cohÃ©rence Ã©ventuelle\n\n\nCA\nCohÃ©rence + DisponibilitÃ©\nPostgreSQL, MySQL (single node)\nPas de tolÃ©rance aux partitions",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#sql-vs-nosql-quand-utiliser-quoi",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "SQL vs NoSQL â€” Quand utiliser quoi ?",
    "text": "SQL vs NoSQL â€” Quand utiliser quoi ?\n\nTableau comparatif\n\n\n\nCritÃ¨re\nSQL (Relationnel)\nNoSQL\n\n\n\n\nSchÃ©ma\nFixe, dÃ©fini Ã  lâ€™avance\nFlexible, dynamique\n\n\nDonnÃ©es\nStructurÃ©es, normalisÃ©es\nSemi/non-structurÃ©es\n\n\nRelations\nJointures natives\nDonnÃ©es embarquÃ©es\n\n\nTransactions\nACID complet\nBASE (Ã©ventuel)\n\n\nScalabilitÃ©\nVerticale (scale-up)\nHorizontale (scale-out)\n\n\n\n\n\n\nUtilise SQL quandâ€¦\n\nDonnÃ©es trÃ¨s structurÃ©es (comptabilitÃ©, RH)\nRelations complexes (ERP, CRM)\nTransactions critiques (banque, e-commerce)\n\n\n\nUtilise NoSQL quandâ€¦\n\nSchÃ©ma variable (catalogues, profils)\nCache haute performance (sessions)\nÃ‰critures massives (IoT, logs)\nScale-out nÃ©cessaire (Big Data)\n\n\n\nEn rÃ©alitÃ© : on utilise les deux !\n\nğŸ’¡ Polyglot Persistence : Utiliser la bonne base pour le bon cas dâ€™usage !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#rÃ©sumÃ©",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "RÃ©sumÃ©",
    "text": "RÃ©sumÃ©\n\nLes 5V du Big Data\n\n\n\nV\nDÃ©fi\n\n\n\n\nVolume\nStocker et traiter des To/Po\n\n\nVelocity\nTraiter en temps rÃ©el\n\n\nVariety\nGÃ©rer tous les formats\n\n\nVeracity\nAssurer la qualitÃ©\n\n\nValue\nExtraire des insights\n\n\n\n\n\nTraitement distribuÃ©\n\n\n\nConcept\nRetenir\n\n\n\n\nScale-out\nCluster de machines ordinaires\n\n\nData locality\nAmener le code aux donnÃ©es\n\n\nFault tolerance\nRÃ©plication, recalcul\n\n\n\n\n\nHadoop vs Spark\n\n\n\n\nHadoop MR\nSpark\n\n\n\n\nStockage intermÃ©diaire\nDisque\nMÃ©moire\n\n\nVitesse\nLent\n100x plus rapide\n\n\nLangages\nJava\nPython, Scala, SQL\n\n\n\n\n\nArchitectures de stockage\n\n\n\nType\nCaractÃ©ristique clÃ©\n\n\n\n\nData Lake\nStockage brut, schema-on-read\n\n\nData Lakehouse\nLake + ACID + Performance\n\n\n\n\n\nArchitectures de traitement\n\n\n\nArchitecture\nApproche\n\n\n\n\nLambda\nBatch + Streaming (2 systÃ¨mes)\n\n\nKappa\nStreaming-first (1 systÃ¨me)\n\n\n\n\n\nSQL vs NoSQL\n\n\n\nType\nQuand utiliser\n\n\n\n\nSQL\nDonnÃ©es structurÃ©es, transactions, intÃ©gritÃ©\n\n\nNoSQL Document\nSchÃ©ma flexible, JSON (MongoDB)\n\n\nNoSQL ClÃ©-valeur\nCache, sessions (Redis)\n\n\nNoSQL Colonnes\nÃ‰critures massives, IoT (Cassandra)\n\n\nNoSQL Graphe\nRelations complexes (Neo4j)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#quiz",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "Quiz",
    "text": "Quiz\n\n\nâ“ Q1. Quels sont les 3V originaux du Big Data ?\n\nVitesse, Valeur, VÃ©ritÃ©\n\nVolume, Velocity, Variety\n\nVolume, Validation, Visualisation\n\nVÃ©locitÃ©, VÃ©racitÃ©, Valorisation\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les 3V originaux (Doug Laney, 2001) sont Volume, Velocity, Variety.\n\n\n\n\nâ“ Q2. Que signifie â€œScale-outâ€ ?\n\nAugmenter la RAM dâ€™un serveur\n\nAjouter des machines au cluster\n\nCompresser les donnÃ©es\n\nRÃ©duire la taille du cluster\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Scale-out (horizontal) = ajouter des machines. Scale-up (vertical) = augmenter les ressources dâ€™une machine.\n\n\n\n\nâ“ Q3. Quel est le principe de â€œData Localityâ€ ?\n\nStocker les donnÃ©es localement sur son PC\n\nDÃ©placer les donnÃ©es vers le serveur de calcul\n\nDÃ©placer le code vers les donnÃ©es\n\nCompresser les donnÃ©es pour les transfÃ©rer\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” On envoie le code (petit) vers les donnÃ©es (grosses), pas lâ€™inverse.\n\n\n\n\nâ“ Q4. Quels sont les 3 composants principaux de Hadoop ?\n\nHDFS, Spark, Kafka\n\nHDFS, MapReduce, YARN\n\nHive, Pig, HBase\n\nMap, Shuffle, Reduce\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” HDFS (stockage), MapReduce (calcul), YARN (ressources).\n\n\n\n\nâ“ Q5. Pourquoi Spark est plus rapide que MapReduce ?\n\nIl utilise un meilleur algorithme\n\nIl stocke les donnÃ©es intermÃ©diaires en mÃ©moire\n\nIl compresse les donnÃ©es\n\nIl utilise plus de serveurs\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Spark garde les donnÃ©es en mÃ©moire (RAM) au lieu dâ€™Ã©crire sur disque entre chaque Ã©tape.\n\n\n\n\nâ“ Q6. Quelle est la diffÃ©rence entre Data Lake et Data Lakehouse ?\n\nAucune diffÃ©rence\n\nLe Lakehouse ajoute les transactions ACID au Data Lake\n\nLe Data Lake est plus rÃ©cent\n\nLe Lakehouse ne supporte que les donnÃ©es structurÃ©es\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Le Data Lakehouse combine la flexibilitÃ© du Data Lake avec les transactions ACID et les performances du Data Warehouse.\n\n\n\n\nâ“ Q7. Quelles sont les 3 couches de lâ€™architecture Lambda ?\n\nBronze, Silver, Gold\n\nBatch, Speed, Serving\n\nExtract, Transform, Load\n\nInput, Process, Output\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Lambda Architecture = Batch Layer + Speed Layer + Serving Layer.\n\n\n\n\nâ“ Q8. Quel est lâ€™avantage principal de lâ€™architecture Kappa sur Lambda ?\n\nPlus rapide\n\nPlus simple (une seule codebase)\n\nMoins de stockage\n\nMeilleure prÃ©cision\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Kappa simplifie lâ€™architecture en nâ€™utilisant quâ€™un seul systÃ¨me (streaming), Ã©vitant de maintenir deux codebases.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Data Swamp ?\n\nUn type de Data Lake optimisÃ©\n\nUn Data Lake mal gÃ©rÃ© et inutilisable\n\nUne architecture de streaming\n\nUn Data Warehouse sur le cloud\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Swamp est un Data Lake devenu ingÃ©rable par manque de gouvernance (donnÃ©es non documentÃ©es, dupliquÃ©es, obsolÃ¨tes).\n\n\n\n\nâ“ Q10. Quel principe du Data Mesh dit que chaque domaine mÃ©tier est responsable de ses donnÃ©es ?\n\nData as a Product\n\nSelf-serve Platform\n\nDomain Ownership\n\nFederated Governance\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Domain Ownership : chaque domaine mÃ©tier possÃ¨de et gÃ¨re ses propres donnÃ©es.\n\n\n\n\nâ“ Q11. Combien de copies HDFS fait-il par dÃ©faut ?\n\n1\n\n2\n\n3\n\n5\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” HDFS rÃ©plique chaque bloc 3 fois par dÃ©faut pour la tolÃ©rance aux pannes.\n\n\n\n\nâ“ Q12. Quelle base NoSQL est idÃ©ale pour le cache et les sessions ?\n\nMongoDB\n\nRedis\n\nCassandra\n\nNeo4j\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Redis est une base in-memory clÃ©-valeur, idÃ©ale pour le cache et les sessions.\n\n\n\n\nâ“ Q13. Dans lâ€™architecture Medallion, quelle couche contient les donnÃ©es brutes non transformÃ©es ?\n\nGold\n\nSilver\n\nBronze\n\nPlatinum\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Bronze contient les donnÃ©es brutes exactement comme elles arrivent des sources.\n\n\n\n\nâ“ Q14. Quel est lâ€™ordre correct de qualitÃ© croissante dans lâ€™architecture Medallion ?\n\nGold â†’ Silver â†’ Bronze\n\nBronze â†’ Gold â†’ Silver\n\nSilver â†’ Bronze â†’ Gold\n\nBronze â†’ Silver â†’ Gold\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… d â€” Bronze (brut) â†’ Silver (nettoyÃ©) â†’ Gold (agrÃ©gÃ©/business-ready). La qualitÃ© augmente Ã  chaque couche.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#ressources",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nBig Data & Hadoop\n\nHadoop Documentation\nSpark Documentation\nThe Google File System (paper)\nMapReduce: Simplified Data Processing (paper)\n\n\n\nArchitectures modernes\n\nDelta Lake Documentation\nMedallion Architecture (Databricks)\nApache Iceberg\nLambda Architecture (Nathan Marz)\nKappa Architecture (Jay Kreps)\nData Mesh (Zhamak Dehghani)\n\n\n\nNoSQL\n\nMongoDB University â€” Cours gratuits\nRedis Documentation\nCAP Theorem Explained",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/08_intro_big_data_distributed.html#prochaine-Ã©tape",
    "title": "Introduction au Big Data & SystÃ¨mes DistribuÃ©s",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu as maintenant les bases thÃ©oriques du Big Data, du traitement distribuÃ©, des architectures modernes et du NoSQL. Place Ã  la pratique avec MongoDB !\nğŸ‘‰ Module suivant : 09_mongodb_for_data_engineers â€” MongoDB pour les Data Engineers\nTu apprendras Ã  : - Installer et utiliser MongoDB - CrÃ©er des collections et documents - Ã‰crire des requÃªtes MQL (MongoDB Query Language) - Utiliser PyMongo depuis Python - ModÃ©liser des donnÃ©es pour MongoDB\n\nğŸ‰ FÃ©licitations ! Tu comprends maintenant le Big Data, les systÃ¨mes distribuÃ©s, les architectures modernes et le NoSQL.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Š Big Data & NoSQL",
      "08 Â· Introduction Big Data"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html",
    "title": "SQL for Data Engineers",
    "section": "",
    "text": "Ce module couvre les fondamentaux et concepts avancÃ©s de SQL, avec une pratique directe via Python + DuckDB.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#approche-de-ce-cours",
    "title": "SQL for Data Engineers",
    "section": "Approche de ce cours",
    "text": "Approche de ce cours\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       FOCUS DU COURS                            â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   Ce cours utilise Python + DuckDB pour apprendre SQL           â”‚\nâ”‚                                                                 â”‚\nâ”‚   âœ… SQL standard (compatible PostgreSQL, MySQL, etc.)          â”‚\nâ”‚   âœ… ExÃ©cution directe dans Jupyter (pas de serveur)            â”‚\nâ”‚   âœ… IntÃ©gration native avec Pandas                             â”‚\nâ”‚   âœ… CompÃ©tences transfÃ©rables Ã  tout SGBD                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’¡ Le SQL appris ici fonctionne sur PostgreSQL, MySQL, SQLite, BigQuery, Snowflake, etc. avec des variations mineures de syntaxe.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prÃ©requis",
    "title": "SQL for Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 06_intro_relational_databases\n\n\nâœ… Requis\nComprendre les concepts de tables, colonnes, clÃ©s\n\n\nâœ… Requis\nBases de Python (module 04 et 05)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#objectifs-du-module",
    "title": "SQL for Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nÃ‰crire des requÃªtes SELECT, WHERE, ORDER BY\nUtiliser les fonctions dâ€™agrÃ©gation (COUNT, SUM, AVG)\nMaÃ®triser GROUP BY et HAVING\nFaire des jointures (JOIN, LEFT JOIN)\nUtiliser les fonctions de date (DATE_TRUNC, EXTRACT)\nUtiliser CASE, les CTEs et les Window Functions\nExÃ©cuter du SQL depuis Python avec DuckDB\n\n\n\nğŸ’¡ Ce notebook est interactif : tu peux exÃ©cuter les cellules de code directement !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cest-quoi-sql",
    "title": "SQL for Data Engineers",
    "section": "Câ€™est quoi SQL ?",
    "text": "Câ€™est quoi SQL ?\nSQL (Structured Query Language) est un langage de requÃªtes pour interagir avec des bases de donnÃ©es relationnelles.\n\n\n\nAction\nCommande SQL\n\n\n\n\nğŸ” Rechercher\nSELECT\n\n\nâœï¸ InsÃ©rer\nINSERT\n\n\nğŸ”„ Modifier\nUPDATE\n\n\nğŸ—‘ï¸ Supprimer\nDELETE\n\n\nğŸ“Š AgrÃ©ger\nCOUNT, SUM, AVG\n\n\nğŸ”— Joindre\nJOIN",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#pourquoi-cest-essentiel-pour-un-data-engineer",
    "title": "SQL for Data Engineers",
    "section": "Pourquoi câ€™est essentiel pour un Data Engineer ?",
    "text": "Pourquoi câ€™est essentiel pour un Data Engineer ?\n\nInterroger les data warehouses (BigQuery, Snowflake, Redshiftâ€¦)\nExtraire / filtrer les donnÃ©es pour les pipelines\nVÃ©rifier la qualitÃ© des donnÃ©es\nCrÃ©er des vues et agrÃ©gats pour les dashboards",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#outils-en-ligne-pour-tester-du-sql",
    "title": "SQL for Data Engineers",
    "section": "Outils en ligne pour tester du SQL",
    "text": "Outils en ligne pour tester du SQL\nAvant de plonger dans Python + DuckDB, sache quâ€™il existe de nombreux outils en ligne gratuits pour tester des requÃªtes SQL sur diffÃ©rents SGBD. Câ€™est utile pour :\n\nTester rapidement une requÃªte\nVÃ©rifier la compatibilitÃ© entre SGBD\nSâ€™entraÃ®ner sans rien installer\nPartager des exemples avec des collÃ¨gues\n\n\nOutils recommandÃ©s\n\n\n\nOutil\nURL\nSGBD supportÃ©s\nPoints forts\n\n\n\n\nDB Fiddle\ndb-fiddle.com\nPostgreSQL, MySQL, SQLite\nInterface claire, partage facile\n\n\nSQL Fiddle\nsqlfiddle.com\nMySQL, PostgreSQL, Oracle, SQL Server\nLe classique, multi-SGBD\n\n\nSQLite Online\nsqliteonline.com\nSQLite, PostgreSQL, MySQL\nTrÃ¨s simple, import CSV\n\n\nProgramiz SQL\nprogramiz.com/sql/online-compiler\nSQLite\nIdÃ©al dÃ©butants, tutoriels intÃ©grÃ©s\n\n\nOneCompiler\nonecompiler.com/mysql\nMySQL, PostgreSQL, SQL Server\nRapide, moderne\n\n\nReplit\nreplit.com\nSQLite, PostgreSQL\nEnvironnement complet, collaboratif\n\n\n\n\n\nğŸ’¡ Exemple avec DB Fiddle\n\nAller sur db-fiddle.com\nChoisir le SGBD (ex: PostgreSQL 15)\nDans le panneau gauche, crÃ©er le schÃ©ma :\n\nCREATE TABLE employes (\n    id INT PRIMARY KEY,\n    nom VARCHAR(50),\n    salaire INT\n);\n\nINSERT INTO employes VALUES \n(1, 'Alice', 50000),\n(2, 'Bob', 60000),\n(3, 'Charlie', 55000);\n\nDans le panneau droit, Ã©crire ta requÃªte :\n\nSELECT nom, salaire \nFROM employes \nWHERE salaire &gt; 52000;\n\nCliquer sur Run et voir le rÃ©sultat !\n\n\n\nâš ï¸ DiffÃ©rences entre SGBD\nLe SQL est standardisÃ©, mais chaque SGBD a ses particularitÃ©s :\n\n\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nPostgreSQL\nMySQL\nSQL Server\nSQLite\n\n\n\n\nConcatÃ©nation\n\\|\\| ou CONCAT()\nCONCAT()\n+ ou CONCAT()\n\\|\\|\n\n\nLimite rÃ©sultats\nLIMIT 10\nLIMIT 10\nTOP 10\nLIMIT 10\n\n\nAuto-increment\nSERIAL\nAUTO_INCREMENT\nIDENTITY\nAUTOINCREMENT\n\n\nDate actuelle\nCURRENT_DATE\nCURDATE()\nGETDATE()\nDATE('now')\n\n\nBoolÃ©ens\nTRUE/FALSE\n1/0\n1/0\n1/0\n\n\n\n\nğŸ’¡ DuckDB utilise une syntaxe proche de PostgreSQL, ce qui est idÃ©al car PostgreSQL est le standard de facto en Data Engineering.\n\n\n\n\nğŸ¯ Pourquoi on utilise Python + DuckDB dans ce cours ?\n\n\n\nAvantage\nExplication\n\n\n\n\nTout-en-un\nSQL + Python dans le mÃªme notebook\n\n\nPas dâ€™installation serveur\nDuckDB tourne en mÃ©moire\n\n\nIntÃ©gration Pandas\nRÃ©sultats directement en DataFrame\n\n\nFichiers directs\nLire CSV/Parquet/JSON sans import\n\n\nSyntaxe standard\nCompatible PostgreSQL â†’ transfÃ©rable\n\n\nPerformance\nOptimisÃ© pour lâ€™analytics (OLAP)\n\n\n\n# Exemple : SQL â†’ DataFrame en 1 ligne\ndf = con.execute(\"SELECT * FROM clients WHERE pays = 'France'\").df()\n\nLes compÃ©tences SQL apprises ici sont directement applicables Ã  PostgreSQL, BigQuery, Snowflake, Redshift, et tout autre SGBD en production.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#python-duckdb-sql-dans-ton-notebook",
    "title": "SQL for Data Engineers",
    "section": "Python + DuckDB â€” SQL dans ton notebook !",
    "text": "Python + DuckDB â€” SQL dans ton notebook !\n\nQuâ€™est-ce que DuckDB ?\nDuckDB est une base de donnÃ©es analytique embarquÃ©e (comme SQLite, mais optimisÃ©e pour lâ€™analytics).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    POURQUOI DUCKDB ?                        â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  âœ… ZÃ©ro installation serveur (fonctionne en mÃ©moire)       â”‚\nâ”‚  âœ… Syntaxe SQL standard (PostgreSQL-like)                  â”‚\nâ”‚  âœ… IntÃ©gration native avec Pandas                         â”‚\nâ”‚  âœ… TrÃ¨s rapide pour l'analytics (colonnar storage)        â”‚\nâ”‚  âœ… Lit directement CSV, Parquet, JSON                      â”‚\nâ”‚  âœ… Parfait pour apprendre SQL dans un notebook             â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nDuckDB vs autres bases\n\n\n\n\n\n\n\n\n\nCritÃ¨re\nSQLite\nDuckDB\nPostgreSQL\n\n\n\n\nInstallation\nEmbarquÃ©\nEmbarquÃ©\nServeur\n\n\nOptimisÃ© pour\nOLTP (transactions)\nOLAP (analytics)\nLes deux\n\n\nIntÃ©gration Pandas\nâš ï¸ Via SQLAlchemy\nâœ… Native\nâš ï¸ Via psycopg2\n\n\nFichiers CSV/Parquet\nâŒ Non\nâœ… Direct\nâŒ Non\n\n\nUsage\nMobile, embarquÃ©\nData Science, ETL\nProduction\n\n\n\n\n\nVoir le code\n# Installation de DuckDB\n!pip install duckdb --quiet\n\nprint(\"âœ… DuckDB installÃ© !\")\n\n\n\n\nVoir le code\nimport duckdb\n\n# CrÃ©er une connexion en mÃ©moire\ncon = duckdb.connect(database=':memory:')\n\nprint(\"âœ… Connexion DuckDB crÃ©Ã©e !\")\nprint(f\"Version : {duckdb.__version__}\")\n\n\n\n\nVoir le code\n# CrÃ©ation des tables de dÃ©monstration\n\n# Table clients\ncon.execute(\"\"\"\nCREATE TABLE clients (\n    id_client INTEGER PRIMARY KEY,\n    nom VARCHAR(50),\n    email VARCHAR(100),\n    pays VARCHAR(50),\n    date_inscription DATE\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO clients VALUES\n    (1, 'Alice', 'alice@mail.com', 'France', '2023-01-15'),\n    (2, 'Bob', 'bob@mail.com', 'France', '2023-02-20'),\n    (3, 'Charlie', 'charlie@mail.com', 'Allemagne', '2023-03-10'),\n    (4, 'Diana', 'diana@mail.com', 'Belgique', '2023-04-05'),\n    (5, 'Eve', 'eve@mail.com', 'France', '2023-05-12');\n\"\"\")\n\n# Table commandes\ncon.execute(\"\"\"\nCREATE TABLE commandes (\n    id_commande INTEGER PRIMARY KEY,\n    id_client INTEGER,\n    produit VARCHAR(50),\n    montant DECIMAL(10,2),\n    date_commande DATE,\n    FOREIGN KEY (id_client) REFERENCES clients(id_client)\n);\n\"\"\")\n\ncon.execute(\"\"\"\nINSERT INTO commandes VALUES\n    (1, 1, 'Clavier', 50.00, '2023-07-12'),\n    (2, 1, 'Souris', 25.00, '2023-07-15'),\n    (3, 2, 'Ã‰cran', 120.00, '2023-08-01'),\n    (4, 1, 'Webcam', 45.00, '2023-08-20'),\n    (5, 4, 'Casque', 80.00, '2023-09-05'),\n    (6, 2, 'Clavier', 55.00, '2023-09-15'),\n    (7, 5, 'Souris', 30.00, '2023-10-01'),\n    (8, 4, 'Ã‰cran', 150.00, '2023-10-20');\n\"\"\")\n\nprint(\"âœ… Tables crÃ©Ã©es : clients (5 lignes), commandes (8 lignes)\")\n\n\n\n\nVoir le code\n# VÃ©rifier les donnÃ©es\nprint(\" Table clients :\")\nprint(con.execute(\"SELECT * FROM clients\").fetchdf())\n\nprint(\"\\n Table commandes :\")\nprint(con.execute(\"SELECT * FROM commandes\").fetchdf())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cheatsheet-sql-commandes-essentielles",
    "title": "SQL for Data Engineers",
    "section": "Cheatsheet SQL â€” Commandes essentielles",
    "text": "Cheatsheet SQL â€” Commandes essentielles\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\nExemple\n\n\n\n\nLecture\nSELECT\nSÃ©lectionner des colonnes\nSELECT nom, email FROM clients\n\n\n\nSELECT *\nToutes les colonnes\nSELECT * FROM clients\n\n\n\nDISTINCT\nValeurs uniques\nSELECT DISTINCT pays FROM clients\n\n\nFiltrage\nWHERE\nFiltrer les lignes\nWHERE pays = 'France'\n\n\n\nAND / OR\nConditions multiples\nWHERE age &gt; 18 AND pays = 'France'\n\n\n\nIN\nListe de valeurs\nWHERE pays IN ('France', 'Belgique')\n\n\n\nBETWEEN\nPlage de valeurs\nWHERE montant BETWEEN 50 AND 100\n\n\n\nLIKE\nRecherche pattern\nWHERE nom LIKE 'A%'\n\n\n\nIS NULL\nValeurs nulles\nWHERE email IS NULL\n\n\nTri\nORDER BY\nTrier les rÃ©sultats\nORDER BY nom ASC\n\n\n\nLIMIT\nLimiter le nombre\nLIMIT 10\n\n\nAgrÃ©gation\nCOUNT()\nCompter\nSELECT COUNT(*) FROM clients\n\n\n\nSUM()\nSomme\nSELECT SUM(montant) FROM commandes\n\n\n\nAVG()\nMoyenne\nSELECT AVG(montant) FROM commandes\n\n\n\nMIN() / MAX()\nMin / Max\nSELECT MAX(montant) FROM commandes\n\n\nGroupement\nGROUP BY\nRegrouper\nGROUP BY pays\n\n\n\nHAVING\nFiltrer aprÃ¨s agrÃ©gation\nHAVING COUNT(*) &gt; 5\n\n\nJointures\nJOIN\nJointure interne\nJOIN commandes ON ...\n\n\n\nLEFT JOIN\nJointure gauche\nLEFT JOIN commandes ON ...",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#select-sÃ©lectionner-des-donnÃ©es",
    "title": "SQL for Data Engineers",
    "section": "1. SELECT â€” SÃ©lectionner des donnÃ©es",
    "text": "1. SELECT â€” SÃ©lectionner des donnÃ©es\n\nSyntaxe de base\nSELECT colonne1, colonne2\nFROM table\nWHERE condition\nORDER BY colonne;\n\n\nVoir le code\n# SELECT * â€” Toutes les colonnes\nquery = \"SELECT * FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# SELECT colonnes spÃ©cifiques\nquery = \"SELECT nom, email, pays FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# WHERE â€” Filtrer les lignes\nquery = \"\"\"\nSELECT nom, email, pays\nFROM clients\nWHERE pays = 'France'\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# ORDER BY + LIMIT\nquery = \"\"\"\nSELECT nom, montant, date_commande\nFROM commandes\nJOIN clients ON clients.id_client = commandes.id_client\nORDER BY montant DESC\nLIMIT 3\n\"\"\"\nprint(\" Top 3 des commandes les plus chÃ¨res :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#agrÃ©gations-count-sum-avg-min-max",
    "title": "SQL for Data Engineers",
    "section": "2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX",
    "text": "2. AgrÃ©gations â€” COUNT, SUM, AVG, MIN, MAX\n\n\nVoir le code\n# COUNT â€” Compter les lignes\nquery = \"SELECT COUNT(*) AS nb_clients FROM clients\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# SUM, AVG, MIN, MAX\nquery = \"\"\"\nSELECT \n    COUNT(*) AS nb_commandes,\n    SUM(montant) AS total,\n    AVG(montant) AS moyenne,\n    MIN(montant) AS min,\n    MAX(montant) AS max\nFROM commandes\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#group-by-regrouper-les-donnÃ©es",
    "title": "SQL for Data Engineers",
    "section": "3. GROUP BY â€” Regrouper les donnÃ©es",
    "text": "3. GROUP BY â€” Regrouper les donnÃ©es\n\n\nVoir le code\n# GROUP BY â€” Nombre de clients par pays\nquery = \"\"\"\nSELECT pays, COUNT(*) AS nb_clients\nFROM clients\nGROUP BY pays\nORDER BY nb_clients DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# Total des achats par client\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# HAVING â€” Filtrer aprÃ¨s agrÃ©gation\n# Clients ayant dÃ©pensÃ© plus de 100â‚¬\nquery = \"\"\"\nSELECT c.nom, SUM(cmd.montant) AS total_achats\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nHAVING SUM(cmd.montant) &gt; 100\nORDER BY total_achats DESC\n\"\"\"\nprint(\"ğŸ† Clients ayant dÃ©pensÃ© plus de 100â‚¬ :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#join-combiner-les-tables",
    "title": "SQL for Data Engineers",
    "section": "4. JOIN â€” Combiner les tables",
    "text": "4. JOIN â€” Combiner les tables\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ INNER JOIN (JOIN)         â”‚ Seulement les correspondances      â”‚\nâ”‚ A âˆ© B                      â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ LEFT JOIN                 â”‚ Tout A + correspondances B         â”‚\nâ”‚ A + (A âˆ© B)               â”‚ (NULL si pas de correspondance)    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ RIGHT JOIN                â”‚ Tout B + correspondances A         â”‚\nâ”‚ B + (A âˆ© B)               â”‚                                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ FULL OUTER JOIN           â”‚ Tout A + Tout B                    â”‚\nâ”‚ A âˆª B                      â”‚                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# INNER JOIN â€” Clients avec leurs commandes\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# LEFT JOIN â€” Tous les clients, mÃªme sans commande\nquery = \"\"\"\nSELECT c.nom, cmd.produit, cmd.montant\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nORDER BY c.nom\n\"\"\"\nprint(\"ğŸ‘€ Remarque : Charlie n'a pas de commande (NULL)\")\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# Trouver les clients sans commande\nquery = \"\"\"\nSELECT c.nom, c.email\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nWHERE cmd.id_commande IS NULL\n\"\"\"\nprint(\"ğŸ˜´ Clients sans aucune commande :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#fonctions-de-date",
    "title": "SQL for Data Engineers",
    "section": "5. Fonctions de date",
    "text": "5. Fonctions de date\n\n\n\n\n\n\n\n\nFonction\nDescription\nExemple\n\n\n\n\nCURRENT_DATE\nDate du jour\n2024-01-15\n\n\nEXTRACT()\nExtraire une partie\nEXTRACT(YEAR FROM date) â†’ 2024\n\n\nDATE_TRUNC()\nTronquer Ã  une pÃ©riode\nDATE_TRUNC('month', date) â†’ 2024-01-01\n\n\nDATE_DIFF()\nDiffÃ©rence entre dates\nDATE_DIFF('day', date1, date2)\n\n\n\n\n\nVoir le code\n# Ventes par mois\nquery = \"\"\"\nSELECT \n    DATE_TRUNC('month', date_commande) AS mois,\n    SUM(montant) AS total_ventes,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois\n\"\"\"\nprint(\" Ventes mensuelles :\")\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# Commandes par jour de la semaine\nquery = \"\"\"\nSELECT \n    EXTRACT(DOW FROM date_commande) AS jour_num,\n    CASE EXTRACT(DOW FROM date_commande)\n        WHEN 0 THEN 'Dimanche'\n        WHEN 1 THEN 'Lundi'\n        WHEN 2 THEN 'Mardi'\n        WHEN 3 THEN 'Mercredi'\n        WHEN 4 THEN 'Jeudi'\n        WHEN 5 THEN 'Vendredi'\n        WHEN 6 THEN 'Samedi'\n    END AS jour,\n    COUNT(*) AS nb_commandes\nFROM commandes\nGROUP BY EXTRACT(DOW FROM date_commande)\nORDER BY jour_num\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#case-conditions-dans-les-requÃªtes",
    "title": "SQL for Data Engineers",
    "section": "6. CASE â€” Conditions dans les requÃªtes",
    "text": "6. CASE â€” Conditions dans les requÃªtes\n\n\nVoir le code\n# CASE â€” CatÃ©goriser les clients\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    CASE\n        WHEN SUM(cmd.montant) &gt;= 150 THEN 'ğŸ¥‡ Premium'\n        WHEN SUM(cmd.montant) &gt;= 100 THEN 'ğŸ¥ˆ Standard'\n        ELSE 'ğŸ¥‰ Basique'\n    END AS categorie\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total_achats DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#cte-common-table-expression-requÃªtes-lisibles",
    "title": "SQL for Data Engineers",
    "section": "7. CTE (Common Table Expression) â€” RequÃªtes lisibles",
    "text": "7. CTE (Common Table Expression) â€” RequÃªtes lisibles\nLes CTEs permettent de crÃ©er des â€œtables temporairesâ€ pour clarifier les requÃªtes complexes.\nWITH nom_cte AS (\n    SELECT ...\n)\nSELECT * FROM nom_cte;\n\n\nVoir le code\n# CTE â€” Calcul intermÃ©diaire rÃ©utilisable\nquery = \"\"\"\nWITH total_par_client AS (\n    SELECT \n        c.id_client,\n        c.nom,\n        c.pays,\n        SUM(cmd.montant) AS total\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n    GROUP BY c.id_client, c.nom, c.pays\n)\nSELECT \n    nom,\n    pays,\n    total,\n    CASE WHEN total &gt; 100 THEN 'âœ… VIP' ELSE 'âŒ' END AS vip\nFROM total_par_client\nORDER BY total DESC\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#window-functions-calculs-avancÃ©s",
    "title": "SQL for Data Engineers",
    "section": "8. Window Functions â€” Calculs avancÃ©s",
    "text": "8. Window Functions â€” Calculs avancÃ©s\nLes Window Functions permettent de faire des calculs sur un groupe de lignes sans les regrouper.\n\n\n\nFonction\nDescription\n\n\n\n\nROW_NUMBER()\nNumÃ©ro de ligne\n\n\nRANK()\nClassement (avec ex-aequo)\n\n\nDENSE_RANK()\nClassement sans saut\n\n\nLAG()\nValeur de la ligne prÃ©cÃ©dente\n\n\nLEAD()\nValeur de la ligne suivante\n\n\nSUM() OVER()\nSomme cumulative\n\n\n\n\n\nVoir le code\n# RANK â€” Classement des clients par total d'achats\nquery = \"\"\"\nSELECT \n    c.nom,\n    SUM(cmd.montant) AS total_achats,\n    RANK() OVER (ORDER BY SUM(cmd.montant) DESC) AS rang\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\n\"\"\"\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# SUM OVER â€” Total cumulatif\nquery = \"\"\"\nSELECT \n    date_commande,\n    montant,\n    SUM(montant) OVER (ORDER BY date_commande) AS cumul\nFROM commandes\nORDER BY date_commande\n\"\"\"\nprint(\" Ã‰volution cumulative des ventes :\")\ncon.execute(query).fetchdf()\n\n\n\n\nVoir le code\n# LAG â€” Comparer avec la pÃ©riode prÃ©cÃ©dente\nquery = \"\"\"\nWITH mensuel AS (\n    SELECT \n        DATE_TRUNC('month', date_commande) AS mois,\n        SUM(montant) AS total\n    FROM commandes\n    GROUP BY DATE_TRUNC('month', date_commande)\n)\nSELECT \n    mois,\n    total,\n    LAG(total) OVER (ORDER BY mois) AS total_precedent,\n    total - LAG(total) OVER (ORDER BY mois) AS evolution\nFROM mensuel\nORDER BY mois\n\"\"\"\nprint(\" Ã‰volution mensuelle :\")\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#intÃ©gration-avec-pandas",
    "title": "SQL for Data Engineers",
    "section": "9. IntÃ©gration avec Pandas",
    "text": "9. IntÃ©gration avec Pandas\nDuckDB sâ€™intÃ¨gre parfaitement avec Pandas !\n\n\nVoir le code\nimport pandas as pd\n\n# RÃ©cupÃ©rer le rÃ©sultat en DataFrame Pandas\nquery = \"\"\"\nSELECT c.nom, c.pays, SUM(cmd.montant) AS total\nFROM clients c\nJOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom, c.pays\nORDER BY total DESC\n\"\"\"\n\ndf = con.execute(query).fetchdf()\nprint(type(df))  # C'est un DataFrame Pandas !\ndf\n\n\n\n\nVoir le code\n# RequÃªter directement un DataFrame Pandas avec SQL !\ndf_exemple = pd.DataFrame({\n    'produit': ['A', 'B', 'C', 'A', 'B'],\n    'ventes': [100, 200, 150, 120, 180]\n})\n\n# DuckDB peut requÃªter le DataFrame directement\nresult = duckdb.query(\"\"\"\n    SELECT produit, SUM(ventes) AS total\n    FROM df_exemple\n    GROUP BY produit\n    ORDER BY total DESC\n\"\"\").fetchdf()\n\nprint(\"SQL directement sur un DataFrame Pandas :\")\nresult\n\n\n\n\nVoir le code\n# DuckDB peut aussi lire directement des fichiers CSV/Parquet\n# Exemple (si tu as un fichier) :\n# duckdb.query(\"SELECT * FROM 'mon_fichier.csv' LIMIT 10\")\n# duckdb.query(\"SELECT * FROM 'data.parquet' WHERE date &gt; '2024-01-01'\")\n\nprint(\"ğŸ’¡ DuckDB peut lire directement :\")\nprint(\"   - CSV  : SELECT * FROM 'fichier.csv'\")\nprint(\"   - Parquet : SELECT * FROM 'fichier.parquet'\")\nprint(\"   - JSON : SELECT * FROM 'fichier.json'\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#exercices-pratiques",
    "title": "SQL for Data Engineers",
    "section": "10. Exercices pratiques",
    "text": "10. Exercices pratiques\nEssaie de rÃ©soudre ces exercices dans les cellules de code ci-dessous !\n\n\nExercice 1 â€” Facile\nAfficher tous les clients de France, triÃ©s par nom.\n\n\nğŸ’¡ Solution\n\nSELECT * FROM clients WHERE pays = 'France' ORDER BY nom;\n\n\n\n\nExercice 2 â€” Facile\nCalculer le montant moyen des commandes.\n\n\nğŸ’¡ Solution\n\nSELECT AVG(montant) AS montant_moyen FROM commandes;\n\n\n\n\nExercice 3 â€” IntermÃ©diaire\nAfficher le total des achats par client, y compris ceux sans commande (afficher 0).\n\n\nğŸ’¡ Solution\n\nSELECT c.nom, COALESCE(SUM(cmd.montant), 0) AS total\nFROM clients c\nLEFT JOIN commandes cmd ON c.id_client = cmd.id_client\nGROUP BY c.nom\nORDER BY total DESC;\n\n\n\n\nExercice 4 â€” IntermÃ©diaire\nCalculer les ventes par mois avec DATE_TRUNC.\n\n\nğŸ’¡ Solution\n\nSELECT DATE_TRUNC('month', date_commande) AS mois, SUM(montant) AS total\nFROM commandes\nGROUP BY DATE_TRUNC('month', date_commande)\nORDER BY mois;\n\n\n\n\nExercice 5 â€” AvancÃ©\nPour chaque client, afficher sa derniÃ¨re commande (produit et date). Indice : ROW_NUMBER()\n\n\nğŸ’¡ Solution\n\nWITH derniere AS (\n    SELECT c.nom, cmd.produit, cmd.date_commande,\n        ROW_NUMBER() OVER (PARTITION BY c.id_client ORDER BY cmd.date_commande DESC) AS rn\n    FROM clients c\n    JOIN commandes cmd ON c.id_client = cmd.id_client\n)\nSELECT nom, produit, date_commande FROM derniere WHERE rn = 1;\n\n\n\nVoir le code\n# âœï¸ Espace pour tes exercices\nquery = \"\"\"\n-- Ã‰cris ta requÃªte ici\nSELECT * FROM clients LIMIT 5\n\"\"\"\ncon.execute(query).fetchdf()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#quiz",
    "title": "SQL for Data Engineers",
    "section": "Quiz",
    "text": "Quiz\n\n\nâ“ Q1. Quelle commande affiche toutes les colonnes dâ€™une table clients ?\n\nSHOW * FROM clients;\n\nSELECT * FROM clients;\n\nLIST * FROM clients;\n\nDISPLAY * FROM clients;\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” SELECT * FROM clients; affiche toutes les colonnes.\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre WHERE et HAVING ?\n\nAucune diffÃ©rence\n\nWHERE sâ€™utilise avant GROUP BY, HAVING aprÃ¨s\n\nHAVING est plus rapide\n\nWHERE ne peut filtrer que les nombres\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” WHERE filtre avant agrÃ©gation, HAVING filtre aprÃ¨s.\n\n\n\n\nâ“ Q3. Quel JOIN retourne UNIQUEMENT les lignes qui ont une correspondance ?\n\nLEFT JOIN\n\nRIGHT JOIN\n\nINNER JOIN\n\nFULL OUTER JOIN\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” INNER JOIN ne garde que les correspondances.\n\n\n\n\nâ“ Q4. Ã€ quoi sert une CTE (WITH) ?\n\nCrÃ©er une table permanente\n\nDÃ©finir une sous-requÃªte rÃ©utilisable\n\nSupprimer des donnÃ©es\n\nCrÃ©er un index\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Les CTEs crÃ©ent des â€œtables temporairesâ€ pour clarifier les requÃªtes.\n\n\n\n\nâ“ Q5. Quelle Window Function permet de classer les lignes ?\n\nCOUNT()\n\nRANK()\n\nSUM()\n\nGROUP BY\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” RANK() OVER (ORDER BY ...) attribue un classement.\n\n\n\n\nâ“ Q6. Quel avantage de DuckDB pour un Data Engineer ?\n\nIl nÃ©cessite un serveur\n\nIl peut requÃªter directement des DataFrames Pandas\n\nIl ne supporte pas SQL standard\n\nIl ne lit pas les fichiers CSV\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” DuckDB peut requÃªter directement des DataFrames Pandas avec SQL.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#ressources",
    "title": "SQL for Data Engineers",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nğŸ® Pratiquer SQL en ligne\n\nSQLBolt â€” Tutoriel interactif\nMode SQL Tutorial â€” Cas business rÃ©els\nLeetCode SQL â€” DÃ©fis progressifs\nHackerRank SQL â€” Exercices variÃ©s\n\n\n\nğŸ¦† DuckDB\n\nDuckDB Documentation\nDuckDB SQL Reference\n\n\n\nğŸ“– Documentation SQL\n\nPostgreSQL Documentation\nW3Schools SQL",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/07_sql_for_data_engineers.html#prochaine-Ã©tape",
    "title": "SQL for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu maÃ®trises maintenant SQL et sais lâ€™exÃ©cuter depuis Python ! DÃ©couvrons les concepts du Big Data et les bases NoSQL.\nğŸ‘‰ Module suivant : 08_intro_big_data_nosql â€” Big Data, traitement distribuÃ© et NoSQL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module SQL pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "07 Â· SQL pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html",
    "title": "Python for Data Processing",
    "section": "",
    "text": "Ce module couvre le traitement de donnÃ©es avancÃ© avec Python : Pandas, visualisation, APIs, et pipelines ETL.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prÃ©requis",
    "title": "Python for Data Processing",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 04_python_basics_for_data_engineers\n\n\nâœ… Requis\nMaÃ®triser les bases de Python (variables, fonctions, boucles)\n\n\nâœ… Requis\nSavoir utiliser pip et les environnements virtuels",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#objectifs-du-module",
    "title": "Python for Data Processing",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nManipuler des donnÃ©es avec Pandas (DataFrames, nettoyage, agrÃ©gations)\nVisualiser des donnÃ©es avec Matplotlib\nCrÃ©er des graphiques statistiques avec Seaborn\nTraiter du texte et utiliser les regex\nConsommer des APIs REST\nValider la qualitÃ© des donnÃ©es\nConstruire un pipeline ETL complet\nGÃ©rer les configurations et secrets",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#installation-des-dÃ©pendances",
    "title": "Python for Data Processing",
    "section": "Installation des dÃ©pendances",
    "text": "Installation des dÃ©pendances\nAvant de commencer, assurons-nous dâ€™avoir toutes les librairies nÃ©cessaires.\n\n\nVoir le code\n# Installation des packages (Ã  exÃ©cuter une seule fois)\n!pip install pandas numpy requests python-dotenv pytest pandera pyarrow openpyxl matplotlib seaborn\n\n\n\n\nVoir le code\n# Imports de base\nimport pandas as pd\nimport numpy as np\nimport json\nimport requests\nfrom datetime import datetime\nimport time\nimport logging\nimport re\nfrom pathlib import Path\n\n# Configuration de l'affichage\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\npd.set_option('display.width', None)\n\nprint(\"âœ… Imports rÃ©ussis !\")\nprint(f\"Version Pandas : {pd.__version__}\")\nprint(f\"Version NumPy : {np.__version__}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#crÃ©er-et-lire-des-donnÃ©es",
    "title": "Python for Data Processing",
    "section": "1.1 CrÃ©er et lire des donnÃ©es",
    "text": "1.1 CrÃ©er et lire des donnÃ©es\n\n\nVoir le code\n# CrÃ©er un DataFrame simple\ndata = {\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'age': [25, 30, 35, None, 28],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon'],\n    'salaire': [45000, 55000, 60000, 50000, None]\n}\n\ndf = pd.DataFrame(data)\nprint(\"ğŸ“Š DataFrame crÃ©Ã© :\")\nprint(df)\n\n\n\n\nVoir le code\n# Sauvegarder en CSV\ndf.to_csv('exemple_employes.csv', index=False)\nprint(\"âœ… Fichier CSV sauvegardÃ©\")\n\n# Lire depuis CSV\ndf_from_csv = pd.read_csv('exemple_employes.csv')\nprint(\"\\nğŸ“‚ Lecture depuis CSV :\")\nprint(df_from_csv.head())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exploration-des-donnÃ©es",
    "title": "Python for Data Processing",
    "section": "1.2 Exploration des donnÃ©es",
    "text": "1.2 Exploration des donnÃ©es\n\n\nVoir le code\n# Informations gÃ©nÃ©rales\nprint(\"ğŸ“‹ Informations du DataFrame :\")\nprint(df.info())\nprint(\"\\n\" + \"=\"*50)\n\n# Statistiques descriptives\nprint(\"\\nğŸ“Š Statistiques descriptives :\")\nprint(df.describe())\n\n# PremiÃ¨res lignes\nprint(\"\\nğŸ” PremiÃ¨res lignes :\")\nprint(df.head(3))\n\n# DerniÃ¨res lignes\nprint(\"\\nğŸ”š DerniÃ¨res lignes :\")\nprint(df.tail(2))",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-des-donnÃ©es",
    "title": "Python for Data Processing",
    "section": "1.3 Nettoyage des donnÃ©es",
    "text": "1.3 Nettoyage des donnÃ©es\n\n\nVoir le code\n# DÃ©tecter les valeurs manquantes\nprint(\"â“ Valeurs manquantes par colonne :\")\nprint(df.isnull().sum())\nprint(f\"\\nTotal de valeurs manquantes : {df.isnull().sum().sum()}\")\n\n# Visualiser les lignes avec des valeurs manquantes\nprint(\"\\nğŸ” Lignes avec des NaN :\")\nprint(df[df.isnull().any(axis=1)])\n\n\n\n\nVoir le code\n# StratÃ©gies de gestion des valeurs manquantes\n\n# 1. Supprimer les lignes avec des NaN\ndf_drop = df.dropna()\nprint(\"ğŸ—‘ï¸ AprÃ¨s suppression des lignes avec NaN :\")\nprint(df_drop)\n\n# 2. Remplir avec une valeur par dÃ©faut\ndf_fill = df.fillna({\n    'age': df['age'].median(),\n    'salaire': df['salaire'].mean()\n})\nprint(\"\\nâœ¨ AprÃ¨s remplissage des NaN :\")\nprint(df_fill)\n\n# 3. Forward fill (propager la valeur prÃ©cÃ©dente)\ndf_ffill = df.fillna(method='ffill')\nprint(\"\\nâ¡ï¸ AprÃ¨s forward fill :\")\nprint(df_ffill)\n\n\n\n\nVoir le code\n# Supprimer les doublons\ndf_with_duplicates = pd.DataFrame({\n    'id': [1, 2, 3, 2, 4],\n    'nom': ['Alice', 'Bob', 'Charlie', 'Bob', 'David']\n})\n\nprint(\"Avant suppression des doublons :\")\nprint(df_with_duplicates)\n\ndf_no_duplicates = df_with_duplicates.drop_duplicates()\nprint(\"\\nAprÃ¨s suppression :\")\nprint(df_no_duplicates)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sÃ©lection-et-filtrage",
    "title": "Python for Data Processing",
    "section": "1.4 SÃ©lection et filtrage",
    "text": "1.4 SÃ©lection et filtrage\n\n\nVoir le code\n# Utilisons le DataFrame nettoyÃ©\ndf_clean = df_fill.copy()\n\n# SÃ©lectionner une colonne\nprint(\"ğŸ“Œ Colonne 'nom' :\")\nprint(df_clean['nom'])\n\n# SÃ©lectionner plusieurs colonnes\nprint(\"\\nğŸ“Œ Colonnes 'nom' et 'ville' :\")\nprint(df_clean[['nom', 'ville']])\n\n# Filtrer les lignes\nprint(\"\\nğŸ” EmployÃ©s de Paris :\")\nprint(df_clean[df_clean['ville'] == 'Paris'])\n\n# Filtres multiples\nprint(\"\\nğŸ” EmployÃ©s de Paris avec salaire &gt; 50000 :\")\nprint(df_clean[(df_clean['ville'] == 'Paris') & (df_clean['salaire'] &gt; 50000)])\n\n\n\n\nVoir le code\n# Indexation avancÃ©e avec loc et iloc\n\n# loc : par label/nom\nprint(\"ğŸ“ loc[0, 'nom'] :\")\nprint(df_clean.loc[0, 'nom'])\n\n# iloc : par position numÃ©rique\nprint(\"\\nğŸ“ iloc[0, 0] (premiÃ¨re ligne, premiÃ¨re colonne) :\")\nprint(df_clean.iloc[0, 0])\n\n# SÃ©lection de plages\nprint(\"\\nğŸ“ loc[0:2, ['nom', 'age']] :\")\nprint(df_clean.loc[0:2, ['nom', 'age']])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#groupby-et-agrÃ©gations",
    "title": "Python for Data Processing",
    "section": "1.5 GroupBy et agrÃ©gations",
    "text": "1.5 GroupBy et agrÃ©gations\n\n\nVoir le code\n# Grouper par ville et calculer des statistiques\nprint(\"ğŸ“Š Statistiques par ville :\")\ngrouped = df_clean.groupby('ville').agg({\n    'nom': 'count',\n    'age': ['mean', 'min', 'max'],\n    'salaire': ['mean', 'sum']\n})\nprint(grouped)\n\n# Renommer les colonnes pour plus de clartÃ©\nprint(\"\\nğŸ“Š Salaire moyen par ville :\")\nsalaire_moyen = df_clean.groupby('ville')['salaire'].mean().round(2)\nprint(salaire_moyen)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#apply-vs-vectorisation",
    "title": "Python for Data Processing",
    "section": "1.6 Apply vs Vectorisation",
    "text": "1.6 Apply vs Vectorisation\n\n\nVoir le code\n# CrÃ©er une colonne calculÃ©e\n\n# MÃ©thode 1 : Apply (plus lent mais flexible)\ndef categoriser_age(age):\n    if age &lt; 30:\n        return 'Junior'\n    elif age &lt; 40:\n        return 'Senior'\n    else:\n        return 'Expert'\n\ndf_clean['categorie_apply'] = df_clean['age'].apply(categoriser_age)\n\n# MÃ©thode 2 : Vectorisation (plus rapide)\ndf_clean['categorie_vect'] = pd.cut(\n    df_clean['age'],\n    bins=[0, 30, 40, 100],\n    labels=['Junior', 'Senior', 'Expert']\n)\n\nprint(\"ğŸ”§ Colonnes calculÃ©es :\")\nprint(df_clean[['nom', 'age', 'categorie_apply', 'categorie_vect']])\n\n\n\n\nVoir le code\n# Comparaison de performance (sur un grand dataset)\nimport time\n\n# CrÃ©er un grand DataFrame\nbig_df = pd.DataFrame({\n    'valeur': np.random.randint(1, 100, 100000)\n})\n\n# MÃ©thode Apply\nstart = time.time()\nbig_df['double_apply'] = big_df['valeur'].apply(lambda x: x * 2)\ntime_apply = time.time() - start\n\n# MÃ©thode VectorisÃ©e\nstart = time.time()\nbig_df['double_vect'] = big_df['valeur'] * 2\ntime_vect = time.time() - start\n\nprint(f\"â±ï¸ Temps Apply : {time_apply:.4f}s\")\nprint(f\"â±ï¸ Temps Vectorisation : {time_vect:.4f}s\")\nprint(f\"ğŸš€ Vectorisation est {time_apply/time_vect:.1f}x plus rapide !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-la-mÃ©moire",
    "title": "Python for Data Processing",
    "section": "1.7 Gestion de la mÃ©moire",
    "text": "1.7 Gestion de la mÃ©moire\n\n\nVoir le code\n# VÃ©rifier l'utilisation mÃ©moire\nprint(\" Utilisation mÃ©moire par colonne :\")\nprint(df_clean.memory_usage(deep=True))\nprint(f\"\\nTotal : {df_clean.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n\n\n\nVoir le code\n# Optimiser les types de donnÃ©es\ndf_optimized = df_clean.copy()\n\n# Avant optimisation\nprint(\"Avant optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n\n# Convertir en types plus efficaces\ndf_optimized['age'] = df_optimized['age'].astype('int8')\ndf_optimized['salaire'] = df_optimized['salaire'].astype('int32')\ndf_optimized['ville'] = df_optimized['ville'].astype('category')\n\nprint(\"\\nAprÃ¨s optimisation :\")\nprint(df_optimized.dtypes)\nprint(f\"MÃ©moire : {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-dates",
    "title": "Python for Data Processing",
    "section": "1.8 Manipulation de dates",
    "text": "1.8 Manipulation de dates\n\n\nVoir le code\n# CrÃ©er un DataFrame avec des dates\ndf_dates = pd.DataFrame({\n    'date_str': ['2024-01-15', '2024-02-20', '2024-03-10', '2024-04-05'],\n    'montant': [1000, 1500, 1200, 1800]\n})\n\n# Convertir en datetime\ndf_dates['date'] = pd.to_datetime(df_dates['date_str'])\n\n# Extraire des composantes\ndf_dates['annee'] = df_dates['date'].dt.year\ndf_dates['mois'] = df_dates['date'].dt.month\ndf_dates['jour'] = df_dates['date'].dt.day\ndf_dates['nom_mois'] = df_dates['date'].dt.month_name()\ndf_dates['jour_semaine'] = df_dates['date'].dt.day_name()\n\nprint(\"ğŸ“… DataFrame avec dates extraites :\")\nprint(df_dates)\n\n\n\n\nVoir le code\n# Calculs avec les dates\ndf_dates['jours_depuis_debut'] = (df_dates['date'] - df_dates['date'].min()).dt.days\n\n# Ajouter/soustraire des pÃ©riodes\ndf_dates['date_plus_30j'] = df_dates['date'] + pd.Timedelta(days=30)\ndf_dates['date_moins_1mois'] = df_dates['date'] - pd.DateOffset(months=1)\n\nprint(\" Calculs de dates :\")\nprint(df_dates[['date', 'jours_depuis_debut', 'date_plus_30j', 'date_moins_1mois']])",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#export-de-donnÃ©es",
    "title": "Python for Data Processing",
    "section": "1.9 Export de donnÃ©es",
    "text": "1.9 Export de donnÃ©es\n\n\nVoir le code\n# Export CSV\ndf_clean.to_csv('employes_clean.csv', index=False)\nprint(\"âœ… Export CSV rÃ©ussi\")\n\n# Export JSON\ndf_clean.to_json('employes_clean.json', orient='records', indent=2)\nprint(\"âœ… Export JSON rÃ©ussi\")\n\n# Export Parquet (format columnar, trÃ¨s efficace)\ndf_clean.to_parquet('employes_clean.parquet', index=False)\nprint(\"âœ… Export Parquet rÃ©ussi\")\n\n# Export Excel\ndf_clean.to_excel('employes_clean.xlsx', index=False, sheet_name='EmployÃ©s')\nprint(\"âœ… Export Excel rÃ©ussi\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-1-pandas",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique 1 : Pandas",
    "text": "Exercice Pratique 1 : Pandas\nObjectif : Analyser un fichier de ventes\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, quantitÃ©, prix, date)\nCalculer le chiffre dâ€™affaires total\nTrouver le produit le plus vendu\nCalculer les ventes mensuelles\nExporter le rÃ©sultat en CSV\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport pandas as pd\nimport numpy as np\n\n# 1. CrÃ©er un DataFrame avec des donnÃ©es de ventes\nnp.random.seed(42)\ndates = pd.date_range(start='2024-01-01', periods=100, freq='D')\nproduits = ['Laptop', 'TÃ©lÃ©phone', 'Tablette', 'Casque', 'Souris']\n\nventes_data = {\n    'date': np.random.choice(dates, 200),\n    'produit': np.random.choice(produits, 200),\n    'quantite': np.random.randint(1, 20, 200),\n    'prix_unitaire': np.random.choice([999, 599, 449, 79, 29], 200)\n}\n\ndf_ventes = pd.DataFrame(ventes_data)\ndf_ventes['montant'] = df_ventes['quantite'] * df_ventes['prix_unitaire']\n\n# 2. Chiffre d'affaires total\nca_total = df_ventes['montant'].sum()\nprint(f\"ğŸ’° CA total : {ca_total:,.0f} â‚¬\")\n\n# 3. Produit le plus vendu\nproduit_top = df_ventes.groupby('produit')['quantite'].sum().idxmax()\nprint(f\"ğŸ† Produit top : {produit_top}\")\n\n# 4. Ventes mensuelles\ndf_ventes['mois'] = pd.to_datetime(df_ventes['date']).dt.to_period('M')\nprint(df_ventes.groupby('mois')['montant'].sum())\n\n# 5. Export CSV\ndf_ventes.to_csv('ventes_analyse.csv', index=False)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#julius.ai-lia-pour-analyser-tes-donnÃ©es",
    "title": "Python for Data Processing",
    "section": "Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es",
    "text": "Julius.ai â€” Lâ€™IA pour analyser tes donnÃ©es\nJulius.ai est une plateforme dâ€™IA qui permet dâ€™analyser des donnÃ©es en langage naturel, sans Ã©crire de code.\n\nAccÃ¨s\nğŸ‘‰ julius.ai â€” Gratuit avec limitations, plans payants disponibles\n\n\nFonctionnalitÃ©s\n\n\n\n\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nUpload de fichiers\nCSV, Excel, JSON, bases de donnÃ©es\n\n\nQuestions en franÃ§ais\nâ€œQuelle est la moyenne des salaires par ville ?â€\n\n\nGÃ©nÃ©ration de code\nPython/Pandas gÃ©nÃ©rÃ© automatiquement\n\n\nVisualisations\nGraphiques crÃ©Ã©s Ã  la demande\n\n\nExport\nCode Python, graphiques, rapports\n\n\n\n\n\nExemples de questions Ã  poser\n- \"Montre-moi les 10 premiÃ¨res lignes\"\n- \"Combien de valeurs manquantes par colonne ?\"\n- \"CrÃ©e un graphique des ventes par mois\"\n- \"Quelle est la corrÃ©lation entre age et salaire ?\"\n- \"Nettoie les doublons et les valeurs aberrantes\"\n- \"GÃ©nÃ¨re un rapport de qualitÃ© des donnÃ©es\"\n\n\nğŸ’¡ Cas dâ€™usage Data Engineering\n\n\n\nSituation\nComment Julius aide\n\n\n\n\nNouveau dataset inconnu\nExploration rapide sans code\n\n\nRÃ©union avec non-techniques\nDÃ©mo interactive\n\n\nPrototypage rapide\nGÃ©nÃ©rer du code Pandas Ã  rÃ©utiliser\n\n\nDebugging\nâ€œPourquoi jâ€™ai des NaN dans cette colonne ?â€\n\n\n\n\n\nâš ï¸ Limitations\n\nDonnÃ©es envoyÃ©es dans le cloud (attention aux donnÃ©es sensibles)\nGratuit limitÃ© en nombre de requÃªtes\nPas adaptÃ© pour la production (utiliser le code gÃ©nÃ©rÃ© plutÃ´t)\n\n\nğŸ’¡ Astuce : Utilise Julius pour explorer, puis copie le code Python gÃ©nÃ©rÃ© dans ton pipeline !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ydata-profiling-rapport-complet-en-1-ligne",
    "title": "Python for Data Processing",
    "section": "ydata-profiling â€” Rapport complet en 1 ligne",
    "text": "ydata-profiling â€” Rapport complet en 1 ligne\nydata-profiling (anciennement pandas-profiling) gÃ©nÃ¨re un rapport HTML interactif complet sur ton DataFrame.\n\nInstallation\npip install ydata-profiling\n\n\nVoir le code\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install ydata-profiling\n\nfrom ydata_profiling import ProfileReport\n\n# CrÃ©er un dataset d'exemple\ndf_exemple = pd.DataFrame({\n    'nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n    'age': [25, 30, 35, None, 28, 45, 32, 29],\n    'ville': ['Paris', 'Lyon', 'Paris', 'Marseille', 'Lyon', 'Paris', 'Lyon', 'Paris'],\n    'salaire': [45000, 55000, 60000, 50000, None, 75000, 52000, 48000],\n    'experience': [2, 5, 8, 3, 4, 15, 7, 3],\n    'date_embauche': pd.to_datetime(['2022-01-15', '2019-06-20', '2016-03-10', \n                                      '2021-09-01', '2020-04-15', '2010-01-01',\n                                      '2017-08-20', '2021-11-30'])\n})\n\n# GÃ©nÃ©rer le rapport (mode minimal pour rapiditÃ©)\nprofile = ProfileReport(\n    df_exemple, \n    title=\"Rapport EmployÃ©s\",\n    minimal=True,  # Mode rapide\n    explorative=True\n)\n\n# Afficher dans le notebook\nprofile.to_notebook_iframe()\n\n# Ou sauvegarder en HTML\n# profile.to_file(\"rapport_employes.html\")\n\n\n\n\nCe que contient le rapport\n\n\n\n\n\n\n\nSection\nContenu\n\n\n\n\nOverview\nNombre de lignes, colonnes, types, taille mÃ©moire\n\n\nVariables\nStats par colonne (min, max, mean, distribution)\n\n\nInteractions\nCorrÃ©lations entre variables\n\n\nCorrelations\nMatrices de corrÃ©lation (Pearson, Spearman)\n\n\nMissing values\nVisualisation des valeurs manquantes\n\n\nDuplicates\nDÃ©tection des doublons\n\n\nAlerts\nâš ï¸ Alertes automatiques (haute cardinalitÃ©, skewness, etc.)\n\n\n\n\n\nOptions utiles\n# Rapport complet (plus lent)\nprofile = ProfileReport(df, minimal=False)\n\n# Comparer deux datasets\nprofile_train = ProfileReport(df_train, title=\"Train\")\nprofile_test = ProfileReport(df_test, title=\"Test\")\ncomparison = profile_train.compare(profile_test)\ncomparison.to_file(\"comparison.html\")\n\n# Exclure certaines analyses (plus rapide)\nprofile = ProfileReport(\n    df,\n    correlations=None,  # DÃ©sactiver les corrÃ©lations\n    interactions=None   # DÃ©sactiver les interactions\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sweetviz-comparaison-visuelle-de-datasets",
    "title": "Python for Data Processing",
    "section": "Sweetviz â€” Comparaison visuelle de datasets",
    "text": "Sweetviz â€” Comparaison visuelle de datasets\nSweetviz est spÃ©cialisÃ© dans la comparaison de datasets (train vs test, avant vs aprÃ¨s nettoyage).\n\nğŸ“¦ Installation\npip install sweetviz\n\n\nVoir le code\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install sweetviz\n\nimport sweetviz as sv\n\n# Rapport simple\nreport = sv.analyze(df_exemple)\nreport.show_notebook()  # Afficher dans le notebook\n# report.show_html(\"sweetviz_report.html\")  # Ou sauvegarder\n\n\n\n\nVoir le code\n# Comparaison de deux datasets (ex: train vs test)\ndf_train = df_exemple.iloc[:5]\ndf_test = df_exemple.iloc[5:]\n\n# GÃ©nÃ©rer le rapport de comparaison\ncomparison_report = sv.compare([df_train, \"Train\"], [df_test, \"Test\"])\ncomparison_report.show_notebook()\n\n# Analyse avec variable cible (pour ML)\n# report = sv.analyze(df, target_feat=\"salaire\")\n\n\n\n\nPoints forts de Sweetviz\n\n\n\nFonctionnalitÃ©\nDescription\n\n\n\n\nComparaison cÃ´te Ã  cÃ´te\nVoir les diffÃ©rences entre 2 datasets\n\n\nVariable cible\nAnalyse par rapport Ã  une target (ML)\n\n\nDesign moderne\nRapports visuellement attractifs\n\n\nRapide\nPlus lÃ©ger que ydata-profiling\n\n\n\n\n\nydata-profiling vs Sweetviz\n\n\n\nCritÃ¨re\nydata-profiling\nSweetviz\n\n\n\n\nProfondeur dâ€™analyse\nâ­â­â­ TrÃ¨s dÃ©taillÃ©\nâ­â­ Essentiel\n\n\nVitesse\nğŸ¢ Plus lent\nğŸ‡ Plus rapide\n\n\nComparaison\nâœ… Possible\nâ­â­â­ Excellent\n\n\nDesign\nClassique\nModerne\n\n\nAlertes\nâœ… Oui\nâŒ Non",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#d-tale-exploration-interactive-comme-excel",
    "title": "Python for Data Processing",
    "section": "D-Tale â€” Exploration interactive (comme Excel)",
    "text": "D-Tale â€” Exploration interactive (comme Excel)\nD-Tale lance une interface web interactive pour explorer tes donnÃ©es comme dans Excel/Google Sheets, mais avec la puissance de Python derriÃ¨re.\n\nInstallation\npip install dtale\n\n\nVoir le code\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install dtale\n\nimport dtale\n\n# Lancer D-Tale\nd = dtale.show(df_exemple)\n\n# Afficher dans le notebook (ou ouvre un nouvel onglet)\nd.notebook()\n\n\n\n\nFonctionnalitÃ©s D-Tale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  D-Tale                                         [Export] [Code]â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  [Filters] [Sort] [Charts] [Correlations] [Describe] [Missing] â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    nom    â”‚  age  â”‚  ville   â”‚ salaire â”‚ experience â”‚          â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚          â”‚\nâ”‚  Alice    â”‚  25   â”‚  Paris   â”‚  45000  â”‚     2      â”‚          â”‚\nâ”‚  Bob      â”‚  30   â”‚  Lyon    â”‚  55000  â”‚     5      â”‚          â”‚\nâ”‚  Charlie  â”‚  35   â”‚  Paris   â”‚  60000  â”‚     8      â”‚          â”‚\nâ”‚  ...      â”‚  ...  â”‚  ...     â”‚  ...    â”‚    ...     â”‚          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nAction\nComment\n\n\n\n\nFiltrer\nCliquer sur une colonne â†’ Filter\n\n\nTrier\nCliquer sur lâ€™en-tÃªte de colonne\n\n\nGraphiques\nMenu Charts â†’ choisir le type\n\n\nStats\nMenu Describe â†’ stats par colonne\n\n\nExporter le code\nBouton â€œCode Exportâ€ â†’ copier le Pandas gÃ©nÃ©rÃ©\n\n\n\n\nğŸ’¡ Killer feature : D-Tale gÃ©nÃ¨re le code Pandas de toutes tes manipulations !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pygwalker-interface-tableau-dans-jupyter",
    "title": "Python for Data Processing",
    "section": "Pygwalker â€” Interface Tableau dans Jupyter",
    "text": "Pygwalker â€” Interface Tableau dans Jupyter\nPygwalker transforme ton DataFrame en une interface drag & drop comme Tableau/Power BI, directement dans Jupyter.\n\nInstallation\npip install pygwalker\n\n\nVoir le code\n# Installation (dÃ©commenter si nÃ©cessaire)\n# !pip install pygwalker\n\nimport pygwalker as pyg\n\n# Lancer l'interface interactive\nwalker = pyg.walk(df_exemple)\n\n\n\n\nComment utiliser Pygwalker\n\nGlisser-dÃ©poser les colonnes sur les axes X, Y, Color, Size\nChoisir le type de graphique (bar, line, scatter, heatmapâ€¦)\nFiltrer les donnÃ©es visuellement\nExporter la configuration pour la rÃ©utiliser\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Pygwalker                                                      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   FIELDS         â”‚                                              â”‚\nâ”‚                  â”‚         [Graphique interactif]               â”‚\nâ”‚      nom         â”‚                                              â”‚\nâ”‚      age         â”‚              â–ˆâ–ˆâ–ˆâ–ˆ                            â”‚\nâ”‚      ville       â”‚         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         â”‚\nâ”‚      salaire     â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      â”‚\nâ”‚      experience  â”‚                                              â”‚\nâ”‚                  â”‚                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  X: ville        â”‚  Y: salaire    Color: experience             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nğŸ’¡ Cas dâ€™usage\n\nExploration visuelle rapide sans Ã©crire de code matplotlib\nPrÃ©sentation Ã  des non-techniques\nPrototypage de dashboards avant de coder",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rÃ©capitulatif-quel-outil-choisir",
    "title": "Python for Data Processing",
    "section": "RÃ©capitulatif â€” Quel outil choisir ?",
    "text": "RÃ©capitulatif â€” Quel outil choisir ?\n\n\n\n\n\n\n\nSituation\nOutil recommandÃ©\n\n\n\n\nPremier aperÃ§u rapide dâ€™un dataset\nydata-profiling (minimal=True)\n\n\nComparer data1 vs data2\nSweetviz\n\n\nExploration interactive (comme Excel)\nD-Tale\n\n\nCrÃ©er des graphiques sans code\nPygwalker\n\n\nPoser des questions en franÃ§ais\nJulius.ai\n\n\nDonnÃ©es sensibles (pas de cloud)\nD-Tale ou ydata-profiling (tout local)\n\n\nGÃ©nÃ©rer du code Pandas\nJulius.ai ou D-Tale\n\n\n\n\nInstallation complÃ¨te\npip install ydata-profiling sweetviz dtale pygwalker\n\n\nâš ï¸ Bonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nUtiliser ces outils pour explorer\nLes utiliser en production\n\n\nCopier le code gÃ©nÃ©rÃ© dans ton pipeline\nDÃ©pendre de lâ€™interface pour le traitement\n\n\nPartager les rapports HTML avec lâ€™Ã©quipe\nEnvoyer des donnÃ©es sensibles sur Julius.ai\n\n\nCombiner plusieurs outils\nSe limiter Ã  un seul\n\n\n\n\nğŸ’¡ Workflow recommandÃ© : 1. Julius.ai pour les premiÃ¨res questions 2. ydata-profiling pour un rapport complet 3. D-Tale pour explorer interactivement 4. Copier le code dans ton pipeline Pandas",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-linÃ©aires-line-plots",
    "title": "Python for Data Processing",
    "section": "Graphiques linÃ©aires (Line Plots)",
    "text": "Graphiques linÃ©aires (Line Plots)\n\n\nVoir le code\n# DonnÃ©es pour un graphique linÃ©aire\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# CrÃ©ation du graphique\nplt.figure(figsize=(12, 6))\nplt.plot(x, y1, label='Sin(x)', color='blue', linewidth=2)\nplt.plot(x, y2, label='Cos(x)', color='red', linestyle='--', linewidth=2)\n\n# Personnalisation\nplt.title('Fonctions trigonomÃ©triques', fontsize=16, fontweight='bold')\nplt.xlabel('x', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.legend(loc='upper right')\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-Ã -barres-bar-plots",
    "title": "Python for Data Processing",
    "section": "Graphiques Ã  barres (Bar Plots)",
    "text": "Graphiques Ã  barres (Bar Plots)\n\n\nVoir le code\n# DonnÃ©es de ventes par mois\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\nventes_2023 = [1200, 1500, 1800, 1600, 2000, 2200]\nventes_2024 = [1400, 1700, 1900, 1800, 2300, 2500]\n\nx = np.arange(len(mois))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Barres groupÃ©es\nbars1 = ax.bar(x - width/2, ventes_2023, width, label='2023', color='steelblue')\nbars2 = ax.bar(x + width/2, ventes_2024, width, label='2024', color='coral')\n\n# Personnalisation\nax.set_title('Comparaison des ventes 2023 vs 2024', fontsize=16, fontweight='bold')\nax.set_xlabel('Mois', fontsize=12)\nax.set_ylabel('Ventes (â‚¬)', fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(mois)\nax.legend()\n\n# Ajouter les valeurs sur les barres\nfor bar in bars1:\n    height = bar.get_height()\n    ax.annotate(f'{height}', xy=(bar.get_x() + bar.get_width()/2, height),\n                xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nuages-de-points-scatter-plots",
    "title": "Python for Data Processing",
    "section": "Nuages de points (Scatter Plots)",
    "text": "Nuages de points (Scatter Plots)\n\n\nVoir le code\n# DonnÃ©es alÃ©atoires avec corrÃ©lation\nnp.random.seed(42)\nx = np.random.randn(100)\ny = 2 * x + np.random.randn(100) * 0.5\ncolors = np.random.rand(100)\nsizes = np.random.rand(100) * 200\n\n# Scatter plot avec couleurs et tailles variables\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(x, y, c=colors, s=sizes, alpha=0.6, cmap='viridis')\n\n# Ajouter une ligne de tendance\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x, p(x), 'r--', linewidth=2, label=f'Tendance: y = {z[0]:.2f}x + {z[1]:.2f}')\n\nplt.colorbar(scatter, label='Valeur')\nplt.title('Nuage de points avec ligne de tendance', fontsize=16, fontweight='bold')\nplt.xlabel('Variable X')\nplt.ylabel('Variable Y')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#histogrammes",
    "title": "Python for Data Processing",
    "section": "Histogrammes",
    "text": "Histogrammes\n\n\nVoir le code\n# DonnÃ©es de distribution\nnp.random.seed(42)\ndata_normal = np.random.normal(loc=50, scale=10, size=1000)\ndata_skewed = np.random.exponential(scale=10, size=1000)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme distribution normale\naxes[0].hist(data_normal, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\naxes[0].axvline(data_normal.mean(), color='red', linestyle='--', label=f'Moyenne: {data_normal.mean():.1f}')\naxes[0].set_title('Distribution Normale', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Valeur')\naxes[0].set_ylabel('FrÃ©quence')\naxes[0].legend()\n\n# Histogramme distribution exponentielle\naxes[1].hist(data_skewed, bins=30, color='coral', edgecolor='black', alpha=0.7)\naxes[1].axvline(data_skewed.mean(), color='red', linestyle='--', label=f'Moyenne: {data_skewed.mean():.1f}')\naxes[1].set_title('Distribution Exponentielle', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Valeur')\naxes[1].set_ylabel('FrÃ©quence')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#graphiques-circulaires-pie-charts",
    "title": "Python for Data Processing",
    "section": "Graphiques circulaires (Pie Charts)",
    "text": "Graphiques circulaires (Pie Charts)\n\n\nVoir le code\n# DonnÃ©es de rÃ©partition\ncategories = ['Produit A', 'Produit B', 'Produit C', 'Produit D', 'Autres']\nparts = [35, 25, 20, 15, 5]\ncolors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc']\nexplode = (0.05, 0, 0, 0, 0)  # Mettre en Ã©vidence le premier segment\n\nplt.figure(figsize=(10, 8))\nwedges, texts, autotexts = plt.pie(parts, labels=categories, colors=colors, explode=explode,\n                                    autopct='%1.1f%%', startangle=90, shadow=True)\n\n# AmÃ©liorer l'apparence du texte\nfor autotext in autotexts:\n    autotext.set_fontsize(11)\n    autotext.set_fontweight('bold')\n\nplt.title('RÃ©partition des ventes par produit', fontsize=16, fontweight='bold')\nplt.axis('equal')  # Assure que le cercle est bien rond\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sous-graphiques-subplots",
    "title": "Python for Data Processing",
    "section": "Sous-graphiques (Subplots)",
    "text": "Sous-graphiques (Subplots)\n\n\nVoir le code\n# CrÃ©er une grille de sous-graphiques\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# DonnÃ©es\nx = np.linspace(0, 10, 50)\n\n# Graphique 1: Ligne\naxes[0, 0].plot(x, np.sin(x), 'b-', linewidth=2)\naxes[0, 0].set_title('Graphique linÃ©aire')\naxes[0, 0].set_xlabel('X')\naxes[0, 0].set_ylabel('Sin(X)')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Graphique 2: Barres\ncategories = ['A', 'B', 'C', 'D']\nvalues = [23, 45, 56, 78]\naxes[0, 1].bar(categories, values, color=['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4'])\naxes[0, 1].set_title('Graphique Ã  barres')\n\n# Graphique 3: Scatter\nx_scatter = np.random.rand(50)\ny_scatter = np.random.rand(50)\naxes[1, 0].scatter(x_scatter, y_scatter, c='purple', alpha=0.6, s=100)\naxes[1, 0].set_title('Nuage de points')\n\n# Graphique 4: Histogramme\ndata = np.random.randn(1000)\naxes[1, 1].hist(data, bins=30, color='orange', edgecolor='black', alpha=0.7)\naxes[1, 1].set_title('Histogramme')\n\nplt.suptitle('Tableau de bord - Vue d\\'ensemble', fontsize=16, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#sauvegarder-des-graphiques",
    "title": "Python for Data Processing",
    "section": "Sauvegarder des graphiques",
    "text": "Sauvegarder des graphiques\n\n\nVoir le code\n# CrÃ©er un graphique Ã  sauvegarder\nfig, ax = plt.subplots(figsize=(10, 6))\nx = np.linspace(0, 10, 100)\nax.plot(x, np.sin(x), 'b-', linewidth=2, label='Sin(x)')\nax.set_title('Graphique Ã  exporter', fontsize=14)\nax.legend()\nax.grid(True, alpha=0.3)\n\n# Sauvegarder dans diffÃ©rents formats\nfig.savefig('graphique.png', dpi=300, bbox_inches='tight')\nfig.savefig('graphique.pdf', bbox_inches='tight')\nfig.savefig('graphique.svg', bbox_inches='tight')\n\nprint(\"âœ… Graphiques sauvegardÃ©s en PNG, PDF et SVG\")\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-matplotlib",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique : Matplotlib",
    "text": "Exercice Pratique : Matplotlib\nObjectif : CrÃ©er un tableau de bord de visualisation\n\nCrÃ©er un DataFrame avec des donnÃ©es de ventes (produit, mois, ventes, profit)\nCrÃ©er 4 sous-graphiques montrant :\n\nÃ‰volution des ventes mensuelles (ligne)\nVentes par produit (barres)\nRelation ventes/profit (scatter)\nDistribution des profits (histogramme)\n\nPersonnaliser les couleurs et ajouter des titres\nSauvegarder le rÃ©sultat en PNG\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(42)\nmois = ['Jan', 'FÃ©v', 'Mar', 'Avr', 'Mai', 'Juin']\ndata = {'mois': mois, 'ventes': np.random.randint(100, 300, 6), \n        'profit': np.random.randint(20, 80, 6)}\ndf = pd.DataFrame(data)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Ligne\naxes[0,0].plot(df['mois'], df['ventes'], marker='o')\naxes[0,0].set_title('ğŸ“ˆ Ã‰volution mensuelle')\n\n# Barres\naxes[0,1].bar(df['mois'], df['ventes'], color='steelblue')\naxes[0,1].set_title('ğŸ“Š Ventes par mois')\n\n# Scatter\naxes[1,0].scatter(df['ventes'], df['profit'], c='coral', s=100)\naxes[1,0].set_title('ğŸ“ Ventes vs Profit')\n\n# Histogramme\naxes[1,1].hist(df['profit'], bins=5, color='green', alpha=0.7)\naxes[1,1].set_title('ğŸ“Š Distribution profits')\n\nplt.tight_layout()\nplt.savefig('dashboard.png', dpi=150)\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#charger-des-jeux-de-donnÃ©es-intÃ©grÃ©s",
    "title": "Python for Data Processing",
    "section": "Charger des jeux de donnÃ©es intÃ©grÃ©s",
    "text": "Charger des jeux de donnÃ©es intÃ©grÃ©s\n\n\nVoir le code\n# Seaborn propose des jeux de donnÃ©es pour s'entraÃ®ner\ntips = sns.load_dataset('tips')\nprint(\"ğŸ“Š Dataset 'tips' :\")\nprint(tips.head())\nprint(f\"\\nDimensions : {tips.shape}\")\nprint(f\"\\nColonnes : {list(tips.columns)}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-distributions",
    "title": "Python for Data Processing",
    "section": "Visualisation des distributions",
    "text": "Visualisation des distributions\n\n\nVoir le code\n# Histogramme avec KDE (Kernel Density Estimation)\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Histogramme simple\nsns.histplot(data=tips, x='total_bill', kde=True, ax=axes[0], color='steelblue')\naxes[0].set_title('Distribution du montant total', fontsize=14, fontweight='bold')\n\n# Histogramme avec hue (groupement)\nsns.histplot(data=tips, x='total_bill', hue='time', kde=True, ax=axes[1])\naxes[1].set_title('Distribution par moment de la journÃ©e', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nVoir le code\n# KDE plot (densitÃ© de probabilitÃ©)\nplt.figure(figsize=(10, 6))\nsns.kdeplot(data=tips, x='total_bill', hue='day', fill=True, alpha=0.5)\nplt.title('DensitÃ© du montant total par jour', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-donnÃ©es-catÃ©gorielles",
    "title": "Python for Data Processing",
    "section": "Visualisation des donnÃ©es catÃ©gorielles",
    "text": "Visualisation des donnÃ©es catÃ©gorielles\n\n\nVoir le code\n# Box plot - Distribution par catÃ©gorie\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Box plot simple\nsns.boxplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Set2')\naxes[0].set_title('Montant total par jour', fontsize=14, fontweight='bold')\n\n# Box plot avec hue\nsns.boxplot(data=tips, x='day', y='total_bill', hue='sex', ax=axes[1], palette='Set1')\naxes[1].set_title('Montant total par jour et sexe', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nVoir le code\n# Violin plot - Combine box plot et KDE\nplt.figure(figsize=(12, 6))\nsns.violinplot(data=tips, x='day', y='total_bill', hue='sex', split=True, palette='muted')\nplt.title('Distribution du montant par jour et sexe (Violin Plot)', fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nVoir le code\n# Bar plot avec estimation statistique\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Bar plot avec intervalle de confiance\nsns.barplot(data=tips, x='day', y='total_bill', ax=axes[0], palette='Blues_d', errorbar='ci')\naxes[0].set_title('Montant moyen par jour (avec IC 95%)', fontsize=14, fontweight='bold')\n\n# Count plot (compte les occurrences)\nsns.countplot(data=tips, x='day', hue='time', ax=axes[1], palette='Set2')\naxes[1].set_title('Nombre de repas par jour', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#visualisation-des-relations-entre-variables",
    "title": "Python for Data Processing",
    "section": "Visualisation des relations entre variables",
    "text": "Visualisation des relations entre variables\n\n\nVoir le code\n# Scatter plot avec rÃ©gression\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Scatter plot simple avec rÃ©gression\nsns.regplot(data=tips, x='total_bill', y='tip', ax=axes[0], color='coral')\naxes[0].set_title('Relation montant/pourboire avec rÃ©gression', fontsize=14, fontweight='bold')\n\n# Scatter plot avec hue et style\nsns.scatterplot(data=tips, x='total_bill', y='tip', hue='time', style='sex', \n                size='size', sizes=(50, 200), ax=axes[1], palette='Set1')\naxes[1].set_title('Relation multidimensionnelle', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nVoir le code\n# lmplot - RÃ©gression avec facettes\ng = sns.lmplot(data=tips, x='total_bill', y='tip', hue='smoker', col='time', \n               height=5, aspect=1.2, palette='Set1')\ng.fig.suptitle('RÃ©gression par moment et statut fumeur', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#heatmaps-et-matrices-de-corrÃ©lation",
    "title": "Python for Data Processing",
    "section": "Heatmaps et matrices de corrÃ©lation",
    "text": "Heatmaps et matrices de corrÃ©lation\n\n\nVoir le code\n# Matrice de corrÃ©lation\n# SÃ©lectionner uniquement les colonnes numÃ©riques\nnumeric_cols = tips.select_dtypes(include=[np.number])\ncorrelation_matrix = numeric_cols.corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            fmt='.2f', linewidths=0.5, square=True)\nplt.title('Matrice de corrÃ©lation', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n\n\n\nVoir le code\n# Heatmap de donnÃ©es pivotÃ©es\npivot_data = tips.pivot_table(values='tip', index='day', columns='time', aggfunc='mean')\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(pivot_data, annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)\nplt.title('Pourboire moyen par jour et moment', fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pair-plots-visualisation-multivariÃ©e",
    "title": "Python for Data Processing",
    "section": "Pair plots (visualisation multivariÃ©e)",
    "text": "Pair plots (visualisation multivariÃ©e)\n\n\nVoir le code\n# Pair plot - Toutes les combinaisons de variables\ng = sns.pairplot(tips, hue='time', palette='Set1', diag_kind='kde', \n                 plot_kws={'alpha': 0.6}, height=2.5)\ng.fig.suptitle('Pair Plot - Dataset Tips', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#facetgrid---graphiques-multi-facettes",
    "title": "Python for Data Processing",
    "section": "FacetGrid - Graphiques multi-facettes",
    "text": "FacetGrid - Graphiques multi-facettes\n\n\nVoir le code\n# CrÃ©er une grille de facettes\ng = sns.FacetGrid(tips, col='time', row='smoker', height=4, aspect=1.2)\ng.map_dataframe(sns.histplot, x='total_bill', kde=True)\ng.add_legend()\ng.fig.suptitle('Distribution du montant par temps et statut fumeur', y=1.02, \n               fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#joint-plots---distributions-jointes",
    "title": "Python for Data Processing",
    "section": "Joint plots - Distributions jointes",
    "text": "Joint plots - Distributions jointes\n\n\nVoir le code\n# Joint plot avec distributions marginales\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='reg', \n                  height=8, ratio=4, color='coral')\ng.fig.suptitle('Distribution jointe montant/pourboire', y=1.02, fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\nVoir le code\n# Joint plot avec hexbin (pour grandes quantitÃ©s de donnÃ©es)\ng = sns.jointplot(data=tips, x='total_bill', y='tip', kind='hex', \n                  height=8, ratio=4, cmap='Blues')\ng.fig.suptitle('Distribution jointe (Hexbin)', y=1.02, fontsize=14, fontweight='bold')\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#personnalisation-des-styles",
    "title": "Python for Data Processing",
    "section": "Personnalisation des styles",
    "text": "Personnalisation des styles\n\n\nVoir le code\n# Explorer diffÃ©rents styles\nstyles = ['white', 'dark', 'whitegrid', 'darkgrid', 'ticks']\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\n\nfor ax, style in zip(axes, styles):\n    with sns.axes_style(style):\n        sns.histplot(tips['total_bill'], ax=ax, color='steelblue')\n        ax.set_title(f\"Style: {style}\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nVoir le code\n# Palettes de couleurs\npalettes = ['deep', 'muted', 'bright', 'pastel', 'dark', 'colorblind']\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor ax, palette in zip(axes, palettes):\n    sns.barplot(data=tips, x='day', y='total_bill', palette=palette, ax=ax)\n    ax.set_title(f\"Palette: {palette}\", fontsize=12)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exemple-tableau-de-bord-complet",
    "title": "Python for Data Processing",
    "section": "Exemple : Tableau de bord complet",
    "text": "Exemple : Tableau de bord complet\n\n\nVoir le code\n# CrÃ©er un tableau de bord d'analyse complet\nsns.set_theme(style='whitegrid')\n\nfig = plt.figure(figsize=(16, 12))\n\n# CrÃ©er une grille personnalisÃ©e\ngs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n\n# 1. Distribution des montants\nax1 = fig.add_subplot(gs[0, 0])\nsns.histplot(data=tips, x='total_bill', kde=True, ax=ax1, color='steelblue')\nax1.set_title('Distribution des montants', fontweight='bold')\n\n# 2. Distribution des pourboires\nax2 = fig.add_subplot(gs[0, 1])\nsns.histplot(data=tips, x='tip', kde=True, ax=ax2, color='coral')\nax2.set_title('Distribution des pourboires', fontweight='bold')\n\n# 3. Relation montant/pourboire\nax3 = fig.add_subplot(gs[0, 2])\nsns.regplot(data=tips, x='total_bill', y='tip', ax=ax3, color='purple', scatter_kws={'alpha':0.5})\nax3.set_title('Montant vs Pourboire', fontweight='bold')\n\n# 4. Boxplot par jour\nax4 = fig.add_subplot(gs[1, 0])\nsns.boxplot(data=tips, x='day', y='total_bill', ax=ax4, palette='Set2')\nax4.set_title('Montants par jour', fontweight='bold')\n\n# 5. Violin plot par temps\nax5 = fig.add_subplot(gs[1, 1])\nsns.violinplot(data=tips, x='time', y='total_bill', hue='sex', split=True, ax=ax5, palette='muted')\nax5.set_title('Distribution par temps et sexe', fontweight='bold')\n\n# 6. Count plot\nax6 = fig.add_subplot(gs[1, 2])\nsns.countplot(data=tips, x='day', hue='time', ax=ax6, palette='Set1')\nax6.set_title('Nombre de repas', fontweight='bold')\n\n# 7. Heatmap de corrÃ©lation (grande)\nax7 = fig.add_subplot(gs[2, :])\npivot = tips.pivot_table(values='tip', index='day', columns='size', aggfunc='mean')\nsns.heatmap(pivot, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax7, linewidths=0.5)\nax7.set_title('Pourboire moyen par jour et taille de groupe', fontweight='bold')\n\nplt.suptitle('ğŸ“Š Tableau de bord - Analyse des pourboires', fontsize=18, fontweight='bold', y=1.01)\nplt.tight_layout()\nplt.savefig('dashboard_seaborn.png', dpi=300, bbox_inches='tight')\nprint(\"âœ… Dashboard sauvegardÃ© en PNG\")\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-seaborn",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique : Seaborn",
    "text": "Exercice Pratique : Seaborn\nObjectif : Analyser le dataset â€˜titanicâ€™ de Seaborn\n\nCharger le dataset avec sns.load_dataset('titanic')\nCrÃ©er un tableau de bord avec :\n\nDistribution des Ã¢ges par classe (violin plot)\nTaux de survie par sexe et classe (bar plot)\nMatrice de corrÃ©lation des variables numÃ©riques (heatmap)\nRelation Ã¢ge/tarif avec survie en couleur (scatter plot)\n\nUtiliser FacetGrid pour analyser les survivants par sexe et classe\nSauvegarder votre tableau de bord\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Charger le dataset\ntitanic = sns.load_dataset('titanic')\nprint(titanic.head())\nprint(f\"\\nDimensions : {titanic.shape}\")\n\n# Votre code de visualisation ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntitanic = sns.load_dataset('titanic')\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Violin plot\nsns.violinplot(data=titanic, x='class', y='age', hue='survived', \n               split=True, ax=axes[0,0])\naxes[0,0].set_title('Ã‚ge par classe')\n\n# Bar plot survie\ntitanic.groupby(['sex','class'])['survived'].mean().unstack().plot(\n    kind='bar', ax=axes[0,1])\naxes[0,1].set_title('Survie par sexe/classe')\n\n# Heatmap\nsns.heatmap(titanic.select_dtypes(include=[np.number]).corr(), \n            annot=True, cmap='coolwarm', ax=axes[1,0])\n\n# Scatter\nsns.scatterplot(data=titanic, x='age', y='fare', hue='survived', ax=axes[1,1])\n\nplt.tight_layout()\nplt.savefig('titanic_dashboard.png')\nplt.show()\n\n# FacetGrid\ng = sns.FacetGrid(titanic, col='sex', row='class', hue='survived')\ng.map(sns.histplot, 'age')\ng.add_legend()\nplt.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#nettoyage-de-base",
    "title": "Python for Data Processing",
    "section": "2.1 Nettoyage de base",
    "text": "2.1 Nettoyage de base\n\n\nVoir le code\n# DonnÃ©es textuelles brutes\ntextes = pd.DataFrame({\n    'texte': [\n        '  BONJOUR   ',\n        'Salut tout le monde!',\n        'Python_est_gÃ©nial',\n        'Data-Engineering-2024'\n    ]\n})\n\n# Nettoyage basique\ntextes['clean'] = textes['texte'].str.strip()  # Supprimer espaces\ntextes['lower'] = textes['texte'].str.lower()  # Minuscules\ntextes['upper'] = textes['texte'].str.upper()  # Majuscules\ntextes['replace'] = textes['texte'].str.replace('_', ' ')  # Remplacer\n\nprint(\"ğŸ§¹ Nettoyage de texte :\")\nprint(textes)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#mÃ©thodes-pandas-string-.str-accessor",
    "title": "Python for Data Processing",
    "section": "2.2 MÃ©thodes Pandas string (.str accessor)",
    "text": "2.2 MÃ©thodes Pandas string (.str accessor)\n\n\nVoir le code\n# DonnÃ©es d'exemple\ndf_text = pd.DataFrame({\n    'email': ['alice@example.com', 'bob@test.org', 'charlie@mail.fr'],\n    'nom_complet': ['Jean Dupont', 'Marie Martin', 'Pierre Durand'],\n    'telephone': ['0612345678', '06-98-76-54-32', '06 11 22 33 44']\n})\n\n# VÃ©rifier si contient\ndf_text['email_gmail'] = df_text['email'].str.contains('gmail')\n\n# Commencer/finir par\ndf_text['email_com'] = df_text['email'].str.endswith('.com')\n\n# Extraire le domaine\ndf_text['domaine'] = df_text['email'].str.split('@').str[1]\n\n# SÃ©parer nom et prÃ©nom\ndf_text[['prenom', 'nom']] = df_text['nom_complet'].str.split(' ', expand=True)\n\n# Longueur\ndf_text['longueur_nom'] = df_text['nom_complet'].str.len()\n\nprint(\"ğŸ”¤ MÃ©thodes string :\")\nprint(df_text)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#expressions-rÃ©guliÃ¨res-regex",
    "title": "Python for Data Processing",
    "section": "2.3 Expressions rÃ©guliÃ¨res (Regex)",
    "text": "2.3 Expressions rÃ©guliÃ¨res (Regex)\n\n\nVoir le code\nimport re\n\n# Exemples de regex courantes\ntexte_test = \"\"\"\nContact: alice@example.com ou bob@test.org\nTÃ©lÃ©phones: 06.12.34.56.78, 01-23-45-67-89\nURL: https://www.example.com\nPrix: 29.99â‚¬, 15.50â‚¬, 100â‚¬\n\"\"\"\n\n# Extraire les emails\nemails = re.findall(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', texte_test)\nprint(\"ğŸ“§ Emails trouvÃ©s :\")\nprint(emails)\n\n# Extraire les tÃ©lÃ©phones\ntelephones = re.findall(r'\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}[-.\\s]?\\d{2}', texte_test)\nprint(\"\\nğŸ“ TÃ©lÃ©phones trouvÃ©s :\")\nprint(telephones)\n\n# Extraire les URLs\nurls = re.findall(r'https?://[^\\s]+', texte_test)\nprint(\"\\nğŸ”— URLs trouvÃ©es :\")\nprint(urls)\n\n# Extraire les prix\nprix = re.findall(r'\\d+\\.?\\d*â‚¬', texte_test)\nprint(\"\\nğŸ’° Prix trouvÃ©s :\")\nprint(prix)\n\n\n\n\nVoir le code\n# Validation avec regex\ndef valider_email(email):\n    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n    return bool(re.match(pattern, email))\n\n# Test\nemails_test = ['alice@example.com', 'bob@invalid', 'charlie.fr', 'david@test.org']\nfor email in emails_test:\n    valide = \"âœ…\" if valider_email(email) else \"âŒ\"\n    print(f\"{valide} {email}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#cas-dusage-rÃ©els-parsing-de-logs",
    "title": "Python for Data Processing",
    "section": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs",
    "text": "2.4 Cas dâ€™usage rÃ©els : Parsing de logs\n\n\nVoir le code\n# Exemple de logs Apache/Nginx\nlogs = \"\"\"\n192.168.1.1 - - [01/Dec/2024:10:15:30 +0000] \"GET /api/users HTTP/1.1\" 200 1234\n192.168.1.2 - - [01/Dec/2024:10:16:45 +0000] \"POST /api/login HTTP/1.1\" 401 567\n192.168.1.3 - - [01/Dec/2024:10:17:20 +0000] \"GET /api/products HTTP/1.1\" 200 8901\n\"\"\"\n\n# Pattern pour parser les logs\npattern = r'(\\S+) - - \\[([^\\]]+)\\] \"(\\S+) (\\S+) (\\S+)\" (\\d+) (\\d+)'\n\n# Extraire les informations\nmatches = re.findall(pattern, logs)\n\n# CrÃ©er un DataFrame\ndf_logs = pd.DataFrame(matches, columns=[\n    'ip', 'timestamp', 'methode', 'endpoint', 'protocole', 'status', 'bytes'\n])\n\n# Convertir les types\ndf_logs['status'] = df_logs['status'].astype(int)\ndf_logs['bytes'] = df_logs['bytes'].astype(int)\n\nprint(\"ğŸ“‹ Logs parsÃ©s :\")\nprint(df_logs)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-de-lencodage",
    "title": "Python for Data Processing",
    "section": "2.5 Gestion de lâ€™encodage",
    "text": "2.5 Gestion de lâ€™encodage\n\n\nVoir le code\n# CrÃ©er un fichier avec encodage spÃ©cifique\ntexte_accentue = \"Voici du texte avec des accents : Ã©Ã Ã¹Ã´ Ã§Ã±\"\n\n# Sauvegarder en UTF-8\nwith open('test_utf8.txt', 'w', encoding='utf-8') as f:\n    f.write(texte_accentue)\n\n# Sauvegarder en Latin-1\nwith open('test_latin1.txt', 'w', encoding='latin-1') as f:\n    f.write(texte_accentue)\n\n# Lire avec le bon encodage\nprint(\"âœ… Lecture UTF-8 :\")\nwith open('test_utf8.txt', 'r', encoding='utf-8') as f:\n    print(f.read())\n\nprint(\"\\nâœ… Lecture Latin-1 :\")\nwith open('test_latin1.txt', 'r', encoding='latin-1') as f:\n    print(f.read())\n\n\n\n\nVoir le code\n# DÃ©tecter l'encodage automatiquement\n!pip install chardet\n\nimport chardet\n\n# DÃ©tecter l'encodage d'un fichier\nwith open('test_utf8.txt', 'rb') as f:\n    result = chardet.detect(f.read())\n    print(f\"Encodage dÃ©tectÃ© : {result['encoding']} (confiance: {result['confidence']*100:.1f}%)\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-2-texte-et-regex",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique 2 : Texte et Regex",
    "text": "Exercice Pratique 2 : Texte et Regex\nObjectif : Nettoyer et valider des donnÃ©es clients\n\nCrÃ©er un DataFrame avec nom, email, tÃ©lÃ©phone\nNettoyer les noms (trim, capitaliser)\nValider les emails avec regex\nNormaliser les numÃ©ros de tÃ©lÃ©phone (format uniforme)\nExporter les donnÃ©es valides uniquement\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({\n    'nom': ['  alice DUPONT  ', 'BOB martin', 'Charlie Brown'],\n    'email': ['alice@gmail.com', 'bob@invalid', 'charlie@test.fr'],\n    'telephone': ['06 12 34 56 78', '+33698765432', '06-11-22-33-44']\n})\n\n# Nettoyer noms\ndf['nom_clean'] = df['nom'].str.strip().str.title()\n\n# Valider emails\nemail_re = r'^[\\w.+-]+@[\\w-]+\\.[a-z]{2,}$'\ndf['email_ok'] = df['email'].apply(lambda x: bool(re.match(email_re, x)))\n\n# Normaliser tÃ©lÃ©phones\ndef norm_tel(t):\n    d = re.sub(r'\\D', '', t)\n    if d.startswith('33'): d = '0' + d[2:]\n    return ' '.join([d[i:i+2] for i in range(0,10,2)]) if len(d)==10 else None\n\ndf['tel_clean'] = df['telephone'].apply(norm_tel)\n\n# Export valides\ndf[df['email_ok'] & df['tel_clean'].notna()].to_csv('clients_ok.csv', index=False)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#manipulation-de-json",
    "title": "Python for Data Processing",
    "section": "3.1 Manipulation de JSON",
    "text": "3.1 Manipulation de JSON\n\n\nVoir le code\nimport json\n\n# CrÃ©er un dictionnaire Python\ndata = {\n    \"nom\": \"Alice\",\n    \"age\": 30,\n    \"competences\": [\"Python\", \"SQL\", \"Pandas\"],\n    \"actif\": True\n}\n\n# Convertir en JSON\njson_str = json.dumps(data, indent=2)\nprint(\"JSON formatÃ© :\")\nprint(json_str)\n\n# Reconvertir en dictionnaire\ndata_reloaded = json.loads(json_str)\nprint(\"\\n RechargÃ© :\")\nprint(data_reloaded)\n\n\n\n\nVoir le code\n# Sauvegarder et lire des fichiers JSON\n\n# Sauvegarder\nwith open('data.json', 'w', encoding='utf-8') as f:\n    json.dump(data, f, indent=2, ensure_ascii=False)\nprint(\"âœ… JSON sauvegardÃ©\")\n\n# Lire\nwith open('data.json', 'r', encoding='utf-8') as f:\n    data_loaded = json.load(f)\nprint(\"\\n JSON chargÃ© :\")\nprint(data_loaded)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#appels-api-avec-requests",
    "title": "Python for Data Processing",
    "section": "3.2 Appels API avec requests",
    "text": "3.2 Appels API avec requests\n\n\nVoir le code\nimport requests\n\n# API publique gratuite : JSONPlaceholder\nurl = \"https://jsonplaceholder.typicode.com/users\"\n\n# GET Request\nresponse = requests.get(url)\n\n# VÃ©rifier le statut\nprint(f\"Status code: {response.status_code}\")\n\nif response.status_code == 200:\n    users = response.json()\n    print(f\"\\nâœ… {len(users)} utilisateurs rÃ©cupÃ©rÃ©s\")\n    print(\"\\nPremier utilisateur :\")\n    print(json.dumps(users[0], indent=2))\nelse:\n    print(\"âŒ Erreur lors de la requÃªte\")\n\n\n\n\nVoir le code\n# Convertir en DataFrame\ndf_users = pd.json_normalize(users)\nprint(\"ğŸ‘¥ DataFrame des utilisateurs :\")\nprint(df_users.head())\nprint(f\"\\nColonnes : {df_users.columns.tolist()}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-erreurs-http",
    "title": "Python for Data Processing",
    "section": "3.3 Gestion des erreurs HTTP",
    "text": "3.3 Gestion des erreurs HTTP\n\n\nVoir le code\ndef fetch_data_safe(url):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec gestion d'erreurs\"\"\"\n    try:\n        response = requests.get(url, timeout=10)\n        response.raise_for_status()  # LÃ¨ve une exception si statut &gt;= 400\n        return response.json()\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : le serveur met trop de temps Ã  rÃ©pondre\")\n        return None\n    except requests.exceptions.HTTPError as e:\n        print(f\"âŒ Erreur HTTP : {e}\")\n        return None\n    except requests.exceptions.RequestException as e:\n        print(f\"âŒ Erreur de connexion : {e}\")\n        return None\n\n# Test avec une URL valide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"âœ… DonnÃ©es rÃ©cupÃ©rÃ©es :\")\n    print(json.dumps(data, indent=2))\n\n# Test avec une URL invalide\ndata = fetch_data_safe(\"https://jsonplaceholder.typicode.com/invalid\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#authentification-api",
    "title": "Python for Data Processing",
    "section": "3.4 Authentification API",
    "text": "3.4 Authentification API\n\n\nVoir le code\n# Exemple 1 : API Key dans les headers\nheaders = {\n    \"Authorization\": \"Bearer YOUR_API_KEY_HERE\",\n    \"Content-Type\": \"application/json\"\n}\n\n# response = requests.get(url, headers=headers)\n\n# Exemple 2 : API Key dans les paramÃ¨tres\nparams = {\n    \"api_key\": \"YOUR_API_KEY_HERE\",\n    \"format\": \"json\"\n}\n\n# response = requests.get(url, params=params)\n\n# Exemple 3 : Basic Auth\nfrom requests.auth import HTTPBasicAuth\n\n# response = requests.get(url, auth=HTTPBasicAuth('username', 'password'))\n\nprint(\"ğŸ’¡ Les exemples ci-dessus montrent diffÃ©rentes mÃ©thodes d'authentification\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pagination-dapis",
    "title": "Python for Data Processing",
    "section": "3.5 Pagination dâ€™APIs",
    "text": "3.5 Pagination dâ€™APIs\n\n\nVoir le code\ndef fetch_all_pages(base_url, max_pages=5):\n    \"\"\"RÃ©cupÃ¨re toutes les pages d'une API paginÃ©e\"\"\"\n    all_data = []\n    \n    for page in range(1, max_pages + 1):\n        url = f\"{base_url}?_page={page}&_limit=10\"\n        print(f\" RÃ©cupÃ©ration page {page}...\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            if not data:  # Plus de donnÃ©es\n                break\n            all_data.extend(data)\n        else:\n            print(f\"âŒ Erreur page {page}\")\n            break\n    \n    return all_data\n\n# Test avec JSONPlaceholder\nposts = fetch_all_pages(\"https://jsonplaceholder.typicode.com/posts\", max_pages=3)\nprint(f\"\\nâœ… Total rÃ©cupÃ©rÃ© : {len(posts)} posts\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#rate-limiting-et-retry-logic",
    "title": "Python for Data Processing",
    "section": "3.6 Rate Limiting et Retry Logic",
    "text": "3.6 Rate Limiting et Retry Logic\n\n\nVoir le code\nimport time\nfrom datetime import datetime\n\ndef fetch_with_retry(url, max_retries=3, delay=2):\n    \"\"\"RÃ©cupÃ¨re des donnÃ©es avec retry et backoff exponentiel\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"Tentative {attempt + 1}/{max_retries}\")\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            print(f\"âŒ Erreur : {e}\")\n            if attempt &lt; max_retries - 1:\n                wait_time = delay * (2 ** attempt)  # Backoff exponentiel\n                print(f\"â³ Attente de {wait_time}s avant nouvelle tentative...\")\n                time.sleep(wait_time)\n            else:\n                print(\"âŒ Ã‰chec aprÃ¨s toutes les tentatives\")\n                return None\n\n# Test\ndata = fetch_with_retry(\"https://jsonplaceholder.typicode.com/users/1\")\nif data:\n    print(\"\\nâœ… SuccÃ¨s !\")\n\n\n\n\nVoir le code\n# Rate limiting simple\ndef fetch_with_rate_limit(urls, requests_per_second=2):\n    \"\"\"RÃ©cupÃ¨re plusieurs URLs en respectant un rate limit\"\"\"\n    delay = 1.0 / requests_per_second\n    results = []\n    \n    for url in urls:\n        start = time.time()\n        print(f\"â¬ RÃ©cupÃ©ration : {url}\")\n        \n        response = requests.get(url)\n        if response.status_code == 200:\n            results.append(response.json())\n        \n        elapsed = time.time() - start\n        sleep_time = max(0, delay - elapsed)\n        if sleep_time &gt; 0:\n            time.sleep(sleep_time)\n    \n    return results\n\n# Test\nurls = [\n    \"https://jsonplaceholder.typicode.com/users/1\",\n    \"https://jsonplaceholder.typicode.com/users/2\",\n    \"https://jsonplaceholder.typicode.com/users/3\"\n]\n\nstart_time = time.time()\nresults = fetch_with_rate_limit(urls, requests_per_second=1)\ntotal_time = time.time() - start_time\n\nprint(f\"\\nâœ… {len(results)} URLs rÃ©cupÃ©rÃ©es en {total_time:.2f}s\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#json-imbriquÃ©-complexe",
    "title": "Python for Data Processing",
    "section": "3.7 JSON imbriquÃ© complexe",
    "text": "3.7 JSON imbriquÃ© complexe\n\n\nVoir le code\n# JSON complexe imbriquÃ©\ncomplex_json = {\n    \"id\": 1,\n    \"nom\": \"Entreprise A\",\n    \"employes\": [\n        {\n            \"id\": 101,\n            \"nom\": \"Alice\",\n            \"competences\": [\"Python\", \"SQL\"],\n            \"adresse\": {\"ville\": \"Paris\", \"code_postal\": \"75001\"}\n        },\n        {\n            \"id\": 102,\n            \"nom\": \"Bob\",\n            \"competences\": [\"Java\", \"Docker\"],\n            \"adresse\": {\"ville\": \"Lyon\", \"code_postal\": \"69001\"}\n        }\n    ]\n}\n\n# Normaliser avec json_normalize\ndf_complex = pd.json_normalize(\n    complex_json,\n    record_path='employes',\n    meta=['nom'],\n    meta_prefix='entreprise_'\n)\n\nprint(\"ğŸ”„ JSON normalisÃ© :\")\nprint(df_complex)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-3-apis",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique 3 : APIs",
    "text": "Exercice Pratique 3 : APIs\nObjectif : RÃ©cupÃ©rer et analyser des donnÃ©es dâ€™une API publique\n\nUtiliser lâ€™API JSONPlaceholder pour rÃ©cupÃ©rer les posts\nConvertir en DataFrame\nCompter le nombre de posts par utilisateur\nRÃ©cupÃ©rer les dÃ©tails des 5 utilisateurs les plus actifs\nExporter le rÃ©sultat en JSON\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport requests\nimport pandas as pd\n\n# 1-2. RÃ©cupÃ©rer les posts\nposts = requests.get(\"https://jsonplaceholder.typicode.com/posts\").json()\ndf_posts = pd.DataFrame(posts)\n\n# 3. Posts par utilisateur\nposts_count = df_posts.groupby('userId').size().reset_index(name='nb_posts')\nposts_count = posts_count.sort_values('nb_posts', ascending=False)\n\n# 4. DÃ©tails des 5 top users\ntop_5 = posts_count.head(5)['userId'].tolist()\nusers = []\nfor uid in top_5:\n    u = requests.get(f\"https://jsonplaceholder.typicode.com/users/{uid}\").json()\n    users.append({'userId': u['id'], 'name': u['name'], 'email': u['email']})\n\ndf_result = pd.DataFrame(users).merge(posts_count, on='userId')\nprint(df_result)\n\n# 5. Export JSON\ndf_result.to_json('top_users.json', orient='records', indent=2)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#vÃ©rifications-basiques",
    "title": "Python for Data Processing",
    "section": "4.1 VÃ©rifications basiques",
    "text": "4.1 VÃ©rifications basiques\n\n\nVoir le code\n# CrÃ©er des donnÃ©es de test\ntest_data = pd.DataFrame({\n    'user_id': [1, 2, 3, 2, 5],\n    'email': ['alice@test.com', 'bob@test', None, 'bob@test', 'eve@test.com'],\n    'age': [25, 150, -5, 30, 28],\n    'salaire': [45000, 55000, 60000, 55000, None]\n})\n\nprint(\"DonnÃ©es de test :\")\nprint(test_data)\n\n# VÃ©rifications\nprint(\"\\n VÃ©rifications :\")\nprint(f\"Colonnes manquantes : {set(['user_id', 'email', 'age', 'salaire']) - set(test_data.columns)}\")\nprint(f\"Valeurs nulles : {test_data.isnull().sum().sum()}\")\nprint(f\"Doublons : {test_data.duplicated().sum()}\")\nprint(f\"Types : \\n{test_data.dtypes}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#classe-de-validation-complÃ¨te",
    "title": "Python for Data Processing",
    "section": "4.2 Classe de validation complÃ¨te",
    "text": "4.2 Classe de validation complÃ¨te\n\n\nVoir le code\nclass DataValidator:\n    \"\"\"Validateur simple pour DataFrames\"\"\"\n    \n    def __init__(self, df):\n        self.df = df\n        self.errors = []\n    \n    def check_columns(self, required_columns):\n        \"\"\"VÃ©rifie prÃ©sence des colonnes requises\"\"\"\n        missing = set(required_columns) - set(self.df.columns)\n        if missing:\n            self.errors.append(f\"Colonnes manquantes: {missing}\")\n            return False\n        return True\n    \n    def check_nulls(self, max_null_pct=10):\n        \"\"\"VÃ©rifie le pourcentage de valeurs nulles\"\"\"\n        null_pct = (self.df.isnull().sum() / len(self.df)) * 100\n        violations = null_pct[null_pct &gt; max_null_pct]\n        if not violations.empty:\n            self.errors.append(f\"Trop de nulls: {violations.to_dict()}\")\n            return False\n        return True\n    \n    def check_range(self, column, min_val, max_val):\n        \"\"\"VÃ©rifie que les valeurs sont dans une plage\"\"\"\n        if column in self.df.columns:\n            violations = self.df[(self.df[column] &lt; min_val) | (self.df[column] &gt; max_val)]\n            if len(violations) &gt; 0:\n                self.errors.append(f\"{column}: {len(violations)} valeurs hors plage [{min_val}, {max_val}]\")\n                return False\n        return True\n    \n    def check_duplicates(self, subset=None):\n        \"\"\"VÃ©rifie les doublons\"\"\"\n        duplicates = self.df.duplicated(subset=subset).sum()\n        if duplicates &gt; 0:\n            self.errors.append(f\"{duplicates} doublons trouvÃ©s\")\n            return False\n        return True\n    \n    def check_types(self, column, expected_type):\n        \"\"\"VÃ©rifie le type d'une colonne\"\"\"\n        if column in self.df.columns:\n            if self.df[column].dtype != expected_type:\n                self.errors.append(f\"{column}: type attendu {expected_type}, obtenu {self.df[column].dtype}\")\n                return False\n        return True\n    \n    def validate(self):\n        \"\"\"Retourne True si valide, False sinon\"\"\"\n        return len(self.errors) == 0\n    \n    def report(self):\n        \"\"\"GÃ©nÃ¨re un rapport de validation\"\"\"\n        return {\n            'is_valid': self.validate(),\n            'total_errors': len(self.errors),\n            'errors': self.errors\n        }\n\n# Utilisation\nvalidator = DataValidator(test_data)\nvalidator.check_columns(['user_id', 'email', 'age'])\nvalidator.check_nulls(max_null_pct=15)\nvalidator.check_range('age', 0, 120)\nvalidator.check_duplicates(subset=['user_id', 'email'])\n\nreport = validator.report()\nprint(\"\\n Rapport de validation:\")\nprint(json.dumps(report, indent=2))",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#validation-avec-schÃ©ma",
    "title": "Python for Data Processing",
    "section": "4.3 Validation avec schÃ©ma",
    "text": "4.3 Validation avec schÃ©ma\n\n\nVoir le code\n# DÃ©finir un schÃ©ma de validation\nschema = {\n    'user_id': {'type': 'int64', 'nullable': False, 'unique': True},\n    'email': {'type': 'object', 'nullable': False, 'pattern': r'.+@.+\\..+'},\n    'age': {'type': 'int64', 'nullable': False, 'min': 0, 'max': 120},\n    'salaire': {'type': 'int64', 'nullable': True, 'min': 0}\n}\n\ndef validate_schema(df, schema):\n    \"\"\"Valide un DataFrame contre un schÃ©ma\"\"\"\n    errors = []\n    \n    for column, rules in schema.items():\n        # VÃ©rifier si la colonne existe\n        if column not in df.columns:\n            errors.append(f\"Colonne manquante: {column}\")\n            continue\n        \n        # VÃ©rifier les nulls\n        if not rules.get('nullable', True) and df[column].isnull().any():\n            errors.append(f\"{column}: contient des valeurs nulles\")\n        \n        # VÃ©rifier l'unicitÃ©\n        if rules.get('unique', False) and df[column].duplicated().any():\n            errors.append(f\"{column}: contient des doublons\")\n        \n        # VÃ©rifier la plage\n        if 'min' in rules:\n            violations = df[df[column] &lt; rules['min']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &lt; {rules['min']}\")\n        \n        if 'max' in rules:\n            violations = df[df[column] &gt; rules['max']]\n            if len(violations) &gt; 0:\n                errors.append(f\"{column}: {len(violations)} valeurs &gt; {rules['max']}\")\n        \n        # VÃ©rifier le pattern (pour les strings)\n        if 'pattern' in rules:\n            pattern = rules['pattern']\n            invalid = df[column].dropna()[~df[column].dropna().str.match(pattern)]\n            if len(invalid) &gt; 0:\n                errors.append(f\"{column}: {len(invalid)} valeurs ne matchent pas le pattern\")\n    \n    return {\n        'is_valid': len(errors) == 0,\n        'errors': errors\n    }\n\n# Test\nresult = validate_schema(test_data, schema)\nprint(\"\\nğŸ“‹ Validation avec schÃ©ma :\")\nprint(json.dumps(result, indent=2))",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-pratique-4-validation",
    "title": "Python for Data Processing",
    "section": "Exercice Pratique 4 : Validation",
    "text": "Exercice Pratique 4 : Validation\nObjectif : CrÃ©er un validateur pour des transactions\n\nCrÃ©er un DataFrame de transactions (id, date, montant, type)\nDÃ©finir un schÃ©ma de validation\nValider que toutes les transactions ont un montant positif\nVÃ©rifier quâ€™il nâ€™y a pas de doublons dâ€™ID\nGÃ©nÃ©rer un rapport de qualitÃ©\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# Votre code ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport pandas as pd\nimport numpy as np\n\n# 1. CrÃ©er transactions (avec erreurs)\nnp.random.seed(42)\ndf = pd.DataFrame({\n    'id': list(range(1,51)) + [25, 30],  # doublons\n    'date': pd.date_range('2024-01-01', periods=52),\n    'montant': list(np.random.uniform(10, 500, 50)) + [-50, 0],  # nÃ©gatifs\n    'type': np.random.choice(['achat', 'remboursement'], 52)\n})\n\n# 2-4. Validation\nerreurs = []\nif (df['montant'] &lt;= 0).any():\n    erreurs.append(f\"âŒ {(df['montant']&lt;=0).sum()} montants invalides\")\nif df['id'].duplicated().any():\n    erreurs.append(f\"âŒ {df['id'].duplicated().sum()} doublons\")\n\n# 5. Rapport\nprint(\"ğŸ“‹ RAPPORT\")\nprint(f\"Valide: {len(erreurs)==0}\")\nfor e in erreurs: print(e)\n\n# Nettoyage\ndf_clean = df[(df['montant']&gt;0) & ~df['id'].duplicated(keep='first')]\nprint(f\"âœ… {len(df_clean)}/{len(df)} lignes valides\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#architecture-du-pipeline",
    "title": "Python for Data Processing",
    "section": "5.1 Architecture du pipeline",
    "text": "5.1 Architecture du pipeline\n\n\nVoir le code\n# CrÃ©er la structure de dossiers\nfrom pathlib import Path\n\ndirs = ['data/raw', 'data/processed', 'data/output', 'logs']\nfor dir_path in dirs:\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n\nprint(\"âœ… Structure de dossiers crÃ©Ã©e\")\nprint(\"\\nğŸ“ Structure :\")\nprint(\"\"\"\nproject/\nâ”œâ”€â”€ data/\nâ”‚   â”œâ”€â”€ raw/\nâ”‚   â”œâ”€â”€ processed/\nâ”‚   â””â”€â”€ output/\nâ””â”€â”€ logs/\n\"\"\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#configuration-et-logging",
    "title": "Python for Data Processing",
    "section": "5.2 Configuration et Logging",
    "text": "5.2 Configuration et Logging\n\n\nVoir le code\nimport logging\nfrom datetime import datetime\n\n# Configuration du logging\nlog_file = f\"logs/pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(log_file),\n        logging.StreamHandler()\n    ]\n)\n\nlogger = logging.getLogger('ETL_Pipeline')\nlogger.info(\"ğŸš€ Pipeline dÃ©marrÃ©\")\n\n\n\n\nVoir le code\n# Configuration centralisÃ©e\nclass Config:\n    \"\"\"Configuration du pipeline\"\"\"\n    # Chemins\n    RAW_DATA_DIR = 'data/raw'\n    PROCESSED_DATA_DIR = 'data/processed'\n    OUTPUT_DIR = 'data/output'\n    \n    # API\n    API_URL = 'https://jsonplaceholder.typicode.com'\n    API_TIMEOUT = 10\n    API_MAX_RETRIES = 3\n    \n    # Validation\n    MAX_NULL_PCT = 10\n    \n    # Export\n    EXPORT_FORMATS = ['csv', 'parquet', 'json']\n\nconfig = Config()\nlogger.info(\"âš™ï¸ Configuration chargÃ©e\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-1-extract",
    "title": "Python for Data Processing",
    "section": "5.3 Ã‰tape 1 : Extract",
    "text": "5.3 Ã‰tape 1 : Extract\n\n\nVoir le code\ndef extract_from_api(url, max_retries=3):\n    \"\"\"Extrait des donnÃ©es depuis une API\"\"\"\n    logger.info(f\"ğŸ“¥ Extraction depuis {url}\")\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=config.API_TIMEOUT)\n            response.raise_for_status()\n            data = response.json()\n            logger.info(f\"âœ… {len(data)} enregistrements extraits\")\n            return data\n        except Exception as e:\n            logger.warning(f\"âš ï¸ Tentative {attempt + 1}/{max_retries} Ã©chouÃ©e: {e}\")\n            if attempt == max_retries - 1:\n                logger.error(\"âŒ Extraction Ã©chouÃ©e\")\n                raise\n            time.sleep(2 ** attempt)\n\n# Test extraction\nusers_data = extract_from_api(f\"{config.API_URL}/users\")\ndf_raw = pd.DataFrame(users_data)\n\n# Sauvegarder les donnÃ©es brutes\nraw_file = f\"{config.RAW_DATA_DIR}/users_raw_{datetime.now().strftime('%Y%m%d')}.csv\"\ndf_raw.to_csv(raw_file, index=False)\nlogger.info(f\"ğŸ’¾ DonnÃ©es brutes sauvegardÃ©es: {raw_file}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-2-transform",
    "title": "Python for Data Processing",
    "section": "5.4 Ã‰tape 2 : Transform",
    "text": "5.4 Ã‰tape 2 : Transform\n\n\nVoir le code\ndef transform_data(df):\n    \"\"\"Transforme et nettoie les donnÃ©es\"\"\"\n    logger.info(\"ğŸ”„ DÃ©but de la transformation\")\n    \n    df_transformed = df.copy()\n    \n    # 1. Normaliser les colonnes imbriquÃ©es\n    if 'address' in df.columns:\n        address_df = pd.json_normalize(df['address'])\n        address_df.columns = ['address_' + col for col in address_df.columns]\n        df_transformed = pd.concat([df_transformed.drop('address', axis=1), address_df], axis=1)\n        logger.info(\"âœ… Colonnes adresse normalisÃ©es\")\n    \n    # 2. Nettoyer les noms de colonnes\n    df_transformed.columns = df_transformed.columns.str.lower().str.replace('.', '_')\n    logger.info(\"âœ… Noms de colonnes nettoyÃ©s\")\n    \n    # 3. GÃ©rer les valeurs manquantes\n    null_counts = df_transformed.isnull().sum()\n    if null_counts.sum() &gt; 0:\n        logger.warning(f\"âš ï¸ {null_counts.sum()} valeurs manquantes dÃ©tectÃ©es\")\n        df_transformed = df_transformed.dropna()\n        logger.info(\"âœ… Valeurs manquantes supprimÃ©es\")\n    \n    # 4. CrÃ©er des colonnes dÃ©rivÃ©es\n    if 'name' in df_transformed.columns:\n        df_transformed['name_length'] = df_transformed['name'].str.len()\n        logger.info(\"âœ… Colonne dÃ©rivÃ©e 'name_length' crÃ©Ã©e\")\n    \n    # 5. Ajouter metadata\n    df_transformed['processed_at'] = datetime.now().isoformat()\n    \n    logger.info(f\"âœ… Transformation terminÃ©e: {len(df_transformed)} lignes\")\n    return df_transformed\n\n# Test transformation\ndf_transformed = transform_data(df_raw)\nprint(\"\\nğŸ“Š DonnÃ©es transformÃ©es :\")\nprint(df_transformed.head())\nprint(f\"\\nColonnes: {df_transformed.columns.tolist()}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-3-validate",
    "title": "Python for Data Processing",
    "section": "5.5 Ã‰tape 3 : Validate",
    "text": "5.5 Ã‰tape 3 : Validate\n\n\nVoir le code\ndef validate_data(df):\n    \"\"\"Valide la qualitÃ© des donnÃ©es\"\"\"\n    logger.info(\"ğŸ” DÃ©but de la validation\")\n    \n    validator = DataValidator(df)\n    \n    # DÃ©finir les rÃ¨gles de validation\n    required_columns = ['id', 'name', 'email']\n    validator.check_columns(required_columns)\n    validator.check_nulls(max_null_pct=config.MAX_NULL_PCT)\n    validator.check_duplicates(subset=['id'])\n    \n    # GÃ©nÃ©rer le rapport\n    report = validator.report()\n    \n    if report['is_valid']:\n        logger.info(\"âœ… Validation rÃ©ussie\")\n    else:\n        logger.error(f\"âŒ Validation Ã©chouÃ©e: {report['total_errors']} erreurs\")\n        for error in report['errors']:\n            logger.error(f\"  - {error}\")\n    \n    return report\n\n# Test validation\nvalidation_report = validate_data(df_transformed)\nprint(\"\\nğŸ“‹ Rapport de validation :\")\nprint(json.dumps(validation_report, indent=2))",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#Ã©tape-4-load",
    "title": "Python for Data Processing",
    "section": "5.6 Ã‰tape 4 : Load",
    "text": "5.6 Ã‰tape 4 : Load\n\n\nVoir le code\ndef load_data(df, base_filename):\n    \"\"\"Exporte les donnÃ©es dans plusieurs formats\"\"\"\n    logger.info(\"ğŸ’¾ DÃ©but de l'export\")\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    files_created = []\n    \n    for format_type in config.EXPORT_FORMATS:\n        filename = f\"{config.OUTPUT_DIR}/{base_filename}_{timestamp}.{format_type}\"\n        \n        try:\n            if format_type == 'csv':\n                df.to_csv(filename, index=False)\n            elif format_type == 'parquet':\n                df.to_parquet(filename, index=False)\n            elif format_type == 'json':\n                df.to_json(filename, orient='records', indent=2)\n            \n            file_size = Path(filename).stat().st_size / 1024  # KB\n            logger.info(f\"âœ… Export {format_type.upper()}: {filename} ({file_size:.2f} KB)\")\n            files_created.append(filename)\n        except Exception as e:\n            logger.error(f\"âŒ Erreur export {format_type}: {e}\")\n    \n    return files_created\n\n# Test export\nexported_files = load_data(df_transformed, 'users_processed')\nprint(\"\\nğŸ“¦ Fichiers exportÃ©s :\")\nfor file in exported_files:\n    print(f\"  - {file}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#pipeline-complet",
    "title": "Python for Data Processing",
    "section": "5.7 Pipeline complet",
    "text": "5.7 Pipeline complet\n\n\nVoir le code\ndef run_pipeline():\n    \"\"\"ExÃ©cute le pipeline complet\"\"\"\n    start_time = time.time()\n    logger.info(\"=\"*50)\n    logger.info(\"ğŸš€ DÃ‰MARRAGE DU PIPELINE\")\n    logger.info(\"=\"*50)\n    \n    try:\n        # EXTRACT\n        logger.info(\"\\nğŸ“¥ PHASE 1: EXTRACTION\")\n        data = extract_from_api(f\"{config.API_URL}/users\")\n        df = pd.DataFrame(data)\n        logger.info(f\"Lignes extraites: {len(df)}\")\n        \n        # TRANSFORM\n        logger.info(\"\\nğŸ”„ PHASE 2: TRANSFORMATION\")\n        df_clean = transform_data(df)\n        logger.info(f\"Lignes aprÃ¨s transformation: {len(df_clean)}\")\n        \n        # VALIDATE\n        logger.info(\"\\nğŸ” PHASE 3: VALIDATION\")\n        validation = validate_data(df_clean)\n        \n        if not validation['is_valid']:\n            logger.error(\"âŒ Validation Ã©chouÃ©e, arrÃªt du pipeline\")\n            return False\n        \n        # LOAD\n        logger.info(\"\\nğŸ’¾ PHASE 4: EXPORT\")\n        files = load_data(df_clean, 'users_final')\n        \n        # STATISTIQUES\n        duration = time.time() - start_time\n        logger.info(\"\\n\" + \"=\"*50)\n        logger.info(\"ğŸ“Š STATISTIQUES DU PIPELINE\")\n        logger.info(\"=\"*50)\n        logger.info(f\"DurÃ©e totale: {duration:.2f}s\")\n        logger.info(f\"Lignes traitÃ©es: {len(df_clean)}\")\n        logger.info(f\"Fichiers crÃ©Ã©s: {len(files)}\")\n        logger.info(f\"Taux de rÃ©ussite: 100%\")\n        logger.info(\"=\"*50)\n        logger.info(\"âœ… PIPELINE TERMINÃ‰ AVEC SUCCÃˆS\")\n        logger.info(\"=\"*50)\n        \n        return True\n        \n    except Exception as e:\n        logger.error(f\"âŒ ERREUR FATALE: {e}\")\n        logger.exception(\"Stack trace:\")\n        return False\n\n# ExÃ©cuter le pipeline\nsuccess = run_pipeline()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#exercice-final-pipeline-complet",
    "title": "Python for Data Processing",
    "section": "Exercice Final : Pipeline Complet",
    "text": "Exercice Final : Pipeline Complet\nObjectif : CrÃ©er votre propre pipeline ETL\n\nExtraire des donnÃ©es de posts depuis JSONPlaceholder\nEnrichir avec les donnÃ©es utilisateurs\nCalculer des statistiques (posts par utilisateur, mots par post, etc.)\nValider la qualitÃ©\nExporter dans tous les formats\nAjouter un logging complet\n\n\n\nVoir le code\n# Ã€ VOUS DE JOUER ! ğŸ®\n# CrÃ©ez votre pipeline complet ici\n\n\n\n\nğŸ’¡ Cliquer pour voir la solution\n\nimport requests, pandas as pd, logging, time\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO, format='%(message)s')\nlog = logging.getLogger()\n\ndef extract():\n    log.info(\"ğŸ“¥ Extract...\")\n    posts = pd.DataFrame(requests.get(\"https://jsonplaceholder.typicode.com/posts\").json())\n    users = pd.DataFrame(requests.get(\"https://jsonplaceholder.typicode.com/users\").json())\n    return posts, users\n\ndef transform(posts, users):\n    log.info(\"ğŸ”„ Transform...\")\n    users = users[['id','name','email']].rename(columns={'id':'userId','name':'author'})\n    df = posts.merge(users, on='userId')\n    df['words'] = df['body'].str.split().str.len()\n    return df\n\ndef validate(df):\n    log.info(\"ğŸ” Validate...\")\n    return not df['id'].isna().any() and not df['id'].duplicated().any()\n\ndef load(df):\n    log.info(\"ğŸ’¾ Load...\")\n    Path('output').mkdir(exist_ok=True)\n    df.to_csv('output/posts.csv', index=False)\n    df.to_json('output/posts.json', orient='records')\n\ndef run():\n    start = time.time()\n    log.info(\"ğŸš€ START\")\n    posts, users = extract()\n    df = transform(posts, users)\n    if not validate(df): return log.error(\"âŒ FAILED\")\n    load(df)\n    log.info(f\"âœ… DONE in {time.time()-start:.1f}s - {len(df)} rows\")\n\nrun()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#gestion-des-configurations",
    "title": "Python for Data Processing",
    "section": "6ï¸âƒ£ Gestion des configurations",
    "text": "6ï¸âƒ£ Gestion des configurations\n\n\nVoir le code\n# Installer python-dotenv\n!pip install python-dotenv\n\n# CrÃ©er un fichier .env (Ã  ne JAMAIS commiter)\nenv_content = \"\"\"\nAPI_KEY=votre_cle_api_secrete\nDATABASE_URL=postgresql://user:password@localhost:5432/db\nENVIRONMENT=development\n\"\"\"\n\nwith open('.env', 'w') as f:\n    f.write(env_content)\n\nprint(\"âœ… Fichier .env crÃ©Ã©\")\nprint(\"âš ï¸ N'oubliez pas d'ajouter .env Ã  votre .gitignore !\")\n\n\n\n\nVoir le code\nfrom dotenv import load_dotenv\nimport os\n\n# Charger les variables d'environnement\nload_dotenv()\n\n# AccÃ©der aux variables\napi_key = os.getenv('API_KEY')\ndb_url = os.getenv('DATABASE_URL')\nenv = os.getenv('ENVIRONMENT')\n\nprint(f\"ğŸ”‘ API Key: {api_key[:10]}...\")\nprint(f\"ğŸ—„ï¸ Database URL: {db_url[:30]}...\")\nprint(f\"ğŸŒ Environment: {env}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#tests-unitaires-basiques",
    "title": "Python for Data Processing",
    "section": "7ï¸âƒ£ Tests unitaires basiques",
    "text": "7ï¸âƒ£ Tests unitaires basiques\n\n\nVoir le code\n# Exemple de fonction Ã  tester\ndef calculer_age_moyen(df, colonne='age'):\n    \"\"\"Calcule l'Ã¢ge moyen d'un DataFrame\"\"\"\n    if colonne not in df.columns:\n        raise ValueError(f\"Colonne '{colonne}' introuvable\")\n    return df[colonne].mean()\n\n# Tests\ndef test_calculer_age_moyen():\n    # Test avec donnÃ©es valides\n    df_test = pd.DataFrame({'age': [20, 30, 40]})\n    assert calculer_age_moyen(df_test) == 30, \"Test 1 Ã©chouÃ©\"\n    print(\"âœ… Test 1: donnÃ©es valides\")\n    \n    # Test avec colonne manquante\n    try:\n        calculer_age_moyen(pd.DataFrame({'nom': ['Alice']}), 'age')\n        print(\"âŒ Test 2 Ã©chouÃ©: devrait lever une exception\")\n    except ValueError:\n        print(\"âœ… Test 2: exception levÃ©e correctement\")\n    \n    # Test avec valeurs nulles\n    df_null = pd.DataFrame({'age': [20, None, 40]})\n    result = calculer_age_moyen(df_null)\n    assert result == 30, \"Test 3 Ã©chouÃ©\"\n    print(\"âœ… Test 3: gestion des nulls\")\n    \n    print(\"\\nğŸ‰ Tous les tests passent !\")\n\n# ExÃ©cuter les tests\ntest_calculer_age_moyen()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ce-que-tu-as-appris",
    "title": "Python for Data Processing",
    "section": "Ce que tu as appris",
    "text": "Ce que tu as appris\n\n\n\n\n\n\n\nSection\nCompÃ©tences acquises\n\n\n\n\nPandas\nManipulation de donnÃ©es, nettoyage, agrÃ©gations, merges\n\n\nMatplotlib\nGraphiques de base, personnalisation, export\n\n\nSeaborn\nVisualisations statistiques, heatmaps, pair plots\n\n\nTexte & Regex\nNettoyage, parsing de logs, expressions rÃ©guliÃ¨res\n\n\nAPIs\nAppels REST, pagination, retry logic\n\n\nValidation\nSchÃ©mas, checks de qualitÃ©\n\n\nPipeline ETL\nArchitecture complÃ¨te Extract-Transform-Load\n\n\nBonnes pratiques\nLogging, configuration, tests",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "Python for Data Processing",
    "section": "Ressources pour aller plus loin",
    "text": "Ressources pour aller plus loin\n\nDocumentation officielle\n\nPandas Documentation\nMatplotlib Documentation\nSeaborn Documentation\nRequests Documentation\n\n\n\nTutoriels et cours\n\nReal Python - Pandas\nKaggle Learn\nDataCamp\n\n\n\nOutils avancÃ©s Ã  explorer\n\nPolars â€” Alternative plus rapide Ã  Pandas\nGreat Expectations â€” Validation de donnÃ©es avancÃ©e\nPandera â€” SchÃ©mas de validation pour DataFrames",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/05_python_data_processing_for_data_engineers.html#prochaine-Ã©tape",
    "title": "Python for Data Processing",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises le traitement de donnÃ©es, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 06_intro_databases â€” Introduction aux bases de donnÃ©es\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Data Processing pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "05 Â· Python Data Processing"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html",
    "href": "notebooks/beginner/06_intro_relational_databases.html",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "",
    "text": "Ce module prÃ©sente les concepts fondamentaux des bases de donnÃ©es relationnelles.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prÃ©requis",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 05_python_data_processing\n\n\nâœ… Requis\nComprendre les structures de donnÃ©es (listes, dictionnaires)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "href": "notebooks/beginner/06_intro_relational_databases.html#objectifs-du-module",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nExpliquer ce quâ€™est une base de donnÃ©es\nComprendre le modÃ¨le relationnel (tables, colonnes, lignes)\nDÃ©finir les clÃ©s primaires et Ã©trangÃ¨res\nIdentifier les types de relations (1-1, 1-N, N-N)\nComprendre les principes de normalisation\nExpliquer les propriÃ©tÃ©s ACID\nDiffÃ©rencier OLTP et OLAP\nExpliquer ce quâ€™est un Data Warehouse et un Data Mart\nComprendre la modÃ©lisation dimensionnelle (Star/Snowflake Schema)\nDistinguer tables de faits et tables de dimensions\n\n\n\nğŸ’¡ Note : Ce module est thÃ©orique. La pratique SQL viendra au module suivant !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#cest-quoi-une-base-de-donnÃ©es",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "1. Câ€™est quoi une Base de DonnÃ©es ?",
    "text": "1. Câ€™est quoi une Base de DonnÃ©es ?\n\nDÃ©finition\nUne base de donnÃ©es est un systÃ¨me organisÃ© pour :\n\n\n\nFonction\nDescription\n\n\n\n\nğŸ’¾ Stocker\nConserver des informations de faÃ§on permanente\n\n\nğŸ” Rechercher\nRetrouver rapidement nâ€™importe quelle donnÃ©e\n\n\nâœï¸ Modifier\nMettre Ã  jour les informations\n\n\nğŸ”’ SÃ©curiser\nContrÃ´ler lâ€™accÃ¨s aux donnÃ©es sensibles\n\n\nğŸ”— Relier\nConnecter diffÃ©rentes informations entre elles\n\n\n\n\n\n\nEn une phrase\n\nâ€œUne base de donnÃ©es, câ€™est comme un classeur numÃ©rique gÃ©ant, ultra-organisÃ© et intelligent, capable de retrouver nâ€™importe quelle information parmi des milliards de donnÃ©es.â€\n\n\n\n\nFichiers vs Base de donnÃ©es\nPourquoi ne pas simplement utiliser des fichiers CSV ou Excel ?\n\n\n\n\n\n\n\n\nCritÃ¨re\nFichiers (CSV, Excel)\nBase de donnÃ©es\n\n\n\n\nAccÃ¨s concurrent\nâŒ Conflits si plusieurs utilisateurs\nâœ… GÃ©rÃ© automatiquement\n\n\nVolume\nâŒ Lent au-delÃ  de ~100K lignes\nâœ… Millions/milliards de lignes\n\n\nIntÃ©gritÃ©\nâŒ Pas de validation\nâœ… Contraintes, types\n\n\nRelations\nâŒ Difficile Ã  gÃ©rer\nâœ… Jointures natives\n\n\nSÃ©curitÃ©\nâŒ Tout ou rien\nâœ… Permissions fines\n\n\nSauvegarde\nâŒ Manuelle\nâœ… Automatique",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-sgbd-systÃ¨me-de-gestion-de-base-de-donnÃ©es",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es",
    "text": "2. Le SGBD â€” SystÃ¨me de Gestion de Base de DonnÃ©es\nUn SGBD (ou DBMS en anglais) est le logiciel qui gÃ¨re la base de donnÃ©es.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        APPLICATION                          â”‚\nâ”‚                  (Python, Java, Web...)                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ RequÃªtes SQL\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                          SGBD                               â”‚\nâ”‚              (PostgreSQL, MySQL, Oracle...)                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\nâ”‚  â”‚   Parser    â”‚  â”‚  Optimizer  â”‚  â”‚   Engine    â”‚         â”‚\nâ”‚  â”‚   (SQL)     â”‚  â”‚  (requÃªtes) â”‚  â”‚  (stockage) â”‚         â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚\n                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     FICHIERS DISQUE                         â”‚\nâ”‚                   (donnÃ©es stockÃ©es)                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nSGBD Relationnels populaires\n\n\n\n\n\n\n\n\n\nSGBD\nType\nPoints forts\nCas dâ€™usage\n\n\n\n\nPostgreSQL\nOpen source\nPuissant, extensible, SQL avancÃ©\nProduction, analytics\n\n\nMySQL\nOpen source\nSimple, rapide, trÃ¨s rÃ©pandu\nWeb, startups\n\n\nSQLite\nEmbarquÃ©\nLÃ©ger, fichier unique, zÃ©ro config\nMobile, prototypage\n\n\nOracle\nCommercial\nEntreprise, haute disponibilitÃ©\nBanques, grandes entreprises\n\n\nSQL Server\nCommercial\nIntÃ©gration Microsoft\nEntreprises Windows\n\n\n\n\nğŸ’¡ Pour ce cours, on utilisera PostgreSQL â€” le plus complet et gratuit.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "href": "notebooks/beginner/06_intro_relational_databases.html#le-modÃ¨le-relationnel",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "3. Le ModÃ¨le Relationnel",
    "text": "3. Le ModÃ¨le Relationnel\nLe modÃ¨le relationnel a Ã©tÃ© inventÃ© par Edgar F. Codd (IBM) en 1970. Câ€™est le modÃ¨le le plus utilisÃ© depuis 50 ans !\n\nVocabulaire de base\nBASE DE DONNÃ‰ES : ma_boutique\nâ”‚\nâ”œâ”€â”€ TABLE : clients\nâ”‚   â”‚\nâ”‚   â”‚    COLONNES (attributs)\nâ”‚   â”‚    â†“      â†“         â†“\nâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   â”‚  â”‚ id  â”‚  nom   â”‚     email       â”‚  â† EN-TÃŠTE\nâ”‚   â”‚  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚   â”‚  â”‚  1  â”‚ Alice  â”‚ alice@mail.com  â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  2  â”‚ Bob    â”‚ bob@mail.com    â”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â”‚  3  â”‚ Charlieâ”‚ charlie@mail.comâ”‚  â† LIGNE (tuple)\nâ”‚   â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚   â”‚    â†‘\nâ”‚   â”‚    CELLULE (valeur)\nâ”‚\nâ”œâ”€â”€ TABLE : produits\nâ”‚   â””â”€â”€ ...\nâ”‚\nâ””â”€â”€ TABLE : commandes\n    â””â”€â”€ ...\n\n\n\nTerminologie\n\n\n\nTerme technique\nTerme courant\nDescription\n\n\n\n\nRelation\nTable\nEnsemble de donnÃ©es du mÃªme type\n\n\nTuple\nLigne / Enregistrement\nUne entrÃ©e (ex: un client)\n\n\nAttribut\nColonne / Champ\nUne propriÃ©tÃ© (ex: nom, email)\n\n\nDomaine\nType\nValeurs possibles (INTEGER, VARCHARâ€¦)\n\n\nSchÃ©ma\nStructure\nDÃ©finition des colonnes et types",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-donnÃ©es",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "4. Types de donnÃ©es",
    "text": "4. Types de donnÃ©es\nChaque colonne a un type qui dÃ©finit les valeurs autorisÃ©es.\n\nTypes courants (PostgreSQL)\n\n\n\nCatÃ©gorie\nType\nDescription\nExemple\n\n\n\n\nEntiers\nINTEGER\nNombre entier\n42\n\n\n\nBIGINT\nGrand entier\n9223372036854775807\n\n\n\nSMALLINT\nPetit entier\n32767 max\n\n\nDÃ©cimaux\nDECIMAL(p,s)\nPrÃ©cision exacte\n19.99\n\n\n\nFLOAT\nApproximatif\n3.14159\n\n\nTexte\nVARCHAR(n)\nTexte variable (max n)\n'Alice'\n\n\n\nTEXT\nTexte illimitÃ©\n'Long texte...'\n\n\n\nCHAR(n)\nTexte fixe (n caractÃ¨res)\n'FR'\n\n\nBoolÃ©en\nBOOLEAN\nVrai/Faux\nTRUE, FALSE\n\n\nDate/Heure\nDATE\nDate seule\n'2024-01-15'\n\n\n\nTIMESTAMP\nDate + heure\n'2024-01-15 14:30:00'\n\n\n\nTIME\nHeure seule\n'14:30:00'\n\n\nAutres\nUUID\nIdentifiant unique\n'a0eebc99-9c0b...'\n\n\n\nJSON\nDonnÃ©es JSON\n'{\"key\": \"value\"}'\n\n\n\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nDECIMAL pour lâ€™argent\nFLOAT pour lâ€™argent (imprÃ©cis)\n\n\nVARCHAR(255) pour emails\nTEXT partout (pas de limite)\n\n\nDATE pour les dates\nVARCHAR pour les dates\n\n\nTypes les plus petits possibles\nTypes trop grands â€œau cas oÃ¹â€",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-primaire-primary-key",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "5. ClÃ© Primaire (Primary Key)",
    "text": "5. ClÃ© Primaire (Primary Key)\nLa clÃ© primaire (PK) identifie de faÃ§on unique chaque ligne dâ€™une table.\n\nRÃ¨gles\n\n\n\nRÃ¨gle\nDescription\n\n\n\n\nUnique\nDeux lignes ne peuvent pas avoir la mÃªme valeur\n\n\nNon NULL\nLa valeur doit toujours Ãªtre prÃ©sente\n\n\nImmuable\nNe devrait jamais changer\n\n\n\n\n\n\nExemple\nTable : clients\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚      email      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚ alice@mail.com  â”‚\nâ”‚    2    â”‚  Bob     â”‚ bob@mail.com    â”‚\nâ”‚    3    â”‚  Charlie â”‚ charlie@mail.comâ”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     â†‘\n     ClÃ© primaire : garantit l'unicitÃ©\n\n\n\nTypes de clÃ©s primaires\n\n\n\n\n\n\n\n\nType\nDescription\nExemple\n\n\n\n\nAuto-incrÃ©mentÃ©e\nGÃ©nÃ©rÃ©e automatiquement (1, 2, 3â€¦)\nSERIAL en PostgreSQL\n\n\nUUID\nIdentifiant universel unique\na0eebc99-9c0b-4ef8...\n\n\nNaturelle\nDonnÃ©e existante unique\nNumÃ©ro de sÃ©curitÃ© sociale\n\n\nComposite\nPlusieurs colonnes combinÃ©es\n(pays, code_postal)\n\n\n\n\nğŸ’¡ Recommandation : Utiliser SERIAL (auto-increment) ou UUID plutÃ´t quâ€™une clÃ© naturelle.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "href": "notebooks/beginner/06_intro_relational_databases.html#clÃ©-Ã©trangÃ¨re-foreign-key",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "6. ClÃ© Ã‰trangÃ¨re (Foreign Key)",
    "text": "6. ClÃ© Ã‰trangÃ¨re (Foreign Key)\nLa clÃ© Ã©trangÃ¨re (FK) crÃ©e un lien entre deux tables.\n\nPrincipe\nTable : clients                    Table : commandes\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id (PK) â”‚   nom    â”‚            â”‚ id (PK) â”‚ client_id(FK)â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚  Alice   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    1    â”‚      1      â”‚ Clavier â”‚\nâ”‚    2    â”‚  Bob     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    2    â”‚      2      â”‚ Souris  â”‚\nâ”‚    3    â”‚  Charlie â”‚            â”‚    3    â”‚      1      â”‚ Ã‰cran   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                              â†‘\n                                    ClÃ© Ã©trangÃ¨re : rÃ©fÃ©rence clients.id\n\n\n\nCe que garantit la FK\n\n\n\n\n\n\n\nGarantie\nDescription\n\n\n\n\nIntÃ©gritÃ© rÃ©fÃ©rentielle\nImpossible de rÃ©fÃ©rencer un client inexistant\n\n\nCohÃ©rence\nSi on supprime un client, que faire des commandes ?\n\n\n\n\n\n\nActions en cascade\nQue se passe-t-il si on supprime ou modifie la ligne rÃ©fÃ©rencÃ©e ?\n\n\n\nAction\nComportement\n\n\n\n\nCASCADE\nSupprime/modifie aussi les lignes liÃ©es\n\n\nSET NULL\nMet la FK Ã  NULL\n\n\nSET DEFAULT\nMet une valeur par dÃ©faut\n\n\nRESTRICT\nInterdit la suppression/modification\n\n\nNO ACTION\nComme RESTRICT (vÃ©rification diffÃ©rÃ©e)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "href": "notebooks/beginner/06_intro_relational_databases.html#types-de-relations",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "7. Types de Relations",
    "text": "7. Types de Relations\n\n1ï¸âƒ£ Relation Un-Ã -Un (1:1)\nUne ligne dans A correspond Ã  exactement une ligne dans B.\nutilisateurs              profils\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  email  â”‚         â”‚ id â”‚ user_id â”‚   bio     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ a@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 1  â”‚    1    â”‚ Dev...    â”‚\nâ”‚ 2  â”‚ b@m.com â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ 2  â”‚    2    â”‚ Designer..â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : SÃ©parer des donnÃ©es rarement utilisÃ©es (optimisation).\n\n\n\n1ï¸âƒ£â¡ï¸ğŸ”¢ Relation Un-Ã -Plusieurs (1:N)\nUne ligne dans A peut correspondre Ã  plusieurs lignes dans B.\nclients                   commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚         â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚  Alice  â”‚â—„â”€â”€â”€â”€â”¬â”€â”€â”€â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚  Bob    â”‚â—„â”€â”€â” â””â”€â”€â”€â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                         â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nCas dâ€™usage : Client â†’ Commandes, Auteur â†’ Articles, Pays â†’ Villes.\n\n\n\nğŸ”¢â†”ï¸ï¸ğŸ”¢ Relation Plusieurs-Ã -Plusieurs (N:N)\nPlusieurs lignes dans A correspondent Ã  plusieurs lignes dans B.\nNÃ©cessite une table de jonction !\netudiants          inscriptions         cours\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚  nom  â”‚    â”‚ etudiant_idâ”‚ cours_id â”‚    â”‚ id â”‚   nom   â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice â”‚â—„â”€â”€â”€â”‚     1      â”‚    1     â”‚â”€â”€â”€â–ºâ”‚ 1  â”‚  Maths  â”‚\nâ”‚ 2  â”‚ Bob   â”‚â—„â”€â”¬â”€â”‚     1      â”‚    2     â”‚â”€â”¬â”€â–ºâ”‚ 2  â”‚ Python  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚     2      â”‚    1     â”‚ â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                â””â”€â”‚     2      â”‚    2     â”‚â”€â”˜\n                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                       Table de jonction\nCas dâ€™usage : Ã‰tudiants â†”ï¸ Cours, Produits â†”ï¸ Tags, Acteurs â†”ï¸ Films.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "href": "notebooks/beginner/06_intro_relational_databases.html#normalisation",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "8. Normalisation",
    "text": "8. Normalisation\nLa normalisation consiste Ã  organiser les donnÃ©es pour :\n\nÃ‰viter la redondance (donnÃ©es dupliquÃ©es)\nÃ‰viter les anomalies (incohÃ©rences lors de modifications)\nGarantir lâ€™intÃ©gritÃ© des donnÃ©es\n\n\n\nExemple NON normalisÃ©\nTable : commandes (MAUVAIS)\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚ client  â”‚ client_email  â”‚ produit â”‚    ville     â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com   â”‚ Clavier â”‚    Paris     â”‚\nâ”‚ 2  â”‚ Alice   â”‚ alice@m.com   â”‚ Souris  â”‚    Paris     â”‚  â† Redondance !\nâ”‚ 3  â”‚ Bob     â”‚ bob@m.com     â”‚ Ã‰cran   â”‚    Lyon      â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâŒ ProblÃ¨mes :\n  - Si Alice change d'email â†’ modifier TOUTES les lignes\n  - Risque d'incohÃ©rence si on oublie une ligne\n  - Espace gaspillÃ©\n\n\n\nâœ… Exemple normalisÃ©\nTable : clients               Table : commandes\nâ”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ id â”‚   nom   â”‚    email    â”‚   â”‚ id â”‚ client_id â”‚ produit â”‚\nâ”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 1  â”‚ Alice   â”‚ alice@m.com â”‚   â”‚ 1  â”‚     1     â”‚ Clavier â”‚\nâ”‚ 2  â”‚ Bob     â”‚ bob@m.com   â”‚   â”‚ 2  â”‚     1     â”‚ Souris  â”‚\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ 3  â”‚     2     â”‚ Ã‰cran   â”‚\n                                 â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… Avantages :\n  - Email modifiÃ© une seule fois\n  - Pas de redondance\n  - DonnÃ©es cohÃ©rentes\n\n\n\nFormes normales (rÃ©sumÃ©)\n\n\n\nForme\nRÃ¨gle principale\n\n\n\n\n1NF\nChaque cellule contient une seule valeur (pas de listes)\n\n\n2NF\n1NF + chaque colonne dÃ©pend de TOUTE la clÃ© primaire\n\n\n3NF\n2NF + pas de dÃ©pendance entre colonnes non-clÃ©s\n\n\n\n\nğŸ’¡ En pratique, la 3NF est gÃ©nÃ©ralement suffisante pour les bases OLTP.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "href": "notebooks/beginner/06_intro_relational_databases.html#propriÃ©tÃ©s-acid",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "9. PropriÃ©tÃ©s ACID",
    "text": "9. PropriÃ©tÃ©s ACID\nACID garantit la fiabilitÃ© des transactions dans une base relationnelle.\n\nLes 4 propriÃ©tÃ©s\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   TRANSACTION   â”‚\n                    â”‚   (ex: virement â”‚\n                    â”‚    bancaire)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n   â”‚ ATOMICITÃ‰ â”‚      â”‚ COHÃ‰RENCE â”‚      â”‚ ISOLATION â”‚\n   â”‚  Tout ou  â”‚      â”‚   Ã‰tat    â”‚      â”‚Transactionsâ”‚\n   â”‚   rien    â”‚      â”‚  valide   â”‚      â”‚ sÃ©parÃ©es  â”‚\n   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                             â–¼\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚ DURABILITÃ‰â”‚\n                      â”‚ Permanent â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nDÃ©tail de chaque propriÃ©tÃ©\n\n\n\n\n\n\n\n\n\nLettre\nPropriÃ©tÃ©\nDescription\nExemple\n\n\n\n\nA\nAtomicitÃ©\nTout ou rien â€” si une partie Ã©choue, tout est annulÃ©\nVirement : dÃ©bit ET crÃ©dit rÃ©ussissent ou rien\n\n\nC\nCohÃ©rence\nLa base reste dans un Ã©tat valide\nLe solde ne peut pas Ãªtre nÃ©gatif\n\n\nI\nIsolation\nLes transactions concurrentes ne sâ€™interfÃ¨rent pas\nDeux virements simultanÃ©s ne se mÃ©langent pas\n\n\nD\nDurabilitÃ©\nUne fois validÃ©e, la transaction est permanente\nMÃªme aprÃ¨s un crash, le virement est enregistrÃ©\n\n\n\n\n\n\nExemple : Virement bancaire\nTRANSACTION : Virer 100â‚¬ de Alice vers Bob\n\n  1. DÃ©biter 100â‚¬ du compte Alice\n  2. CrÃ©diter 100â‚¬ sur le compte Bob\n\nATOMICITÃ‰ :\n  âœ… Les deux opÃ©rations rÃ©ussissent â†’ COMMIT\n  âŒ Une opÃ©ration Ã©choue â†’ ROLLBACK (rien ne change)\n\nCOHÃ‰RENCE :\n  âœ… Alice : 500â‚¬ â†’ 400â‚¬\n  âœ… Bob   : 200â‚¬ â†’ 300â‚¬\n  âœ… Total : 700â‚¬ â†’ 700â‚¬ (inchangÃ©)\n\nISOLATION :\n  Un autre virement simultanÃ© ne voit pas l'Ã©tat intermÃ©diaire\n\nDURABILITÃ‰ :\n  MÃªme si le serveur crash juste aprÃ¨s le COMMIT,\n  le virement sera toujours lÃ  au redÃ©marrage",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#oltp-vs-olap-deux-mondes-diffÃ©rents",
    "href": "notebooks/beginner/06_intro_relational_databases.html#oltp-vs-olap-deux-mondes-diffÃ©rents",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "10. OLTP vs OLAP â€” Deux mondes diffÃ©rents",
    "text": "10. OLTP vs OLAP â€” Deux mondes diffÃ©rents\nLes bases de donnÃ©es relationnelles peuvent servir Ã  deux usages trÃ¨s diffÃ©rents. Comprendre cette distinction est fondamental en Data Engineering.\n\n\n10.1 OLTP â€” Online Transaction Processing\nLes bases OLTP gÃ¨rent les opÃ©rations quotidiennes dâ€™une entreprise.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nBut\nGÃ©rer les transactions courantes\n\n\nOpÃ©rations\nINSERT, UPDATE, DELETE frÃ©quents\n\n\nRequÃªtes\nSimples, sur peu de lignes\n\n\nUtilisateurs\nApplications, employÃ©s\n\n\nVolume par requÃªte\nQuelques lignes\n\n\nPrioritÃ©\nRapiditÃ©, disponibilitÃ©\n\n\nSchÃ©ma\nNormalisÃ© (3NF)\n\n\n\nExemples : - Application e-commerce (commandes, paiements) - SystÃ¨me bancaire (virements, retraits) - Gestion de stock (entrÃ©es, sorties) - RÃ©servation de billets\n\n\n\n10.2 OLAP â€” Online Analytical Processing\nLes bases OLAP sont conÃ§ues pour lâ€™analyse de donnÃ©es et le reporting.\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nBut\nAnalyser les donnÃ©es historiques\n\n\nOpÃ©rations\nSELECT complexes (agrÃ©gations, jointures)\n\n\nRequÃªtes\nComplexes, sur des millions de lignes\n\n\nUtilisateurs\nAnalystes, Data Scientists, dirigeants\n\n\nVolume par requÃªte\nDes millions/milliards de lignes\n\n\nPrioritÃ©\nPerformance analytique\n\n\nSchÃ©ma\nDÃ©normalisÃ© (Star Schema, Snowflake)\n\n\n\nExemples : - Rapport des ventes par rÃ©gion/mois - Analyse du comportement client - Tableaux de bord (dashboards) - PrÃ©visions et tendances\n\n\n\n10.3 Comparaison OLTP vs OLAP\n           OLTP                              OLAP\n    (Transactionnel)                    (Analytique)\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚  INSERT   â”‚                      â”‚  SELECT   â”‚\n    â”‚  UPDATE   â”‚                      â”‚  GROUP BY â”‚\n    â”‚  DELETE   â”‚                      â”‚  JOIN     â”‚\n    â”‚  (CRUD)   â”‚                      â”‚  (Analyse)â”‚\n    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚ Quelques  â”‚                      â”‚ Millions  â”‚\n    â”‚  lignes   â”‚                      â”‚ de lignes â”‚\n    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n          â”‚                                  â”‚\n    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”\n    â”‚   App     â”‚                      â”‚  Rapport  â”‚\n    â”‚   Web     â”‚                      â”‚ Dashboard â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nCritÃ¨re\nOLTP\nOLAP\n\n\n\n\nObjectif\nOpÃ©rations quotidiennes\nAnalyse, dÃ©cision\n\n\nDonnÃ©es\nActuelles\nHistoriques\n\n\nRequÃªtes\nSimples, frÃ©quentes\nComplexes, ponctuelles\n\n\nTemps de rÃ©ponse\nMillisecondes\nSecondes Ã  minutes\n\n\nUtilisateurs\nMilliers (applications)\nDizaines (analystes)\n\n\nSchÃ©ma\nNormalisÃ©\nDÃ©normalisÃ©\n\n\nExemples\nPostgreSQL, MySQL\nSnowflake, BigQuery, Redshift\n\n\n\n\n\n\n10.4 Data Warehouse â€” Lâ€™entrepÃ´t de donnÃ©es\nUn Data Warehouse (DWH) est une base de donnÃ©es OLAP centralisÃ©e qui stocke les donnÃ©es de toute lâ€™entreprise pour lâ€™analyse.\n\nDÃ©finition\n\nâ€œUn Data Warehouse est une copie des donnÃ©es transactionnelles, structurÃ©e spÃ©cifiquement pour lâ€™analyse et le reporting.â€ â€” Bill Inmon\n\n\n\nCaractÃ©ristiques\n\n\n\n\n\n\n\nCaractÃ©ristique\nDescription\n\n\n\n\nOrientÃ© sujet\nOrganisÃ© par domaine mÃ©tier (ventes, clients, produits)\n\n\nIntÃ©grÃ©\nDonnÃ©es de sources multiples, harmonisÃ©es\n\n\nHistorisÃ©\nConserve lâ€™historique (pas de suppression)\n\n\nNon volatile\nDonnÃ©es en lecture seule (pas de UPDATE)\n\n\n\n\n\nExemples de Data Warehouses\n\n\n\nProduit\nType\nParticularitÃ©\n\n\n\n\nSnowflake\nCloud\nSÃ©paration stockage/calcul, trÃ¨s scalable\n\n\nAmazon Redshift\nCloud AWS\nIntÃ©gration AWS native\n\n\nGoogle BigQuery\nCloud GCP\nServerless, pay-per-query\n\n\nAzure Synapse\nCloud Azure\nIntÃ©gration Microsoft\n\n\nTeradata\nOn-premise\nHistorique, grandes entreprises\n\n\n\n\n\n\n\n10.5 Data Mart â€” Le magasin spÃ©cialisÃ©\nUn Data Mart est un sous-ensemble du Data Warehouse, focalisÃ© sur un domaine mÃ©tier spÃ©cifique.\n\nDÃ©finition\n\nâ€œUn Data Mart est une vue spÃ©cialisÃ©e du Data Warehouse, optimisÃ©e pour les besoins dâ€™un dÃ©partement ou dâ€™une fonction mÃ©tier.â€\n\n\n\nExemples de Data Marts\n\n\n\nData Mart\nDonnÃ©es\nUtilisateurs\n\n\n\n\nMarketing\nCampagnes, conversions, segments\nÃ‰quipe Marketing\n\n\nFinance\nRevenus, coÃ»ts, budgets\nDirection financiÃ¨re\n\n\nRH\nEmployÃ©s, salaires, turnover\nRessources Humaines\n\n\nVentes\nCommandes, clients, produits\nÃ‰quipe commerciale\n\n\n\n\n\n\n\n10.6 Architecture globale\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                         SOURCES DE DONNÃ‰ES                              â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\nâ”‚  â”‚   ERP    â”‚  â”‚   CRM    â”‚  â”‚  E-comm  â”‚  â”‚   Logs   â”‚  ...          â”‚\nâ”‚  â”‚  (OLTP)  â”‚  â”‚  (OLTP)  â”‚  â”‚  (OLTP)  â”‚  â”‚          â”‚               â”‚\nâ”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”€â”˜\n        â”‚             â”‚             â”‚             â”‚\n        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   ETL / ELT     â”‚\n                    â”‚  (Extraction,   â”‚\n                    â”‚  Transformation,â”‚\n                    â”‚  Chargement)    â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  DATA WAREHOUSE â”‚\n                    â”‚     (OLAP)      â”‚\n                    â”‚                 â”‚\n                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n                    â”‚ â”‚  DonnÃ©es    â”‚ â”‚\n                    â”‚ â”‚  intÃ©grÃ©es  â”‚ â”‚\n                    â”‚ â”‚  historisÃ©esâ”‚ â”‚\n                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  DATA MART  â”‚     â”‚  DATA MART  â”‚     â”‚  DATA MART  â”‚\n  â”‚   Ventes    â”‚     â”‚  Marketing  â”‚     â”‚   Finance   â”‚\n  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n  â”‚  Dashboard  â”‚     â”‚  Rapports   â”‚     â”‚    KPIs     â”‚\n  â”‚   Power BI  â”‚     â”‚  Campaigns  â”‚     â”‚  Financiers â”‚\n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nğŸ’¡ RÃ©sumÃ© OLTP / OLAP / DWH / Data Mart\n\n\n\nConcept\nRÃ´le\nAnalogie\n\n\n\n\nOLTP\nOpÃ©rations quotidiennes\nLa caisse enregistreuse\n\n\nOLAP\nAnalyse des donnÃ©es\nLe bureau de lâ€™analyste\n\n\nData Warehouse\nEntrepÃ´t centralisÃ©\nLe grand entrepÃ´t\n\n\nData Mart\nVue mÃ©tier spÃ©cialisÃ©e\nLe rayon dâ€™un magasin",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#modÃ©lisation-dimensionnelle-star-snowflake-schema",
    "href": "notebooks/beginner/06_intro_relational_databases.html#modÃ©lisation-dimensionnelle-star-snowflake-schema",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "11. ModÃ©lisation Dimensionnelle â€” Star & Snowflake Schema",
    "text": "11. ModÃ©lisation Dimensionnelle â€” Star & Snowflake Schema\nLa modÃ©lisation dimensionnelle est la technique utilisÃ©e pour structurer les donnÃ©es dans un Data Warehouse. Elle est optimisÃ©e pour lâ€™analyse (OLAP), pas pour les transactions (OLTP).\n\nğŸ’¡ Cette approche a Ã©tÃ© popularisÃ©e par Ralph Kimball dans les annÃ©es 1990.\n\n\n\n11.1 Tables de Faits (Fact Tables)\nUne table de faits contient les mesures (mÃ©triques) que lâ€™on veut analyser.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      TABLE DE FAITS                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚    Contient :                                                  â”‚\nâ”‚     â€¢ Les MESURES (chiffres Ã  analyser)                        â”‚\nâ”‚     â€¢ Les CLÃ‰S Ã‰TRANGÃˆRES vers les dimensions                  â”‚\nâ”‚                                                                 â”‚\nâ”‚    Exemples de mesures :                                       â”‚\nâ”‚     â€¢ Montant de la vente                                      â”‚\nâ”‚     â€¢ QuantitÃ© vendue                                          â”‚\nâ”‚     â€¢ CoÃ»t                                                     â”‚\nâ”‚     â€¢ Nombre de clics                                          â”‚\nâ”‚                                                                 â”‚\nâ”‚    CaractÃ©ristiques :                                          â”‚\nâ”‚     â€¢ TrÃ¨s grande (millions/milliards de lignes)               â”‚\nâ”‚     â€¢ Une ligne = un Ã‰VÃ‰NEMENT (une vente, un clic, etc.)      â”‚\nâ”‚     â€¢ GranularitÃ© fine (niveau de dÃ©tail)                      â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemple : Table de faits fact_sales\nfact_sales\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ sale_id(PK)â”‚ date_id(FK) â”‚ product_id(FK) â”‚ customer_id(FK)â”‚ quantity â”‚  amount  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚     1      â”‚  20240115   â”‚      101       â”‚      501       â”‚    2     â”‚  199.98  â”‚\nâ”‚     2      â”‚  20240115   â”‚      102       â”‚      502       â”‚    1     â”‚   49.99  â”‚\nâ”‚     3      â”‚  20240116   â”‚      101       â”‚      501       â”‚    1     â”‚   99.99  â”‚\nâ”‚    ...     â”‚    ...      â”‚      ...       â”‚      ...       â”‚   ...    â”‚   ...    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                   â”‚              â”‚                â”‚\n                FK vers        FK vers          FK vers\n               dim_date     dim_product      dim_customer\n\n\n\n11.2 Tables de Dimensions (Dimension Tables)\nUne table de dimension contient le contexte descriptif des mesures.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    TABLE DE DIMENSION                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚    Contient :                                                  â”‚\nâ”‚     â€¢ Les ATTRIBUTS descriptifs (le \"quoi\", \"qui\", \"oÃ¹\", \"quand\") â”‚\nâ”‚     â€¢ La clÃ© primaire rÃ©fÃ©rencÃ©e par la table de faits         â”‚\nâ”‚                                                                 â”‚\nâ”‚    Exemples de dimensions :                                    â”‚\nâ”‚     â€¢ dim_date : annÃ©e, mois, jour, trimestre, jour_semaine    â”‚\nâ”‚     â€¢ dim_product : nom, catÃ©gorie, marque, prix_liste         â”‚\nâ”‚     â€¢ dim_customer : nom, segment, ville, pays                 â”‚\nâ”‚     â€¢ dim_store : nom, rÃ©gion, type, surface                   â”‚\nâ”‚                                                                 â”‚\nâ”‚    CaractÃ©ristiques :                                          â”‚\nâ”‚     â€¢ Relativement petite (milliers Ã  millions de lignes)      â”‚\nâ”‚     â€¢ DÃ©normalisÃ©e (toutes les infos dans une seule table)     â”‚\nâ”‚     â€¢ Permet le \"slicing & dicing\" (filtres et regroupements)  â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemples de tables de dimensions :\ndim_date                                    dim_product\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ date_id  â”‚ year â”‚monthâ”‚ day â”‚quarterâ”‚   â”‚ product_id â”‚   name   â”‚  category â”‚  brand  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ 20240115 â”‚ 2024 â”‚  1  â”‚ 15  â”‚  Q1  â”‚    â”‚    101     â”‚ iPhone 15â”‚ Smartphoneâ”‚  Apple  â”‚\nâ”‚ 20240116 â”‚ 2024 â”‚  1  â”‚ 16  â”‚  Q1  â”‚    â”‚    102     â”‚Galaxy S24â”‚ Smartphoneâ”‚ Samsung â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\ndim_customer\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ customer_id â”‚   name   â”‚ segment  â”‚  city   â”‚ country â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚    501      â”‚  Alice   â”‚ Premium  â”‚  Paris  â”‚ France  â”‚\nâ”‚    502      â”‚   Bob    â”‚ Standard â”‚  Lyon   â”‚ France  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n11.3 Star Schema (SchÃ©ma en Ã©toile)\nLe Star Schema est le modÃ¨le dimensionnel le plus simple et le plus utilisÃ©.\n                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                            â”‚    dim_date     â”‚\n                            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                            â”‚ date_id (PK)    â”‚\n                            â”‚ year            â”‚\n                            â”‚ month           â”‚\n                            â”‚ quarter         â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   dim_customer  â”‚         â”‚   fact_sales    â”‚         â”‚   dim_product   â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\nâ”‚ customer_id(PK) â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”‚ sale_id (PK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ product_id (PK) â”‚\nâ”‚ name            â”‚         â”‚ date_id (FK)    â”‚         â”‚ name            â”‚\nâ”‚ segment         â”‚         â”‚ product_id (FK) â”‚         â”‚ category        â”‚\nâ”‚ city            â”‚         â”‚ customer_id(FK) â”‚         â”‚ brand           â”‚\nâ”‚ country         â”‚         â”‚ store_id (FK)   â”‚         â”‚ price           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ quantity        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                            â”‚ amount          â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                     â”‚\n                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                            â”‚   dim_store     â”‚\n                            â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                            â”‚ store_id (PK)   â”‚\n                            â”‚ name            â”‚\n                            â”‚ region          â”‚\n                            â”‚ type            â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nPourquoi â€œÃ©toileâ€ ? La table de faits est au centre, entourÃ©e des dimensions â€” comme une Ã©toile !\n\n\n\n\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nSimple Ã  comprendre\nRedondance dans les dimensions\n\n\nRequÃªtes rapides (peu de jointures)\nDimensions peuvent Ãªtre grandes\n\n\nFacile Ã  maintenir\nPas adaptÃ© aux hiÃ©rarchies trÃ¨s complexes\n\n\n\n\n\n\n11.4 Snowflake Schema (SchÃ©ma en flocon)\nLe Snowflake Schema est une variante oÃ¹ les dimensions sont normalisÃ©es (dÃ©coupÃ©es en sous-tables).\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   dim_year    â”‚     â”‚   dim_month     â”‚\nâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\nâ”‚ year_id (PK)  â”‚â—„â”€â”€â”€â”€â”‚ month_id (PK)   â”‚\nâ”‚ year          â”‚     â”‚ year_id (FK)    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ month           â”‚\n                      â”‚ quarter         â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                               â”‚\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚    dim_date     â”‚         â”‚  dim_category   â”‚\n                      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                      â”‚ date_id (PK)    â”‚         â”‚ category_id(PK) â”‚\n                      â”‚ month_id (FK)   â”‚         â”‚ category_name   â”‚\n                      â”‚ day             â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\n                               â”‚                           â”‚\n                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”\n                      â”‚   fact_sales    â”‚         â”‚   dim_product   â”‚\n                      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚\n                      â”‚ date_id (FK)    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ product_id (PK) â”‚\n                      â”‚ product_id (FK) â”‚         â”‚ category_id(FK) â”‚\n                      â”‚ quantity        â”‚         â”‚ name            â”‚\n                      â”‚ amount          â”‚         â”‚ brand           â”‚\n                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nPourquoi â€œfloconâ€ ? Les branches se subdivisent comme un flocon de neige.\n\n\n\nâœ… Avantages\nâŒ InconvÃ©nients\n\n\n\n\nMoins de redondance\nPlus de jointures (plus lent)\n\n\nÃ‰conomie dâ€™espace\nPlus complexe Ã  comprendre\n\n\nMeilleur pour hiÃ©rarchies\nMaintenance plus difficile\n\n\n\n\n\n\n11.5 Star vs Snowflake â€” Quand utiliser quoi ?\n\n\n\nCritÃ¨re\nStar Schema â­\nSnowflake Schema â„ï¸\n\n\n\n\nPerformance\nâš¡ Plus rapide\nğŸ¢ Plus de jointures\n\n\nSimplicitÃ©\nâœ… Simple\nâš ï¸ Complexe\n\n\nRedondance\nâš ï¸ Plus de duplication\nâœ… Moins de duplication\n\n\nCas dâ€™usage\nReporting, dashboards\nHiÃ©rarchies complexes\n\n\n\n\nğŸ’¡ En pratique : Le Star Schema est recommandÃ© dans 90% des cas. Utilise Snowflake uniquement si tu as des contraintes spÃ©cifiques de stockage ou des hiÃ©rarchies trÃ¨s profondes.\n\n\n\n\n11.6 Exemple de requÃªte analytique\nAvec un Star Schema, les requÃªtes analytiques sont simples et intuitives :\n-- Ventes par catÃ©gorie de produit et par trimestre\nSELECT \n    d.year,\n    d.quarter,\n    p.category,\n    SUM(f.amount) as total_sales,\n    COUNT(*) as num_transactions\nFROM fact_sales f\nJOIN dim_date d ON f.date_id = d.date_id\nJOIN dim_product p ON f.product_id = p.product_id\nWHERE d.year = 2024\nGROUP BY d.year, d.quarter, p.category\nORDER BY d.quarter, total_sales DESC;\n\n\n\nyear\nquarter\ncategory\ntotal_sales\nnum_transactions\n\n\n\n\n2024\nQ1\nSmartphone\n1,250,000\n8,500\n\n\n2024\nQ1\nLaptop\n980,000\n3,200\n\n\n2024\nQ1\nAccessoires\n450,000\n15,000\n\n\n\n\nğŸ”® Preview : Dans le module 08, tu dÃ©couvriras les concepts de Data Lake, Data Lakehouse, Medallion Architecture (Bronze/Silver/Gold) et les architectures Lambda/Kappa pour le Big Data !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne-oltp",
    "href": "notebooks/beginner/06_intro_relational_databases.html#exemple-complet-schÃ©ma-dune-boutique-en-ligne-oltp",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "12. Exemple complet : SchÃ©ma dâ€™une boutique en ligne (OLTP)",
    "text": "12. Exemple complet : SchÃ©ma dâ€™une boutique en ligne (OLTP)\nVoici un exemple de schÃ©ma OLTP normalisÃ© pour les opÃ©rations quotidiennes :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        BASE DE DONNÃ‰ES : ma_boutique                    â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚    CLIENTS      â”‚       â”‚    COMMANDES    â”‚       â”‚   PRODUITS    â”‚ â”‚\nâ”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚\nâ”‚  â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)      â”‚       â”‚ ğŸ”‘ id (PK)    â”‚ â”‚\nâ”‚  â”‚ nom             â”‚â—„â”€â”€â”€â”€â”€â”€â”‚ ğŸ”— client_id(FK)â”‚       â”‚ nom           â”‚ â”‚\nâ”‚  â”‚ email           â”‚       â”‚ date            â”‚       â”‚ prix          â”‚ â”‚\nâ”‚  â”‚ telephone       â”‚       â”‚ statut          â”‚       â”‚ stock         â”‚ â”‚\nâ”‚  â”‚ created_at      â”‚       â”‚ total           â”‚       â”‚ categorie     â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                                     â”‚                        â”‚         â”‚\nâ”‚                                     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\nâ”‚                                     â”‚    â”‚                             â”‚\nâ”‚                                     â–¼    â–¼                             â”‚\nâ”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚\nâ”‚                            â”‚   LIGNES_COMMANDE   â”‚                     â”‚\nâ”‚                            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                     â”‚\nâ”‚                            â”‚ ğŸ”‘ id (PK)          â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— commande_id (FK) â”‚                     â”‚\nâ”‚                            â”‚ ğŸ”— produit_id (FK)  â”‚                     â”‚\nâ”‚                            â”‚ quantite            â”‚                     â”‚\nâ”‚                            â”‚ prix_unitaire       â”‚                     â”‚\nâ”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRELATIONS :\n  â€¢ clients (1) â”€â”€â”€â–º (N) commandes        : Un client a plusieurs commandes\n  â€¢ commandes (1) â”€â”€â”€â–º (N) lignes_commande : Une commande a plusieurs lignes\n  â€¢ produits (1) â”€â”€â”€â–º (N) lignes_commande  : Un produit dans plusieurs lignes\nCe schÃ©ma est normalisÃ© (3NF) car câ€™est un systÃ¨me OLTP pour les opÃ©rations quotidiennes.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "href": "notebooks/beginner/06_intro_relational_databases.html#rÃ©sumÃ©",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "RÃ©sumÃ©",
    "text": "RÃ©sumÃ©\n\nVocabulaire relationnel\n\n\n\nTerme\nDescription\n\n\n\n\nTable\nCollection de donnÃ©es structurÃ©es\n\n\nColonne\nAttribut (nom, email, prixâ€¦)\n\n\nLigne\nUn enregistrement (un client, une commandeâ€¦)\n\n\nClÃ© Primaire (PK)\nIdentifiant unique dâ€™une ligne\n\n\nClÃ© Ã‰trangÃ¨re (FK)\nRÃ©fÃ©rence vers une autre table\n\n\nSGBD\nLogiciel de gestion (PostgreSQL, MySQLâ€¦)\n\n\n\n\n\nRelations\n\n\n\nType\nNotation\nExemple\n\n\n\n\nUn-Ã -Un\n1:1\nUtilisateur â†”ï¸ Profil\n\n\nUn-Ã -Plusieurs\n1:N\nClient â†’ Commandes\n\n\nPlusieurs-Ã -Plusieurs\nN:N\nÃ‰tudiants â†”ï¸ Cours\n\n\n\n\n\nACID\n\n\n\nLettre\nPropriÃ©tÃ©\n\n\n\n\nA\nAtomicitÃ© â€” Tout ou rien\n\n\nC\nCohÃ©rence â€” Ã‰tat valide\n\n\nI\nIsolation â€” Transactions sÃ©parÃ©es\n\n\nD\nDurabilitÃ© â€” Permanent\n\n\n\n\n\nOLTP vs OLAP\n\n\n\nConcept\nOLTP\nOLAP\n\n\n\n\nUsage\nTransactions\nAnalyse\n\n\nRequÃªtes\nSimples, rapides\nComplexes, agrÃ©gÃ©es\n\n\nSchÃ©ma\nNormalisÃ©\nDÃ©normalisÃ©\n\n\n\n\n\nData Warehouse & Data Mart\n\n\n\nConcept\nDescription\n\n\n\n\nData Warehouse\nEntrepÃ´t centralisÃ© pour lâ€™analyse\n\n\nData Mart\nSous-ensemble orientÃ© mÃ©tier\n\n\n\n\n\nModÃ©lisation dimensionnelle\n\n\n\nConcept\nDescription\n\n\n\n\nTable de faits\nMesures numÃ©riques (ventes, quantitÃ©s)\n\n\nTable de dimension\nContexte descriptif (date, produit, client)\n\n\nStar Schema\nFaits au centre, dimensions autour\n\n\nSnowflake Schema\nStar avec dimensions normalisÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "href": "notebooks/beginner/06_intro_relational_databases.html#quiz",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "Quiz",
    "text": "Quiz\n\n\nâ“ Q1. Quâ€™est-ce quâ€™une clÃ© primaire ?\n\nUne clÃ© de chiffrement\n\nUn identifiant unique pour chaque ligne\n\nLe nom de la premiÃ¨re colonne\n\nUn mot de passe\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© primaire identifie de faÃ§on unique chaque ligne dâ€™une table.\n\n\n\n\nâ“ Q2. Ã€ quoi sert une clÃ© Ã©trangÃ¨re ?\n\nÃ€ chiffrer les donnÃ©es\n\nÃ€ crÃ©er un lien entre deux tables\n\nÃ€ indexer les colonnes\n\nÃ€ supprimer des lignes\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La clÃ© Ã©trangÃ¨re rÃ©fÃ©rence la clÃ© primaire dâ€™une autre table pour crÃ©er une relation.\n\n\n\n\nâ“ Q3. Quelle relation nÃ©cessite une table de jonction ?\n\nUn-Ã -Un (1:1)\n\nUn-Ã -Plusieurs (1:N)\n\nPlusieurs-Ã -Plusieurs (N:N)\n\nAucune\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” Les relations N:N nÃ©cessitent une table intermÃ©diaire (jonction) pour stocker les associations.\n\n\n\n\nâ“ Q4. Que signifie le A de ACID ?\n\nAuthentification\n\nAtomicitÃ©\n\nAutomatisation\n\nArchivage\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” AtomicitÃ© : une transaction est indivisible (tout rÃ©ussit ou tout Ã©choue).\n\n\n\n\nâ“ Q5. Pourquoi normaliser une base de donnÃ©es ?\n\nPour la rendre plus rapide\n\nPour Ã©viter la redondance et les incohÃ©rences\n\nPour ajouter du chiffrement\n\nPour compresser les donnÃ©es\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La normalisation Ã©vite les donnÃ©es dupliquÃ©es et garantit la cohÃ©rence.\n\n\n\n\nâ“ Q6. Quel SGBD est open source ET trÃ¨s complet ?\n\nOracle\n\nSQL Server\n\nPostgreSQL\n\nAccess\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” PostgreSQL est open source, gratuit et trÃ¨s complet (le plus recommandÃ©).\n\n\n\n\nâ“ Q7. Quel type utiliser pour stocker des montants en euros ?\n\nFLOAT\n\nINTEGER\n\nDECIMAL\n\nVARCHAR\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… c â€” DECIMAL garantit une prÃ©cision exacte pour les montants financiers (FLOAT est imprÃ©cis).\n\n\n\n\nâ“ Q8. Quelle est la diffÃ©rence principale entre OLTP et OLAP ?\n\nOLTP est plus rÃ©cent\n\nOLTP gÃ¨re les transactions, OLAP lâ€™analyse\n\nOLAP est plus rapide\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” OLTP (Online Transaction Processing) gÃ¨re les opÃ©rations quotidiennes, OLAP (Online Analytical Processing) est optimisÃ© pour lâ€™analyse et le reporting.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Data Warehouse ?\n\nUn serveur trÃ¨s puissant\n\nUn entrepÃ´t centralisÃ© pour lâ€™analyse des donnÃ©es\n\nUn type de base de donnÃ©es NoSQL\n\nUn logiciel de visualisation\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Warehouse est une base de donnÃ©es OLAP centralisÃ©e qui stocke les donnÃ©es de lâ€™entreprise pour lâ€™analyse et le reporting.\n\n\n\n\nâ“ Q10. Quâ€™est-ce quâ€™un Data Mart ?\n\nUn magasin de donnÃ©es brutes\n\nUn sous-ensemble du Data Warehouse orientÃ© mÃ©tier\n\nUne base de donnÃ©es transactionnelle\n\nUn outil ETL\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Un Data Mart est une vue spÃ©cialisÃ©e du Data Warehouse, focalisÃ©e sur un domaine mÃ©tier (Marketing, Finance, Ventesâ€¦).\n\n\n\n\nâ“ Q11. Dans un Star Schema, que contient la table de faits ?\n\nLes descriptions des produits\n\nLes mesures (mÃ©triques) Ã  analyser\n\nLes informations clients\n\nLes dates uniquement\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” La table de faits contient les mesures numÃ©riques (montants, quantitÃ©s, clicsâ€¦) et les clÃ©s Ã©trangÃ¨res vers les dimensions.\n\n\n\n\nâ“ Q12. Quelle est la diffÃ©rence entre Star Schema et Snowflake Schema ?\n\nStar Schema est plus rÃ©cent\n\nDans Snowflake, les dimensions sont normalisÃ©es (dÃ©coupÃ©es)\n\nSnowflake nâ€™a pas de table de faits\n\nIls sont identiques\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Dans un Snowflake Schema, les tables de dimensions sont normalisÃ©es (dÃ©coupÃ©es en sous-tables), ce qui ressemble Ã  un flocon de neige.\n\n\n\n\nâ“ Q13. Une table de dimension contientâ€¦\n\nLes chiffres Ã  analyser\n\nLe contexte descriptif (qui, quoi, oÃ¹, quand)\n\nLes clÃ©s Ã©trangÃ¨res uniquement\n\nLes donnÃ©es en temps rÃ©el\n\n\n\nğŸ’¡ RÃ©ponse\n\nâœ… b â€” Une table de dimension contient les attributs descriptifs qui donnent du contexte aux mesures (produit, client, date, magasinâ€¦).",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "href": "notebooks/beginner/06_intro_relational_databases.html#ressources",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "ğŸ“š Ressources",
    "text": "ğŸ“š Ressources\n\nBases de donnÃ©es relationnelles\n\nPostgreSQL Documentation\nSQLBolt â€” Tutoriel interactif\nDB Diagram â€” CrÃ©er des schÃ©mas visuels\nDatabase Normalization (Wikipedia)\n\n\n\nOLTP, OLAP et Data Warehousing\n\nOLTP vs OLAP (AWS)\nWhat is a Data Warehouse? (Snowflake)\nData Mart vs Data Warehouse (IBM)\n\n\n\nModÃ©lisation dimensionnelle\n\nStar Schema vs Snowflake Schema (Databricks)\nThe Data Warehouse Toolkit â€” Ralph Kimball\nDimensional Modeling (Kimball Group)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/06_intro_relational_databases.html#prochaine-Ã©tape",
    "title": "Introduction aux Bases de DonnÃ©es Relationnelles",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nTu connais maintenant la thÃ©orie des bases de donnÃ©es relationnelles, OLTP/OLAP, Data Warehouse, et la modÃ©lisation dimensionnelle. Passons Ã  la pratique !\nğŸ‘‰ Module suivant : 07_sql_for_data_engineers â€” Ã‰crire des requÃªtes SQL\n\nğŸ‰ FÃ©licitations ! Tu maÃ®trises les concepts fondamentaux des bases relationnelles et de lâ€™architecture Data.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ—„ï¸ Bases de DonnÃ©es",
      "06 Â· Introduction aux BDD Relationnelles"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html",
    "href": "notebooks/beginner/03_git_for_data_engineers.html",
    "title": "Git pour Data Engineers",
    "section": "",
    "text": "Objectif : Comprendre Git, GitHub et GitLab, savoir versionner ses scripts, notebooks et configurations.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prÃ©requis",
    "title": "Git pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 02_bash_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal\n\n\nâœ… Requis\nAvoir un compte GitHub (gratuit) â€” voir section ci-dessous",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#objectifs-du-module",
    "title": "Git pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les concepts de versioning\nInitialiser et configurer un dÃ©pÃ´t Git\nMaÃ®triser le workflow : add â†’ commit â†’ push\nTravailler avec les branches\nCollaborer efficacement avec une Ã©quipe\nUtiliser .gitignore pour protÃ©ger les donnÃ©es sensibles",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#pourquoi-git-est-essentiel-pour-un-data-engineer",
    "title": "Git pour Data Engineers",
    "section": "Pourquoi Git est essentiel pour un Data Engineer ?",
    "text": "Pourquoi Git est essentiel pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nVersionner les pipelines\nSuivre lâ€™Ã©volution de tes scripts ETL\n\n\nCollaborer en Ã©quipe\nTravailler Ã  plusieurs sur le mÃªme projet data\n\n\nRevenir en arriÃ¨re\nRestaurer une version qui fonctionnait aprÃ¨s un bug\n\n\nCode review\nValider les modifications avant mise en production\n\n\nCI/CD\nDÃ©clencher automatiquement des tests et dÃ©ploiements\n\n\nDocumentation\nHistorique complet de qui a fait quoi et pourquoi\n\n\n\n\nğŸ’¡ En bref : Git est le systÃ¨me nerveux de tout projet data moderne. Sans Git, pas de collaboration efficace ni de traÃ§abilitÃ©.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#dÃ©finitions-git-github-et-gitlab",
    "title": "Git pour Data Engineers",
    "section": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab",
    "text": "1ï¸âƒ£ DÃ©finitions : Git, GitHub et GitLab\n\nQuâ€™est-ce que Git ?\nGit est un logiciel de gestion de versions distribuÃ©. Il permet de :\n\nSuivre lâ€™Ã©volution de vos fichiers au fil du temps\nCrÃ©er des points de restauration (commits)\nTravailler en parallÃ¨le sur diffÃ©rentes fonctionnalitÃ©s (branches)\nFusionner le travail de plusieurs personnes\n\nğŸ“ Ton projet\n    â”‚\n    â”œâ”€â”€ pipeline.py      â† VersionnÃ© par Git\n    â”œâ”€â”€ config.yaml      â† VersionnÃ© par Git  \n    â”œâ”€â”€ .git/            â† Dossier cachÃ© contenant l'historique\n    â””â”€â”€ data/            â† Ã€ NE PAS versionner !\n\n\nQuâ€™est-ce que GitHub ?\nGitHub est une plateforme cloud (propriÃ©tÃ© de Microsoft) qui hÃ©berge vos dÃ©pÃ´ts Git.\n\nInterface web moderne\nTrÃ¨s populaire pour lâ€™open source\nGitHub Actions pour la CI/CD\n\n\n\nQuâ€™est-ce que GitLab ?\nGitLab est une alternative open source Ã  GitHub :\n\nPeut Ãªtre auto-hÃ©bergÃ© (on-premise)\nCI/CD intÃ©grÃ© trÃ¨s puissant\nSouvent utilisÃ© en entreprise",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#github-vs-gitlab-comparatif",
    "title": "Git pour Data Engineers",
    "section": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif",
    "text": "2ï¸âƒ£ GitHub vs GitLab â€” Comparatif\n\n\n\n\n\n\n\n\nFonctionnalitÃ©\nGitHub\nGitLab\n\n\n\n\nHÃ©bergement\nCloud (Microsoft)\nCloud ou auto-hÃ©bergÃ©\n\n\nCI/CD\nGitHub Actions\nGitLab CI/CD (intÃ©grÃ©)\n\n\nCommunautÃ©\nTrÃ¨s vaste (open source)\nOrientÃ© entreprise\n\n\nInterface\nModerne, simple\nComplÃ¨te, personnalisable\n\n\nPrix\nGratuit + plans payants\nGratuit + plans payants\n\n\nSÃ©curitÃ©\nDÃ©pend du plan\nPeut Ãªtre auto-hÃ©bergÃ©\n\n\nUsage recommandÃ©\nProjets publics, open source\nProjets internes, entreprise\n\n\n\n\nğŸ’¡ Pour ce cours, tu peux utiliser lâ€™un ou lâ€™autre. Les commandes Git sont identiques !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#crÃ©er-un-compte-github-gratuit",
    "title": "Git pour Data Engineers",
    "section": "CrÃ©er un compte GitHub gratuit",
    "text": "CrÃ©er un compte GitHub gratuit\n\nÃ‰tape 1 : Inscription\n\nAller sur github.com\nCliquer sur â€œSign upâ€ (en haut Ã  droite)\nRemplir le formulaire :\n\nEmail : ton adresse email\nPassword : un mot de passe fort\nUsername : ton pseudo (visible publiquement)\n\nRÃ©soudre le puzzle de vÃ©rification\nValider lâ€™email (vÃ©rifier ta boÃ®te mail)\n\nâœ… Câ€™est gratuit ! Le plan gratuit inclut : - DÃ©pÃ´ts publics illimitÃ©s - DÃ©pÃ´ts privÃ©s illimitÃ©s - 500 Mo de stockage pour GitHub Packages - 2000 minutes/mois de GitHub Actions\n\n\n\nÃ‰tape 2 : Configurer lâ€™authentification\nDepuis 2021, GitHub nâ€™accepte plus les mots de passe pour git push. Tu dois utiliser :\n\n\n\nMÃ©thode\nDifficultÃ©\nRecommandation\n\n\n\n\nHTTPS + Token\nâ­ Facile\nâœ… Pour dÃ©buter\n\n\nSSH\nâ­â­ Moyen\nâœ… Pour usage rÃ©gulier\n\n\n\n\n\n\nOption A : HTTPS + Personal Access Token (PAT)\nCrÃ©er un token :\n\nConnecte-toi sur github.com\nClique sur ta photo de profil â†’ Settings\nDans le menu gauche, descends jusquâ€™Ã  Developer settings\nClique sur Personal access tokens â†’ Tokens (classic)\nClique sur Generate new token â†’ Generate new token (classic)\nConfigure le token :\n\nNote : git-access (ou un nom descriptif)\nExpiration : 90 days (ou plus)\nScopes : cocher repo (accÃ¨s complet aux dÃ©pÃ´ts)\n\nClique sur Generate token\nCOPIE LE TOKEN MAINTENANT â€” tu ne pourras plus le voir aprÃ¨s !\n\nUtiliser le token :\n# Quand Git demande ton mot de passe, colle le TOKEN (pas ton mot de passe !)\ngit push origin main\nUsername: ton-username\nPassword: ghp_xxxxxxxxxxxxxxxxxxxx   # â† Coller le token ici\nSauvegarder le token (optionnel) :\n# MÃ©moriser les credentials pour 1 heure\ngit config --global credential.helper cache\n\n# Ou mÃ©moriser indÃ©finiment (moins sÃ©curisÃ©)\ngit config --global credential.helper store\n\n\n\nOption B : ClÃ© SSH (recommandÃ© pour usage rÃ©gulier)\n1. GÃ©nÃ©rer une clÃ© SSH :\n# GÃ©nÃ©rer une paire de clÃ©s (appuie sur EntrÃ©e pour les valeurs par dÃ©faut)\nssh-keygen -t ed25519 -C \"ton.email@exemple.com\"\n\n# DÃ©marrer l'agent SSH\neval \"$(ssh-agent -s)\"\n\n# Ajouter la clÃ© Ã  l'agent\nssh-add ~/.ssh/id_ed25519\n2. Ajouter la clÃ© Ã  GitHub :\n# Copier la clÃ© publique\ncat ~/.ssh/id_ed25519.pub\n# Copie le rÃ©sultat (commence par ssh-ed25519...)\n\nSur GitHub : Settings â†’ SSH and GPG keys â†’ New SSH key\nColler la clÃ© publique et sauvegarder\n\n3. Tester la connexion :\nssh -T git@github.com\n# RÃ©ponse attendue : \"Hi username! You've successfully authenticated...\"\n4. Utiliser SSH pour cloner :\n# Cloner avec SSH (au lieu de HTTPS)\ngit clone git@github.com:username/repo.git\n\n# Ou changer un dÃ©pÃ´t existant vers SSH\ngit remote set-url origin git@github.com:username/repo.git\n\n\n\nÃ‰tape 3 : CrÃ©er ton premier dÃ©pÃ´t sur GitHub\nVia lâ€™interface web :\n\nConnecte-toi sur github.com\nClique sur â€œ+â€ (en haut Ã  droite) â†’ New repository\nConfigure le dÃ©pÃ´t :\n\nRepository name : mon-projet-data\nDescription : â€œMon premier projet Data Engineeringâ€\nVisibility : Public ou Private\nâŒ Ne PAS cocher â€œAdd a README fileâ€ (on le fera localement)\n\nClique sur Create repository\nGitHub affiche les commandes Ã  exÃ©cuter â€” copie-les !\n\nLier ton projet local au dÃ©pÃ´t GitHub :\n# Si tu as dÃ©jÃ  un projet local avec des commits\ncd mon_projet_data\ngit remote add origin https://github.com/ton-username/mon-projet-data.git\ngit branch -M main\ngit push -u origin main\n# Ou cloner un dÃ©pÃ´t existant\ngit clone https://github.com/ton-username/mon-projet-data.git\ncd mon-projet-data\n\n\n\nRÃ©capitulatif : Workflow complet\n1. CrÃ©er compte GitHub â”€â”€â”€â”€â–º github.com/signup\n                                    â”‚\n2. Configurer auth â”€â”€â”€â”€â”€â”€â”€â”€â–º Token (HTTPS) ou SSH\n                                    â”‚\n3. CrÃ©er dÃ©pÃ´t sur GitHub â”€â–º github.com â†’ New repository\n                                    â”‚\n4. Lier projet local â”€â”€â”€â”€â”€â”€â–º git remote add origin ...\n                                    â”‚\n5. Pousser le code â”€â”€â”€â”€â”€â”€â”€â”€â–º git push -u origin main\n                                    â”‚\n                              âœ… Code sur GitHub !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#installation-et-configuration-de-git",
    "title": "Git pour Data Engineers",
    "section": "3ï¸âƒ£ Installation et configuration de Git",
    "text": "3ï¸âƒ£ Installation et configuration de Git\n\nInstallation\n\n\n\nSystÃ¨me\nCommande\n\n\n\n\nğŸªŸ Windows\nTÃ©lÃ©charger depuis git-scm.com\n\n\nğŸ macOS\nbrew install git\n\n\nğŸ§ Linux (Debian/Ubuntu)\nsudo apt install git\n\n\nğŸ§ Linux (Fedora)\nsudo dnf install git\n\n\n\n\n\nVoir le code\n%%bash\n# VÃ©rifier la version installÃ©e\ngit --version\n\n# Configuration obligatoire (identitÃ©)\ngit config --global user.name \"Ton Nom\"\ngit config --global user.email \"ton.email@exemple.com\"\n\n# Configuration recommandÃ©e\ngit config --global init.defaultBranch main    # Branche par dÃ©faut\ngit config --global core.editor \"code --wait\"  # Ã‰diteur (VS Code)\ngit config --global pull.rebase false          # Merge par dÃ©faut lors du pull\n\n# VÃ©rifier la configuration\ngit config --list",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#comprendre-le-workflow-git",
    "title": "Git pour Data Engineers",
    "section": "4ï¸âƒ£ Comprendre le workflow Git",
    "text": "4ï¸âƒ£ Comprendre le workflow Git\n\nLes 4 zones de Git\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git add     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Working Dir    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Staging Area   â”‚\nâ”‚  (tes fichiers) â”‚                â”‚  (index)        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                            â”‚ git commit\n                                            â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    git push    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Remote         â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Local Repo     â”‚\nâ”‚  (GitHub/GitLab)â”‚                â”‚  (.git)         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nCycle de vie dâ€™un fichier\n\n\n\nÃ‰tat\nDescription\nCommande pour passer Ã  lâ€™Ã©tat suivant\n\n\n\n\nUntracked\nNouveau fichier, non suivi\ngit add &lt;fichier&gt;\n\n\nStaged\nPrÃªt Ã  Ãªtre commitÃ©\ngit commit -m \"message\"\n\n\nCommitted\nEnregistrÃ© localement\ngit push\n\n\nPushed\nEnvoyÃ© sur le serveur distant\nâœ… TerminÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#clients-git-alternatives-aux-commandes",
    "title": "Git pour Data Engineers",
    "section": "Clients Git (alternatives aux commandes)",
    "text": "Clients Git (alternatives aux commandes)\nTu nâ€™es pas obligÃ© dâ€™utiliser le terminal ! Il existe des interfaces graphiques (GUI) pour Git qui facilitent la visualisation et certaines opÃ©rations.\n\nğŸ“± Clients populaires\n\n\n\nClient\nPlateforme\nPoints forts\nPrix\n\n\n\n\nGitHub Desktop\nWindows, Mac\nSimple, parfait pour dÃ©buter, intÃ©gration GitHub\nGratuit\n\n\nGitKraken\nWindows, Mac, Linux\nInterface visuelle puissante, graphe des branches\nGratuit (public)\n\n\nSourcetree\nWindows, Mac\nComplet, supporte Git et Mercurial\nGratuit\n\n\nVS Code\nTous\nGit intÃ©grÃ© + extension GitLens\nGratuit\n\n\nPyCharm / IntelliJ\nTous\nGit intÃ©grÃ© dans lâ€™IDE\nGratuit / Payant\n\n\n\n\n\nTerminal vs GUI : quand utiliser quoi ?\n\n\n\n\n\n\n\nSituation\nRecommandation\n\n\n\n\nVisualiser lâ€™historique et les branches\nğŸ–¥ï¸ GUI â€” Plus clair visuellement\n\n\nRÃ©soudre des conflits de merge\nğŸ–¥ï¸ GUI â€” Comparaison cÃ´te Ã  cÃ´te\n\n\nOpÃ©rations quotidiennes (add, commit, push)\nâŒ¨ï¸ Terminal ou ğŸ–¥ï¸ GUI â€” Au choix\n\n\nScripts et automatisation (CI/CD)\nâŒ¨ï¸ Terminal â€” Obligatoire\n\n\nServeurs distants (SSH)\nâŒ¨ï¸ Terminal â€” Pas de GUI disponible\n\n\nApprendre Git en profondeur\nâŒ¨ï¸ Terminal â€” Comprendre ce qui se passe\n\n\n\n\nğŸ’¡ Conseil : Apprends dâ€™abord les commandes pour comprendre Git, puis utilise un client GUI pour gagner en productivitÃ© au quotidien.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#utilisation-de-git-pas-Ã -pas",
    "title": "Git pour Data Engineers",
    "section": "5ï¸âƒ£ Utilisation de Git pas Ã  pas",
    "text": "5ï¸âƒ£ Utilisation de Git pas Ã  pas\n\nÃ‰tape 1 : CrÃ©er un projet et initialiser Git\n\n\nVoir le code\n%%bash\n# CrÃ©er un dossier de projet\nmkdir mon_projet_data\ncd mon_projet_data\n\n# Initialiser Git (crÃ©e le dossier .git)\ngit init\n\n# VÃ©rifier le statut\ngit status\n\n\n\n\nÃ‰tape 2 : Ajouter des fichiers et commiter\n\n\nVoir le code\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un fichier Python\necho \"print('Hello Data Engineering!')\" &gt; main.py\n\n# Voir le statut (fichier untracked)\ngit status\n\n# Ajouter le fichier Ã  la staging area\ngit add main.py\n\n# Voir le statut (fichier staged)\ngit status\n\n# Commiter avec un message descriptif\ngit commit -m \"feat: ajouter script principal\"\n\n# Voir l'historique\ngit log --oneline\n\n\n\n\nConventions de commits (Conventional Commits)\nUtilise des prÃ©fixes standardisÃ©s pour des messages clairs :\n\n\n\n\n\n\n\n\nPrÃ©fixe\nUsage\nExemple\n\n\n\n\nfeat:\nNouvelle fonctionnalitÃ©\nfeat: ajouter extraction API\n\n\nfix:\nCorrection de bug\nfix: corriger parsing dates\n\n\ndocs:\nDocumentation\ndocs: mettre Ã  jour README\n\n\nrefactor:\nRefactoring (sans changer le comportement)\nrefactor: simplifier fonction ETL\n\n\ntest:\nAjout/modification de tests\ntest: ajouter tests unitaires\n\n\nchore:\nMaintenance, config\nchore: mettre Ã  jour dÃ©pendances\n\n\n\nExemple de bon message :\nfeat: ajouter pipeline d'extraction des donnÃ©es clients\n\n- Connexion Ã  l'API CRM\n- Transformation des donnÃ©es JSON\n- Export en format Parquet",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#le-fichier-.gitignore-essentiel-pour-data-engineers",
    "title": "Git pour Data Engineers",
    "section": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers",
    "text": "6ï¸âƒ£ Le fichier .gitignore â€” ESSENTIEL pour Data Engineers\nLe .gitignore indique Ã  Git quels fichiers NE PAS versionner.\n\nâš ï¸ Ne JAMAIS versionner :\n\nFichiers de donnÃ©es (CSV, Parquet, JSON volumineux)\nSecrets et credentials (mots de passe, clÃ©s API)\nDÃ©pendances (node_modules, venv)\nFichiers temporaires (cache, logs)\n\n\n\nVoir le code\n%%bash\ncd mon_projet_data\n\n# CrÃ©er un .gitignore pour projet Data Engineering\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# ==== DONNÃ‰ES ====\n*.csv\n*.parquet\n*.json\n*.xlsx\ndata/\nraw/\nprocessed/\n\n# ==== SECRETS ====\n.env\n*.pem\n*.key\ncredentials.json\nsecrets.yaml\n\n# ==== PYTHON ====\n__pycache__/\n*.py[cod]\nvenv/\n.venv/\n*.egg-info/\n.pytest_cache/\n\n# ==== JUPYTER ====\n.ipynb_checkpoints/\n*.ipynb_checkpoints\n\n# ==== IDE ====\n.idea/\n.vscode/\n*.swp\n\n# ==== LOGS ====\n*.log\nlogs/\n\n# ==== OS ====\n.DS_Store\nThumbs.db\nEOF\n\necho \"âœ… .gitignore crÃ©Ã©\"\ncat .gitignore\n\n\n\n\nğŸ’¡ Astuces .gitignore\n# Ignorer un dossier\ndata/\n\n# Ignorer tous les .csv sauf un\n*.csv\n!schema.csv\n\n# Ignorer les fichiers dans tous les sous-dossiers\n**/*.log\n\n# VÃ©rifier ce qui est ignorÃ©\ngit status --ignored\n\nğŸ”— GÃ©nÃ©rateur de .gitignore : gitignore.io",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#lier-Ã -un-dÃ©pÃ´t-distant-githubgitlab",
    "title": "Git pour Data Engineers",
    "section": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)",
    "text": "7ï¸âƒ£ Lier Ã  un dÃ©pÃ´t distant (GitHub/GitLab)\n\nÃ‰tape 1 : CrÃ©er un dÃ©pÃ´t sur GitHub/GitLab\n\nVa sur github.com ou gitlab.com\nClique sur â€œNew repositoryâ€ / â€œNew projectâ€\nDonne un nom (ex: mon_projet_data)\nNe coche PAS â€œInitialize with READMEâ€ (on a dÃ©jÃ  un repo local)\nCopie lâ€™URL HTTPS\n\n\n\nÃ‰tape 2 : Connecter le dÃ©pÃ´t local\n\n\nVoir le code\n%%bash\ncd mon_projet_data\n\n# Ajouter le dÃ©pÃ´t distant (remplace par ton URL)\ngit remote add origin https://github.com/ton-username/mon_projet_data.git\n\n# VÃ©rifier les remotes\ngit remote -v\n\n# S'assurer d'Ãªtre sur la branche main\ngit branch -M main\n\n# Pousser le code (premiÃ¨re fois : -u pour lier la branche)\ngit push -u origin main\n\n\n\n\nCloner un projet existant\n# Cloner un dÃ©pÃ´t\ngit clone https://github.com/username/projet.git\n\n# Cloner dans un dossier spÃ©cifique\ngit clone https://github.com/username/projet.git mon_dossier\n\n\nSynchroniser avec le distant\n# RÃ©cupÃ©rer les modifications (fetch + merge)\ngit pull\n\n# Voir les modifications distantes sans les appliquer\ngit fetch\ngit log origin/main --oneline",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#travailler-avec-les-branches",
    "title": "Git pour Data Engineers",
    "section": "8ï¸âƒ£ Travailler avec les branches",
    "text": "8ï¸âƒ£ Travailler avec les branches\nLes branches permettent de travailler sur des fonctionnalitÃ©s en parallÃ¨le sans affecter le code principal.\nmain         â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—  (code stable)\n                  â”‚               â†‘\nfeature/etl       â””â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â—â”€â”€â”€â”˜     (nouvelle fonctionnalitÃ©)\n\n\nVoir le code\n%%bash\ncd mon_projet_data\n\n# Voir les branches existantes\ngit branch\n\n# CrÃ©er une nouvelle branche\ngit branch feature/add-etl\n\n# Basculer sur la branche\ngit switch feature/add-etl\n\n# OU crÃ©er + basculer en une commande\ngit switch -c feature/add-validation\n\n# Faire des modifications\necho \"def validate(df): pass\" &gt; validation.py\ngit add validation.py\ngit commit -m \"feat: ajouter module de validation\"\n\n# Revenir sur main\ngit switch main\n\n# Fusionner la branche\ngit merge feature/add-validation\n\n# Supprimer la branche fusionnÃ©e\ngit branch -d feature/add-validation\n\n\n\nWorkflow de branches recommandÃ© pour Data Engineers\nmain (production)\n  â”‚\n  â”œâ”€â”€ develop (intÃ©gration)\n  â”‚     â”‚\n  â”‚     â”œâ”€â”€ feature/etl-clients\n  â”‚     â”œâ”€â”€ feature/dashboard-ventes  \n  â”‚     â””â”€â”€ fix/bug-parsing-dates\n  â”‚\n  â””â”€â”€ hotfix/critical-fix (urgences)\n\n\n\nBranche\nUsage\n\n\n\n\nmain\nCode en production, toujours stable\n\n\ndevelop\nIntÃ©gration des features avant release\n\n\nfeature/*\nNouvelles fonctionnalitÃ©s\n\n\nfix/*\nCorrections de bugs\n\n\nhotfix/*\nCorrections urgentes en production",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#rÃ©soudre-les-conflits-de-merge",
    "title": "Git pour Data Engineers",
    "section": "9ï¸âƒ£ RÃ©soudre les conflits de merge",
    "text": "9ï¸âƒ£ RÃ©soudre les conflits de merge\nUn conflit survient quand deux personnes modifient la mÃªme ligne.\n\nÃ€ quoi ressemble un conflit ?\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\ndef process_data(df):\n    return df.dropna()\n=======\ndef process_data(dataframe):\n    return dataframe.fillna(0)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature/autre-branche\n\n\nComment rÃ©soudre ?\n\nOuvrir le fichier et choisir la bonne version\nSupprimer les marqueurs (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nTester que le code fonctionne\nCommiter la rÃ©solution\n\n# AprÃ¨s avoir Ã©ditÃ© le fichier\ngit add fichier_resolu.py\ngit commit -m \"fix: rÃ©soudre conflit sur process_data\"\n\nğŸ’¡ Astuce : Utilise un outil visuel comme VS Code pour rÃ©soudre les conflits plus facilement.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#commandes-utiles-avancÃ©es",
    "title": "Git pour Data Engineers",
    "section": "ğŸ”§ Commandes utiles avancÃ©es",
    "text": "ğŸ”§ Commandes utiles avancÃ©es\n\ngit stash â€” Mettre de cÃ´tÃ© temporairement\n# Sauvegarder les modifications en cours\ngit stash\n\n# Voir les stash\ngit stash list\n\n# RÃ©cupÃ©rer le dernier stash\ngit stash pop\n\n# RÃ©cupÃ©rer un stash spÃ©cifique\ngit stash apply stash@{0}\n\n\nAnnuler des changements\n# Annuler les modifications d'un fichier (non commitÃ©)\ngit checkout -- fichier.py\n\n# Retirer un fichier de la staging area\ngit reset HEAD fichier.py\n\n# Annuler le dernier commit (garde les fichiers)\ngit reset --soft HEAD~1\n\n# Annuler le dernier commit (supprime les fichiers)\ngit reset --hard HEAD~1  # âš ï¸ DANGEREUX\n\n# CrÃ©er un commit qui annule un commit prÃ©cÃ©dent\ngit revert &lt;commit-hash&gt;\n\n\nInspecter lâ€™historique\n# Historique compact\ngit log --oneline\n\n# Historique graphique\ngit log --oneline --graph --all\n\n# Voir les modifications d'un commit\ngit show &lt;commit-hash&gt;\n\n# Voir qui a modifiÃ© chaque ligne\ngit blame fichier.py\n\n# Chercher un commit par message\ngit log --grep=\"ETL\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#cheatsheet-commandes-essentielles",
    "title": "Git pour Data Engineers",
    "section": "Cheatsheet â€” Commandes essentielles",
    "text": "Cheatsheet â€” Commandes essentielles\n\n\n\n\n\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nSetup\ngit init\nInitialiser un dÃ©pÃ´t\n\n\n\ngit clone &lt;url&gt;\nCloner un dÃ©pÃ´t distant\n\n\n\ngit config --global user.name\nConfigurer son nom\n\n\nBasique\ngit status\nVoir lâ€™Ã©tat des fichiers\n\n\n\ngit add &lt;fichier&gt;\nAjouter Ã  la staging area\n\n\n\ngit add .\nAjouter tous les fichiers\n\n\n\ngit commit -m \"msg\"\nEnregistrer les modifications\n\n\nHistorique\ngit log --oneline\nVoir lâ€™historique compact\n\n\n\ngit diff\nVoir les modifications\n\n\n\ngit blame &lt;fichier&gt;\nVoir qui a modifiÃ© quoi\n\n\nBranches\ngit branch\nLister les branches\n\n\n\ngit switch -c &lt;nom&gt;\nCrÃ©er et basculer\n\n\n\ngit merge &lt;branche&gt;\nFusionner une branche\n\n\n\ngit branch -d &lt;nom&gt;\nSupprimer une branche\n\n\nRemote\ngit remote add origin &lt;url&gt;\nAjouter un dÃ©pÃ´t distant\n\n\n\ngit push\nEnvoyer sur le serveur\n\n\n\ngit pull\nRÃ©cupÃ©rer du serveur\n\n\n\ngit fetch\nVÃ©rifier les changements\n\n\nAnnuler\ngit stash\nMettre de cÃ´tÃ©\n\n\n\ngit reset --soft HEAD~1\nAnnuler dernier commit\n\n\n\ngit revert &lt;hash&gt;\nCrÃ©er un commit dâ€™annulation\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Git Cheatsheet officiel (PDF)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "Git pour Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\n\n\nâ“ Q1. Git est un outil de :\n\nDesign graphique\n\nGestion de versions\n\nStockage cloud\n\nDÃ©ploiement automatique\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Git est un systÃ¨me de gestion de versions distribuÃ©.\n\n\n\n\nâ“ Q2. Quelle commande initialise un dÃ©pÃ´t Git ?\n\ngit start\n\ngit init\n\ngit create\n\ngit new\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” git init crÃ©e un nouveau dÃ©pÃ´t Git.\n\n\n\n\nâ“ Q3. Quelle est la diffÃ©rence entre git commit et git push ?\n\ncommit enregistre localement, push envoie au serveur distant\n\ncommit supprime des fichiers\n\npush crÃ©e un dÃ©pÃ´t local\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” commit sauvegarde localement, push synchronise avec le serveur.\n\n\n\n\nâ“ Q4. Que faut-il mettre dans le .gitignore pour un projet Data ?\n\nLes fichiers Python\n\nLes fichiers de donnÃ©es (CSV, Parquet) et les secrets\n\nLe README\n\nTous les fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Ne jamais versionner les donnÃ©es volumineuses ni les credentials !\n\n\n\n\nâ“ Q5. Quelle commande crÃ©e une nouvelle branche et bascule dessus ?\n\ngit branch new\n\ngit create-branch nom\n\ngit switch -c nom\n\ngit branch -m nom\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” git switch -c nom crÃ©e et bascule sur la branche.\n\n\n\n\nâ“ Q6. Comment annuler le dernier commit tout en gardant les fichiers ?\n\ngit delete commit\n\ngit reset --hard HEAD~1\n\ngit reset --soft HEAD~1\n\ngit undo\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” --soft garde les fichiers, --hard les supprime.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#exercice-pratique",
    "title": "Git pour Data Engineers",
    "section": "Exercice pratique",
    "text": "Exercice pratique\n\nObjectif\nCrÃ©er un projet Data Engineering versionnÃ© et le pousser sur GitHub/GitLab.\n\n\nInstructions\n\nCrÃ©er un dossier projet_etl\nInitialiser Git\nCrÃ©er un .gitignore appropriÃ©\nCrÃ©er un fichier etl.py avec un script simple\nCrÃ©er un fichier README.md\nFaire un premier commit\nCrÃ©er une branche feature/add-config\nAjouter un fichier config.yaml sur cette branche\nMerger dans main\nCrÃ©er un dÃ©pÃ´t sur GitHub et pousser le code\n\n\n\nâœ… Solution\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n# 1. CrÃ©er le dossier\nmkdir projet_etl && cd projet_etl\n\n# 2. Initialiser Git\ngit init\n\n# 3. CrÃ©er le .gitignore\ncat &lt;&lt; 'EOF' &gt; .gitignore\n# DonnÃ©es\n*.csv\n*.parquet\ndata/\n\n# Secrets\n.env\ncredentials.json\n\n# Python\n__pycache__/\nvenv/\nEOF\n\n# 4. CrÃ©er le script ETL\ncat &lt;&lt; 'EOF' &gt; etl.py\n#!/usr/bin/env python3\n\"\"\"Simple ETL Pipeline\"\"\"\n\ndef extract():\n    print(\"ğŸ“¥ Extracting data...\")\n    return {\"data\": [1, 2, 3]}\n\ndef transform(data):\n    print(\"ğŸ”„ Transforming data...\")\n    return {\"data\": [x * 2 for x in data[\"data\"]]}\n\ndef load(data):\n    print(\"ğŸ’¾ Loading data...\")\n    print(f\"Result: {data}\")\n\nif __name__ == \"__main__\":\n    raw = extract()\n    transformed = transform(raw)\n    load(transformed)\n    print(\"âœ… ETL completed!\")\nEOF\n\n# 5. CrÃ©er le README\ncat &lt;&lt; 'EOF' &gt; README.md\n# Projet ETL\n\nUn pipeline ETL simple pour apprendre Git.\n\n## Usage\n\n```bash\npython etl.py\nEOF",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "Git pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ® Apprendre en pratiquant\n\nLearn Git Branching â€” Tutoriel interactif visuel\nOh My Git! â€” Jeu pour apprendre Git\nGit Katas â€” Exercices pratiques\n\n\n\nğŸ“– Documentation\n\nPro Git Book â€” Livre gratuit (en franÃ§ais)\nGitHub Docs\nGitLab Docs\n\n\n\nğŸ› ï¸ Outils\n\nGitHub Desktop â€” Interface graphique\nGitKraken â€” Client Git visuel\nConventional Commits â€” Standard de messages",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#conclusion",
    "title": "Git pour Data Engineers",
    "section": "âœ… Conclusion",
    "text": "âœ… Conclusion\nTu sais maintenant :\n\nâœ… Ce quâ€™est Git, GitHub et GitLab\nâœ… Comment initialiser et configurer un projet\nâœ… Le workflow : add â†’ commit â†’ push\nâœ… Travailler avec les branches\nâœ… Utiliser .gitignore pour protÃ©ger les donnÃ©es sensibles\nâœ… RÃ©soudre les conflits de merge\nâœ… Les commandes avancÃ©es (stash, reset, revert)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/03_git_for_data_engineers.html#prochaine-Ã©tape",
    "title": "Git pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu sais versionner ton code, passons aux bases de donnÃ©es !\nğŸ‘‰ Module suivant : 04_python_basics_for_data_engineers â€” Les fondamentaux de Python\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Git pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "03 Â· Git & Versioning"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "",
    "text": "Ce notebook donne les bases de Python nÃ©cessaires pour la suite du parcours Data Engineering From Zero to Hero.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#version-python-recommandÃ©e",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Version Python recommandÃ©e",
    "text": "âš ï¸ Version Python recommandÃ©e\n\n\n\n\n\n\n\n\nVersion\nStatut\nRecommandation\n\n\n\n\nPython 3.12\nâœ… DerniÃ¨re stable\nRecommandÃ©e pour nouveaux projets\n\n\nPython 3.11\nâœ… Stable\nExcellent choix, trÃ¨s performant\n\n\nPython 3.10\nâœ… SupportÃ©e\nMinimum pour les type hints modernes\n\n\nPython 3.9\nâš ï¸ Maintenance\nÃ‰viter pour nouveaux projets\n\n\nPython 3.8 et avant\nâŒ ObsolÃ¨te\nNe pas utiliser\n\n\n\n\nğŸ’¡ Pour ce cours, utilise Python 3.11 ou 3.12.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prÃ©requis",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 03_git_for_data_engineers\n\n\nâœ… Requis\nSavoir utiliser un terminal (Bash)\n\n\nğŸŸ¡ Optionnel\nNotions de programmation dans un autre langage",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#objectifs-du-module",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce notebook, tu seras capable de :\n\nInstaller et configurer ton environnement Python\nCrÃ©er et gÃ©rer des environnements virtuels\nManipuler les types de base (nombres, chaÃ®nes, listes, dictionnaires)\nUtiliser les conditions et boucles\nÃ‰crire des fonctions et des classes simples\nGÃ©rer les erreurs avec try / except\nLire et Ã©crire des fichiers (texte, JSON)\nUtiliser le module logging pour tracer un script",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-python-pour-le-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Pourquoi Python pour le Data Engineering ?",
    "text": "Pourquoi Python pour le Data Engineering ?\n\n\n\n\n\n\n\nRaison\nDÃ©tail\n\n\n\n\nÃ‰cosystÃ¨me riche\nPandas, PySpark, Airflow, dbt, FastAPIâ€¦\n\n\nPolyvalent\nScripts, APIs, pipelines, ML, automation\n\n\nStandard de lâ€™industrie\nUtilisÃ© par Netflix, Spotify, Airbnbâ€¦\n\n\nFacile Ã  apprendre\nSyntaxe claire et lisible\n\n\nIntÃ©grations\nConnecteurs pour toutes les bases de donnÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installation-environnement",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "0. Installation & Environnement ğŸ› ï¸",
    "text": "0. Installation & Environnement ğŸ› ï¸\nCette section explique comment installer Python, VS Code, Jupyter et vÃ©rifier que tout fonctionne. Les commandes sont donnÃ©es pour Windows, Linux et macOS.\n\n0.1 Installer Python\n\nSous Windows\n\nAller sur le site officiel : https://www.python.org/downloads/\nTÃ©lÃ©charger la derniÃ¨re version stable de Python 3.x.\nLors de lâ€™installation :\n\nCocher â€œAdd Python to PATHâ€ en bas de la premiÃ¨re fenÃªtre ;\npuis cliquer sur Install Now.\n\n\n\n\nSous Linux (Ubuntu / Debian)\nsudo apt update\nsudo apt install -y python3 python3-pip\n\n\nSous macOS\n\nOption 1 : paquet officiel :\n\nTÃ©lÃ©charger un .pkg depuis https://www.python.org/downloads/mac-osx/\nInstaller comme une application classique.\n\nOption 2 (si Homebrew est installÃ©) :\n\nbrew install python\n\n\n\n0.2 VÃ©rifier lâ€™installation de Python\nOuvrir un terminal (ou PowerShell sous Windows) puis taper :\npython --version   # ou parfois: python3 --version\nTu dois voir une version du type : Python 3.12.x.\nâš ï¸ Erreurs frÃ©quentes : - python nâ€™est pas reconnu â†’ Python nâ€™est pas dans le PATH ; - sur Linux/macOS, il faut parfois utiliser python3 au lieu de python.\n\n\n0.3 Installer Visual Studio Code (VS Code)\n\nTÃ©lÃ©charger VS Code : https://code.visualstudio.com/\nInstaller la version adaptÃ©e Ã  ton systÃ¨me (Windows, Linux, macOS).\nLancer VS Code.\n\nVS Code servira Ã  : - Ã©diter des scripts .py ; - ouvrir des notebooks Jupyter (.ipynb) ; - organiser un projet de data engineering complet.\n\n\n0.4 Extensions VS Code : Python & Jupyter\nDans VS Code, aller dans lâ€™onglet Extensions (icÃ´ne de blocs Ã  gauche), puis :\n\nRechercher â€œPythonâ€ (Ã©diteur : Microsoft) et lâ€™installer ;\nRechercher â€œJupyterâ€ (Ã©diteur : Microsoft) et lâ€™installer.\n\nEnsuite, ouvrir un fichier .py ou .ipynb : VS Code proposera de sÃ©lectionner un interprÃ©teur Python (en bas Ã  droite). Choisir ton installation Python 3.x.\n\n\n0.5 Jupyter Notebook â€” Installation et utilisation\nJupyter Notebook est un environnement interactif qui permet dâ€™Ã©crire du code, de lâ€™exÃ©cuter, et de voir les rÃ©sultats immÃ©diatement. Câ€™est lâ€™outil idÃ©al pour apprendre, explorer des donnÃ©es et documenter son travail.\n\nğŸ’¡ Ce cours est lui-mÃªme un Notebook Jupyter (fichier .ipynb) !\n\n\n\nInstallation\n# Installer Jupyter\npip install notebook\n\n# Ou avec Anaconda (dÃ©jÃ  inclus)\n# Rien Ã  faire, c'est installÃ© par dÃ©faut\n\n\n\nLancer Jupyter Notebook\n# Dans ton terminal, place-toi dans ton dossier de travail\ncd /chemin/vers/mon/projet\n\n# Lancer Jupyter\njupyter notebook\nCe qui se passe :\n\nUn serveur local dÃ©marre\nTon navigateur sâ€™ouvre automatiquement sur http://localhost:8888\nTu vois la liste des fichiers de ton dossier\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Jupyter                                    [Quit] [Logout]     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Files    Running    Clusters                                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  data/                                                       â”‚\nâ”‚  01_intro.ipynb                                              â”‚\nâ”‚  02_basics.ipynb                                             â”‚\nâ”‚  script.py                                                   â”‚\nâ”‚                                                                 â”‚\nâ”‚  [New â–¼]  â† Cliquer ici pour crÃ©er un nouveau notebook          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nCrÃ©er un nouveau Notebook\n\nCliquer sur New (en haut Ã  droite)\nSÃ©lectionner Python 3 (ou Python 3 (ipykernel))\nUn nouveau notebook sâ€™ouvre : Untitled.ipynb\nCliquer sur â€œUntitledâ€ pour le renommer (ex: mon_premier_notebook.ipynb)\n\n\n\n\nLâ€™interface Jupyter\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  mon_notebook.ipynb                              [Trusted]      â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  File  Edit  View  Insert  Cell  Kernel  Help                   â”‚\nâ”‚  [ğŸ’¾] [+] [âœ‚ï¸] [ğŸ“‹] [â–¶ï¸ Run] [â¹ï¸] [ğŸ”„]    | Code â–¼ |             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  In [1]: â–ˆ                              â† Cellule de code       â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nÃ‰lÃ©ment\nDescription\n\n\n\n\nCellule\nBloc oÃ¹ tu Ã©cris du code ou du texte\n\n\nIn [1]:\nNumÃ©ro dâ€™exÃ©cution de la cellule\n\n\nâ–¶ï¸ Run\nExÃ©cute la cellule sÃ©lectionnÃ©e\n\n\nCode â–¼\nType de cellule (Code ou Markdown)\n\n\n\n\n\n\nâŒ¨Raccourcis clavier essentiels\n\n\n\nRaccourci\nAction\n\n\n\n\nShift + Enter\nâ–¶ï¸ ExÃ©cuter la cellule et passer Ã  la suivante\n\n\nCtrl + Enter\nExÃ©cuter la cellule (rester dessus)\n\n\nEsc\nPasser en mode commande (cellule bleue)\n\n\nEnter\nPasser en mode Ã©dition (cellule verte)\n\n\nA (mode commande)\nInsÃ©rer une cellule au-dessus\n\n\nB (mode commande)\nInsÃ©rer une cellule en-dessous\n\n\nDD (mode commande)\nSupprimer la cellule\n\n\nM (mode commande)\nConvertir en cellule Markdown\n\n\nY (mode commande)\nConvertir en cellule Code\n\n\nCtrl + S\nSauvegarder le notebook\n\n\n\n\nğŸ’¡ Astuce : Apprends Shift + Enter en premier â€” câ€™est le raccourci que tu utiliseras le plus !\n\n\n\n\nPremier test dans Jupyter\n\nDans une cellule, tape :\n\nprint(\"Hello Data Engineer !\")\n\nAppuie sur Shift + Enter\nTu dois voir :\n\nIn [1]: print(\"Hello Data Engineer !\")\n\nHello Data Engineer !\n\n\n\nTypes de cellules\n\n\n\nType\nUsage\nExemple\n\n\n\n\nCode\nExÃ©cuter du Python\nprint(\"Hello\")\n\n\nMarkdown\nDocumenter, titres, explications\n# Mon titre\n\n\n\n# Titre principal\n## Sous-titre\n\nDu texte en **gras** et en *italique*.\n\n- Liste Ã  puces\n- Autre Ã©lÃ©ment\n\n\n\nğŸ›‘ ArrÃªter Jupyter\n\nSauvegarder ton notebook (Ctrl + S)\nFermer lâ€™onglet du navigateur\nDans le terminal oÃ¹ Jupyter tourne : appuyer sur Ctrl + C deux fois\n\n^C\nShutdown this notebook server (y/[n])? y\n\n\n\nğŸ’¡ Alternative : Jupyter dans VS Code\nTu peux aussi ouvrir des notebooks directement dans VS Code (avec lâ€™extension Jupyter installÃ©e) :\n\nOuvrir VS Code\nFile &gt; Open File â†’ sÃ©lectionner un fichier .ipynb\nOu crÃ©er un nouveau fichier avec lâ€™extension .ipynb\n\nCâ€™est souvent plus pratique car tu as tout dans le mÃªme Ã©diteur !\n\n\n\n0.6 Premier test Python\nPython peut sâ€™utiliser de 3 faÃ§ons diffÃ©rentes :\n\n\n\nMode\nUsage\nCommande\n\n\n\n\nInteractif\nTester rapidement du code\npython ou python3\n\n\nScript\nExÃ©cuter un fichier .py\npython mon_script.py\n\n\nNotebook\nExploration, visualisation\nJupyter / VS Code\n\n\n\n\n\nMode interactif (REPL)\nREPL = Read-Eval-Print Loop (Lire-Ã‰valuer-Afficher en boucle)\n1. Lancer lâ€™interprÃ©teur Python :\n# Dans ton terminal (PowerShell, CMD, Bash...)\npython\n# ou sur Linux/macOS\npython3\n2. Tu verras apparaÃ®tre le prompt &gt;&gt;&gt; :\nPython 3.12.0 (main, Oct  2 2024, 12:00:00)\n[GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n3. Taper du code Python directement :\n&gt;&gt;&gt; print(\"Hello Data Engineer\")\nHello Data Engineer\n\n&gt;&gt;&gt; 2 + 3\n5\n\n&gt;&gt;&gt; nom = \"Alice\"\n&gt;&gt;&gt; print(f\"Bonjour {nom}\")\nBonjour Alice\n4. Quitter lâ€™interprÃ©teur :\n&gt;&gt;&gt; exit()\nOu utiliser le raccourci clavier : - Windows : Ctrl + Z puis EntrÃ©e - Linux/macOS : Ctrl + D\n\n\n\nQuand utiliser le mode interactif ?\n\n\n\nâœ… Bon usage\nâŒ Mauvais usage\n\n\n\n\nTester une syntaxe rapidement\nÃ‰crire un programme complet\n\n\nVÃ©rifier le rÃ©sultat dâ€™une expression\nCode quâ€™on veut sauvegarder\n\n\nExplorer une librairie (help(fonction))\nPipeline de production\n\n\nCalculatrice avancÃ©e\nTravail collaboratif\n\n\n\n\nğŸ’¡ Pour ce cours, tu utiliseras surtout les Notebooks Jupyter (exploration) et les scripts .py (production). Le mode interactif est utile pour des tests rapides.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#environnements-virtuels",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "0.7 Environnements virtuels",
    "text": "0.7 Environnements virtuels\nUn environnement virtuel isole les dÃ©pendances de chaque projet. Câ€™est indispensable en Data Engineering pour Ã©viter les conflits de versions.\n\nPourquoi utiliser un environnement virtuel ?\n\n\n\n\n\n\n\nâŒ Sans environnement virtuel\nâœ… Avec environnement virtuel\n\n\n\n\nTous les projets partagent les mÃªmes packages\nChaque projet a ses propres packages\n\n\nConflits de versions\nIsolation complÃ¨te\n\n\nDifficile Ã  reproduire\nReproductible avec requirements.txt\n\n\n\n\n\nOption 1 : venv (intÃ©grÃ© Ã  Python)\n# CrÃ©er un environnement virtuel\npython -m venv mon_env\n\n# Activer l'environnement\n# Windows\nmon_env\\Scripts\\activate\n\n# Linux / macOS\nsource mon_env/bin/activate\n\n# Tu verras (mon_env) au dÃ©but de ta ligne de commande\n\n# DÃ©sactiver l'environnement\ndeactivate\n\n\nOption 2 : conda (Anaconda/Miniconda)\n# CrÃ©er un environnement\nconda create -n mon_projet python=3.11\n\n# Activer\nconda activate mon_projet\n\n# DÃ©sactiver\nconda deactivate\n\n# Lister les environnements\nconda env list\n\n\nğŸ’¡ Recommandation pour Data Engineers\n\n\n\nOutil\nQuand lâ€™utiliser\n\n\n\n\nvenv\nProjets Python purs, lÃ©gers, CI/CD\n\n\nconda\nData Science, dÃ©pendances complexes (NumPy, Spark)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-des-packages-avec-pip",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "0.8 Gestion des packages avec pip",
    "text": "0.8 Gestion des packages avec pip\npip est le gestionnaire de packages Python. Tu lâ€™utiliseras pour installer les librairies Data Engineering.\n\nCommandes essentielles\n# Installer un package\npip install pandas\n\n# Installer une version spÃ©cifique\npip install pandas==2.0.0\n\n# Installer plusieurs packages\npip install pandas numpy requests\n\n# Mettre Ã  jour un package\npip install --upgrade pandas\n\n# DÃ©sinstaller\npip uninstall pandas\n\n# Lister les packages installÃ©s\npip list\n\n# Voir les infos d'un package\npip show pandas\n\n\nLe fichier requirements.txt\nCe fichier liste toutes les dÃ©pendances dâ€™un projet. Indispensable pour la reproductibilitÃ©.\n# GÃ©nÃ©rer le fichier Ã  partir de l'environnement actuel\npip freeze &gt; requirements.txt\n\n# Installer toutes les dÃ©pendances d'un projet\npip install -r requirements.txt\n\n\nExemple de requirements.txt pour Data Engineering\n# Data Processing\npandas&gt;=2.0.0\nnumpy&gt;=1.24.0\npyarrow&gt;=12.0.0\n\n# APIs\nrequests&gt;=2.28.0\nfastapi&gt;=0.100.0\n\n# Database\nsqlalchemy&gt;=2.0.0\npsycopg2-binary&gt;=2.9.0\n\n# Testing\npytest&gt;=7.0.0\n\nğŸ’¡ Bonne pratique : Toujours travailler dans un environnement virtuel avant dâ€™installer des packages !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#variables-et-types",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "1. Variables et Types",
    "text": "1. Variables et Types\nUne variable est un nom qui rÃ©fÃ©rence une valeur en mÃ©moire.\nPython possÃ¨de plusieurs types intÃ©grÃ©s (builtins). Voici ceux Ã  maÃ®triser absolument en Data Engineering :\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nType\nExemple\nUsage Data Engineering\n\n\n\n\nNumÃ©rique\nint\n3, 42, -5\nComptage, index, tailles, IDs\n\n\n\nfloat\n3.14, 0.99\nPrix, mesures, statistiques\n\n\nTexte\nstr\n\"Abidjan\"\nParsing CSV/JSON, nettoyage, logs\n\n\nBoolÃ©en\nbool\nTrue, False\nConditions, filtres, validation\n\n\nSÃ©quences ordonnÃ©es\nlist\n[1,2,3]\nLignes CSV, sÃ©ries numÃ©riques\n\n\n\ntuple\n(200, \"OK\")\nValeurs fixes, clÃ©s composites\n\n\nMapping\ndict\n{\"id\":1,\"ville\":\"Paris\"}\nJSON, API, MongoDB\n\n\nEnsemble (unique)\nset\n{\"python\",\"data\"}\nDÃ©duplication (emails, tags)\n\n\nBinaire\nbytes\nb\"abc\"\nFichiers binaires, images, rÃ©seau\n\n\n\n\n\nExemples de variables\nage = 30              # int\npi = 3.14             # float\nnom = \"Alice\"         # str\nest_data_engineer = True  # bool\n\nprint(age, type(age))\nprint(pi, type(pi))\nprint(nom, type(nom))\nprint(est_data_engineer, type(est_data_engineer))\n\n\nConversion de types\nIl est frÃ©quent de convertir des chaÃ®nes en nombres, par exemple aprÃ¨s lecture dâ€™un fichier.\nage_str = \"25\"\nage_int = int(age_str)\nâš ï¸ Erreurs frÃ©quentes : - Essayer de convertir une chaÃ®ne non numÃ©rique : int(\"abc\") â†’ ValueError ; - Additionner directement un str et un int : - \"25\" + 3 âŒ - int(\"25\") + 3 âœ”ï¸",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#conditions-et-boucles",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "2. Conditions et Boucles",
    "text": "2. Conditions et Boucles\nLes conditions permettent de prendre des dÃ©cisions dans le code.\nLes boucles permettent de rÃ©pÃ©ter des actions automatiquement.\n\nğŸ’¡ En Data Engineering, ces structures sont essentielles pour : - Filtrer des donnÃ©es (conditions) - Traiter plusieurs fichiers (boucles) - Valider des rÃ¨gles mÃ©tier (conditions) - Parcourir des bases de donnÃ©es (boucles)\n\n\n\n2.1 Conditions (if / elif / else)\nLes conditions testent des critÃ¨res et exÃ©cutent du code selon le rÃ©sultat.\n\nSyntaxe de base\nage = 20\n\nif age &lt; 18:\n    print(\"Mineur\")\nelif age == 18:\n    print(\"Tout juste majeur\")\nelse:\n    print(\"Majeur\")\nFonctionnement :\n\nif : PremiÃ¨re condition testÃ©e\nelif : Condition alternative (si if est fausse)\nelse : Cas par dÃ©faut (si toutes les conditions sont fausses)\n\n\n\nOpÃ©rateurs de comparaison\n\n\n\nOpÃ©rateur\nSignification\nExemple\n\n\n\n\n==\nÃ‰gal Ã \nage == 18\n\n\n!=\nDiffÃ©rent de\nage != 18\n\n\n&lt;\nInfÃ©rieur Ã \nage &lt; 18\n\n\n&lt;=\nInfÃ©rieur ou Ã©gal\nage &lt;= 18\n\n\n&gt;\nSupÃ©rieur Ã \nage &gt; 18\n\n\n&gt;=\nSupÃ©rieur ou Ã©gal\nage &gt;= 18\n\n\n\n\n\nConditions multiples\nage = 25\npays = \"France\"\n\n# OpÃ©rateur AND (et)\nif age &gt;= 18 and pays == \"France\":\n    print(\"Peut voter en France\")\n\n# OpÃ©rateur OR (ou)\nif age &lt; 18 or age &gt; 65:\n    print(\"Tarif rÃ©duit\")\n\n# OpÃ©rateur NOT (nÃ©gation)\nif not (age &lt; 18):\n    print(\"Majeur\")\n\n\n\n\n2.2 Boucles (for et while)\nLes boucles permettent dâ€™exÃ©cuter du code de maniÃ¨re rÃ©pÃ©tÃ©e.\n\nBoucle for : Parcourir une sÃ©quence\n# Parcourir une liste\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\n\nfor nom in noms:\n    print(f\"Bonjour {nom}\")\n\n# RÃ©sultat :\n# Bonjour Alice\n# Bonjour Bob\n# Bonjour Charlie\n\n\nBoucle avec range()\n# range(n) gÃ©nÃ¨re les nombres de 0 Ã  n-1\nfor i in range(5):\n    print(f\"ItÃ©ration {i}\")\n\n# RÃ©sultat : 0, 1, 2, 3, 4\n\n# range(dÃ©but, fin, pas)\nfor i in range(0, 10, 2):\n    print(i)  # 0, 2, 4, 6, 8\n\n\nBoucle while : RÃ©pÃ©ter tant quâ€™une condition est vraie\ncompteur = 0\n\nwhile compteur &lt; 3:\n    print(f\"Compteur = {compteur}\")\n    compteur += 1  # âš ï¸ IMPORTANT : incrÃ©mentation obligatoire\n\n# RÃ©sultat :\n# Compteur = 0\n# Compteur = 1\n# Compteur = 2\n\n\nContrÃ´le de flux : break et continue\n# break : Sortir immÃ©diatement de la boucle\nfor i in range(10):\n    if i == 5:\n        break  # ArrÃªte la boucle Ã  5\n    print(i)  # Affiche 0, 1, 2, 3, 4\n\n# continue : Passer Ã  l'itÃ©ration suivante\nfor i in range(5):\n    if i == 2:\n        continue  # Saute l'itÃ©ration quand i=2\n    print(i)  # Affiche 0, 1, 3, 4\n\n\nCas dâ€™usage Data Engineering\n# Exemple 1 : Traitement de fichiers multiples\nfichiers = [\"users_2024_01.csv\", \"users_2024_02.csv\", \"users_2024_03.csv\"]\n\nfor fichier in fichiers:\n    print(f\"Traitement de {fichier}...\")\n    # Ici : logique de lecture/transformation\n    # df = pd.read_csv(fichier)\n    # process(df)\n\n# Exemple 2 : Nettoyage de donnÃ©es\nposts = [\n    {\"text\": \"Hello\", \"likes\": 10},\n    {\"text\": \"\", \"likes\": 5},       # âš ï¸ Texte vide\n    {\"text\": \"Python\", \"likes\": 20}\n]\n\nposts_valides = []\n\nfor post in posts:\n    # Skip les posts vides\n    if not post[\"text\"].strip():\n        continue\n    \n    # Nettoyer et garder\n    post[\"text\"] = post[\"text\"].strip().lower()\n    posts_valides.append(post)\n\nprint(posts_valides)\n# [{'text': 'hello', 'likes': 10}, {'text': 'python', 'likes': 20}]\n\n# Exemple 3 : Retry logic (tentatives multiples)\nmax_tentatives = 3\ntentative = 0\nsucces = False\n\nwhile tentative &lt; max_tentatives and not succes:\n    print(f\"Tentative {tentative + 1}...\")\n    \n    # Simulation d'une connexion\n    # succes = tenter_connexion()\n    \n    tentative += 1\n    \n    if not succes and tentative &lt; max_tentatives:\n        print(\"Ã‰chec, nouvelle tentative...\")\n\n\n\n\n2.3 Boucles avancÃ©es : enumerate() et zip()\n\nenumerate() : Obtenir lâ€™index ET la valeur\nfruits = [\"pomme\", \"banane\", \"orange\"]\n\n# Sans enumerate (moins pratique)\nfor i in range(len(fruits)):\n    print(f\"{i}: {fruits[i]}\")\n\n# Avec enumerate (recommandÃ©)\nfor index, fruit in enumerate(fruits):\n    print(f\"{index}: {fruit}\")\n\n# Avec enumerate dÃ©marrant Ã  1\nfor num, fruit in enumerate(fruits, start=1):\n    print(f\"Fruit #{num}: {fruit}\")\n\n\nzip() : Parcourir plusieurs listes simultanÃ©ment\nnoms = [\"Alice\", \"Bob\", \"Charlie\"]\nages = [25, 30, 35]\nvilles = [\"Paris\", \"Lyon\", \"Marseille\"]\n\nfor nom, age, ville in zip(noms, ages, villes):\n    print(f\"{nom} a {age} ans et habite Ã  {ville}\")\n\n# RÃ©sultat :\n# Alice a 25 ans et habite Ã  Paris\n# Bob a 30 ans et habite Ã  Lyon\n# Charlie a 35 ans et habite Ã  Marseille\n\n\n\n\nErreurs frÃ©quentes et bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ Erreur\nâœ… Correction\nğŸ’¡ Explication\n\n\n\n\nOublier : aprÃ¨s if/for/while\nif age &gt; 18:\nSyntaxe obligatoire\n\n\nMauvaise indentation\nUtiliser 4 espaces\nPython est sensible Ã  lâ€™indentation\n\n\nBoucle infinie while\nToujours incrÃ©menter\ncompteur += 1\n\n\nModifier liste pendant for\nCrÃ©er nouvelle liste\nÃ‰vite comportements imprÃ©visibles\n\n\n= au lieu de ==\nif age == 18:\n= assigne, == compare\n\n\n\nExemples dâ€™erreurs :\n# âŒ Erreur 1 : Oublier le :\nif age &gt; 18\n    print(\"Majeur\")  # SyntaxError\n\n# âœ… Correction\nif age &gt; 18:\n    print(\"Majeur\")\n\n# âŒ Erreur 2 : Boucle infinie\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    # Oubli d'incrÃ©menter â†’ boucle infinie !\n\n# âœ… Correction\ncompteur = 0\nwhile compteur &lt; 5:\n    print(compteur)\n    compteur += 1\n\n# âŒ Erreur 3 : Modifier liste pendant boucle\nnombres = [1, 2, 3, 4, 5]\nfor n in nombres:\n    if n % 2 == 0:\n        nombres.remove(n)  # âš ï¸ Comportement imprÃ©visible\n\n# âœ… Correction : List comprehension\nnombres = [1, 2, 3, 4, 5]\nnombres_impairs = [n for n in nombres if n % 2 != 0]\n\n\n\nRÃ©capitulatif\n\n\n\nStructure\nUsage\nExemple\n\n\n\n\nif/elif/else\nPrendre des dÃ©cisions\nValidation, filtrage\n\n\nfor\nParcourir sÃ©quences\nTraiter fichiers, lignes\n\n\nwhile\nRÃ©pÃ©ter tant queâ€¦\nRetry logic, polling\n\n\nbreak\nSortir de boucle\nArrÃªt prÃ©maturÃ©\n\n\ncontinue\nSauter itÃ©ration\nSkip donnÃ©es invalides\n\n\nenumerate()\nIndex + valeur\nNumÃ©rotation\n\n\nzip()\nCombiner listes\nJoindre donnÃ©es parallÃ¨les\n\n\n\n\n\n\nPoints clÃ©s Ã  retenir\n\nConditions : Utilisent == pour comparer (pas =)\nIndentation : 4 espaces obligatoires aprÃ¨s :\nwhile : Toujours prÃ©voir une sortie de boucle\nfor : PrÃ©fÃ©rer enumerate() si besoin de lâ€™index\nList comprehension : Alternative Ã©lÃ©gante aux boucles simples",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#structures-de-donnÃ©es",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "3. Structures de donnÃ©es",
    "text": "3. Structures de donnÃ©es\nPython propose plusieurs structures trÃ¨s utilisÃ©es en data engineering.\n\n\n\nStructure\nOrdonnÃ©\nModifiable\nDuplicats\nAccÃ¨s principal\n\n\n\n\nlist\nâœ”ï¸\nâœ”ï¸\nâœ”ï¸\nindex (0, 1, 2â€¦)\n\n\ndict\nâœ”ï¸ (3.7+)\nâœ”ï¸\nclÃ©s uniques\nclÃ© (\"nom\")\n\n\ntuple\nâœ”ï¸\nâŒ\nâœ”ï¸\nindex\n\n\nset\nâŒ\nâœ”ï¸\nâŒ\nappartenance (in)\n\n\n\n\n3.1 Listes (list)\nUne liste est une sÃ©quence ordonnÃ©e et modifiable.\n# CrÃ©ation d'une liste\nnombres = [10, 20, 30, 40]\n\n# AccÃ¨s par index\nprint(nombres[0])   # 10\nprint(nombres[2])   # 30\n\n# Ajout en fin de liste\nnombres.append(50)\nprint(\"AprÃ¨s append :\", nombres)\n\n# Insertion Ã  une position prÃ©cise\nnombres.insert(1, 15)\nprint(\"AprÃ¨s insert :\", nombres)\n\n# Modification d'un Ã©lÃ©ment\nnombres[0] = 5\nprint(\"AprÃ¨s modification :\", nombres)\n\n# Suppression par valeur\nnombres.remove(30)\nprint(\"AprÃ¨s remove :\", nombres)\n\n# Suppression par index\ndel nombres[0]\nprint(\"AprÃ¨s del :\", nombres)\n\nCrÃ©ation dynamique de listes\nOn crÃ©e trÃ¨s souvent des listes Ã  partir dâ€™autres listes, avec une boucle ou une list comprehension.\nnombres = [1, 2, 3, 4, 5, 6]\n\n# Version avec boucle\npairs = []\nfor n in nombres:\n    if n % 2 == 0:\n        pairs.append(n)\n\nprint(\"Pairs (boucle) :\", pairs)\n\n# Version list comprehension\npairs2 = [n for n in nombres if n % 2 == 0]\nprint(\"Pairs (list comprehension) :\", pairs2)\nâš ï¸ Erreurs frÃ©quentes avec les listes : - nombres[10] alors que la liste a moins dâ€™Ã©lÃ©ments â†’ IndexError ; - nombres.remove(999) alors que 999 nâ€™est pas dans la liste â†’ ValueError.\n\n\n\n3.2 Dictionnaires (dict)\nUn dictionnaire stocke des paires clÃ© â†’ valeur. Câ€™est lâ€™Ã©quivalent naturel des objets JSON, trÃ¨s utilisÃ© pour les APIs et NoSQL.\nutilisateur = {\n    \"id\": 1,\n    \"nom\": \"Alice\",\n    \"ville\": \"Abidjan\"\n}\n\n# AccÃ¨s Ã  une valeur par clÃ©\nprint(utilisateur[\"nom\"])  # Alice\n\n# Ajout / modification\nutilisateur[\"age\"] = 30\nutilisateur[\"ville\"] = \"BouakÃ©\"\n\n# Suppression\ndel utilisateur[\"id\"]\n\nprint(utilisateur)\n\nAccÃ¨s sÃ©curisÃ© avec .get()\nUtiliser dict.get() permet dâ€™Ã©viter un KeyError si la clÃ© nâ€™existe pas.\nprint(utilisateur.get(\"email\"))           # None\nprint(utilisateur.get(\"email\", \"Inconnu\"))  # Inconnu\n\n\nCrÃ©ation dynamique : comptage dâ€™occurrences\nnoms = [\"bob\", \"alice\", \"bob\", \"charlie\", \"alice\"]\ncompte = {}\n\nfor n in noms:\n    compte[n] = compte.get(n, 0) + 1\n\nprint(compte)  # {'bob': 2, 'alice': 2, 'charlie': 1}\nâš ï¸ Erreurs frÃ©quentes avec les dictionnaires : - utilisateur[\"email\"] alors que la clÃ© nâ€™existe pas â†’ KeyError ; - supposer quâ€™un dictionnaire est indexÃ© comme une liste (utilisateur[0]).\n\n\n\n3.3 Tuples (tuple)\nUn tuple est comme une liste non modifiable (immutable). On lâ€™utilise pour reprÃ©senter des collections fixes de valeurs : coordonnÃ©es, dates, etc.\ncoord = (5.0, 10.0)\nprint(coord[0])  # 5.0\nprint(coord[1])  # 10.0\n\n# coord[0] = 20.0  # âŒ TypeError : un tuple n'est pas modifiable\nEn data engineering, les tuples sont utiles pour : - retourner plusieurs valeurs depuis une fonction ; - reprÃ©senter des clÃ©s composites (ex : (annÃ©e, mois)).\n\n\n3.4 Ensembles (set)\nUn ensemble (set) contient des valeurs uniques, sans ordre garanti. TrÃ¨s utile pour dÃ©dupliquer une liste.\ntags = {\"python\", \"data\", \"python\"}\nprint(tags)  # 'python' n'apparaÃ®t qu'une seule fois\n\n# Ajout\ntags.add(\"engineer\")\n\n# Suppression (erreur si absent)\ntags.remove(\"data\")\n\n# Suppression sans erreur si absent\ntags.discard(\"ai\")\n\nprint(tags)\nğŸ’¡ Exemple de dÃ©duplication :\nemails = [\"a@test.com\", \"b@test.com\", \"a@test.com\"]\nemails_uniques = list(set(emails))\nprint(emails_uniques)\nâš ï¸ Erreurs frÃ©quentes : - Compter sur lâ€™ordre dâ€™un set (lâ€™ordre nâ€™est pas garanti) ; - Utiliser remove() pour un Ã©lÃ©ment possiblement absent â†’ prÃ©fÃ©rer discard().\n\n\n3.5 Mini-exercice â€” Structures de donnÃ©es\nDonnÃ©es\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\nObjectifs 1. CrÃ©er la liste de tous les utilisateurs (list) 2. CrÃ©er la liste unique des utilisateurs (set) 3. CrÃ©er un dictionnaire qui compte le nombre de requÃªtes par utilisateur (dict)\n# Ã€ toi de jouer ğŸ˜Š\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []            # TODO\nutilisateurs_uniques = set() # TODO\ncompte_par_user = {}         # TODO\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\nlogs = [\n    {\"user\": \"alice\", \"status\": 200},\n    {\"user\": \"bob\", \"status\": 500},\n    {\"user\": \"alice\", \"status\": 404},\n    {\"user\": \"bob\", \"status\": 200},\n]\n\nutilisateurs = []\nutilisateurs_uniques = set()\ncompte_par_user = {}\n\nfor log in logs:\n    user = log[\"user\"]\n    utilisateurs.append(user)\n    utilisateurs_uniques.add(user)\n    compte_par_user[user] = compte_par_user.get(user, 0) + 1\n\nprint(utilisateurs)\nprint(utilisateurs_uniques)\nprint(compte_par_user)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#comprehensions-syntaxe-puissante-pour-transformer-les-donnÃ©es",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es",
    "text": "Comprehensions â€” Syntaxe puissante pour transformer les donnÃ©es\nLes comprehensions sont une syntaxe Python Ã©lÃ©gante et performante pour crÃ©er des listes, dictionnaires ou sets en une seule ligne. Câ€™est fondamental en Data Engineering pour transformer des donnÃ©es efficacement.\n\n\nList Comprehension\nSyntaxe : [expression for item in iterable if condition]\n# âŒ MÃ©thode classique (verbose)\nnombres = [1, 2, 3, 4, 5]\ncarres = []\nfor n in nombres:\n    carres.append(n ** 2)\nprint(carres)  # [1, 4, 9, 16, 25]\n\n# âœ… List comprehension (recommandÃ©)\ncarres = [n ** 2 for n in nombres]\nprint(carres)  # [1, 4, 9, 16, 25]\n\nAvec condition (filtre)\n# Garder seulement les nombres pairs\nnombres = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\npairs = [n for n in nombres if n % 2 == 0]\nprint(pairs)  # [2, 4, 6, 8, 10]\n\n# Exemple Data Engineering : filtrer les emails valides\nemails = [\"alice@test.com\", \"invalid\", \"bob@company.org\", \"\"]\nemails_valides = [e for e in emails if \"@\" in e and e]\nprint(emails_valides)  # ['alice@test.com', 'bob@company.org']\n\n\nCas dâ€™usage Data Engineering\n# Extraire les noms d'une liste de dictionnaires\nusers = [\n    {\"nom\": \"Alice\", \"actif\": True},\n    {\"nom\": \"Bob\", \"actif\": False},\n    {\"nom\": \"Charlie\", \"actif\": True}\n]\n\n# Noms des utilisateurs actifs en majuscules\nnoms_actifs = [u[\"nom\"].upper() for u in users if u[\"actif\"]]\nprint(noms_actifs)  # ['ALICE', 'CHARLIE']\n\n# Nettoyer une liste de fichiers\nfichiers = [\"data.csv\", \"readme.txt\", \"users.csv\", \"config.yaml\"]\ncsv_files = [f for f in fichiers if f.endswith(\".csv\")]\nprint(csv_files)  # ['data.csv', 'users.csv']\n\n\n\n\nDict Comprehension\nSyntaxe : {key: value for item in iterable if condition}\n# CrÃ©er un dictionnaire Ã  partir de deux listes\nnoms = [\"alice\", \"bob\", \"charlie\"]\nages = [25, 30, 35]\n\nusers_dict = {nom: age for nom, age in zip(noms, ages)}\nprint(users_dict)  # {'alice': 25, 'bob': 30, 'charlie': 35}\n\n# Inverser un dictionnaire\noriginal = {\"a\": 1, \"b\": 2, \"c\": 3}\ninverse = {v: k for k, v in original.items()}\nprint(inverse)  # {1: 'a', 2: 'b', 3: 'c'}\n\nCas dâ€™usage Data Engineering\n# Transformer des donnÃ©es JSON\nraw_data = [\n    {\"id\": 1, \"value\": \"100\"},\n    {\"id\": 2, \"value\": \"200\"},\n    {\"id\": 3, \"value\": \"300\"}\n]\n\n# CrÃ©er un mapping id -&gt; valeur (convertie en int)\nid_to_value = {d[\"id\"]: int(d[\"value\"]) for d in raw_data}\nprint(id_to_value)  # {1: 100, 2: 200, 3: 300}\n\n\n\n\nSet Comprehension\nSyntaxe : {expression for item in iterable if condition}\n# Valeurs uniques\nnombres = [1, 2, 2, 3, 3, 3, 4, 4, 4, 4]\nuniques = {n for n in nombres}\nprint(uniques)  # {1, 2, 3, 4}\n\n# Domaines uniques des emails\nemails = [\"alice@gmail.com\", \"bob@yahoo.com\", \"charlie@gmail.com\"]\ndomaines = {e.split(\"@\")[1] for e in emails}\nprint(domaines)  # {'gmail.com', 'yahoo.com'}\n\n\n\nBonnes pratiques\n\n\n\n\n\n\n\nâœ… Faire\nâŒ Ã‰viter\n\n\n\n\nComprehensions simples et lisibles\nComprehensions imbriquÃ©es complexes\n\n\nUne seule transformation\nPlusieurs opÃ©rations chaÃ®nÃ©es\n\n\nUtiliser pour crÃ©er des collections\nUtiliser pour des effets de bord\n\n\n\n\n\n\nMini-exercice\nTransformer cette liste de transactions :\ntransactions = [\n    {\"id\": 1, \"montant\": 100, \"devise\": \"EUR\"},\n    {\"id\": 2, \"montant\": 50, \"devise\": \"USD\"},\n    {\"id\": 3, \"montant\": 200, \"devise\": \"EUR\"},\n    {\"id\": 4, \"montant\": 75, \"devise\": \"USD\"}\n]\n\nCrÃ©er une liste des montants en EUR uniquement\nCrÃ©er un dict {id: montant} pour les transactions &gt; 60\n\n\n\nğŸ’¡ Solution\n\n# 1. Montants EUR\nmontants_eur = [t[\"montant\"] for t in transactions if t[\"devise\"] == \"EUR\"]\nprint(montants_eur)  # [100, 200]\n\n# 2. Dict id -&gt; montant (&gt; 60)\nid_montant = {t[\"id\"]: t[\"montant\"] for t in transactions if t[\"montant\"] &gt; 60}\nprint(id_montant)  # {1: 100, 3: 200, 4: 75}",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#fonctions",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "4. Fonctions",
    "text": "4. Fonctions\nUne fonction permet de :\n\nrÃ©utiliser du code ;\nencapsuler de la logique mÃ©tier (nettoyage, validationâ€¦) ;\nsÃ©curiser la qualitÃ© de donnÃ©es (type, structureâ€¦).\n\nSyntaxe gÃ©nÃ©rale :\ndef nom(param1, param2=valeur_par_defaut) -&gt; type_retour:\n    # traitement\n    return resultat\nExemple :\ndef somme(a: int, b: int) -&gt; int:\n    \"\"\"Retourne la somme de deux entiers.\"\"\"\n    return a + b\n\nresultat = somme(3, 5)\nprint(\"RÃ©sultat :\", resultat)\n\n4.1 Fonction pure (sans effet externe)\ndef normaliser_nom(nom: str) -&gt; str:\n    \"\"\"Nettoie un nom : supprime espaces, met en minuscule et capitalise.\"\"\"\n    return nom.strip().lower().capitalize()\n\nprint(normaliser_nom(\"  aLiCe  \"))  # Alice\n\n\n4.2 ParamÃ¨tres + valeurs par dÃ©faut\ndef calculer_total(prix: float, quantite: int = 1, taxe: float = 0.18) -&gt; float:\n    \"\"\"Retourne le prix total avec taxe.\"\"\"\n    return prix * quantite * (1 + taxe)\n\nprint(calculer_total(2000))  \nprint(calculer_total(2000, quantite=3, taxe=0.09))\n\n\n4.3 Retourner plusieurs valeurs (tuple)\ndef stats_notes(notes: list[int]) -&gt; tuple[float, float]:\n    \"\"\"Retourne moyenne et maximum d'une liste de notes.\"\"\"\n    moyenne = sum(notes) / len(notes)\n    maxi = max(notes)\n    return moyenne, maxi\n\nm, mx = stats_notes([14, 9, 18])\nprint(\"Moyenne:\", m, \"Max:\", mx)\n\n\n4.4 Fonctions qui manipulent un dictionnaire\ndef extraire_champ(data: dict, champ: str, default=None):\n    \"\"\"RÃ©cupÃ¨re un champ d'un dict, Ã©vite KeyError.\"\"\"\n    return data.get(champ, default)\n\nuser = {\"nom\": \"Sara\", \"ville\": \"Paris\"}\nprint(extraire_champ(user, \"nom\"))         # Sara\nprint(extraire_champ(user, \"age\", \"N/A\"))  # N/A\nâš ï¸ Erreurs frÃ©quentes : - Oublier les parenthÃ¨ses lors de lâ€™appel : somme au lieu de somme(3, 5) ; - Oublier return â†’ la fonction retourne None ; - Ne pas respecter le nombre dâ€™arguments attendus.\nâŒ Mauvaise pratique :\ndef ajouter(element, liste=[]):  # liste partagÃ©e !\n    liste.append(element)\n    return liste\nâœ”ï¸ Correct :\ndef ajouter(element, liste=None):\n    if liste is None: liste = []\n    liste.append(element)\n    return liste\n\n\nMini-exercice â€” Fonctions sur des posts\n\nDataset simulÃ©\nposts = [\n  {\"user\": \"alice\", \"text\": \"  Hello World  \"},\n  {\"user\": \"bob\", \"text\": \"Data Engineer ici\"},\n  {\"user\": \"alice\", \"text\": \"Python est top \"}\n]\n\n\nInstructions\nCrÃ©er 3 fonctions :\n\nnettoyer_texte(text: str) -&gt; str\nNettoie le texte (supprime espaces, convertit en minuscules)\nlongueur_post(post: dict) -&gt; int\nRetourne la longueur du texte nettoyÃ© dâ€™un post\nstats_posts(posts: list[dict]) -&gt; tuple[float, int, int]\nRetour attendu : (moyenne, maximum, minimum) des longueurs de texte\n\n# Ã€ toi de jouer ğŸ˜Š\n\n# TODO\n\n\nğŸ’¡ Correction (cliquer pour afficher)\n\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\ndef longueur_post(post: dict) -&gt; int:\n    return len(nettoyer_texte(post[\"text\"]))\n\ndef stats_posts(posts: list[dict]) -&gt; tuple[float, int, int]:\n    longueurs = [longueur_post(p) for p in posts]\n    return (sum(longueurs)/len(longueurs), max(longueurs), min(longueurs))\n\nprint(stats_posts(posts))\nRÃ©sultat attendu : (15.333333333333334, 19, 11)\n\n\n\n\n4.5 *args et **kwargs â€” Fonctions flexibles\nCes syntaxes permettent de crÃ©er des fonctions qui acceptent un nombre variable dâ€™arguments. TrÃ¨s utilisÃ© pour crÃ©er des wrappers, des dÃ©corateurs, ou des fonctions gÃ©nÃ©riques.\n\n\n*args â€” Arguments positionnels variables\ndef somme(*args):\n    \"\"\"Accepte n'importe quel nombre d'arguments.\"\"\"\n    print(f\"Arguments reÃ§us : {args}\")  # C'est un tuple\n    return sum(args)\n\nprint(somme(1, 2))           # 3\nprint(somme(1, 2, 3, 4, 5))  # 15\nprint(somme())               # 0\n\n\n\n**kwargs â€” Arguments nommÃ©s variables\ndef afficher_info(**kwargs):\n    \"\"\"Accepte n'importe quel argument nommÃ©.\"\"\"\n    print(f\"Arguments reÃ§us : {kwargs}\")  # C'est un dict\n    for cle, valeur in kwargs.items():\n        print(f\"  {cle} = {valeur}\")\n\nafficher_info(nom=\"Alice\", age=30, ville=\"Paris\")\n# Arguments reÃ§us : {'nom': 'Alice', 'age': 30, 'ville': 'Paris'}\n#   nom = Alice\n#   age = 30\n#   ville = Paris\n\n\n\nCombiner les deux\ndef fonction_flexible(obligatoire, *args, option=\"defaut\", **kwargs):\n    \"\"\"Ordre : obligatoire, *args, avec dÃ©faut, **kwargs\"\"\"\n    print(f\"Obligatoire : {obligatoire}\")\n    print(f\"Args : {args}\")\n    print(f\"Option : {option}\")\n    print(f\"Kwargs : {kwargs}\")\n\nfonction_flexible(\"premier\", 1, 2, 3, option=\"custom\", extra=\"valeur\")\n\n\n\nCas dâ€™usage Data Engineering\ndef log_etl(etape: str, *messages, niveau: str = \"INFO\", **metadata):\n    \"\"\"Logger flexible pour pipeline ETL.\"\"\"\n    print(f\"[{niveau}] {etape}\")\n    for msg in messages:\n        print(f\"  - {msg}\")\n    if metadata:\n        print(f\"  Metadata: {metadata}\")\n\n# Utilisation\nlog_etl(\n    \"EXTRACT\",\n    \"Connexion Ã©tablie\",\n    \"1000 lignes lues\",\n    niveau=\"INFO\",\n    source=\"postgres\",\n    table=\"users\"\n)\n# Wrapper pour ajouter du logging Ã  n'importe quelle fonction\ndef avec_logging(func):\n    def wrapper(*args, **kwargs):\n        print(f\"Appel de {func.__name__} avec args={args}, kwargs={kwargs}\")\n        result = func(*args, **kwargs)\n        print(f\"RÃ©sultat : {result}\")\n        return result\n    return wrapper",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-simple-modÃ©liser-un-utilisateur",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤",
    "text": "5.1 Exemple simple : ModÃ©liser un utilisateur ğŸ‘¤\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\nu = Utilisateur(1, \"Alice\", \"Abidjan\")\nprint(u.presentation())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ajouter-une-mÃ©thode-utile",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.2 Ajouter une mÃ©thode utile",
    "text": "5.2 Ajouter une mÃ©thode utile\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, ville: str) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.ville}.\"\n\n    def changer_ville(self, nouvelle_ville: str) -&gt; None:\n        self.ville = nouvelle_ville",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#encapsulation-attributs-protÃ©gÃ©sprivÃ©s",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s",
    "text": "5.3 Encapsulation : attributs protÃ©gÃ©s/privÃ©s\nclass Compte:\n    def __init__(self, solde: float):\n        self._solde = solde        # usage interne\n        self.__secret = \"XYZ123\"   # privÃ© via name mangling",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#reprÃ©sentation-textuelle-str",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.4 ReprÃ©sentation textuelle (str)",
    "text": "5.4 ReprÃ©sentation textuelle (str)\nclass Utilisateur:\n    def __init__(self, identifiant, nom, ville):\n        self.identifiant = identifiant\n        self.nom = nom\n        self.ville = ville\n\n    def __str__(self) -&gt; str:\n        return f\"[{self.identifiant}] {self.nom} ({self.ville})\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#composition-dobjets-objet-dans-un-objet",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.5 Composition dâ€™objets (objet dans un objet)",
    "text": "5.5 Composition dâ€™objets (objet dans un objet)\nclass Adresse:\n    def __init__(self, rue: str, ville: str, pays: str) -&gt; None:\n        self.rue = rue\n        self.ville = ville\n        self.pays = pays\n\n    def __str__(self) -&gt; str:\n        return f\"{self.rue}, {self.ville} ({self.pays})\"\n\nclass Utilisateur:\n    def __init__(self, identifiant: int, nom: str, adresse: Adresse) -&gt; None:\n        self.identifiant = identifiant\n        self.nom = nom\n        self.adresse = adresse\n\n    def presentation(self) -&gt; str:\n        return f\"Je m'appelle {self.nom} et j'habite Ã  {self.adresse}.\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-des-classes-en-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.6 Pourquoi utiliser des classes en Data Engineering ?",
    "text": "5.6 Pourquoi utiliser des classes en Data Engineering ?\nclass Transaction:\n    def __init__(self, id, montant, devise, timestamp):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n        self.timestamp = timestamp\n\nclass Transaction:\n    def __init__(self, id, montant, devise):\n        self.id = id\n        self.montant = montant\n        self.devise = devise\n\n    def montant_fcfa(self):\n        taux = {\"EUR\": 655, \"USD\": 600}\n        return self.montant * taux.get(self.devise, 1)\n\nclass ExtracteurCSV:\n    def __init__(self, chemin):\n        self.chemin = chemin\n\n    def extract(self):\n        with open(self.chemin) as f:\n            return f.readlines()\n\nevent = EventHubRecord(payload)\nevent.clean()\nevent.validate()\nevent.to_parquet()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "5.7 Erreurs frÃ©quentes",
    "text": "5.7 Erreurs frÃ©quentes\n\nOublier self dans les mÃ©thodes.\nAccÃ©der Ã  un attribut avant de lâ€™avoir crÃ©Ã©.\nUtiliser une valeur mutable dans __init__ (list, dict).\nConfondre composition et hÃ©ritage.\nFaire une classe Â« God Object Â» avec trop de responsabilitÃ©s.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dataclasses-classes-simplifiÃ©es-pour-les-donnÃ©es",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es",
    "text": "Dataclasses â€” Classes simplifiÃ©es pour les donnÃ©es\nLes dataclasses (Python 3.7+) simplifient la crÃ©ation de classes qui servent principalement Ã  stocker des donnÃ©es. Câ€™est le standard moderne en Data Engineering pour modÃ©liser des structures de donnÃ©es.\n\n\nProblÃ¨me avec les classes classiques\n# âŒ Classe classique â€” verbose et rÃ©pÃ©titif\nclass User:\n    def __init__(self, id: int, nom: str, email: str, actif: bool = True):\n        self.id = id\n        self.nom = nom\n        self.email = email\n        self.actif = actif\n    \n    def __repr__(self):\n        return f\"User(id={self.id}, nom='{self.nom}', email='{self.email}', actif={self.actif})\"\n    \n    def __eq__(self, other):\n        return self.id == other.id and self.nom == other.nom\n\n\n\nâœ… Solution avec dataclass\nfrom dataclasses import dataclass\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n    actif: bool = True  # Valeur par dÃ©faut\n\n# Utilisation\nuser = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user)  # User(id=1, nom='Alice', email='alice@test.com', actif=True)\n\n# Comparaison automatique\nuser2 = User(id=1, nom=\"Alice\", email=\"alice@test.com\")\nprint(user == user2)  # True\n\n\n\nAvantages des dataclasses\n\n\n\nAvantage\nDescription\n\n\n\n\n__init__ auto-gÃ©nÃ©rÃ©\nPlus besoin dâ€™Ã©crire le constructeur\n\n\n__repr__ auto-gÃ©nÃ©rÃ©\nAffichage lisible pour le debug\n\n\n__eq__ auto-gÃ©nÃ©rÃ©\nComparaison par valeur\n\n\nType hints intÃ©grÃ©s\nDocumentation et validation IDE\n\n\nValeurs par dÃ©faut\nSyntaxe simple\n\n\n\n\n\n\nCas dâ€™usage Data Engineering\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\n@dataclass\nclass Transaction:\n    id: int\n    montant: float\n    devise: str\n    timestamp: str\n    metadata: dict = field(default_factory=dict)  # Pour les types mutables\n\n# CrÃ©er une transaction\ntx = Transaction(\n    id=1001,\n    montant=150.50,\n    devise=\"EUR\",\n    timestamp=\"2024-01-15T10:30:00Z\"\n)\nprint(tx)\n\n# AccÃ©der aux attributs\nprint(f\"Montant: {tx.montant} {tx.devise}\")\n\nDataclass immuable (frozen)\n@dataclass(frozen=True)  # Immuable comme un tuple\nclass Config:\n    host: str\n    port: int\n    database: str\n\nconfig = Config(host=\"localhost\", port=5432, database=\"warehouse\")\n# config.port = 3306  # âŒ Erreur : FrozenInstanceError\n\n\n\n\nConvertir en dict/tuple\nfrom dataclasses import asdict, astuple\n\n@dataclass\nclass User:\n    id: int\n    nom: str\n    email: str\n\nuser = User(1, \"Alice\", \"alice@test.com\")\n\n# Conversion en dict (utile pour JSON)\nuser_dict = asdict(user)\nprint(user_dict)  # {'id': 1, 'nom': 'Alice', 'email': 'alice@test.com'}\n\n# Conversion en tuple\nuser_tuple = astuple(user)\nprint(user_tuple)  # (1, 'Alice', 'alice@test.com')\n\n\n\nâš ï¸ Attention : valeurs par dÃ©faut mutables\n# âŒ ERREUR : liste partagÃ©e entre instances\n@dataclass\nclass BadExample:\n    items: list = []  # ValueError!\n\n# âœ… CORRECT : utiliser field(default_factory=...)\nfrom dataclasses import field\n\n@dataclass\nclass GoodExample:\n    items: list = field(default_factory=list)\n    metadata: dict = field(default_factory=dict)\n\n\n\nğŸ’¡ Quand utiliser dataclass vs classe classique ?\n\n\n\nSituation\nRecommandation\n\n\n\n\nStocker des donnÃ©es (models, records)\nâœ… @dataclass\n\n\nLogique mÃ©tier complexe\nClasse classique\n\n\nConfiguration, paramÃ¨tres\nâœ… @dataclass(frozen=True)\n\n\nValidation avancÃ©e\nPydantic (module suivant)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gestion-derreurs-indispensable-en-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "6. Gestion dâ€™erreurs â€” (Indispensable en Data Engineering)",
    "text": "6. Gestion dâ€™erreurs â€” (Indispensable en Data Engineering)\nEn Data Engineering, les erreurs sont inÃ©vitables :\n\nfichiers manquants\n\nAPI indisponible\n\nJSON mal formÃ©\n\ndivision par zÃ©ro\n\nconnexion BD Ã©chouÃ©e\n\ntypage incorrect\n\nğŸ‘‰ La bonne pratique consiste Ã  capturer, expliquer, puis continuer proprement.\nCâ€™est le rÃ´le de try / except.\n\n\nğŸ¯ Exemple simple â€” division sÃ©curisÃ©e\ndef division(a: float, b: float) -&gt; float | None:\n    try:\n        return a / b\n    except ZeroDivisionError:\n        print(\"âŒ Erreur : division par zÃ©ro.\")\n        return None\nprint(division(10, 2))  # OK\nprint(division(10, 0))  # Erreur gÃ©rÃ©e\n\n\n\nExemple plus rÃ©aliste â€” lecture de fichier\ndef lire_csv(chemin: str) -&gt; list | None:\n    try:\n        with open(chemin, \"r\") as f:\n            return f.readlines()\n    except FileNotFoundError:\n        print(f\"âŒ Fichier introuvable : {chemin}\")\n        return None\n\n\n\nğŸ’¡ else et finally\nOn peut amÃ©liorer la lisibilitÃ© avec else et finally :\ntry:\n    result = 10 / 2\nexcept ZeroDivisionError:\n    print(\"Erreur\")\nelse:\n    print(\"Aucune erreur, rÃ©sultat =\", result)\nfinally:\n    print(\"Bloc exÃ©cutÃ© dans tous les cas\")\n\n\n\nExemple Data Engineering : appel API sÃ©curisÃ©\nimport requests\n\ndef fetch_json(url: str) -&gt; dict | None:\n    try:\n        response = requests.get(url, timeout=3)\n        response.raise_for_status()   # GÃ©nÃ¨re une erreur HTTP si code â‰  200\n        return response.json()\n    except requests.exceptions.HTTPError as e:\n        print(\"âŒ Erreur HTTP :\", e)\n    except requests.exceptions.Timeout:\n        print(\"â±ï¸ Timeout : serveur trop lent\")\n    except ValueError:\n        print(\"âŒ JSON mal formÃ©\")\n    except Exception as e:\n        print(\"âš ï¸ Erreur inconnue :\", e)\n    return None\n\n\n\nâš ï¸ Erreurs frÃ©quentes Ã  Ã©viter\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nâœ… Bonne pratique\n\n\n\n\nexcept: (attrape tout)\nSpÃ©cifier lâ€™erreur : except ValueError:\n\n\nCacher lâ€™erreur sans message\nFournir un contexte ğŸ—ƒï¸\n\n\nRetourner nâ€™importe quoi\nRetour cohÃ©rent (None ou valeur par dÃ©faut)\n\n\nMettre trop de logique dans try\nLimiter au strict nÃ©cessaire\n\n\nIgnorer les erreurs silencieusement\nLogguer ou notifier\n\n\n\n\n\n\nConseil pro (trÃ¨s utile en Data Engineering)\nUtiliser raise pour propager lâ€™erreur si elle doit Ãªtre traitÃ©e ailleurs :\ndef parse_json(data: str) -&gt; dict:\n    try:\n        return json.loads(data)\n    except json.JSONDecodeError as e:\n        raise ValueError(f\"JSON invalide : {e}\")\n\n\n\nÃ€ retenir\n\nToujours attraper le type exact dâ€™erreur.\n\nToujours expliquer lâ€™erreur (message clair).\n\nToujours garder un comportement cohÃ©rent (retour None ou valeur dÃ©faut).\n\nLes erreurs silencieuses sont pires que les erreurs visibles.\n\nLes pipelines cassent souvent â€” gÃ©rer les erreurs = Ãªtre un vrai Data Engineer.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#modules-et-imports-structurer-son-code-comme-un-data-engineer",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "7. Modules et Imports â€” Structurer son code comme un Data Engineer",
    "text": "7. Modules et Imports â€” Structurer son code comme un Data Engineer\nEn Python :\n\nUn module = un fichier .py\nUn package = un dossier contenant plusieurs modules + un fichier __init__.py\n\nğŸ‘‰ Cela permet dâ€™organiser un projet data en blocs logiques :\ningestion, nettoyage, transformation, validation, etc.\n\n\nExemple de structure de projet (propre & professionnelle)\nproject/\nâ”œâ”€â”€ utils/               â† Package (outils rÃ©utilisables)\nâ”‚   â”œâ”€â”€ __init__.py      â† Indique que `utils` est un package\nâ”‚   â””â”€â”€ math_utils.py     â† Module\nâ”œâ”€â”€ processors/           â† Package pour le traitement\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â””â”€â”€ text_cleaner.py   â† Module\nâ””â”€â”€ main.py               â† Point dâ€™entrÃ©e du projet\n\n\n\nContenu du module : utils/math_utils.py\ndef somme(a, b):\n    return a + b\n\n\n\nContenu dâ€™un autre module : processors/text_cleaner.py\ndef nettoyer_texte(text: str) -&gt; str:\n    return text.strip().lower()\n\n\n\nImporter dans main.py\nfrom utils.math_utils import somme\nfrom processors.text_cleaner import nettoyer_texte\n\nprint(somme(2, 3))\nprint(nettoyer_texte(\"  Hello WORLD  \"))",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#import-absolu-vs-import-relatif",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Import absolu vs import relatif",
    "text": "Import absolu vs import relatif\n\nImport absolu (recommandÃ©)\nfrom utils.math_utils import somme\nâ¡ï¸ Le plus lisible, idÃ©al pour projets pro / Data Engineering.\n\n\nImport relatif (utile dans les packages)\nfrom .math_utils import somme\nâ¡ï¸ Courant dans les gros projets et dans les packages pip.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-cest-important-en-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Pourquoi câ€™est important en Data Engineering ?",
    "text": "Pourquoi câ€™est important en Data Engineering ?\n\nCrÃ©er des modules = rendre ton code rÃ©utilisable (API, pipelines, notebooks)\nStructurer ton projet = Ã©viter le â€œscript spaghettiâ€\nFaciliter les tests unitaires\nFaciliter la maintenance dâ€™un pipeline data\nRÃ©duire les erreurs de duplication de logique",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-solutions",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & solutions",
    "text": "âš ï¸ Erreurs frÃ©quentes & solutions\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nModuleNotFoundError\nexÃ©cuter Python depuis le mauvais dossier\nToujours lancer Python depuis la racine du projet\n\n\nImport relatif impossible\nabsence du fichier __init__.py\nAjouter __init__.py dans le dossier\n\n\nModules dupliquÃ©s\nfichiers ayant le mÃªme nom dans deux dossiers\nRenommer ou structurer les packages\n\n\nimport *\nimports imprÃ©cis\nToujours importer explicitement\n\n\n\n\n\nAstuce pro : vÃ©rifier ton PYTHONPATH\nimport sys\nprint(sys.path)\nCela indique oÃ¹ Python cherche les modules.\n\n\n\nRappel essentiel\n\nImporter = RÃ©utiliser.\nStructurer = Devenir un vrai Data Engineer.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-de-fichiers-compÃ©tence-essentielle-du-data-engineer",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "8. Manipulation de fichiers â€” CompÃ©tence essentielle du Data Engineer",
    "text": "8. Manipulation de fichiers â€” CompÃ©tence essentielle du Data Engineer\nDans un pipeline Data, on manipule en permanence des fichiers :\n\nfichiers texte (logs, outputs)\nfichiers CSV (exports mÃ©tier, ingestion)\nfichiers JSON (APIs, NoSQL, Ã©vÃ©nements Kafka)\ndossiers qui contiennent les donnÃ©es\n\nPython fournit des outils natifs trÃ¨s puissants pour cela :\n\npathlib.Path â†’ gÃ©rer les chemins et dossiers\n\nopen() â†’ lire/Ã©crire des fichiers texte\n\njson â†’ sÃ©rialiser/dÃ©sÃ©rialiser des donnÃ©es JSON\n\n\n\nInitialisation du dossier data/\nfrom pathlib import Path\nimport json\n\n# CrÃ©ation du dossier de travail\nDATA_DIR = Path(\"data\")\nDATA_DIR.mkdir(exist_ok=True)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-texte-logs-outputs",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "8.1 Manipulation dâ€™un fichier texte (logs, outputsâ€¦)",
    "text": "8.1 Manipulation dâ€™un fichier texte (logs, outputsâ€¦)\ntexte_path = DATA_DIR / \"exemple.txt\"\n\n# Ã‰criture\nwith open(texte_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"Bonjour Data Engineer\\n\")\n\n# Lecture\nwith open(texte_path, \"r\", encoding=\"utf-8\") as f:\n    contenu = f.read()\n\nprint(\"Contenu du fichier :\", contenu)\nBonnes pratiques :\n\nToujours utiliser encoding=\"utf-8\"\n\nToujours utiliser with ... (fermeture automatique du fichier)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#manipulation-dun-fichier-json",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "8.2 Manipulation dâ€™un fichier JSON",
    "text": "8.2 Manipulation dâ€™un fichier JSON\nFormat le plus utilisÃ© dans : - APIs REST - MongoDB - Events Kafka / Kinesis - Configurations de job\njson_path = DATA_DIR / \"utilisateur.json\"\nutilisateur = {\"nom\": \"Alice\", \"age\": 30}\n\n# Ã‰criture JSON\nwith open(json_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(utilisateur, f, ensure_ascii=False, indent=2)\n\n# Lecture JSON\nwith open(json_path, \"r\", encoding=\"utf-8\") as f:\n    utilisateur_charge = json.load(f)\n\nprint(\"Utilisateur chargÃ© :\", utilisateur_charge)\nğŸ’¡ ensure_ascii=False permet dâ€™Ã©crire proprement les accents.\nğŸ’¡ indent=2 rend le JSON lisible (log, debug, auditsâ€¦).",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#optionnel-manipulation-dun-csv-avec-pandas",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "8.3 (Optionnel) Manipulation dâ€™un CSV avec Pandas",
    "text": "8.3 (Optionnel) Manipulation dâ€™un CSV avec Pandas\nUtilisÃ© en ingestion de donnÃ©es. On en reparlera plus au module 5.\nimport pandas as pd\n\ncsv_path = DATA_DIR / \"exemple.csv\"\n\ndf = pd.DataFrame({\n    \"nom\": [\"Alice\", \"Bob\"],\n    \"age\": [30, 25]\n})\n\ndf.to_csv(csv_path, index=False)\n\ndf_loaded = pd.read_csv(csv_path)\nprint(df_loaded)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\nFileNotFoundError\nMauvais chemin\nToujours utiliser Path() et vÃ©rifier path.exists()\n\n\nAccents cassÃ©s (Ã©, Ã â€¦)\nMauvais encoding\nToujours encoding=\"utf-8\"\n\n\nFichier non fermÃ©\nopen() sans contexte\nToujours utiliser with open(...)\n\n\nJSON mal formÃ©\nÃ©crit Ã  la main\nToujours utiliser json.dump / json.load\n\n\nChemins relatifs non fiables\nmauvais dossier dâ€™exÃ©cution\nUtiliser Path(__file__).resolve().parent dans un projet rÃ©el",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#points-clÃ©s-Ã -retenir-1",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Points clÃ©s Ã  retenir",
    "text": "Points clÃ©s Ã  retenir\n\npathlib.Path simplifie la gestion des chemins.\n\nwith open(...) est obligatoire pour Ã©viter les fuites de fichier.\n\nJSON = format standard du Data Engineering (MongoDB, API, logsâ€¦).\n\nToujours contrÃ´ler lâ€™encoding lors de la lecture/Ã©criture.\n\nLe dossier data/ centralise vos fichiers dans un projet propre.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#packages-et-pip-gÃ©rer-les-dÃ©pendances-comme-un-data-engineer",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "9. Packages et pip â€” GÃ©rer les dÃ©pendances comme un Data Engineer",
    "text": "9. Packages et pip â€” GÃ©rer les dÃ©pendances comme un Data Engineer\nPython devient puissant grÃ¢ce Ã  ses packages externes :\nPandas, Requests, SQLAlchemy, PyMongo, Polars, FastAPI, etc.\nPour installer ces packages, on utilise pip, le gestionnaire officiel de Python.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-un-package-avec-pip",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Installer un package avec pip",
    "text": "Installer un package avec pip\nExemple : installer requests pour faire des appels HTTP (API REST).\npython -m pip install requests\nğŸ‘‰ Pourquoi python -m pip et pas juste pip install ?\nParce que cela garantit quâ€™on utilise le pip liÃ© Ã  la bonne version de Python, surtout si plusieurs versions sont installÃ©es.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#utilisation-dans-un-script",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Utilisation dans un script",
    "text": "Utilisation dans un script\nimport requests\n\nresponse = requests.get(\"https://api.github.com\")\nprint(response.status_code)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#installer-plusieurs-packages-requirements.txt",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Installer plusieurs packages â€” requirements.txt",
    "text": "Installer plusieurs packages â€” requirements.txt\nDans un vrai projet Data Engineering, on liste les dÃ©pendances dans un fichier :\nrequests\npandas\nsqlalchemy\npymongo\nPuis on installe tout dâ€™un coup :\npython -m pip install -r requirements.txt",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#mettre-Ã -jour-un-package",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Mettre Ã  jour un package",
    "text": "Mettre Ã  jour un package\npython -m pip install --upgrade requests",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#dÃ©sinstaller-un-package",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "DÃ©sinstaller un package",
    "text": "DÃ©sinstaller un package\npython -m pip uninstall requests",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#oÃ¹-sont-installÃ©s-les-packages",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "OÃ¹ sont installÃ©s les packages ?",
    "text": "OÃ¹ sont installÃ©s les packages ?\npython -m pip show requests\nDonne : version, emplacement, dÃ©pendances, etc.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques",
    "text": "âš ï¸ Erreurs frÃ©quentes & bonnes pratiques\n\n\n\n\n\n\n\n\nâŒ ProblÃ¨me\nğŸ’¡ Cause\nâœ… Solution\n\n\n\n\npip: command not found\nPython mal installÃ©\nUtiliser python -m pip\n\n\nInstaller dans le mauvais Python\nPlusieurs versions installÃ©es\nToujours python -m pip install\n\n\nVersion incompatible\nConflit de dÃ©pendances\nSpÃ©cifier une version : requests==2.31.0\n\n\nInstaller globalement\nRisque de casser le systÃ¨me\nUtiliser un venv (python -m venv .venv)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-figer-les-versions-pour-la-production",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce Pro : figer les versions pour la production",
    "text": "â­ Astuce Pro : figer les versions pour la production\npython -m pip freeze &gt; requirements.txt\nâ¡ï¸ Cela capture exactement les versions utilisÃ©es dans ton environnement.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#gÃ©nÃ©rateurs-yield-traiter-de-gros-volumes-sans-saturer-la-mÃ©moire",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire",
    "text": "GÃ©nÃ©rateurs (yield) â€” Traiter de gros volumes sans saturer la mÃ©moire\nLes gÃ©nÃ©rateurs produisent des valeurs une par une, sans tout charger en mÃ©moire. Câ€™est essentiel en Data Engineering pour traiter des fichiers volumineux ou des flux de donnÃ©es.\n\n\nProblÃ¨me : charger tout en mÃ©moire\n# âŒ Charge TOUT le fichier en mÃ©moire\ndef lire_fichier_complet(chemin):\n    with open(chemin) as f:\n        return f.readlines()  # Liste de TOUTES les lignes\n\n# Si le fichier fait 10 Go... ğŸ’¥ MemoryError\nlignes = lire_fichier_complet(\"huge_file.csv\")\n\n\n\nâœ… Solution : gÃ©nÃ©rateur avec yield\n# âœ… Lit UNE ligne Ã  la fois\ndef lire_fichier_ligne_par_ligne(chemin):\n    with open(chemin) as f:\n        for ligne in f:\n            yield ligne.strip()  # Retourne et \"pause\"\n\n# Utilisation : ne charge qu'une ligne Ã  la fois\nfor ligne in lire_fichier_ligne_par_ligne(\"huge_file.csv\"):\n    process(ligne)  # Traite ligne par ligne\n\n\n\nDiffÃ©rence return vs yield\n\n\n\n\n\n\n\nreturn\nyield\n\n\n\n\nRetourne une valeur et termine\nRetourne une valeur et pause\n\n\nFonction normale\nGÃ©nÃ©rateur\n\n\nTout en mÃ©moire\nUne valeur Ã  la fois\n\n\n\n# Fonction normale\ndef carres_liste(n):\n    result = []\n    for i in range(n):\n        result.append(i ** 2)\n    return result  # Retourne toute la liste\n\n# GÃ©nÃ©rateur\ndef carres_generateur(n):\n    for i in range(n):\n        yield i ** 2  # Retourne un par un\n\n# Test mÃ©moire\nimport sys\nliste = carres_liste(1000000)\ngen = carres_generateur(1000000)\n\nprint(sys.getsizeof(liste))  # ~8 Mo\nprint(sys.getsizeof(gen))    # ~200 octets !\n\n\n\nCas dâ€™usage Data Engineering\n\nLire un CSV volumineux\ndef lire_csv_en_chunks(chemin, chunk_size=1000):\n    \"\"\"Lit un CSV par lots de chunk_size lignes.\"\"\"\n    with open(chemin) as f:\n        header = next(f).strip().split(\",\")\n        chunk = []\n        \n        for ligne in f:\n            valeurs = ligne.strip().split(\",\")\n            row = dict(zip(header, valeurs))\n            chunk.append(row)\n            \n            if len(chunk) &gt;= chunk_size:\n                yield chunk\n                chunk = []\n        \n        if chunk:  # Dernier lot\n            yield chunk\n\n# Utilisation\nfor batch in lire_csv_en_chunks(\"users.csv\", chunk_size=500):\n    print(f\"Traitement de {len(batch)} lignes...\")\n    # insert_to_database(batch)\n\n\nPipeline de transformation\ndef extraire(source):\n    for item in source:\n        yield item\n\ndef transformer(items):\n    for item in items:\n        item[\"nom\"] = item[\"nom\"].upper()\n        yield item\n\ndef filtrer(items, condition):\n    for item in items:\n        if condition(item):\n            yield item\n\n# Pipeline chaÃ®nÃ© â€” Ã©valuation paresseuse (lazy)\nsource = [{\"nom\": \"alice\"}, {\"nom\": \"bob\"}, {\"nom\": \"charlie\"}]\npipeline = filtrer(\n    transformer(extraire(source)),\n    lambda x: len(x[\"nom\"]) &gt; 3\n)\n\nfor item in pipeline:\n    print(item)  # {'nom': 'ALICE'}, {'nom': 'CHARLIE'}\n\n\n\n\nGenerator Expressions (syntaxe courte)\n# List comprehension â€” crÃ©e une liste en mÃ©moire\ncarres_liste = [x**2 for x in range(1000000)]\n\n# Generator expression â€” crÃ©e un gÃ©nÃ©rateur\ncarres_gen = (x**2 for x in range(1000000))  # ParenthÃ¨ses !\n\n# Utile pour les agrÃ©gations\ntotal = sum(x**2 for x in range(1000000))  # Efficient",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-python-.py",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "CrÃ©er et exÃ©cuter un script Python (.py)",
    "text": "CrÃ©er et exÃ©cuter un script Python (.py)\nJusquâ€™ici, tu as travaillÃ© dans un Notebook Jupyter. Mais en Data Engineering, on Ã©crit surtout des scripts .py qui sâ€™exÃ©cutent depuis le terminal ou sont orchestrÃ©s par Airflow, cron, etc.\n\n\nNotebook vs Script â€” Quand utiliser quoi ?\n\n\n\nCritÃ¨re\nNotebook (.ipynb)\nScript (.py)\n\n\n\n\nExploration\nâœ… IdÃ©al\nâŒ Pas adaptÃ©\n\n\nVisualisation\nâœ… Graphiques inline\nâš ï¸ Doit sauvegarder\n\n\nProduction\nâŒ Difficile Ã  orchestrer\nâœ… Standard\n\n\nTests unitaires\nâŒ CompliquÃ©\nâœ… Facile avec pytest\n\n\nGit / Code review\nâš ï¸ Diffs illisibles\nâœ… Diffs propres\n\n\nCI/CD\nâŒ Pas adaptÃ©\nâœ… Standard\n\n\nAirflow / Orchestration\nâš ï¸ Possible mais pas idÃ©al\nâœ… Natif\n\n\n\n\nğŸ’¡ RÃ¨gle : Notebook pour explorer, Script pour produire.\n\n\n\n\nCrÃ©er ton premier script\n\nOuvre VS Code\nCrÃ©e un nouveau fichier : File &gt; New File\nSauvegarde-le avec lâ€™extension .py : mon_script.py\n\n\nExemple : hello.py\n# hello.py\nprint(\"Hello, Data Engineer!\")\n\n\n\n\nâ–¶ï¸ ExÃ©cuter depuis le terminal\n# Se placer dans le dossier du script\ncd /chemin/vers/mon/projet\n\n# ExÃ©cuter le script\npython hello.py\n# ou\npython3 hello.py\nRÃ©sultat :\nHello, Data Engineer!\n\n\n\nStructure standard : if __name__ == \"__main__\"\nCâ€™est LA structure que tu verras dans tous les scripts Python professionnels.\n# etl_simple.py\n\ndef extract():\n    \"\"\"Extrait les donnÃ©es.\"\"\"\n    print(\" Extraction...\")\n    return [{\"id\": 1, \"nom\": \"Alice\"}, {\"id\": 2, \"nom\": \"Bob\"}]\n\ndef transform(data):\n    \"\"\"Transforme les donnÃ©es.\"\"\"\n    print(\" Transformation...\")\n    return [{**d, \"nom\": d[\"nom\"].upper()} for d in data]\n\ndef load(data):\n    \"\"\"Charge les donnÃ©es.\"\"\"\n    print(\" Chargement...\")\n    for row in data:\n        print(f\"  InsÃ©rÃ© : {row}\")\n\ndef main():\n    \"\"\"Point d'entrÃ©e principal.\"\"\"\n    print(\" DÃ©marrage du pipeline ETL\")\n    \n    data = extract()\n    data = transform(data)\n    load(data)\n    \n    print(\" Pipeline terminÃ© !\")\n\n# Ce bloc s'exÃ©cute SEULEMENT si le script est lancÃ© directement\nif __name__ == \"__main__\":\n    main()\n\nPourquoi if __name__ == \"__main__\" ?\n\n\n\nSituation\n__name__ vaut\nLe bloc sâ€™exÃ©cute ?\n\n\n\n\npython etl_simple.py\n\"__main__\"\nâœ… Oui\n\n\nimport etl_simple\n\"etl_simple\"\nâŒ Non\n\n\n\nCela permet de : - RÃ©utiliser les fonctions dans dâ€™autres scripts (import etl_simple) - ExÃ©cuter le script directement (python etl_simple.py)\n\n\n\n\nPasser des arguments au script\n\nMÃ©thode simple : sys.argv\n# script_args.py\nimport sys\n\ndef main():\n    print(f\"Arguments reÃ§us : {sys.argv}\")\n    \n    # sys.argv[0] = nom du script\n    # sys.argv[1], sys.argv[2], ... = arguments\n    \n    if len(sys.argv) &lt; 2:\n        print(\"Usage : python script_args.py &lt;fichier&gt;\")\n        sys.exit(1)\n    \n    fichier = sys.argv[1]\n    print(f\"Traitement de : {fichier}\")\n\nif __name__ == \"__main__\":\n    main()\npython script_args.py data.csv\n# Arguments reÃ§us : ['script_args.py', 'data.csv']\n# Traitement de : data.csv\n\n\nMÃ©thode pro : argparse\n# etl_avec_args.py\nimport argparse\n\ndef main():\n    # CrÃ©er le parser\n    parser = argparse.ArgumentParser(\n        description=\"Pipeline ETL simple\"\n    )\n    \n    # DÃ©finir les arguments\n    parser.add_argument(\n        \"--input\", \"-i\",\n        required=True,\n        help=\"Fichier d'entrÃ©e (CSV)\"\n    )\n    parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.csv\",\n        help=\"Fichier de sortie (dÃ©faut: output.csv)\"\n    )\n    parser.add_argument(\n        \"--verbose\", \"-v\",\n        action=\"store_true\",\n        help=\"Mode verbeux\"\n    )\n    \n    # Parser les arguments\n    args = parser.parse_args()\n    \n    # Utiliser les arguments\n    print(f\"Input  : {args.input}\")\n    print(f\"Output : {args.output}\")\n    print(f\"Verbose: {args.verbose}\")\n\nif __name__ == \"__main__\":\n    main()\n# Afficher l'aide\npython etl_avec_args.py --help\n\n# ExÃ©cuter avec arguments\npython etl_avec_args.py --input data.csv --output result.csv --verbose\npython etl_avec_args.py -i data.csv -o result.csv -v\n\n\n\n\nStructure dâ€™un projet Data Engineering\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â”œâ”€â”€ transform.py\nâ”‚   â””â”€â”€ load.py\nâ”œâ”€â”€ scripts/\nâ”‚   â””â”€â”€ run_etl.py      â† Point d'entrÃ©e\nâ”œâ”€â”€ tests/\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ data/               â† âš ï¸ Dans .gitignore\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ .gitignore\nâ””â”€â”€ README.md\n\n\n\nExercice pratique\nCrÃ©er un script compter_lignes.py qui : 1. Prend un fichier en argument 2. Compte le nombre de lignes 3. Affiche le rÃ©sultat\n\n\nğŸ’¡ Solution\n\n# compter_lignes.py\nimport argparse\nfrom pathlib import Path\n\ndef compter_lignes(chemin: str) -&gt; int:\n    \"\"\"Compte les lignes d'un fichier.\"\"\"\n    path = Path(chemin)\n    if not path.exists():\n        raise FileNotFoundError(f\"Fichier non trouvÃ© : {chemin}\")\n    \n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        return sum(1 for _ in f)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Compte les lignes d'un fichier\")\n    parser.add_argument(\"fichier\", help=\"Chemin du fichier Ã  analyser\")\n    args = parser.parse_args()\n    \n    try:\n        nb_lignes = compter_lignes(args.fichier)\n        print(f\"Le fichier '{args.fichier}' contient {nb_lignes} lignes.\")\n    except FileNotFoundError as e:\n        print(f\"Erreur : {e}\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()\npython compter_lignes.py mon_fichier.csv",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#context-managers-gÃ©rer-les-ressources-proprement",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Context Managers â€” GÃ©rer les ressources proprement",
    "text": "Context Managers â€” GÃ©rer les ressources proprement\nTu as dÃ©jÃ  utilisÃ© with open(...) pour les fichiers. Câ€™est un context manager ! Ce pattern garantit que les ressources sont toujours libÃ©rÃ©es, mÃªme en cas dâ€™erreur.\n\n\nLe problÃ¨me sans context manager\n# âŒ Risque : fichier non fermÃ© si erreur\nf = open(\"data.txt\", \"r\")\ndata = f.read()\n# Si une erreur se produit ici...\nprocess(data)  # Le fichier reste ouvert !\nf.close()\n# âœ… Avec context manager : fermeture garantie\nwith open(\"data.txt\", \"r\") as f:\n    data = f.read()\n    process(data)\n# Fichier automatiquement fermÃ©, mÃªme en cas d'erreur\n\n\n\nCas dâ€™usage Data Engineering\n\n\n\nRessource\nPourquoi un context manager\n\n\n\n\nFichiers\nFermer aprÃ¨s lecture/Ã©criture\n\n\nConnexions DB\nLibÃ©rer la connexion\n\n\nTransactions\nCommit ou rollback\n\n\nFichiers temporaires\nSupprimer aprÃ¨s usage\n\n\nVerrous (locks)\nLibÃ©rer le verrou\n\n\n\n\n\n\nCrÃ©er son propre context manager\n\nMÃ©thode 1 : avec une classe\nclass DatabaseConnection:\n    def __init__(self, host: str, database: str):\n        self.host = host\n        self.database = database\n        self.connection = None\n    \n    def __enter__(self):\n        \"\"\"AppelÃ© au dÃ©but du bloc 'with'.\"\"\"\n        print(f\"ğŸ”Œ Connexion Ã  {self.host}/{self.database}...\")\n        self.connection = f\"Connection to {self.database}\"  # Simule une connexion\n        return self.connection\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"AppelÃ© Ã  la fin du bloc 'with' (mÃªme si erreur).\"\"\"\n        print(f\"ğŸ”Œ Fermeture de la connexion...\")\n        self.connection = None\n        # Retourner False pour propager les exceptions\n        return False\n\n# Utilisation\nwith DatabaseConnection(\"localhost\", \"warehouse\") as conn:\n    print(f\"Connexion active : {conn}\")\n    # Faire des requÃªtes...\n# Connexion automatiquement fermÃ©e ici\n\n\nMÃ©thode 2 : avec @contextmanager (plus simple)\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(nom: str):\n    \"\"\"Mesure le temps d'exÃ©cution d'un bloc.\"\"\"\n    import time\n    start = time.time()\n    print(f\" DÃ©but : {nom}\")\n    \n    yield  # Le bloc 'with' s'exÃ©cute ici\n    \n    elapsed = time.time() - start\n    print(f\" Fin : {nom} ({elapsed:.2f}s)\")\n\n# Utilisation\nwith timer(\"Traitement ETL\"):\n    # Simuler un traitement long\n    import time\n    time.sleep(1)\n    print(\"Traitement en cours...\")\n\n# Output:\n# DÃ©but : Traitement ETL\n# Traitement en cours...\n# Fin : Traitement ETL (1.00s)\n\n\n\n\nExemple complet : Transaction DB\nfrom contextlib import contextmanager\n\n@contextmanager\ndef transaction(connection):\n    \"\"\"GÃ¨re une transaction avec commit/rollback automatique.\"\"\"\n    try:\n        print(\" DÃ©but de la transaction\")\n        yield connection\n        print(\" COMMIT\")\n        # connection.commit()\n    except Exception as e:\n        print(f\" ROLLBACK : {e}\")\n        # connection.rollback()\n        raise\n\n# Utilisation\nwith transaction(\"ma_connexion\") as conn:\n    print(\"Insertion de donnÃ©es...\")\n    # Si une erreur ici â†’ rollback automatique\n\n\n\nğŸ’¡ Context managers utiles de la stdlib\nfrom contextlib import suppress, redirect_stdout\nimport tempfile\n\n# Ignorer silencieusement une erreur\nwith suppress(FileNotFoundError):\n    os.remove(\"fichier_peut_etre_absent.txt\")\n\n# Fichier temporaire (supprimÃ© automatiquement)\nwith tempfile.NamedTemporaryFile(mode=\"w\", delete=True) as tmp:\n    tmp.write(\"donnÃ©es temporaires\")\n    print(f\"Fichier temp : {tmp.name}\")\n# Fichier supprimÃ© ici",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#logging-traÃ§abilitÃ©-indispensable-en-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "10. Logging â€” TraÃ§abilitÃ© indispensable en Data Engineering",
    "text": "10. Logging â€” TraÃ§abilitÃ© indispensable en Data Engineering\nDans un vrai pipeline Data (ingestion, nettoyage, transformationâ€¦), on doit suivre ce quâ€™il se passe :\n\nFichier introuvable ?\nAPI trop lente ?\nFormat JSON invalide ?\nDonnÃ©es anormales ?\nETL en retard ?\n\nâ¡ï¸ print() ne suffit pas, car il ne permet ni filtrage, ni niveaux, ni logs dans un fichier.\nâ¡ï¸ Le module logging est le standard utilisÃ© en entreprise.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#pourquoi-utiliser-logging",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "10.1 Pourquoi utiliser logging ?",
    "text": "10.1 Pourquoi utiliser logging ?\n\n\n\n\n\n\n\nAvantage\nDescription\n\n\n\n\nNiveaux de log\nDEBUG, INFO, WARNING, ERROR, CRITICAL\n\n\nFiltrage des messages\nOn peut afficher seulement WARNING+\n\n\nLogs dans un fichier\nIndispensable en production\n\n\nStandard Python\nCompatible Airflow, FastAPI, ETL, microservices\n\n\nThread-safe\nFonctionne mÃªme avec du multithreading",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#configuration-minimale-recommandÃ©e",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "ğŸ”§ Configuration minimale recommandÃ©e",
    "text": "ğŸ”§ Configuration minimale recommandÃ©e\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n)\nExplications : - level=INFO â†’ DEBUG est ignorÃ© - %(asctime)s â†’ timestamp (important dans les pipelines) - %(levelname)s â†’ niveau (INFO, ERRORâ€¦) - %(message)s â†’ contenu du message",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-dutilisation",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Exemple dâ€™utilisation",
    "text": "Exemple dâ€™utilisation\nlogging.debug(\"Message DEBUG (non affichÃ© en mode INFO)\")\nlogging.info(\"DÃ©marrage du mini-script\")\nlogging.warning(\"Attention : donnÃ©es manquantes\")\nlogging.error(\"Erreur : Ã©chec de connexion API\")\nlogging.critical(\"CRITIQUE : le pipeline doit s'arrÃªter !\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã©crire-les-logs-dans-un-fichier-cas-rÃ©el-data-engineering",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)",
    "text": "10.2 Ã‰crire les logs dans un fichier (cas rÃ©el Data Engineering)\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    filename=\"pipeline.log\",\n    filemode=\"a\",  # append au lieu de rÃ©Ã©crire\n)\nâ¡ï¸ TrÃ¨s utilisÃ© dans : - scripts Airflow - traitements batch - pipelines de production",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exemple-typique-dans-une-fonction",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "10.3 Exemple typique dans une fonction",
    "text": "10.3 Exemple typique dans une fonction\ndef charger_json(chemin: str) -&gt; dict | None:\n    logging.info(f\"Chargement du fichier : {chemin}\")\n    try:\n        with open(chemin, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except FileNotFoundError:\n        logging.error(f\"Fichier introuvable : {chemin}\")\n    except json.JSONDecodeError:\n        logging.error(\"JSON invalide\")\n    return None",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#erreurs-frÃ©quentes-comment-les-Ã©viter-1",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter",
    "text": "âš ï¸ Erreurs frÃ©quentes & comment les Ã©viter\n\n\n\n\n\n\n\n\nâŒ Mauvaise pratique\nğŸ’¡ Pourquoi\nâœ… Bonne pratique\n\n\n\n\nUtiliser print() partout\nimpossible Ã  filtrer/logguer\nUtiliser logging.info()\n\n\nAppeler logging.basicConfig() plusieurs fois\nne fonctionne que la 1Ã¨re fois\nConfigurer un seul logger global\n\n\nLogger des donnÃ©es sensibles\nfuite de secrets/mots de passe\nFiltrer/masquer les champs sensibles\n\n\nLogs trop verbeux\nralentissent les pipelines\nUtiliser DEBUG seulement en dev\n\n\nLogs insuffisants\ndifficile de diagnostiquer\nLogger les erreurs + contexte",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#astuce-pro-logger-dans-la-console-et-dans-un-fichier",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "â­ Astuce pro : Logger dans la console et dans un fichier",
    "text": "â­ Astuce pro : Logger dans la console et dans un fichier\nlogger = logging.getLogger(\"pipeline\")\nlogger.setLevel(logging.INFO)\n\n# Handler console\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\n\n# Handler fichier\nfile = logging.FileHandler(\"pipeline.log\")\nfile.setLevel(logging.INFO)\n\n# Format\nfmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\nconsole.setFormatter(fmt)\nfile.setFormatter(fmt)\n\n# Ajouter handlers\nlogger.addHandler(console)\nlogger.addHandler(file)\n\nlogger.info(\"Pipeline dÃ©marrÃ©\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#Ã -retenir-1",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Ã€ retenir",
    "text": "Ã€ retenir\n\nlogging = indispensable en Data Engineering\n\nToujours configurer : niveau + format\n\nUtiliser un fichier log en production\n\nJamais de print() dans un vrai pipeline\n\nBien choisir le niveau (INFO, WARNING, ERRORâ€¦)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#exercices-pratiques",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "11. Exercices pratiques",
    "text": "11. Exercices pratiques\nEssaie de rÃ©soudre ces exercices avant dâ€™afficher les corrections.\n\nExercice 1 â€“ Validation dâ€™Ã¢ge\nÃ‰crire une fonction est_majeur(age: int) -&gt; bool qui : - retourne True si age est supÃ©rieur ou Ã©gal Ã  18 ; - sinon retourne False.\n# Ã€ toi de jouer\ndef est_majeur(age: int) -&gt; bool:\n    # TODO: complÃ©ter\n    pass\n\nprint(est_majeur(15))  # attendu: False\nprint(est_majeur(18))  # attendu: True\n\n\nAfficher une solution possible\n\ndef est_majeur(age: int) -&gt; bool:\n    return age &gt;= 18\n\nprint(est_majeur(15))  # False\nprint(est_majeur(18))  # True\n\n\n\nExercice 2 â€“ Compter les posts par utilisateur\nOn dispose dâ€™une liste de dictionnaires reprÃ©sentant des posts :\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\nÃ‰crire une fonction compter_posts_par_utilisateur(posts) qui retourne :\n{\"alice\": 2, \"bob\": 1}\n# Ã€ toi de jouer\nposts = [\n    {\"user\": \"alice\", \"text\": \"Hello\"},\n    {\"user\": \"bob\", \"text\": \"Salut\"},\n    {\"user\": \"alice\", \"text\": \"Rebonjour\"},\n]\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    # TODO: complÃ©ter\n    result = {}\n    return result\n\nprint(compter_posts_par_utilisateur(posts))\n\n\nAfficher une solution possible\n\ndef compter_posts_par_utilisateur(posts: list[dict]) -&gt; dict:\n    result = {}\n    for p in posts:\n        user = p[\"user\"]\n        if user not in result:\n            result[user] = 0\n        result[user] += 1\n    return result\n\nprint(compter_posts_par_utilisateur(posts))  # {'alice': 2, 'bob': 1}\n\n\n\nExercice 3 â€“ Classe Post\nCrÃ©er une classe Post avec : - attributs : auteur (str), texte (str) ; - mÃ©thode longueur() qui retourne la longueur du texte.\n# Ã€ toi de jouer\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        # TODO: stocker les attributs\n        pass\n\n    def longueur(self) -&gt; int:\n        # TODO: retourner la longueur du texte\n        return 0\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())  # attendu: longueur de la phrase\n\n\nAfficher une solution possible\n\nclass Post:\n    def __init__(self, auteur: str, texte: str) -&gt; None:\n        self.auteur = auteur\n        self.texte = texte\n\n    def longueur(self) -&gt; int:\n        return len(self.texte)\n\np = Post(\"alice\", \"Bonjour tout le monde\")\nprint(p.longueur())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#introduction-aux-tests-avec-pytest",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "Introduction aux tests avec pytest",
    "text": "Introduction aux tests avec pytest\nTester son code est essentiel en Data Engineering. Un bug dans un pipeline peut corrompre des millions de lignes de donnÃ©es ! pytest est le framework de test standard en Python.\n\n\nInstallation\npip install pytest\n\n\n\nPremier test avec assert\nassert vÃ©rifie quâ€™une condition est vraie. Si elle est fausse â†’ erreur.\n# test_basics.py\n\ndef addition(a, b):\n    return a + b\n\ndef test_addition():\n    \"\"\"Test simple avec assert.\"\"\"\n    assert addition(2, 3) == 5\n    assert addition(0, 0) == 0\n    assert addition(-1, 1) == 0\n\ndef test_addition_floats():\n    \"\"\"Test avec des floats.\"\"\"\n    result = addition(0.1, 0.2)\n    assert abs(result - 0.3) &lt; 0.0001  # Comparaison floats\n# ExÃ©cuter les tests\npytest test_basics.py\n\n# Avec plus de dÃ©tails\npytest test_basics.py -v\n\n\n\nTester des fonctions Data Engineering\n# src/transform.py\ndef nettoyer_email(email: str) -&gt; str:\n    \"\"\"Nettoie un email : minuscules, strip.\"\"\"\n    if not email or \"@\" not in email:\n        raise ValueError(\"Email invalide\")\n    return email.strip().lower()\n\ndef filtrer_actifs(users: list[dict]) -&gt; list[dict]:\n    \"\"\"Garde uniquement les users actifs.\"\"\"\n    return [u for u in users if u.get(\"actif\", False)]\n# tests/test_transform.py\nimport pytest\nfrom src.transform import nettoyer_email, filtrer_actifs\n\nclass TestNettoyerEmail:\n    \"\"\"Tests pour nettoyer_email.\"\"\"\n    \n    def test_email_valide(self):\n        assert nettoyer_email(\"Alice@Test.COM\") == \"alice@test.com\"\n    \n    def test_email_avec_espaces(self):\n        assert nettoyer_email(\"  bob@test.com  \") == \"bob@test.com\"\n    \n    def test_email_invalide_sans_arobase(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"invalid_email\")\n    \n    def test_email_vide(self):\n        with pytest.raises(ValueError):\n            nettoyer_email(\"\")\n\n\nclass TestFiltrerActifs:\n    \"\"\"Tests pour filtrer_actifs.\"\"\"\n    \n    def test_filtre_users_actifs(self):\n        users = [\n            {\"nom\": \"Alice\", \"actif\": True},\n            {\"nom\": \"Bob\", \"actif\": False},\n            {\"nom\": \"Charlie\", \"actif\": True}\n        ]\n        result = filtrer_actifs(users)\n        assert len(result) == 2\n        assert all(u[\"actif\"] for u in result)\n    \n    def test_liste_vide(self):\n        assert filtrer_actifs([]) == []\n    \n    def test_tous_inactifs(self):\n        users = [{\"nom\": \"X\", \"actif\": False}]\n        assert filtrer_actifs(users) == []\n\n\n\nExÃ©cuter les tests\n# Tous les tests du dossier\npytest\n\n# Un fichier spÃ©cifique\npytest tests/test_transform.py\n\n# Une classe spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail\n\n# Un test spÃ©cifique\npytest tests/test_transform.py::TestNettoyerEmail::test_email_valide\n\n# Avec couverture de code\npip install pytest-cov\npytest --cov=src\n\n\n\nStructure recommandÃ©e\nmon_projet/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ extract.py\nâ”‚   â””â”€â”€ transform.py\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ test_extract.py\nâ”‚   â””â”€â”€ test_transform.py\nâ”œâ”€â”€ pytest.ini          # Configuration pytest (optionnel)\nâ””â”€â”€ requirements.txt\n\n\n\nğŸ’¡ Bonnes pratiques\n\n\n\nPratique\nExplication\n\n\n\n\nNommer test_*.py\npytest les dÃ©tecte automatiquement\n\n\nUn assert par test\nPlus facile Ã  dÃ©buguer\n\n\nTester les cas limites\nListes vides, None, valeurs nÃ©gatives\n\n\nTester les erreurs\npytest.raises(Exception)\n\n\nExÃ©cuter avant chaque commit\nÃ‰vite les rÃ©gressions",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#quiz-final",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "12. Quiz final",
    "text": "12. Quiz final\nTeste tes connaissances ! RÃ©ponds mentalement puis vÃ©rifie les rÃ©ponses.\n\n\nâ“ Q1. Quel type correspond Ã  une chaÃ®ne de caractÃ¨res ?\n\nstr\n\ntext\n\nchar\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” str est le type pour les chaÃ®nes de caractÃ¨res.\n\n\n\n\nâ“ Q2. Quelle structure est la plus adaptÃ©e pour reprÃ©senter un objet JSON ?\n\nlist\n\ndict\n\ntuple\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” dict avec des paires clÃ©/valeur, comme JSON.\n\n\n\n\nâ“ Q3. Laquelle de ces boucles risque le plus de devenir infinie ?\n\nfor\n\nwhile\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” while continue tant que la condition est vraie.\n\n\n\n\nâ“ Q4. Quel mot-clÃ© permet de gÃ©rer une erreur ?\n\nerror\n\nexcept\n\ncatch\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” try/except pour gÃ©rer les exceptions.\n\n\n\n\nâ“ Q5. Quel module est utilisÃ© pour le logging ?\n\nlogs\n\nlogging\n\nlogger\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” import logging\n\n\n\n\nâ“ Q6. Comment crÃ©er un environnement virtuel avec venv ?\n\npython -m venv mon_env\n\npip create venv mon_env\n\npython --venv mon_env\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” python -m venv nom_environnement\n\n\n\n\nâ“ Q7. Quel fichier liste les dÃ©pendances dâ€™un projet Python ?\n\npackages.txt\n\nrequirements.txt\n\ndependencies.json\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” requirements.txt avec pip freeze",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸ“– Documentation officielle\n\nPython.org Documentation â€” RÃ©fÃ©rence complÃ¨te\nPython Tutorial â€” Tutoriel officiel\n\n\n\nğŸ“ Cours et tutoriels\n\nReal Python â€” Tutoriels de qualitÃ©\nPython for Data Engineering (DataCamp) â€” Cours interactifs\nAutomate the Boring Stuff â€” Livre gratuit\n\n\n\nğŸ› ï¸ Outils recommandÃ©s\n\nPyPI â€” Repository de packages Python\nRuff â€” Linter ultra-rapide\nBlack â€” Formateur de code\nmypy â€” VÃ©rification des types\n\n\n\nğŸ“Š Packages Data Engineering essentiels\n\n\n\nPackage\nUsage\n\n\n\n\npandas\nManipulation de donnÃ©es tabulaires\n\n\nnumpy\nCalcul numÃ©rique\n\n\nrequests\nAppels API HTTP\n\n\nsqlalchemy\nORM et connexions bases de donnÃ©es\n\n\npydantic\nValidation de donnÃ©es\n\n\npytest\nTests unitaires\n\n\nclick\nCLI (Command Line Interface)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/04_python_basics_for_data_engineers.html#prochaine-Ã©tape",
    "title": "Python â€“ Bases pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les bases de Python, passons au traitement de donnÃ©es !\nğŸ‘‰ Module suivant : 05_python_data_processing_for_data_engineers.ipynb â€” Pandas, Matplotlib, Seaborn et ETL\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Python Basics pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ Python",
      "04 Â· Python Fondamental"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html",
    "title": "Bash pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre les commandes Bash essentielles pour manipuler des fichiers, automatiser des tÃ¢ches, et interagir avec ton environnement systÃ¨me â€” des compÃ©tences indispensables pour un Data Engineer !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prÃ©requis",
    "title": "Bash pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 01_intro_data_engineering\n\n\nâœ… Requis\nAvoir accÃ¨s Ã  un terminal (Linux, Mac, ou Windows avec WSL/Git Bash)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#objectifs-du-module",
    "title": "Bash pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nNaviguer dans lâ€™arborescence de fichiers\nManipuler des fichiers et dossiers\nFiltrer et rechercher dans des donnÃ©es\nÃ‰crire des scripts Bash pour automatiser des tÃ¢ches\nPlanifier des jobs avec cron",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cest-quoi-le-langage-bash",
    "title": "Bash pour Data Engineers",
    "section": "Câ€™est quoi le langage Bash ?",
    "text": "Câ€™est quoi le langage Bash ?\nBash (abrÃ©viation de Bourne Again SHell) est un langage de commande et de script utilisÃ© dans la majoritÃ© des systÃ¨mes Unix/Linux (et mÃªme sous Windows via WSL ou Git Bash).\nIl te permet de :\n\nNaviguer dans les dossiers\n\nManipuler des fichiers et des donnÃ©es\n\nAutomatiser des tÃ¢ches rÃ©pÃ©titives\n\nÃ‰crire des scripts shell pour lancer des traitements de donnÃ©es\n\n\n\nPourquoi câ€™est utile pour un Data Engineer ?\n\n\n\n\n\n\n\nCas dâ€™usage\nExemple concret\n\n\n\n\nLancer des pipelines ETL\npython etl_pipeline.py && echo \"Success\" \\|\\| echo \"Failed\"\n\n\nÃ‰crire des jobs cron\nExtraction automatique de donnÃ©es chaque nuit Ã  2h\n\n\nManipuler des fichiers\nFusionner 100 fichiers CSV en un seul\n\n\nOrchestrer des outils\nLancer Docker, Spark, ou Airflow depuis un script\n\n\nAnalyser des logs\nTrouver toutes les erreurs dans les logs du jour\n\n\n\n\nğŸ’¡ En bref : le Bash est ton couteau suisse pour parler avec ton ordinateur et piloter lâ€™Ã©cosystÃ¨me data.\n\n\nâ„¹ï¸ Le savais-tu ?\nLe mot Bash signifie â€œBourne Again SHellâ€, un jeu de mots sur :\n\nLe shell Unix original : le Bourne Shell (sh), dÃ©veloppÃ© dans les annÃ©es 1970 par Stephen Bourne\nLâ€™expression anglaise â€œborn againâ€ = renaÃ®tre\n\nBash est donc une nouvelle version amÃ©liorÃ©e du shell Bourne, libre, puissante, et utilisÃ©e par dÃ©faut dans la plupart des systÃ¨mes Unix/Linux modernes.\nğŸ“– Biographie de Stephen R. Bourne sur Wikipedia",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#comment-accÃ©der-Ã -bash",
    "title": "Bash pour Data Engineers",
    "section": "Comment accÃ©der Ã  Bash ?",
    "text": "Comment accÃ©der Ã  Bash ?\n\n\n\n\n\n\n\nSystÃ¨me\nComment y accÃ©der\n\n\n\n\nğŸ§ Linux\nBash est installÃ© par dÃ©faut. Ouvre un Terminal\n\n\nğŸ macOS\nOuvre Terminal (Applications â†’ Utilitaires â†’ Terminal)\n\n\nğŸªŸ Windows\nInstalle WSL (Windows Subsystem for Linux) ou Git Bash\n\n\n\n\nInstallation de WSL sur Windows\n# Dans PowerShell en administrateur\nwsl --install\nAprÃ¨s redÃ©marrage, tu auras accÃ¨s Ã  un terminal Linux complet !\n\n\nVÃ©rifier ta version de Bash\nbash --version",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#navigation-exploration",
    "title": "Bash pour Data Engineers",
    "section": "1. Navigation & exploration",
    "text": "1. Navigation & exploration\nLes commandes de base pour se dÃ©placer dans lâ€™arborescence :\n\n\nVoir le code\n%%bash\n# Affiche le chemin du dossier courant (Print Working Directory)\npwd\n\n# Liste les fichiers du dossier courant\nls\n\n# Liste avec dÃ©tails (permissions, taille, date)\nls -lh\n\n# Liste incluant les fichiers cachÃ©s\nls -la\n\n# Affiche l'arborescence (si installÃ©)\n# tree\n\n# Change de dossier\ncd /tmp\npwd\n\n# Revenir au dossier prÃ©cÃ©dent\ncd -\n\n# Aller au dossier home\ncd ~\n\n\n\nRaccourcis de navigation essentiels\n\n\n\nSymbole\nSignification\nExemple\n\n\n\n\n.\nDossier courant\n./script.sh\n\n\n..\nDossier parent\ncd ..\n\n\n~\nDossier home\ncd ~\n\n\n/\nRacine du systÃ¨me\ncd /\n\n\n-\nDossier prÃ©cÃ©dent\ncd -",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©ation-et-manipulation-de-fichiers",
    "title": "Bash pour Data Engineers",
    "section": "2. CrÃ©ation et manipulation de fichiers",
    "text": "2. CrÃ©ation et manipulation de fichiers\nCrÃ©er, copier, dÃ©placer, supprimer :\n\n\nVoir le code\n%%bash\n# CrÃ©er un dossier\nmkdir data\n\n# CrÃ©er un dossier avec ses parents (pas d'erreur si existe)\nmkdir -p data/raw/2024\n\n# CrÃ©er un fichier vide\ntouch data/fichier.csv\n\n# CrÃ©er plusieurs fichiers\ntouch data/file1.csv data/file2.csv data/file3.csv\n\n# Copier un fichier\ncp data/fichier.csv data/fichier_backup.csv\n\n# Copier un dossier entier (rÃ©cursif)\ncp -r data/ data_backup/\n\n# DÃ©placer / Renommer un fichier\nmv data/fichier.csv data/nouveau_nom.csv\n\n# Supprimer un fichier\nrm data/file1.csv\n\n# Supprimer un dossier vide\nrmdir data/raw/2024\n\n# Supprimer un dossier et son contenu (âš ï¸ DANGEREUX)\nrm -r data_backup/",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#lecture-de-contenu",
    "title": "Bash pour Data Engineers",
    "section": "3. Lecture de contenu",
    "text": "3. Lecture de contenu\nLire, afficher, compter les lignes :\n\n\nVoir le code\n%%bash\n# CrÃ©ons d'abord un fichier exemple\ncat &lt;&lt; 'EOF' &gt; ventes.csv\ndate,produit,quantite,prix\n2024-01-01,Laptop,5,999.99\n2024-01-02,Souris,20,29.99\n2024-01-03,Clavier,15,79.99\n2024-01-04,Ã‰cran,8,299.99\n2024-01-05,Laptop,3,999.99\n2024-01-06,Souris,25,29.99\n2024-01-07,Casque,12,149.99\nEOF\n\necho \"âœ… Fichier ventes.csv crÃ©Ã©\"\n\n\n\n\nVoir le code\n%%bash\n# Affiche le contenu entier\necho \"=== cat ===\"\ncat ventes.csv\n\necho \"\"\necho \"=== head (3 premiÃ¨res lignes) ===\"\nhead -n 3 ventes.csv\n\necho \"\"\necho \"=== tail (2 derniÃ¨res lignes) ===\"\ntail -n 2 ventes.csv\n\necho \"\"\necho \"=== wc (comptage) ===\"\nwc -l ventes.csv    # Nombre de lignes\nwc -w ventes.csv    # Nombre de mots\nwc -c ventes.csv    # Nombre de caractÃ¨res\n\n\n\nLire des gros fichiers avec less\nPour les fichiers volumineux, utilise less qui permet de naviguer :\nless gros_fichier.csv\n\n\n\nTouche\nAction\n\n\n\n\nâ†“ ou j\nLigne suivante\n\n\nâ†‘ ou k\nLigne prÃ©cÃ©dente\n\n\nSpace\nPage suivante\n\n\nb\nPage prÃ©cÃ©dente\n\n\n/mot\nRechercher â€œmotâ€\n\n\nn\nOccurrence suivante\n\n\nq\nQuitter",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#recherche-filtrage",
    "title": "Bash pour Data Engineers",
    "section": "4. Recherche & filtrage",
    "text": "4. Recherche & filtrage\nExtraire des informations prÃ©cises â€” essentiel pour un Data Engineer !\n\n\nVoir le code\n%%bash\necho \"=== grep : recherche de motifs ===\"\n\n# Rechercher les lignes contenant \"Laptop\"\necho \"Lignes avec 'Laptop':\"\ngrep \"Laptop\" ventes.csv\n\necho \"\"\n# Recherche insensible Ã  la casse\necho \"Recherche insensible Ã  la casse (-i):\"\ngrep -i \"laptop\" ventes.csv\n\necho \"\"\n# Compter le nombre de correspondances\necho \"Nombre de lignes avec 'Souris':\"\ngrep -c \"Souris\" ventes.csv\n\necho \"\"\n# Afficher les numÃ©ros de ligne\necho \"Avec numÃ©ros de ligne (-n):\"\ngrep -n \"99.99\" ventes.csv\n\necho \"\"\n# Inverser la recherche (lignes qui NE contiennent PAS)\necho \"Lignes SANS 'Laptop' (-v):\"\ngrep -v \"Laptop\" ventes.csv\n\n\n\n\nVoir le code\n%%bash\necho \"=== find : trouver des fichiers ===\"\n\n# CrÃ©er quelques fichiers pour l'exemple\nmkdir -p projet/data projet/scripts\ntouch projet/data/users.csv projet/data/sales.csv projet/data/old.json\ntouch projet/scripts/etl.py projet/scripts/utils.py\n\n# Trouver tous les fichiers .csv\necho \"Fichiers .csv:\"\nfind projet/ -name \"*.csv\"\n\necho \"\"\n# Trouver tous les fichiers .py\necho \"Fichiers .py:\"\nfind projet/ -name \"*.py\"\n\necho \"\"\n# Trouver les fichiers modifiÃ©s dans les derniÃ¨res 24h\necho \"Fichiers modifiÃ©s rÃ©cemment:\"\nfind projet/ -mtime -1 -type f\n\n# Nettoyage\nrm -r projet/\n\n\n\n\nVoir le code\n%%bash\necho \"=== cut : extraire des colonnes ===\"\n\n# Extraire la 2Ã¨me colonne (produit)\necho \"Colonne 'produit':\"\ncut -d',' -f2 ventes.csv\n\necho \"\"\necho \"=== sort : trier ===\"\n# Trier par produit (2Ã¨me colonne)\necho \"TriÃ© par produit:\"\ntail -n +2 ventes.csv | sort -t',' -k2\n\necho \"\"\necho \"=== uniq : valeurs uniques ===\"\n# Liste des produits uniques\necho \"Produits uniques:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq\n\necho \"\"\n# Compter les occurrences\necho \"Comptage par produit:\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#pipes-redirections",
    "title": "Bash pour Data Engineers",
    "section": "5. Pipes & redirections",
    "text": "5. Pipes & redirections\nLe pipe (|) est lâ€™outil le plus puissant de Bash : il permet de chaÃ®ner des commandes en envoyant la sortie dâ€™une commande vers lâ€™entrÃ©e de la suivante.\n\n\nVoir le code\n%%bash\necho \"=== Exemples de pipes ===\"\n\n# Trouver les ventes de Laptop et compter\necho \"Nombre de ventes Laptop:\"\ncat ventes.csv | grep \"Laptop\" | wc -l\n\necho \"\"\n# Top 3 des produits les plus vendus\necho \"Top 3 produits (par nombre de lignes):\"\ncut -d',' -f2 ventes.csv | tail -n +2 | sort | uniq -c | sort -rn | head -3\n\necho \"\"\n# Pipeline complexe : produits avec prix &gt; 100\necho \"Produits avec prix &gt; 100:\"\ntail -n +2 ventes.csv | awk -F',' '$4 &gt; 100 {print $2, $4}' | sort -u\n\n\n\n\nVoir le code\n%%bash\necho \"=== Redirections ===\"\n\n# Rediriger vers un fichier (Ã©crase)\ngrep \"Laptop\" ventes.csv &gt; laptops.txt\necho \"Contenu de laptops.txt:\"\ncat laptops.txt\n\necho \"\"\n# Ajouter Ã  un fichier (append)\ngrep \"Ã‰cran\" ventes.csv &gt;&gt; laptops.txt\necho \"AprÃ¨s ajout:\"\ncat laptops.txt\n\necho \"\"\n# Rediriger les erreurs\nls fichier_inexistant 2&gt; erreurs.log\necho \"Erreur capturÃ©e:\"\ncat erreurs.log\n\n# Nettoyage\nrm -f laptops.txt erreurs.log\n\n\n\nRÃ©capitulatif des redirections\n\n\n\n\n\n\n\n\nSymbole\nDescription\nExemple\n\n\n\n\n&gt;\nRedirige stdout vers fichier (Ã©crase)\necho \"hello\" &gt; file.txt\n\n\n&gt;&gt;\nRedirige stdout vers fichier (ajoute)\necho \"world\" &gt;&gt; file.txt\n\n\n2&gt;\nRedirige stderr vers fichier\ncmd 2&gt; errors.log\n\n\n&&gt;\nRedirige stdout ET stderr\ncmd &&gt; all.log\n\n\n&lt;\nUtilise fichier comme entrÃ©e\nwc -l &lt; file.txt\n\n\n\\|\nPipe : stdout â†’ stdin suivant\ncat file \\| grep mot",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#variables-et-boucles",
    "title": "Bash pour Data Engineers",
    "section": "6. Variables et boucles",
    "text": "6. Variables et boucles\nAutomatiser avec des scripts bash :\n\n\nVoir le code\n%%bash\necho \"=== Variables ===\"\n\n# DÃ©clarer une variable (PAS d'espace autour du =)\nnom=\"Data Engineer\"\nannee=2024\ndossier_data=\"/home/user/data\"\n\n# Utiliser une variable avec $\necho \"Bienvenue $nom !\"\necho \"Nous sommes en $annee\"\n\n# Utiliser ${} pour Ã©viter l'ambiguÃ¯tÃ©\necho \"Fichier: ${dossier_data}/ventes.csv\"\n\necho \"\"\necho \"=== Variables d'environnement ===\"\necho \"Home: $HOME\"\necho \"User: $USER\"\necho \"Shell: $SHELL\"\necho \"Path: $PATH\" | cut -c1-50  # TronquÃ© pour l'affichage\n\n\n\n\nVoir le code\n%%bash\necho \"=== Boucle for ===\"\n\n# CrÃ©er des fichiers de test\nmkdir -p data_test\ntouch data_test/jan.csv data_test/feb.csv data_test/mar.csv\n\n# Boucle sur les fichiers CSV\nfor fichier in data_test/*.csv; do\n    echo \"Traitement de: $fichier\"\n    echo \"   Nom: $(basename \"$fichier\")\"\ndone\n\necho \"\"\necho \"=== Boucle avec sÃ©quence ===\"\nfor i in {1..5}; do\n    echo \"ItÃ©ration $i\"\ndone\n\necho \"\"\necho \"=== Boucle while ===\"\ncompteur=1\nwhile [ $compteur -le 3 ]; do\n    echo \"Compteur: $compteur\"\n    ((compteur++))\ndone\n\n# Nettoyage\nrm -r data_test/",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#conditions-ifelse",
    "title": "Bash pour Data Engineers",
    "section": "7. Conditions (if/else)",
    "text": "7. Conditions (if/else)\nPrendre des dÃ©cisions dans tes scripts :\n\n\nVoir le code\n%%bash\necho \"=== Conditions de base ===\"\n\n# VÃ©rifier si un fichier existe\nif [ -f \"ventes.csv\" ]; then\n    echo \"âœ… Le fichier ventes.csv existe\"\nelse\n    echo \"âŒ Le fichier n'existe pas\"\nfi\n\necho \"\"\n# VÃ©rifier si un dossier existe\nif [ -d \"/tmp\" ]; then\n    echo \"âœ… Le dossier /tmp existe\"\nfi\n\necho \"\"\n# Comparer des nombres\nnb_lignes=$(wc -l &lt; ventes.csv)\necho \"Nombre de lignes: $nb_lignes\"\n\nif [ $nb_lignes -gt 5 ]; then\n    echo \"ğŸ“Š Fichier volumineux (&gt; 5 lignes)\"\nelse\n    echo \"ğŸ“„ Petit fichier\"\nfi\n\n\n\nOpÃ©rateurs de test\n\n\n\nTest fichiers\nDescription\n\n\n\n\n-f fichier\nFichier existe\n\n\n-d dossier\nDossier existe\n\n\n-r fichier\nFichier lisible\n\n\n-w fichier\nFichier modifiable\n\n\n-s fichier\nFichier non vide\n\n\n\n\n\n\nTest nombres\nDescription\n\n\n\n\n-eq\nÃ‰gal\n\n\n-ne\nDiffÃ©rent\n\n\n-gt\nPlus grand que\n\n\n-lt\nPlus petit que\n\n\n-ge\nPlus grand ou Ã©gal\n\n\n-le\nPlus petit ou Ã©gal\n\n\n\n\n\n\nTest chaÃ®nes\nDescription\n\n\n\n\n=\nÃ‰gal\n\n\n!=\nDiffÃ©rent\n\n\n-z\nChaÃ®ne vide\n\n\n-n\nChaÃ®ne non vide",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#crÃ©er-et-exÃ©cuter-un-script-bash",
    "title": "Bash pour Data Engineers",
    "section": "8. CrÃ©er et exÃ©cuter un script Bash",
    "text": "8. CrÃ©er et exÃ©cuter un script Bash\nUn script Bash est simplement un fichier texte contenant des commandes :\n\n\nVoir le code\n%%bash\n# CrÃ©er un script complet\ncat &lt;&lt; 'EOF' &gt; mon_script.sh\n#!/bin/bash\n# Script de traitement de donnÃ©es\n# Auteur: Data Engineer\n# Date: 2024\n\necho \"ğŸš€ DÃ©marrage du script\"\necho \"ğŸ“… Date: $(date)\"\necho \"ğŸ‘¤ Utilisateur: $USER\"\necho \"ğŸ“‚ Dossier: $(pwd)\"\n\n# VÃ©rifier si un argument est passÃ©\nif [ -z \"$1\" ]; then\n    echo \"âš ï¸ Usage: ./mon_script.sh &lt;nom_fichier&gt;\"\n    exit 1\nfi\n\necho \"ğŸ“„ Fichier Ã  traiter: $1\"\necho \"âœ… Script terminÃ©\"\nEOF\n\n# Rendre exÃ©cutable\nchmod +x mon_script.sh\n\n# ExÃ©cuter le script\necho \"=== ExÃ©cution sans argument ===\"\n./mon_script.sh\n\necho \"\"\necho \"=== ExÃ©cution avec argument ===\"\n./mon_script.sh ventes.csv\n\n# Nettoyage\nrm mon_script.sh",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#automatisation-avec-cron",
    "title": "Bash pour Data Engineers",
    "section": "9. Automatisation avec Cron",
    "text": "9. Automatisation avec Cron\nCron permet de planifier lâ€™exÃ©cution automatique de scripts â€” indispensable pour les pipelines ETL !\n\nFormat dâ€™une ligne crontab\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0 - 59)\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0 - 23)\nâ”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1 - 31)\nâ”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mois (1 - 12)\nâ”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour de la semaine (0 - 6) (dimanche = 0)\nâ”‚ â”‚ â”‚ â”‚ â”‚\n* * * * * commande Ã  exÃ©cuter\n\n\nExemples courants pour Data Engineers\n\n\n\nExpression\nDescription\nCas dâ€™usage\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h\nETL nocturne\n\n\n*/15 * * * *\nToutes les 15 minutes\nMonitoring\n\n\n0 0 * * 0\nChaque dimanche Ã  minuit\nRapport hebdomadaire\n\n\n0 9 1 * *\nLe 1er de chaque mois Ã  9h\nRapport mensuel\n\n\n0 */4 * * *\nToutes les 4 heures\nSynchronisation donnÃ©es\n\n\n\n\n\nCommandes cron\n# Ã‰diter la crontab\ncrontab -e\n\n# Lister les jobs planifiÃ©s\ncrontab -l\n\n# Supprimer tous les jobs\ncrontab -r\n\n\nExemple de crontab pour Data Engineer\n# ETL quotidien Ã  2h du matin\n0 2 * * * /home/user/scripts/etl_pipeline.sh &gt;&gt; /var/log/etl.log 2&gt;&1\n\n# Backup des donnÃ©es chaque dimanche Ã  3h\n0 3 * * 0 /home/user/scripts/backup.sh\n\n# Nettoyage des fichiers temporaires chaque jour Ã  4h\n0 4 * * * find /tmp -mtime +7 -delete\n\nğŸ’¡ Astuce : Utilise crontab.guru pour gÃ©nÃ©rer facilement des expressions cron !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#exercice-pratique",
    "title": "Bash pour Data Engineers",
    "section": "Exercice pratique",
    "text": "Exercice pratique\n\nInstructions\n\nCrÃ©e un dossier de travail nommÃ© mon_premier_script\nEntre dans ce dossier\nCrÃ©e un fichier script appelÃ© bonjour.sh\nÃ‰dite ce fichier et Ã©cris un script qui :\n\nAffiche â€œBonjour Data Engineer ğŸ‘‹â€\nAffiche la date du jour\nTe souhaite une bonne session\n\nRends le script exÃ©cutable\nCrÃ©e un sous-dossier nommÃ© data/ et place-y quelques fichiers .csv (mÃªme vides)\nAjoute une Ã©tape dans le script pour :\n\nAfficher tous les fichiers .csv prÃ©sents dans le dossier data/\nPour chaque fichier .csv, afficher son nom avec un message comme :\nğŸ‘‰ â€œFichier trouvÃ© : nom_du_fichier.csv âœ…â€\n\n\nğŸ“Œ Quelle structure utiliser ? (indice : boucle for)\n\n\nâœ… Correction\n\n\nğŸ“¥ Afficher la correction complÃ¨te\n\n#!/bin/bash\n\n# 1. ğŸ“¢ Afficher un message de bienvenue\necho \"Bonjour Data Engineer ğŸ‘‹\"\n\n# 2. ğŸ—“ï¸ Afficher la date du jour\necho \"Date: $(date)\"\n\n# 3. ğŸ’¬ Souhaiter une bonne session\necho \"Bonne session de travail ğŸ’ª\"\n\n# 4. ğŸ“ CrÃ©er le dossier 'data/' s'il n'existe pas\nmkdir -p data\n\n# 5. ğŸ—‚ï¸ CrÃ©er quelques fichiers de test\ntouch data/fichier1.csv data/fichier2.csv data/fichier3.csv\n\n# 6. ğŸ” Lister les fichiers CSV\necho \"\"\necho \"ğŸ” Recherche de fichiers CSV dans ./data...\"\n\nfor fichier in data/*.csv; do\n    if [ -f \"$fichier\" ]; then\n        echo \"Fichier trouvÃ© : $(basename \"$fichier\") âœ…\"\n    fi\ndone\n\n# 7. âœ… Fin du script\necho \"\"\necho \"Traitement terminÃ© âœ…\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#cheatsheet-bash-commandes-essentielles",
    "title": "Bash pour Data Engineers",
    "section": "Cheatsheet Bash â€“ Commandes essentielles",
    "text": "Cheatsheet Bash â€“ Commandes essentielles\n\n\n\nCatÃ©gorie\nCommande\nDescription\n\n\n\n\nNavigation\npwd\nAffiche le chemin actuel\n\n\n\ncd dossier/\nSe dÃ©placer dans un dossier\n\n\n\nls -lh\nListe les fichiers avec dÃ©tails\n\n\nFichiers\ntouch nom.txt\nCrÃ©er un fichier vide\n\n\n\ncp fichier.txt dossier/\nCopier un fichier\n\n\n\nmv fichier.txt nouveau.txt\nRenommer ou dÃ©placer\n\n\n\nrm fichier.txt\nSupprimer un fichier\n\n\nDossiers\nmkdir dossier/\nCrÃ©er un dossier\n\n\n\nmkdir -p a/b/c\nCrÃ©er avec parents\n\n\n\nrm -r dossier/\nSupprimer dossier + contenu\n\n\nLecture\ncat fichier.txt\nAfficher tout le contenu\n\n\n\nhead -n 10 fichier.txt\n10 premiÃ¨res lignes\n\n\n\ntail -n 10 fichier.txt\n10 derniÃ¨res lignes\n\n\n\nwc -l fichier.txt\nCompter les lignes\n\n\nRecherche\ngrep \"mot\" fichier.txt\nRechercher un mot\n\n\n\nfind . -name \"*.csv\"\nTrouver des fichiers\n\n\n\ncut -d',' -f1 fichier.csv\nExtraire une colonne\n\n\nPipes\ncmd1 \\| cmd2\nChaÃ®ner des commandes\n\n\n\ncmd &gt; fichier.txt\nRediriger vers fichier\n\n\n\ncmd &gt;&gt; fichier.txt\nAjouter Ã  un fichier\n\n\nScripts\nchmod +x script.sh\nRendre exÃ©cutable\n\n\n\n./script.sh\nLancer un script\n\n\nBoucles\nfor f in *.csv; do ...; done\nBoucle sur fichiers\n\n\nCron\ncrontab -e\nÃ‰diter les tÃ¢ches planifiÃ©es\n\n\n\ncrontab -l\nLister les tÃ¢ches\n\n\n\nğŸ“¥ TÃ©lÃ©charger le Bash Cheatsheet PDF (fr)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#erreurs-classiques-Ã -Ã©viter",
    "title": "Bash pour Data Engineers",
    "section": "Erreurs classiques Ã  Ã©viter",
    "text": "Erreurs classiques Ã  Ã©viter\n\n\n\n\n\n\n\n\nâŒ Erreur\nğŸ’¥ ConsÃ©quence\nâœ… Bonne pratique\n\n\n\n\nrm -rf /\nSupprime TOUT le systÃ¨me !\nToujours vÃ©rifier le chemin avant rm -rf\n\n\n$fichier sans guillemets\nBug si espaces dans le nom\nUtiliser \"$fichier\"\n\n\nsudo sans rÃ©flÃ©chir\nÃ‰crase des fichiers systÃ¨me\nComprendre la commande avant dâ€™utiliser sudo\n\n\nScript non testÃ© en prod\nPerte de donnÃ©es\nToujours tester en sandbox dâ€™abord\n\n\nVAR = valeur (avec espaces)\nErreur de syntaxe\nVAR=valeur (sans espaces)\n\n\nOublier #!/bin/bash\nScript peut mal sâ€™exÃ©cuter\nToujours commencer par le shebang\n\n\n\n\nğŸ§  Conseil : Avant dâ€™exÃ©cuter une commande destructive (rm, mv), utilise echo pour voir ce qui serait affectÃ© :\n# Au lieu de :\nrm -rf data/*.csv\n\n# D'abord tester avec :\necho data/*.csv",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "Bash pour Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle commande affiche le chemin du dossier courant ?\n\ncd\n\npwd\n\nls\n\npath\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” pwd = Print Working Directory\n\n\n\n\nâ“ Q2. Que fait la commande rm -rf mon_dossier/ ?\n\nRedÃ©marre lâ€™ordinateur\n\nRÃ©organise un fichier\n\nSupprime un dossier et son contenu\n\nReformate le disque\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” -r = rÃ©cursif, -f = force (sans confirmation)\n\n\n\n\nâ“ Q3. Quelle commande affiche les 10 premiÃ¨res lignes dâ€™un fichier ?\n\nhead -n 10\n\ncat -10\n\nstart 10\n\ntop 10\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : a â€” head -n 10 fichier.txt\n\n\n\n\nâ“ Q4. Pourquoi Ã©crire \"$fichier\" au lieu de $fichier ?\n\nPour que Bash reconnaisse les fichiers CSV\n\nPour faire du style\n\nPour Ã©viter les bugs avec les noms contenant des espaces\n\nÃ‡a nâ€™a pas dâ€™importance\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les guillemets protÃ¨gent les valeurs contenant des espaces\n\n\n\n\nâ“ Q5. Que signifie le | (pipe) en Bash ?\n\nInterrompre une commande\n\nExÃ©cuter un script\n\nEnvoyer la sortie dâ€™une commande vers lâ€™entrÃ©e dâ€™une autre\n\nCrÃ©er un fichier temporaire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le pipe chaÃ®ne les commandes : cmd1 | cmd2\n\n\n\n\nâ“ Q6. Quelle expression cron exÃ©cute un script tous les jours Ã  2h du matin ?\n\n2 0 * * *\n\n0 2 * * *\n\n* 2 * * *\n\n0 0 2 * *\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Format : minute heure jour mois jour_semaine",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#mini-projet-archiver-intelligemment-des-fichiers-csv",
    "title": "Bash pour Data Engineers",
    "section": "Mini-projet : Archiver intelligemment des fichiers CSV",
    "text": "Mini-projet : Archiver intelligemment des fichiers CSV\n\nObjectif\nCrÃ©er un script Bash rÃ©aliste qui automatise lâ€™archivage de fichiers .csv selon leur anciennetÃ©.\n\n\nContexte\nTu travailles dans une Ã©quipe data. Chaque jour, des fichiers .csv sont dÃ©posÃ©s dans un dossier data/.\nTu dois crÃ©er un script qui :\n\nRepÃ¨re tous les fichiers .csv modifiÃ©s il y a plus de 7 jours\nLes archive dans un fichier .tar.gz nommÃ© archive_YYYYMMDD.tar.gz\nDÃ©place ces fichiers dans un dossier archive/\n\n\n\nContraintes\n\nLe script doit fonctionner mÃªme si aucun fichier nâ€™est Ã©ligible\nLâ€™archive doit Ãªtre horodatÃ©e automatiquement\nLe dossier archive/ doit Ãªtre crÃ©Ã© sâ€™il nâ€™existe pas\nAjouter du logging pour tracer les actions\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n#!/bin/bash\n#\n# Script: archive_csv.sh\n# Description: Archive les fichiers CSV de plus de 7 jours\n# Auteur: Data Engineer\n#\n\n# Configuration\nDATA_DIR=\"./data\"\nARCHIVE_DIR=\"./archive\"\nDAYS_OLD=7\nDATE_TAG=$(date +%Y%m%d)\nARCHIVE_NAME=\"archive_${DATE_TAG}.tar.gz\"\nLOG_FILE=\"archive.log\"\n\n# Fonction de logging\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" | tee -a \"$LOG_FILE\"\n}\n\nlog \"ğŸš€ DÃ©marrage du script d'archivage\"\n\n# VÃ©rifier que le dossier source existe\nif [ ! -d \"$DATA_DIR\" ]; then\n    log \"âŒ Erreur: Le dossier $DATA_DIR n'existe pas\"\n    exit 1\nfi\n\n# CrÃ©er le dossier d'archive si nÃ©cessaire\nmkdir -p \"$ARCHIVE_DIR\"\nlog \"ğŸ“ Dossier d'archive: $ARCHIVE_DIR\"\n\n# Trouver les fichiers CSV de plus de 7 jours\nOLD_FILES=$(find \"$DATA_DIR\" -name \"*.csv\" -mtime +$DAYS_OLD -type f)\n\n# VÃ©rifier s'il y a des fichiers Ã  archiver\nif [ -z \"$OLD_FILES\" ]; then\n    log \"â„¹ï¸ Aucun fichier CSV de plus de $DAYS_OLD jours trouvÃ©\"\n    exit 0\nfi\n\n# Compter les fichiers\nNB_FILES=$(echo \"$OLD_FILES\" | wc -l)\nlog \"ğŸ“Š $NB_FILES fichier(s) Ã  archiver\"\n\n# CrÃ©er l'archive\nlog \"ğŸ“¦ CrÃ©ation de l'archive $ARCHIVE_NAME...\"\necho \"$OLD_FILES\" | tar -czvf \"$ARCHIVE_DIR/$ARCHIVE_NAME\" -T -\n\nif [ $? -eq 0 ]; then\n    log \"âœ… Archive crÃ©Ã©e avec succÃ¨s\"\n    \n    # DÃ©placer les fichiers archivÃ©s\n    for file in $OLD_FILES; do\n        mv \"$file\" \"$ARCHIVE_DIR/\"\n        log \"   â†³ DÃ©placÃ©: $(basename \"$file\")\"\n    done\n    \n    log \"ğŸ‰ Archivage terminÃ© avec succÃ¨s\"\nelse\n    log \"âŒ Erreur lors de la crÃ©ation de l'archive\"\n    exit 1\nfi\nPour lâ€™utiliser :\nchmod +x archive_csv.sh\n./archive_csv.sh\nPour lâ€™automatiser avec cron (tous les jours Ã  3h) :\n0 3 * * * /home/user/scripts/archive_csv.sh",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "Bash pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Sites & outils\n\nExplainShell â€” Explique nâ€™importe quelle commande Bash\nShellCheck â€” VÃ©rifie la syntaxe de tes scripts\nCrontab Guru â€” GÃ©nÃ©rateur dâ€™expressions cron\nLinux Command â€” Tutoriel complet\n\n\n\nğŸ“– Documentation\n\nGNU Bash Manual\nAdvanced Bash-Scripting Guide\n\n\n\nğŸ® Pratique\n\nOverTheWire - Bandit â€” Jeu pour apprendre Bash\nCmdchallenge â€” DÃ©fis en ligne de commande",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/beginner/02_bash_for_data_engineers.html#prochaine-Ã©tape",
    "title": "Bash pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Bash, passons Ã  un autre outil essentiel : Git !\nğŸ‘‰ Module suivant : 03_git_for_data_engineers â€” Versionner ton code et collaborer\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Bash pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ¦ Niveau 1 : DÃ©butant",
      "ğŸ“Œ Fondations",
      "02 Â· Linux & Bash"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  dÃ©ployer Apache Spark sur Kubernetes. Câ€™est lâ€™architecture moderne de rÃ©fÃ©rence pour exÃ©cuter des workloads Spark en production.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#prÃ©requis",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#prÃ©requis",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nModule\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 14\nDocker Fundamentals\n\n\nâœ… Requis\nModule 15\nKubernetes Fundamentals\n\n\nâœ… Requis\nModule 16\nKubernetes for Data Workloads\n\n\nâœ… Requis\nModule 19\nPySpark Advanced\n\n\nâœ… Requis\nModule 20\nSpark SQL Deep Dive\n\n\nğŸ’¡ RecommandÃ©\n-\nExpÃ©rience avec kubectl et Helm",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#objectifs-du-module",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#objectifs-du-module",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre lâ€™architecture Spark on Kubernetes\nConstruire une image Docker Spark optimisÃ©e\nConfigurer les ressources Kubernetes (RBAC, Secrets, PVC)\nLancer des jobs avec spark-submit sur K8s\nUtiliser Spark Operator pour la production\nConfigurer lâ€™autoscaling (Dynamic Allocation, KEDA)\nMettre en place le monitoring (Spark UI, Prometheus, Grafana)\nDebugger les erreurs courantes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#introduction-pourquoi-spark-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#introduction-pourquoi-spark-sur-kubernetes",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "1. Introduction â€” Pourquoi Spark sur Kubernetes ?",
    "text": "1. Introduction â€” Pourquoi Spark sur Kubernetes ?\n\n1.1 Lâ€™Ã©volution de lâ€™infrastructure Spark\n2010s                              2020s+\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Hadoop/YARN   â”‚                â”‚   Kubernetes    â”‚\nâ”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚   â”‚  Spark  â”‚   â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â–¶   â”‚   â”‚  Spark  â”‚   â”‚\nâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ”‚   On-premise    â”‚                â”‚   Cloud-native  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Avantages de Kubernetes\n\n\n\nAvantage\nDescription\n\n\n\n\nCloud-native\nMÃªme infra que le reste de tes applications\n\n\nMulti-cloud\nAWS, GCP, Azure, on-premise\n\n\nIsolation\nNamespaces, RBAC, Network Policies\n\n\nAutoscaling\nHPA, VPA, KEDA, Cluster Autoscaler\n\n\nCI/CD\nImages Docker versionnÃ©es, GitOps\n\n\nCost optimization\nSpot instances, autoscaling down\n\n\nStandardisation\nPlus besoin dâ€™expertise Hadoop/YARN\n\n\n\n\n\n1.3 Support officiel\n\nSpark 2.3 : Support expÃ©rimental K8s\nSpark 3.0 : Support stable (GA)\nSpark 3.1+ : AmÃ©liorations (PVC, Pod templates)\nSpark 3.4+ : External shuffle service sur K8s\n\n\n\n1.4 Comparaison YARN vs Kubernetes\n\n\n\n\n\n\n\n\nAspect\nYARN\nKubernetes\n\n\n\n\nÃ‰cosystÃ¨me\nHadoop-centric\nCloud-native, polyglot\n\n\nScheduling\nResourceManager\nkube-scheduler\n\n\nIsolation\nContainers/cgroups\nPods, Namespaces, Network Policies\n\n\nScaling\nManual ou scripts\nHPA, VPA, KEDA, Cluster Autoscaler\n\n\nPackaging\nJAR/ZIP sur HDFS\nImages Docker\n\n\nSecrets\nHadoop credentials\nK8s Secrets, Vault\n\n\nMonitoring\nYARN UI, Ganglia\nPrometheus, Grafana, native\n\n\nMulti-tenancy\nQueues\nNamespaces + RBAC\n\n\nData locality\nâœ… Excellent\nâš ï¸ LimitÃ© (shuffle coÃ»teux)\n\n\nCourbe dâ€™apprentissage\nHadoop stack\nK8s stack\n\n\nAdoption 2024+\nLegacy\nStandard moderne\n\n\n\nğŸ“Š Quand choisir quoi ?\n\nYARN si :                           Kubernetes si :\nâ”œâ”€â”€ Cluster Hadoop existant         â”œâ”€â”€ Infrastructure cloud-native\nâ”œâ”€â”€ Data locality critique          â”œâ”€â”€ Multi-cloud ou hybrid\nâ”œâ”€â”€ Ã‰quipe Hadoop experte           â”œâ”€â”€ CI/CD moderne (GitOps)\nâ””â”€â”€ Pas de migration prÃ©vue         â””â”€â”€ Autoscaling avancÃ© requis",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#architecture-spark-on-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#architecture-spark-on-kubernetes",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "2. Architecture Spark on Kubernetes",
    "text": "2. Architecture Spark on Kubernetes\n\n2.1 Vue dâ€™ensemble\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        KUBERNETES CLUSTER                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚                     Namespace: spark                      â”‚  â”‚\nâ”‚  â”‚                                                           â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚  â”‚   â”‚   Driver Pod    â”‚      â”‚     Executor Pods       â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”‚  Spark    â”‚  â”‚â—€â”€â”€â”€â”€â–¶â”‚  â”‚ Exec  â”‚ â”‚ Exec  â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â”‚  Driver   â”‚  â”‚      â”‚  â”‚  #1   â”‚ â”‚  #2   â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - Spark UI     â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - Coordinateur â”‚      â”‚  â”‚ Exec  â”‚ â”‚ Exec  â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚  #3   â”‚ â”‚  #4   â”‚   â”‚   â”‚  â”‚\nâ”‚  â”‚           â”‚                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  â”‚\nâ”‚  â”‚           â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚  â”‚           â–¼                                               â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚\nâ”‚  â”‚   â”‚  ConfigMaps     â”‚      â”‚       Secrets           â”‚   â”‚  â”‚\nâ”‚  â”‚   â”‚  - spark-config â”‚      â”‚  - cloud credentials    â”‚   â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚\nâ”‚  â”‚                                                           â”‚  â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚\nâ”‚  â”‚   â”‚              PersistentVolumeClaims                 â”‚â”‚  â”‚\nâ”‚  â”‚   â”‚  - checkpoints  - logs  - shuffle (optional)       â”‚â”‚  â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 RÃ´les et responsabilitÃ©s\n\n\n\nComposant\nGÃ©rÃ© par\nResponsabilitÃ©\n\n\n\n\nPods scheduling\nKubernetes\nPlacement sur les nodes\n\n\nNetworking\nKubernetes\nCommunication Driver â†”ï¸ Executors\n\n\nSecrets\nKubernetes\nCredentials cloud/DB\n\n\nStorage\nKubernetes\nPVC pour checkpoints/logs\n\n\nDriver\nSpark\nCoordination du job\n\n\nExecutors\nSpark\nExÃ©cution des tasks\n\n\nShuffle\nSpark\nÃ‰change de donnÃ©es entre stages\n\n\n\n\n\n2.3 Client Mode vs Cluster Mode\nCLIENT MODE                              CLUSTER MODE\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Local Machineâ”‚                        â”‚      Kubernetes Cluster      â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚  â”‚ Driver â”‚  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚  â”‚ Driver â”‚â—€â”€â”€â”€â”€â”            â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          â”‚             â”‚  â”‚  Pod   â”‚     â”‚            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚            â”‚\n                          â”‚             â”‚        â–²        â”‚            â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚        â”‚        â”‚            â”‚\nâ”‚      K8s Cluster        â”‚         â”‚   â”‚        â–¼        â–¼            â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚Executorâ”‚ â”‚Executorâ”‚â—€â”€â”˜         â”‚   â”‚  â”‚Executorâ”‚ â”‚Executorâ”‚      â”‚\nâ”‚  â”‚  Pod   â”‚ â”‚  Pod   â”‚            â”‚   â”‚  â”‚  Pod   â”‚ â”‚  Pod   â”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\nAspect\nClient Mode\nCluster Mode\n\n\n\n\nDriver\nMachine locale\nPod Kubernetes\n\n\nUsage\nDÃ©veloppement, debug\nProduction\n\n\nSpark UI\nlocalhost:4040\nVia Service/Ingress\n\n\nRÃ©seau\nDoit atteindre le cluster\nInterne au cluster\n\n\nRÃ©silience\nâŒ Si local crash â†’ job perdu\nâœ… Pod peut Ãªtre reschedulÃ©\n\n\nCI/CD\nâŒ Difficile\nâœ… Natif\n\n\n\n\nğŸ’¡ RÃ¨gle : Client mode pour le dev/debug, Cluster mode pour la production.\n\n\n\nExercice 1 : Identifier les composants\nRÃ©ponds aux questions suivantes :\n\nDans quel mode le Driver tourne-t-il en tant que Pod K8s ?\nQui gÃ¨re le scheduling des Executor Pods ?\nOÃ¹ sont stockÃ©s les credentials cloud ?\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\nCluster mode â€” Le Driver est un Pod K8s\nKubernetes (kube-scheduler) â€” Spark demande des Pods, K8s les place\nKubernetes Secrets â€” MontÃ©s dans les Pods Driver/Executor",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#construire-une-image-docker-spark",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#construire-une-image-docker-spark",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "3. Construire une Image Docker Spark",
    "text": "3. Construire une Image Docker Spark\n\n3.1 Image de base\nPlusieurs options :\n\n\n\nImage\nAvantage\nInconvÃ©nient\n\n\n\n\napache/spark\nOfficielle\nBasique\n\n\nbitnami/spark\nBien maintenue, non-root\nTaille moyenne\n\n\ngcr.io/spark-operator/spark\nPour Spark Operator\nSpÃ©cifique\n\n\nCustom\nContrÃ´le total\nPlus de travail\n\n\n\n\n\nVoir le code\n%%writefile /tmp/spark-docker/Dockerfile.basic\n# Image Spark de base\nFROM bitnami/spark:3.5\n\n# Passer en root pour installer des packages\nUSER root\n\n# Installer des dÃ©pendances Python\nRUN pip install --no-cache-dir \\\n    boto3 \\\n    pyarrow \\\n    pandas\n\n# Copier l'application\nCOPY app/ /app/\n\n# Revenir Ã  l'utilisateur non-root (sÃ©curitÃ©)\nUSER 1001\n\n# DÃ©finir le working directory\nWORKDIR /app\n\n\n\n\n3.2 Image production-grade avec JARs\n\n\nVoir le code\n%%writefile /tmp/spark-docker/Dockerfile.production\n# ============================================\n# Spark Production Image\n# ============================================\nFROM bitnami/spark:3.5 AS base\n\nUSER root\n\n# ---- DÃ©pendances systÃ¨me ----\nRUN apt-get update && apt-get install -y --no-install-recommends \\\n    curl \\\n    && rm -rf /var/lib/apt/lists/*\n\n# ---- DÃ©pendances Python ----\nCOPY requirements.txt /tmp/\nRUN pip install --no-cache-dir -r /tmp/requirements.txt\n\n# ---- JARs pour connecteurs cloud ----\n# AWS S3\nRUN curl -sL https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \\\n    -o /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar\nRUN curl -sL https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar \\\n    -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar\n\n# ---- Application ----\nCOPY app/ /app/\n\n# ---- SÃ©curitÃ© : non-root ----\nRUN chown -R 1001:1001 /app\nUSER 1001\n\nWORKDIR /app\n\n# ---- Healthcheck ----\nHEALTHCHECK --interval=30s --timeout=10s --retries=3 \\\n    CMD curl -f http://localhost:4040/api/v1/applications || exit 1\n\n\n\n\nVoir le code\n%%writefile /tmp/spark-docker/requirements.txt\n# Python dependencies for Spark jobs\nboto3&gt;=1.28.0\npyarrow&gt;=14.0.0\npandas&gt;=2.0.0\nrequests&gt;=2.31.0\n\n\n\n\n3.3 Best practices Docker pour Spark\n\n\n\nPratique\nPourquoi\nComment\n\n\n\n\nNon-root\nSÃ©curitÃ©\nUSER 1001\n\n\nMulti-stage builds\nImage plus lÃ©gÃ¨re\nFROM ... AS builder\n\n\nLayer caching\nBuilds plus rapides\nDÃ©pendances avant code\n\n\nNo cache pip\nImage plus lÃ©gÃ¨re\n--no-cache-dir\n\n\nHealthcheck\nK8s liveness probe\nHEALTHCHECK\n\n\nVersioning\nReproductibilitÃ©\nTags explicites (pas latest)\n\n\n\n\n\nVoir le code\n# Commandes pour construire et pousser l'image\ndocker_commands = \"\"\"\n# Construire l'image\ndocker build -t my-registry/spark-app:1.0.0 -f Dockerfile.production .\n\n# Tester localement\ndocker run --rm my-registry/spark-app:1.0.0 spark-submit --version\n\n# Pousser vers un registry\ndocker push my-registry/spark-app:1.0.0\n\n# Pour Minikube (utiliser le Docker daemon de Minikube)\neval $(minikube docker-env)\ndocker build -t spark-app:1.0.0 .\n\"\"\"\nprint(docker_commands)\n\n\n\n\nExercice 2 : Construire une image Spark\nObjectif : CrÃ©er un Dockerfile Spark avec :\n\nBase bitnami/spark:3.5\nDÃ©pendance Python : numpy\nUn script hello.py qui affiche â€œHello Spark on K8s!â€\n\n\n\nğŸ’¡ Solution\n\nFROM bitnami/spark:3.5\n\nUSER root\nRUN pip install --no-cache-dir numpy\n\nCOPY hello.py /app/hello.py\n\nUSER 1001\nWORKDIR /app\n# hello.py\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Hello\").getOrCreate()\nprint(\"Hello Spark on K8s!\")\nspark.stop()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#configuration-kubernetes-pour-spark",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#configuration-kubernetes-pour-spark",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "4. Configuration Kubernetes pour Spark",
    "text": "4. Configuration Kubernetes pour Spark\n\n4.1 Namespace dÃ©diÃ©\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark\n  labels:\n    app: spark\n    environment: development\n\n\n\n\n4.2 ServiceAccount & RBAC\nSpark a besoin de permissions pour :\n\nCrÃ©er des Pods (executors)\nLister/supprimer des Pods\nAccÃ©der aux ConfigMaps et Secrets\n\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/rbac.yaml\n# ServiceAccount pour Spark\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark-sa\n  namespace: spark\n---\n# Role avec permissions minimales\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: spark-role\n  namespace: spark\nrules:\n  # GÃ©rer les pods (executors)\n  - apiGroups: [\"\"]\n    resources: [\"pods\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"delete\"]\n  # Logs des pods\n  - apiGroups: [\"\"]\n    resources: [\"pods/log\"]\n    verbs: [\"get\", \"list\"]\n  # ConfigMaps pour Spark config\n  - apiGroups: [\"\"]\n    resources: [\"configmaps\"]\n    verbs: [\"create\", \"get\", \"list\", \"watch\", \"delete\"]\n  # Services pour Spark UI\n  - apiGroups: [\"\"]\n    resources: [\"services\"]\n    verbs: [\"create\", \"get\", \"delete\"]\n  # PersistentVolumeClaims\n  - apiGroups: [\"\"]\n    resources: [\"persistentvolumeclaims\"]\n    verbs: [\"create\", \"get\", \"list\", \"delete\"]\n---\n# Binding Role â†’ ServiceAccount\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: spark-role-binding\n  namespace: spark\nsubjects:\n  - kind: ServiceAccount\n    name: spark-sa\n    namespace: spark\nroleRef:\n  kind: Role\n  name: spark-role\n  apiGroup: rbac.authorization.k8s.io\n\n\n\n\n4.3 Secrets (credentials cloud)\n\nâš ï¸ Note : En production, utilise un gestionnaire de secrets (Vault, AWS Secrets Manager, etc.)\n\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/secrets.yaml\n# Secret pour MinIO (ou S3)\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-credentials\n  namespace: spark\ntype: Opaque\nstringData:\n  AWS_ACCESS_KEY_ID: \"minioadmin\"\n  AWS_SECRET_ACCESS_KEY: \"minioadmin\"\n  S3_ENDPOINT: \"http://minio.minio.svc.cluster.local:9000\"\n\n\n\n\nVoir le code\n# Alternative : crÃ©er le secret via CLI\nsecret_command = \"\"\"\nkubectl create secret generic minio-credentials \\\n  --namespace=spark \\\n  --from-literal=AWS_ACCESS_KEY_ID=minioadmin \\\n  --from-literal=AWS_SECRET_ACCESS_KEY=minioadmin \\\n  --from-literal=S3_ENDPOINT=http://minio.minio.svc.cluster.local:9000\n\"\"\"\nprint(secret_command)\n\n\n\n\n4.4 PersistentVolumeClaims\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/pvc.yaml\n# PVC pour les logs Spark\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: spark-logs-pvc\n  namespace: spark\nspec:\n  accessModes:\n    - ReadWriteMany  # Plusieurs pods peuvent Ã©crire\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: standard  # Adapter selon ton cluster\n---\n# PVC pour les checkpoints (streaming)\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: spark-checkpoints-pvc\n  namespace: spark\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 20Gi\n  storageClassName: standard\n\n\n\n\n4.5 ResourceQuotas & LimitRanges (optionnel mais recommandÃ©)\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/quotas.yaml\n# Limiter les ressources du namespace Spark\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: spark-quota\n  namespace: spark\nspec:\n  hard:\n    requests.cpu: \"20\"        # Max 20 CPU demandÃ©s\n    requests.memory: \"40Gi\"   # Max 40 Gi mÃ©moire demandÃ©e\n    limits.cpu: \"40\"          # Max 40 CPU limites\n    limits.memory: \"80Gi\"     # Max 80 Gi mÃ©moire limites\n    pods: \"50\"                # Max 50 pods\n---\n# Defaults pour les pods sans spec\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: spark-limits\n  namespace: spark\nspec:\n  limits:\n    - type: Container\n      default:\n        cpu: \"1\"\n        memory: \"2Gi\"\n      defaultRequest:\n        cpu: \"500m\"\n        memory: \"1Gi\"\n      max:\n        cpu: \"8\"\n        memory: \"16Gi\"\n\n\n\n\nExercice 3 : CrÃ©er les manifests RBAC\nQuestion : Pourquoi le ServiceAccount Spark a-t-il besoin de la permission pods/log ?\n\n\nğŸ’¡ RÃ©ponse\n\nLe Driver Spark a besoin de lire les logs des Executor Pods pour :\n\nAfficher les erreurs dans le Spark UI\nPermettre le debugging via kubectl logs\nCollecter les mÃ©triques dâ€™exÃ©cution",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-submit-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-submit-sur-kubernetes",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "5. spark-submit sur Kubernetes",
    "text": "5. spark-submit sur Kubernetes\n\n5.1 Syntaxe de base\n\n\nVoir le code\nspark_submit_basic = \"\"\"\n# Spark-submit basique sur K8s (cluster mode)\nspark-submit \\\n  --master k8s://https://&lt;kubernetes-api-server&gt;:6443 \\\n  --deploy-mode cluster \\\n  --name spark-pi \\\n  --conf spark.kubernetes.container.image=spark-app:1.0.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  local:///app/pi.py\n\"\"\"\nprint(spark_submit_basic)\n\n\n\n\n5.2 Commande complÃ¨te production-grade\n\n\nVoir le code\nspark_submit_full = \"\"\"\n# Spark-submit complet pour production\nspark-submit \\\n  # === Master & Mode ===\n  --master k8s://https://$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}') \\\n  --deploy-mode cluster \\\n  --name etl-job-$(date +%Y%m%d-%H%M%S) \\\n  \\\n  # === Image & Namespace ===\n  --conf spark.kubernetes.container.image=my-registry/spark-app:1.0.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  \\\n  # === Driver Resources ===\n  --conf spark.driver.cores=2 \\\n  --conf spark.driver.memory=4g \\\n  --conf spark.kubernetes.driver.request.cores=1 \\\n  --conf spark.kubernetes.driver.limit.cores=2 \\\n  \\\n  # === Executor Resources ===\n  --conf spark.executor.instances=4 \\\n  --conf spark.executor.cores=2 \\\n  --conf spark.executor.memory=4g \\\n  --conf spark.kubernetes.executor.request.cores=1 \\\n  --conf spark.kubernetes.executor.limit.cores=2 \\\n  \\\n  # === Secrets (environnement) ===\n  --conf spark.kubernetes.driver.secretKeyRef.AWS_ACCESS_KEY_ID=minio-credentials:AWS_ACCESS_KEY_ID \\\n  --conf spark.kubernetes.driver.secretKeyRef.AWS_SECRET_ACCESS_KEY=minio-credentials:AWS_SECRET_ACCESS_KEY \\\n  --conf spark.kubernetes.executor.secretKeyRef.AWS_ACCESS_KEY_ID=minio-credentials:AWS_ACCESS_KEY_ID \\\n  --conf spark.kubernetes.executor.secretKeyRef.AWS_SECRET_ACCESS_KEY=minio-credentials:AWS_SECRET_ACCESS_KEY \\\n  \\\n  # === S3/MinIO Config ===\n  --conf spark.hadoop.fs.s3a.endpoint=http://minio.minio.svc.cluster.local:9000 \\\n  --conf spark.hadoop.fs.s3a.path.style.access=true \\\n  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n  \\\n  # === Application ===\n  local:///app/etl_job.py \\\n  --input s3a://bronze/data \\\n  --output s3a://silver/data\n\"\"\"\nprint(spark_submit_full)\n\n\n\n\n5.3 Configurations essentielles\n\n\n\n\n\n\n\n\nConfiguration\nDescription\nExemple\n\n\n\n\nspark.kubernetes.container.image\nImage Docker Spark\nregistry/spark:1.0\n\n\nspark.kubernetes.namespace\nNamespace K8s\nspark\n\n\nspark.kubernetes.authenticate.driver.serviceAccountName\nServiceAccount\nspark-sa\n\n\nspark.executor.instances\nNombre dâ€™executors\n4\n\n\nspark.executor.cores\nCores par executor\n2\n\n\nspark.executor.memory\nMÃ©moire par executor\n4g\n\n\nspark.kubernetes.driver.secretKeyRef.*\nSecrets â†’ env vars\nVoir exemple\n\n\nspark.kubernetes.executor.deleteOnTermination\nCleanup\ntrue\n\n\n\n\n\n5.4 AccÃ©der au Spark UI\nEn cluster mode, le Spark UI est dans le Pod Driver.\n\n\nVoir le code\nspark_ui_access = \"\"\"\n# 1. Trouver le pod Driver\nkubectl get pods -n spark -l spark-role=driver\n\n# 2. Port-forward vers le Spark UI\nkubectl port-forward -n spark &lt;driver-pod-name&gt; 4040:4040\n\n# 3. Ouvrir dans le navigateur\n# http://localhost:4040\n\n# Alternative : crÃ©er un Service\nkubectl expose pod &lt;driver-pod-name&gt; -n spark \\\n  --port=4040 --target-port=4040 \\\n  --name=spark-ui --type=NodePort\n\"\"\"\nprint(spark_ui_access)\n\n\n\n\nExercice 4 : Lancer un job Spark sur Minikube\nÃ‰tapes : 1. DÃ©marrer Minikube : minikube start --cpus=4 --memory=8g 2. CrÃ©er le namespace et RBAC 3. Lancer le job SparkPi intÃ©grÃ©\n\n\nğŸ’¡ Solution\n\n# 1. Setup\nminikube start --cpus=4 --memory=8g\nkubectl create namespace spark\nkubectl apply -f rbac.yaml\n\n# 2. Obtenir l'URL de l'API K8s\nK8S_API=$(kubectl config view -o jsonpath='{.clusters[0].cluster.server}')\n\n# 3. Lancer SparkPi\nspark-submit \\\n  --master k8s://$K8S_API \\\n  --deploy-mode cluster \\\n  --name spark-pi \\\n  --conf spark.kubernetes.container.image=apache/spark:3.5.0 \\\n  --conf spark.kubernetes.namespace=spark \\\n  --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-sa \\\n  --conf spark.executor.instances=2 \\\n  local:///opt/spark/examples/src/main/python/pi.py 100\n\n# 4. VÃ©rifier\nkubectl get pods -n spark\nkubectl logs -n spark &lt;driver-pod&gt; | tail -20",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-operator-production-grade",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#spark-operator-production-grade",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "6. Spark Operator â€” Production-Grade",
    "text": "6. Spark Operator â€” Production-Grade\n\n6.1 Pourquoi utiliser Spark Operator ?\n\n\n\nspark-submit\nSpark Operator\n\n\n\n\nCommande CLI\nManifeste YAML dÃ©claratif\n\n\nPas de retry automatique\nRetry policy configurable\n\n\nPas de scheduling\nCronJob-like scheduling\n\n\nDifficile Ã  intÃ©grer CI/CD\nGitOps natif\n\n\nLogs dispersÃ©s\nLogs centralisÃ©s\n\n\n\n\n\n6.2 Installation avec Helm\n\n\nVoir le code\noperator_install = \"\"\"\n# Ajouter le repo Helm\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\nhelm repo update\n\n# Installer l'opÃ©rateur\nhelm install spark-operator spark-operator/spark-operator \\\n  --namespace spark-operator \\\n  --create-namespace \\\n  --set webhook.enable=true \\\n  --set sparkJobNamespace=spark\n\n# VÃ©rifier\nkubectl get pods -n spark-operator\n\"\"\"\nprint(operator_install)\n\n\n\n\n6.3 SparkApplication CRD\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/spark-application.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: etl-job\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: my-registry/spark-app:1.0.0\n  imagePullPolicy: Always\n  mainApplicationFile: local:///app/etl_job.py\n  arguments:\n    - \"--input\"\n    - \"s3a://bronze/data\"\n    - \"--output\"\n    - \"s3a://silver/data\"\n  sparkVersion: \"3.5.0\"\n  \n  # Configuration Spark\n  sparkConf:\n    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio.minio.svc.cluster.local:9000\"\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\"\n    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n  \n  # Restart policy\n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 3\n    onFailureRetryInterval: 30\n    onSubmissionFailureRetries: 3\n  \n  # Driver config\n  driver:\n    cores: 1\n    coreLimit: \"1200m\"\n    memory: \"2g\"\n    serviceAccount: spark-sa\n    labels:\n      app: spark-etl\n      role: driver\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n  \n  # Executor config\n  executor:\n    cores: 2\n    coreLimit: \"2400m\"\n    instances: 4\n    memory: \"4g\"\n    labels:\n      app: spark-etl\n      role: executor\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n\n\n\n\n6.4 ScheduledSparkApplication (Cron jobs)\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/scheduled-spark.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: ScheduledSparkApplication\nmetadata:\n  name: daily-etl\n  namespace: spark\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h du matin\n  concurrencyPolicy: Forbid  # Ne pas lancer si le prÃ©cÃ©dent tourne encore\n  successfulRunHistoryLimit: 5\n  failedRunHistoryLimit: 3\n  \n  template:\n    type: Python\n    pythonVersion: \"3\"\n    mode: cluster\n    image: my-registry/spark-app:1.0.0\n    mainApplicationFile: local:///app/daily_etl.py\n    sparkVersion: \"3.5.0\"\n    \n    restartPolicy:\n      type: OnFailure\n      onFailureRetries: 2\n    \n    driver:\n      cores: 1\n      memory: \"2g\"\n      serviceAccount: spark-sa\n    \n    executor:\n      cores: 2\n      instances: 3\n      memory: \"4g\"\n\n\n\n\nVoir le code\noperator_commands = \"\"\"\n# Appliquer une SparkApplication\nkubectl apply -f spark-application.yaml\n\n# Voir le statut\nkubectl get sparkapplication -n spark\nkubectl describe sparkapplication etl-job -n spark\n\n# Voir les logs du driver\nkubectl logs -n spark -l spark-role=driver -f\n\n# Supprimer\nkubectl delete sparkapplication etl-job -n spark\n\"\"\"\nprint(operator_commands)\n\n\n\n\nExercice 5 : DÃ©ployer avec SparkOperator\nObjectif : CrÃ©er une SparkApplication qui calcule Pi avec 1000 itÃ©rations.\n\n\nğŸ’¡ Solution\n\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: apache/spark:3.5.0\n  mainApplicationFile: local:///opt/spark/examples/src/main/python/pi.py\n  arguments: [\"1000\"]\n  sparkVersion: \"3.5.0\"\n  \n  restartPolicy:\n    type: Never\n  \n  driver:\n    cores: 1\n    memory: \"1g\"\n    serviceAccount: spark-sa\n  \n  executor:\n    cores: 1\n    instances: 2\n    memory: \"1g\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#autoscaling-resource-management",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#autoscaling-resource-management",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "7. Autoscaling & Resource Management",
    "text": "7. Autoscaling & Resource Management\n\n7.1 Dynamic Allocation (Spark natif)\nSpark peut ajuster dynamiquement le nombre dâ€™executors.\n\n\nVoir le code\ndynamic_allocation_config = \"\"\"\n# Dans sparkConf ou spark-submit\nspark.dynamicAllocation.enabled=true\nspark.dynamicAllocation.minExecutors=1\nspark.dynamicAllocation.maxExecutors=10\nspark.dynamicAllocation.initialExecutors=2\nspark.dynamicAllocation.executorIdleTimeout=60s\nspark.dynamicAllocation.schedulerBacklogTimeout=1s\n\n# Shuffle tracking (requis pour K8s)\nspark.dynamicAllocation.shuffleTracking.enabled=true\nspark.dynamicAllocation.shuffleTracking.timeout=600s\n\"\"\"\nprint(dynamic_allocation_config)\n\n\n\n\n7.2 Options dâ€™autoscaling K8s\n\n\n\n\n\n\n\n\nType\nDescription\nUse case\n\n\n\n\nDynamic Allocation\nSpark gÃ¨re les executors\nâœ… RecommandÃ© pour batch\n\n\nHPA\nScale sur CPU/memory\nâš ï¸ Pas idÃ©al pour Spark\n\n\nVPA\nAjuste les ressources\nğŸ’¡ Pour dimensionnement initial\n\n\nKEDA\nScale sur Ã©vÃ©nements externes\nâœ… IdÃ©al pour streaming (Kafka lag)\n\n\nCluster Autoscaler\nAjoute/retire des nodes\nâœ… CombinÃ© avec Dynamic Allocation\n\n\n\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/keda-scaledobject.yaml\n# KEDA ScaledObject pour Spark Streaming basÃ© sur Kafka lag\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: spark-streaming-scaler\n  namespace: spark\nspec:\n  scaleTargetRef:\n    name: spark-streaming  # Deployment Ã  scaler\n  minReplicaCount: 1\n  maxReplicaCount: 10\n  triggers:\n    - type: kafka\n      metadata:\n        bootstrapServers: kafka.kafka.svc.cluster.local:9092\n        consumerGroup: spark-streaming-group\n        topic: events\n        lagThreshold: \"100\"  # Scale si lag &gt; 100\n\n\n\n\n7.3 Node Affinity & Tolerations\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/spark-with-affinity.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: gpu-job\n  namespace: spark\nspec:\n  type: Python\n  mode: cluster\n  image: my-registry/spark-gpu:1.0.0\n  mainApplicationFile: local:///app/gpu_job.py\n  sparkVersion: \"3.5.0\"\n  \n  driver:\n    cores: 1\n    memory: \"2g\"\n    serviceAccount: spark-sa\n  \n  executor:\n    cores: 4\n    instances: 2\n    memory: \"8g\"\n    # Placer les executors sur des nodes avec GPU\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - gpu\n    # TolÃ©rer les taints GPU\n    tolerations:\n      - key: \"nvidia.com/gpu\"\n        operator: \"Exists\"\n        effect: \"NoSchedule\"\n\n\n\n\nExercice 6 : Configurer Dynamic Allocation\nObjectif : Modifier la SparkApplication pour avoir :\n\nMin 2 executors\nMax 8 executors\nInitial 3 executors\n\n\n\nğŸ’¡ Solution\n\nsparkConf:\n  \"spark.dynamicAllocation.enabled\": \"true\"\n  \"spark.dynamicAllocation.minExecutors\": \"2\"\n  \"spark.dynamicAllocation.maxExecutors\": \"8\"\n  \"spark.dynamicAllocation.initialExecutors\": \"3\"\n  \"spark.dynamicAllocation.shuffleTracking.enabled\": \"true\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#monitoring-observability",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#monitoring-observability",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "8. Monitoring & Observability",
    "text": "8. Monitoring & Observability\n\n8.1 Spark UI\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/spark-ui-ingress.yaml\n# Service pour Spark UI\napiVersion: v1\nkind: Service\nmetadata:\n  name: spark-ui\n  namespace: spark\nspec:\n  selector:\n    spark-role: driver\n  ports:\n    - port: 4040\n      targetPort: 4040\n  type: ClusterIP\n---\n# Ingress pour accÃ¨s externe\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: spark-ui-ingress\n  namespace: spark\nspec:\n  rules:\n    - host: spark-ui.mycompany.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: spark-ui\n                port:\n                  number: 4040\n\n\n\n\n8.2 Spark History Server\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/history-server.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: spark-history-server\n  namespace: spark\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: spark-history-server\n  template:\n    metadata:\n      labels:\n        app: spark-history-server\n    spec:\n      containers:\n        - name: history-server\n          image: bitnami/spark:3.5\n          command:\n            - /opt/bitnami/spark/sbin/start-history-server.sh\n          env:\n            - name: SPARK_HISTORY_OPTS\n              value: \"-Dspark.history.fs.logDirectory=s3a://spark-logs/history\"\n          ports:\n            - containerPort: 18080\n          envFrom:\n            - secretRef:\n                name: minio-credentials\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: spark-history-server\n  namespace: spark\nspec:\n  selector:\n    app: spark-history-server\n  ports:\n    - port: 18080\n      targetPort: 18080\n  type: ClusterIP\n\n\n\n\nExercice 7 : Exposer Spark UI\nObjectif : AccÃ©der au Spark UI dâ€™un job en cours.\n# 1. Lister les pods driver\nkubectl get pods -n spark -l spark-role=driver\n\n# 2. Port-forward\nkubectl port-forward -n spark &lt;driver-pod&gt; 4040:4040\n\n# 3. Ouvrir http://localhost:4040",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#debugging-troubleshooting",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#debugging-troubleshooting",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "9. Debugging & Troubleshooting",
    "text": "9. Debugging & Troubleshooting\n\n9.1 Erreurs courantes et solutions\n\n\n\n\n\n\n\n\nErreur\nCause\nSolution\n\n\n\n\nImagePullBackOff\nImage introuvable\nVÃ©rifier registry, credentials, tag\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter memory, rÃ©duire donnÃ©es\n\n\nForbidden\nRBAC manquant\nVÃ©rifier Role/RoleBinding\n\n\nPending (pods)\nRessources insuffisantes\nVÃ©rifier quotas, cluster capacity\n\n\nConnection refused\nDriver inaccessible\nVÃ©rifier network policies, services\n\n\nClassNotFoundException\nJAR manquant\nAjouter dans lâ€™image Docker\n\n\n\n\n\nVoir le code\ndebug_commands = \"\"\"\n# === Debugging Spark on K8s ===\n\n# 1. Voir les pods et leur statut\nkubectl get pods -n spark -o wide\n\n# 2. DÃ©tails d'un pod (Ã©vÃ©nements)\nkubectl describe pod &lt;pod-name&gt; -n spark\n\n# 3. Logs du driver\nkubectl logs -n spark -l spark-role=driver\n\n# 4. Logs d'un executor spÃ©cifique\nkubectl logs -n spark &lt;executor-pod-name&gt;\n\n# 5. Logs en temps rÃ©el (follow)\nkubectl logs -n spark -l spark-role=driver -f\n\n# 6. Shell dans un pod (debug)\nkubectl exec -it &lt;pod-name&gt; -n spark -- /bin/bash\n\n# 7. VÃ©rifier les ressources du namespace\nkubectl describe resourcequota -n spark\n\n# 8. VÃ©rifier les events\nkubectl get events -n spark --sort-by='.lastTimestamp'\n\n# 9. SparkApplication status\nkubectl get sparkapplication -n spark\nkubectl describe sparkapplication &lt;name&gt; -n spark\n\"\"\"\nprint(debug_commands)\n\n\n\n\n9.2 Debugging OOMKilled\nğŸ” SymptÃ´me : Pod executor en Ã©tat OOMKilled\n\nCauses possibles :\nâ”œâ”€â”€ spark.executor.memory trop bas\nâ”œâ”€â”€ spark.executor.memoryOverhead mal configurÃ©\nâ”œâ”€â”€ Trop de donnÃ©es par partition\nâ”œâ”€â”€ Broadcast variables trop grandes\nâ””â”€â”€ Fuite mÃ©moire dans le code\n\nSolutions :\nâ”œâ”€â”€ Augmenter spark.executor.memory\nâ”œâ”€â”€ spark.executor.memoryOverhead = max(0.1 Ã— memory, 384m)\nâ”œâ”€â”€ Repartitionner : df.repartition(200)\nâ”œâ”€â”€ RÃ©duire spark.sql.shuffle.partitions\nâ””â”€â”€ Utiliser spark.memory.fraction = 0.6 (default)\n\n\n9.3 Debugging Shuffle failures\nLe shuffle est plus coÃ»teux sur K8s (pas de data locality).\n\n\nVoir le code\nshuffle_optimizations = \"\"\"\n# Optimisations shuffle pour K8s\n\n# Augmenter les timeouts\nspark.network.timeout=600s\nspark.shuffle.io.maxRetries=10\nspark.shuffle.io.retryWait=30s\n\n# Compression\nspark.shuffle.compress=true\nspark.shuffle.spill.compress=true\n\n# Buffer sizes\nspark.shuffle.file.buffer=64k\nspark.reducer.maxSizeInFlight=96m\n\n# Shuffle tracking (pour Dynamic Allocation)\nspark.dynamicAllocation.shuffleTracking.enabled=true\n\"\"\"\nprint(shuffle_optimizations)\n\n\n\n\nExercice 8 : Diagnostiquer un job qui Ã©choue\nScÃ©nario : Un job Spark Ã©choue avec le message â€œPod OOMKilledâ€.\nQuestions :\n\nQuelle commande pour voir les events du pod ?\nQuelles configs Spark vÃ©rifier ?\n\n\n\nğŸ’¡ Solution\n\n\nCommandes :\n\nkubectl describe pod &lt;pod-name&gt; -n spark\nkubectl get events -n spark --field-selector involvedObject.name=&lt;pod-name&gt;\n\nConfigs Ã  vÃ©rifier :\n\nspark.executor.memory\nspark.executor.memoryOverhead\nspark.kubernetes.executor.limit.memory",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#optimisations-spark-on-k8s",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#optimisations-spark-on-k8s",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "10. Optimisations Spark on K8s",
    "text": "10. Optimisations Spark on K8s\n\n10.1 Data Locality\nâš ï¸ ProblÃ¨me : Pas de data locality sur K8s\n\nSur YARN avec HDFS :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Node 1               â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚ â”‚Executorâ”‚â—€â”‚ Data  â”‚ â”‚  â† Data local (fast)\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nSur K8s avec Object Storage :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Node 1               â”‚      â”‚ S3/MinIO    â”‚\nâ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚â—€â”€â”€â”€â”€â”€â”‚ (remote)    â”‚  â† Network transfer\nâ”‚ â”‚Executorâ”‚           â”‚      â”‚             â”‚\nâ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nSolutions :\n\nUtiliser des formats colonnaires (Parquet) optimisÃ©s\nConfigurer S3A/GCS pour le parallÃ©lisme\nExternal Shuffle Service (Spark 3.4+)\n\n\n\nVoir le code\ns3a_config = \"\"\"\n# Optimisations S3A pour Spark on K8s\n\n# ParallÃ©lisme\nspark.hadoop.fs.s3a.connection.maximum=200\nspark.hadoop.fs.s3a.threads.max=64\nspark.hadoop.fs.s3a.threads.core=16\n\n# Fast upload\nspark.hadoop.fs.s3a.fast.upload=true\nspark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer\nspark.hadoop.fs.s3a.multipart.size=104857600  # 100MB\n\n# Committer (Ã©viter les renames S3)\nspark.sql.sources.commitProtocolClass=org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\nspark.sql.parquet.output.committer.class=org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n\"\"\"\nprint(s3a_config)\n\n\n\n\n10.2 Pod Templates\nPour des configurations avancÃ©es des pods Spark.\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/executor-pod-template.yaml\napiVersion: v1\nkind: Pod\nspec:\n  containers:\n    - name: spark-executor\n      volumeMounts:\n        - name: spark-local-dir\n          mountPath: /tmp/spark\n      resources:\n        requests:\n          ephemeral-storage: \"10Gi\"\n        limits:\n          ephemeral-storage: \"20Gi\"\n  volumes:\n    - name: spark-local-dir\n      emptyDir:\n        medium: Memory  # Utiliser RAM pour shuffle local\n        sizeLimit: \"4Gi\"\n\n\n\n\nVoir le code\npod_template_usage = \"\"\"\n# Utiliser le pod template dans spark-submit\nspark-submit \\\n  --conf spark.kubernetes.executor.podTemplateFile=/path/to/executor-pod-template.yaml \\\n  ...\n\n# Ou dans SparkApplication\nspec:\n  executor:\n    podTemplateFile: /path/to/executor-pod-template.yaml\n\"\"\"\nprint(pod_template_usage)\n\n\n\n\n10.3 Spot/Preemptible Instances\nÃ‰conomiser jusquâ€™Ã  90% sur les coÃ»ts compute.\n\n\nVoir le code\n%%writefile /tmp/spark-k8s/spark-with-spot.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spot-etl\n  namespace: spark\nspec:\n  type: Python\n  mode: cluster\n  image: my-registry/spark-app:1.0.0\n  mainApplicationFile: local:///app/etl.py\n  sparkVersion: \"3.5.0\"\n  \n  # Driver sur nodes stables (on-demand)\n  driver:\n    cores: 1\n    memory: \"2g\"\n    serviceAccount: spark-sa\n    affinity:\n      nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          nodeSelectorTerms:\n            - matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - on-demand\n  \n  # Executors sur spot instances\n  executor:\n    cores: 2\n    instances: 5\n    memory: \"4g\"\n    affinity:\n      nodeAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n                - key: node-type\n                  operator: In\n                  values:\n                    - spot\n    tolerations:\n      - key: \"kubernetes.azure.com/scalesetpriority\"\n        operator: \"Equal\"\n        value: \"spot\"\n        effect: \"NoSchedule\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#mini-projet-etl-pipeline-sur-kubernetes",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#mini-projet-etl-pipeline-sur-kubernetes",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "11. Mini-Projet : ETL Pipeline sur Kubernetes",
    "text": "11. Mini-Projet : ETL Pipeline sur Kubernetes\n\nObjectif\nDÃ©ployer un pipeline Spark complet sur K8s avec MinIO (Object Storage local).\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     Kubernetes Cluster                          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚   MinIO     â”‚     â”‚      Spark       â”‚     â”‚   MinIO     â”‚  â”‚\nâ”‚  â”‚  (source)   â”‚â”€â”€â”€â”€â–¶â”‚    on K8s        â”‚â”€â”€â”€â”€â–¶â”‚  (output)   â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚                  â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚ bucket:     â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â”‚ bucket:     â”‚  â”‚\nâ”‚  â”‚ bronze/     â”‚     â”‚  â”‚  Driver    â”‚  â”‚     â”‚ silver/     â”‚  â”‚\nâ”‚  â”‚   data.csv  â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚   data.parquetâ”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”   â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â”‚Execâ”‚ â”‚Execâ”‚   â”‚     â”‚             â”‚  â”‚\nâ”‚  â”‚             â”‚     â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜   â”‚     â”‚             â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nStructure du projet\nspark-k8s-project/\nâ”œâ”€â”€ docker/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â””â”€â”€ requirements.txt\nâ”œâ”€â”€ app/\nâ”‚   â””â”€â”€ etl_job.py\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ rbac.yaml\nâ”‚   â”œâ”€â”€ minio-secret.yaml\nâ”‚   â””â”€â”€ spark-application.yaml\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sample.csv\nâ””â”€â”€ README.md\n\n\nÃ‰tape 1 : DÃ©ployer MinIO sur K8s\n\n\nVoir le code\nminio_deploy = \"\"\"\n# DÃ©ployer MinIO avec Helm\nhelm repo add minio https://charts.min.io/\nhelm repo update\n\nhelm install minio minio/minio \\\n  --namespace minio \\\n  --create-namespace \\\n  --set rootUser=minioadmin \\\n  --set rootPassword=minioadmin \\\n  --set mode=standalone \\\n  --set resources.requests.memory=512Mi \\\n  --set persistence.size=10Gi\n\n# Port-forward pour accÃ©der Ã  la console\nkubectl port-forward -n minio svc/minio-console 9001:9001\n\n# CrÃ©er les buckets\nmc alias set myminio http://localhost:9000 minioadmin minioadmin\nmc mb myminio/bronze\nmc mb myminio/silver\n\n# Uploader des donnÃ©es de test\nmc cp data/sample.csv myminio/bronze/\n\"\"\"\nprint(minio_deploy)\n\n\n\n\nÃ‰tape 2 : Code de lâ€™application ETL\n\n\nVoir le code\n%%writefile /tmp/spark-k8s-project/app/etl_job.py\n\"\"\"\nETL Job : Bronze â†’ Silver\nLit des CSV depuis MinIO, transforme, et Ã©crit en Parquet.\n\"\"\"\nimport argparse\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, upper, current_timestamp\n\n\ndef create_spark_session():\n    \"\"\"CrÃ©er une SparkSession configurÃ©e pour MinIO.\"\"\"\n    return SparkSession.builder \\\n        .appName(\"ETL Bronze to Silver\") \\\n        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n        .getOrCreate()\n\n\ndef extract(spark, input_path):\n    \"\"\"Lire les donnÃ©es brutes depuis le bucket Bronze.\"\"\"\n    print(f\"ğŸ“¥ Reading from {input_path}\")\n    df = spark.read \\\n        .option(\"header\", \"true\") \\\n        .option(\"inferSchema\", \"true\") \\\n        .csv(input_path)\n    print(f\"   Loaded {df.count()} rows\")\n    return df\n\n\ndef transform(df):\n    \"\"\"Appliquer les transformations mÃ©tier.\"\"\"\n    print(\"ğŸ”„ Transforming data\")\n    transformed = df \\\n        .dropDuplicates() \\\n        .dropna() \\\n        .withColumn(\"processed_at\", current_timestamp())\n    \n    # Normalisation des colonnes string\n    for col_name, dtype in transformed.dtypes:\n        if dtype == \"string\":\n            transformed = transformed.withColumn(col_name, upper(col(col_name)))\n    \n    print(f\"   Transformed to {transformed.count()} rows\")\n    return transformed\n\n\ndef load(df, output_path):\n    \"\"\"Ã‰crire les donnÃ©es transformÃ©es dans le bucket Silver.\"\"\"\n    print(f\"ğŸ“¤ Writing to {output_path}\")\n    df.write \\\n        .mode(\"overwrite\") \\\n        .parquet(output_path)\n    print(\"   Done!\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"ETL Job\")\n    parser.add_argument(\"--input\", required=True, help=\"Input path (s3a://...)\")\n    parser.add_argument(\"--output\", required=True, help=\"Output path (s3a://...)\")\n    args = parser.parse_args()\n    \n    spark = create_spark_session()\n    \n    try:\n        # ETL Pipeline\n        df = extract(spark, args.input)\n        transformed = transform(df)\n        load(transformed, args.output)\n        \n        print(\"âœ… ETL completed successfully!\")\n    except Exception as e:\n        print(f\"âŒ ETL failed: {e}\")\n        raise\n    finally:\n        spark.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\nÃ‰tape 3 : Dockerfile\n\n\nVoir le code\n%%writefile /tmp/spark-k8s-project/docker/Dockerfile\nFROM bitnami/spark:3.5\n\nUSER root\n\n# DÃ©pendances Python\nRUN pip install --no-cache-dir pyarrow pandas\n\n# JARs pour S3/MinIO\nRUN curl -sL https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar \\\n    -o /opt/bitnami/spark/jars/hadoop-aws-3.3.4.jar && \\\n    curl -sL https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar \\\n    -o /opt/bitnami/spark/jars/aws-java-sdk-bundle-1.12.262.jar\n\n# Application\nCOPY app/ /app/\nRUN chown -R 1001:1001 /app\n\nUSER 1001\nWORKDIR /app\n\n\n\n\nÃ‰tape 4 : Manifests Kubernetes\n\n\nVoir le code\n%%writefile /tmp/spark-k8s-project/manifests/spark-application.yaml\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: etl-bronze-silver\n  namespace: spark\nspec:\n  type: Python\n  pythonVersion: \"3\"\n  mode: cluster\n  image: spark-etl:1.0.0  # Image locale (Minikube)\n  imagePullPolicy: Never  # Pour Minikube\n  mainApplicationFile: local:///app/etl_job.py\n  arguments:\n    - \"--input\"\n    - \"s3a://bronze/sample.csv\"\n    - \"--output\"\n    - \"s3a://silver/data\"\n  sparkVersion: \"3.5.0\"\n  \n  sparkConf:\n    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio.minio.svc.cluster.local:9000\"\n    \"spark.hadoop.fs.s3a.path.style.access\": \"true\"\n    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\"\n  \n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 2\n  \n  driver:\n    cores: 1\n    memory: \"1g\"\n    serviceAccount: spark-sa\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n  \n  executor:\n    cores: 1\n    instances: 2\n    memory: \"1g\"\n    envSecretKeyRefs:\n      AWS_ACCESS_KEY_ID:\n        name: minio-credentials\n        key: AWS_ACCESS_KEY_ID\n      AWS_SECRET_ACCESS_KEY:\n        name: minio-credentials\n        key: AWS_SECRET_ACCESS_KEY\n\n\n\n\nÃ‰tape 5 : DÃ©ploiement complet\n\n\nVoir le code\ndeployment_script = \"\"\"\n#!/bin/bash\nset -e\n\necho \"ğŸš€ DÃ©ploiement du pipeline ETL Spark on K8s\"\n\n# 1. CrÃ©er le namespace et RBAC\necho \"ğŸ“ CrÃ©ation namespace et RBAC...\"\nkubectl apply -f manifests/namespace.yaml\nkubectl apply -f manifests/rbac.yaml\nkubectl apply -f manifests/minio-secret.yaml\n\n# 2. Build de l'image (Minikube)\necho \"ğŸ³ Build de l'image Docker...\"\neval $(minikube docker-env)\ndocker build -t spark-etl:1.0.0 -f docker/Dockerfile .\n\n# 3. VÃ©rifier que MinIO est prÃªt\necho \"â³ Attente de MinIO...\"\nkubectl wait --for=condition=ready pod -l app=minio -n minio --timeout=120s\n\n# 4. Lancer le job Spark\necho \"ğŸ”¥ Lancement du job Spark...\"\nkubectl apply -f manifests/spark-application.yaml\n\n# 5. Suivre le job\necho \"ğŸ“Š Suivi du job...\"\nkubectl get sparkapplication -n spark -w\n\"\"\"\nprint(deployment_script)\n\n\n\n\nÃ‰tape 6 : VÃ©rification\n\n\nVoir le code\nverify_commands = \"\"\"\n# VÃ©rifier le statut du job\nkubectl get sparkapplication -n spark\n\n# Voir les logs du driver\nkubectl logs -n spark -l spark-role=driver\n\n# VÃ©rifier les outputs dans MinIO\nmc ls myminio/silver/data/\n\n# Lire un sample des donnÃ©es\nmc cat myminio/silver/data/part-00000.parquet | head\n\"\"\"\nprint(verify_commands)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#quiz-de-fin-de-module",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ§ª Quiz de fin de module",
    "text": "ğŸ§ª Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre Client mode et Cluster mode ?\n\nClient mode est plus rapide\n\nEn Cluster mode, le Driver tourne dans un Pod K8s\n\nClient mode ne supporte pas les executors\n\nCluster mode nÃ©cessite YARN\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” En Cluster mode, le Driver est un Pod K8s. En Client mode, il reste sur la machine locale.\n\n\n\n\nâ“ Q2. Pourquoi utiliser Spark Operator plutÃ´t que spark-submit directement ?\n\nSpark Operator est plus rapide\n\nspark-submit ne fonctionne pas sur K8s\n\nSpark Operator permet des manifestes YAML dÃ©claratifs et des retries automatiques\n\nSpark Operator ne nÃ©cessite pas de ServiceAccount\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Spark Operator offre une approche dÃ©clarative (YAML), des retry policies, du scheduling, et une meilleure intÃ©gration CI/CD.\n\n\n\n\nâ“ Q3. Quel problÃ¨me rÃ©sout Dynamic Allocation ?\n\nLa sÃ©curitÃ© des pods\n\nLâ€™ajustement automatique du nombre dâ€™executors\n\nLe stockage des logs\n\nLa communication rÃ©seau\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Dynamic Allocation permet Ã  Spark dâ€™ajuster automatiquement le nombre dâ€™executors selon la charge.\n\n\n\n\nâ“ Q4. Quelle config est requise pour Dynamic Allocation sur K8s ?\n\nspark.dynamicAllocation.enabled=true uniquement\n\nspark.dynamicAllocation.shuffleTracking.enabled=true\n\nspark.kubernetes.allocation.batch.size=5\n\nAucune config spÃ©ciale\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Sur K8s (sans External Shuffle Service), shuffle tracking est requis pour que Dynamic Allocation fonctionne.\n\n\n\n\nâ“ Q5. Que signifie lâ€™erreur OOMKilled ?\n\nLe pod nâ€™a pas dâ€™image\n\nLe pod a dÃ©passÃ© sa limite mÃ©moire\n\nLe rÃ©seau est indisponible\n\nLe ServiceAccount est invalide\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” OOMKilled signifie que le container a dÃ©passÃ© sa limite mÃ©moire et a Ã©tÃ© tuÃ© par K8s.\n\n\n\n\nâ“ Q6. Quel est lâ€™avantage principal de K8s sur YARN pour Spark ?\n\nMeilleure data locality\n\nInfrastructure cloud-native et multi-cloud\n\nPlus rapide\n\nMoins de configuration\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” K8s offre une infrastructure cloud-native, standardisÃ©e, multi-cloud, avec autoscaling avancÃ©.\n\n\n\n\nâ“ Q7. Comment exposer le Spark UI dâ€™un job en cluster mode ?\n\nIl est automatiquement accessible sur localhost:4040\n\nVia port-forward, Service, ou Ingress\n\nVia YARN ResourceManager\n\nCe nâ€™est pas possible en cluster mode\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” On utilise kubectl port-forward, un Service K8s, ou un Ingress pour exposer le Spark UI.\n\n\n\n\nâ“ Q8. Pourquoi le shuffle est-il plus coÃ»teux sur K8s que sur YARN/HDFS ?\n\nK8s est plus lent\n\nPas de data locality â€” les donnÃ©es doivent transiter par le rÃ©seau\n\nSpark nâ€™est pas optimisÃ© pour K8s\n\nLes executors ont moins de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Sur YARN/HDFS, les donnÃ©es peuvent Ãªtre locales. Sur K8s avec Object Storage, tout passe par le rÃ©seau.\n\n\n\n\nâ“ Q9. Quel est le rÃ´le du ServiceAccount dans Spark on K8s ?\n\nStocker les credentials\n\nPermettre au Driver de crÃ©er des Executor Pods\n\nGÃ©rer le Spark UI\n\nConfigurer le rÃ©seau\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le ServiceAccount donne au Driver les permissions RBAC pour crÃ©er, lister et supprimer des Pods (executors).\n\n\n\n\nâ“ Q10. Comment Ã©conomiser sur les coÃ»ts compute avec Spark on K8s ?\n\nUtiliser moins dâ€™executors\n\nDÃ©sactiver le monitoring\n\nUtiliser des Spot/Preemptible instances pour les executors\n\nNe pas utiliser de PVC\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les Spot instances peuvent coÃ»ter jusquâ€™Ã  90% moins cher. Le Driver reste sur des nodes stables, les executors sur Spot.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#ressources-pour-aller-plus-loin",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark on Kubernetes\nSpark Operator GitHub\nSpark Operator User Guide\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Spark on K8s Best Practices\nGoogle Cloud - Running Spark on GKE\nAWS - EMR on EKS",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/21_spark_on_kubernetes.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/21_spark_on_kubernetes.html#prochaine-Ã©tape",
    "title": "Spark on Kubernetes â€” Production-Grade Deployment",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu sais dÃ©ployer Spark sur Kubernetes, passons au Cloud et Object Storage !\nğŸ‘‰ Module suivant : 22_cloud_object_storage â€” Cloud & Object Storage\nTu vas apprendre :\n\nCloud Computing : IaaS, PaaS, SaaS\nAWS, GCP, Azure : Services Data Engineering\nObject Storage : S3, GCS, Azure Blob\nMinIO : Pratiquer localement\n\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nArchitecture\nDriver/Executor Pods, Client vs Cluster mode\n\n\nDocker Image\nBuild, JARs, best practices\n\n\nK8s Config\nRBAC, Secrets, PVC\n\n\nspark-submit\nConfigurations essentielles\n\n\nSpark Operator\nSparkApplication, ScheduledSparkApplication\n\n\nAutoscaling\nDynamic Allocation, KEDA\n\n\nMonitoring\nSpark UI, Prometheus, Grafana\n\n\nDebugging\nErreurs courantes, commandes kubectl\n\n\nOptimisations\nS3A config, Pod templates, Spot instances\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Spark on Kubernetes.\n\n\nVoir le code\n# Commandes de nettoyage\ncleanup_commands = \"\"\"\n# Supprimer les ressources Spark\nkubectl delete sparkapplication --all -n spark\nkubectl delete namespace spark\n\n# Supprimer Spark Operator\nhelm uninstall spark-operator -n spark-operator\nkubectl delete namespace spark-operator\n\n# Supprimer MinIO\nhelm uninstall minio -n minio\nkubectl delete namespace minio\n\n# ArrÃªter Minikube (optionnel)\nminikube stop\n\"\"\"\nprint(cleanup_commands)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "21 Â· Spark on Kubernetes"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html",
    "title": "Kubernetes pour Workloads Data",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  dÃ©ployer et gÃ©rer des charges de travail data sur Kubernetes. Tu dÃ©couvriras les patterns avancÃ©s pour les Jobs ETL, les bases de donnÃ©es, le scaling et le monitoring â€” le tout appliquÃ© au Data Engineering !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#prÃ©requis",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#prÃ©requis",
    "title": "Kubernetes pour Workloads Data",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi le module 15_kubernetes_fundamentals\n\n\nâœ… Requis\nMaÃ®triser Pod, Deployment, Service, ConfigMap, Secret, PVC\n\n\nâœ… Requis\nSavoir utiliser kubectl\n\n\nğŸ’¡ RecommandÃ©\nCluster K8s local fonctionnel (Docker Desktop ou Minikube)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#objectifs-du-module",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#objectifs-du-module",
    "title": "Kubernetes pour Workloads Data",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre ce quâ€™est un workload data et ses caractÃ©ristiques\nConfigurer des Jobs et CronJobs avancÃ©s pour des ETL robustes\nDÃ©ployer des bases de donnÃ©es avec StatefulSets\nUtiliser Helm pour dÃ©ployer des stacks data\nGÃ©rer le scaling et les ressources pour des workloads gourmands\nMettre en place le monitoring de tes pipelines\nAvoir un aperÃ§u de Spark et Airflow sur Kubernetes",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#cest-quoi-un-workload-data",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#cest-quoi-un-workload-data",
    "title": "Kubernetes pour Workloads Data",
    "section": "Câ€™est quoi un â€œWorkload Dataâ€ ?",
    "text": "Câ€™est quoi un â€œWorkload Dataâ€ ?\n\nğŸ“Š Un workload data (charge de travail data) dÃ©signe toute tÃ¢che ou application dÃ©diÃ©e au traitement, transformation, ou dÃ©placement de donnÃ©es.\n\nEn Data Engineering, les workloads typiques incluent :\n\n\n\n\n\n\n\n\nType\nDescription\nExemple concret\n\n\n\n\nBatch ETL\nTraitement planifiÃ© de donnÃ©es\nJob Python qui transforme des CSV chaque nuit\n\n\nIngestion\nChargement de donnÃ©es dans un systÃ¨me\nCSV â†’ PostgreSQL, API â†’ Data Lake\n\n\nTransformation\nCalculs et agrÃ©gations\nJointures, agrÃ©gations, nettoyage\n\n\nProcessing lourd\nCalculs intensifs en ressources\nFeature engineering, ML preprocessing\n\n\nOrchestration\nCoordination de plusieurs tÃ¢ches\nDAG Airflow avec 10 Ã©tapes\n\n\n\n\nCaractÃ©ristiques des workloads data\n\n\n\n\n\n\n\nCaractÃ©ristique\nExplication\n\n\n\n\nÃ‰phÃ©mÃ¨res\nSâ€™exÃ©cutent puis se terminent (run-to-completion)\n\n\nGourmands\nBesoin de CPU et RAM significatifs\n\n\nI/O intensifs\nLecture/Ã©criture de grandes quantitÃ©s de donnÃ©es\n\n\nPlanifiÃ©s\nSouvent exÃ©cutÃ©s selon un schedule (quotidien, horaire)\n\n\nReproductibles\nDoivent pouvoir Ãªtre relancÃ©s en cas dâ€™Ã©chec\n\n\n\n\n\nWorkloads data vs Applications classiques\n\n\n\nAspect\nApplication web\nWorkload data\n\n\n\n\nDurÃ©e de vie\nContinue (24/7)\nÃ‰phÃ©mÃ¨re (minutes/heures)\n\n\nRessource K8s\nDeployment\nJob / CronJob\n\n\nScaling\nHorizontal (replicas)\nVertical (plus de RAM/CPU)\n\n\nÃ‰tat final\nToujours running\nCompleted ou Failed\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nLe terme â€œworkloadâ€ vient du monde des mainframes IBM des annÃ©es 1960, oÃ¹ il dÃ©signait la quantitÃ© de travail quâ€™une machine devait traiter.\nAujourdâ€™hui, dans le contexte cloud-native et Kubernetes, un workload dÃ©signe toute unitÃ© de travail dÃ©ployÃ©e sur un cluster : une app web, un job batch, un service de streaming, etc.\nLes â€œdata workloadsâ€ sont simplement les workloads spÃ©cialisÃ©s dans le traitement de donnÃ©es !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#rappels-kubernetes-essentiels",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#rappels-kubernetes-essentiels",
    "title": "Kubernetes pour Workloads Data",
    "section": "1. Rappels Kubernetes essentiels",
    "text": "1. Rappels Kubernetes essentiels\nAvant dâ€™aller plus loin, voici un rÃ©capitulatif rapide des concepts K8s vus dans le module prÃ©cÃ©dent :\n\n\n\n\n\n\n\n\nRessource\nRÃ´le\nUsage Data Engineering\n\n\n\n\nPod\nUnitÃ© de base (1+ containers)\nExÃ©cute ton script ETL\n\n\nDeployment\nGÃ¨re des replicas de pods\nApps long-running (API, workers)\n\n\nJob\nTÃ¢che one-shot\nETL ponctuel, migration\n\n\nCronJob\nJob planifiÃ©\nETL quotidien, rapport hebdo\n\n\nService\nExpose des pods\nAccÃ¨s Ã  PostgreSQL, APIs\n\n\nConfigMap\nConfig non sensible\nChemins, URLs, paramÃ¨tres\n\n\nSecret\nConfig sensible\nPasswords, API keys\n\n\nPVC\nStockage persistant\nDonnÃ©es PostgreSQL, fichiers\n\n\nNamespace\nIsolation logique\nUn namespace par projet\n\n\n\n\nğŸ’¡ Si ces concepts ne sont pas clairs, revois le module 15_kubernetes_fundamentals avant de continuer.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#jobs-cronjobs-avancÃ©s",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#jobs-cronjobs-avancÃ©s",
    "title": "Kubernetes pour Workloads Data",
    "section": "2. Jobs & CronJobs avancÃ©s",
    "text": "2. Jobs & CronJobs avancÃ©s\nLes Jobs et CronJobs sont les ressources K8s idÃ©ales pour les workloads data.\n\n2.1 Anatomie complÃ¨te dâ€™un Job (ligne par ligne)\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  JOB : TÃ¢che qui s'exÃ©cute jusqu'Ã  complÃ©tion                            â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: batch/v1              # API pour les Jobs et CronJobs\nkind: Job                         # Type = Job (tÃ¢che one-shot)\n\nmetadata:\n  name: etl-advanced-job          # Nom unique du job\n  namespace: data-pipeline        # Namespace cible\n  labels:\n    app: etl                      # Labels pour filtrer/monitorer\n    team: data-engineering\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  SPEC DU JOB : ParamÃ¨tres de comportement                                â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nspec:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # COMPLETION ET PARALLÃ‰LISME\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  completions: 1                  # Nombre de succÃ¨s requis pour terminer\n                                  # Ex: 10 = le job doit rÃ©ussir 10 fois\n  \n  parallelism: 1                  # Combien de pods tournent EN MÃŠME TEMPS\n                                  # Ex: 3 = 3 pods en parallÃ¨le\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # GESTION DES Ã‰CHECS\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  backoffLimit: 3                 # Nombre de RETRIES avant Ã©chec dÃ©finitif\n                                  # Le pod sera relancÃ© 3 fois max si erreur\n  \n  activeDeadlineSeconds: 3600     # TIMEOUT global = 1 heure (3600 secondes)\n                                  # Si le job dÃ©passe ce temps â†’ Ã©chec\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # NETTOYAGE AUTOMATIQUE\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  ttlSecondsAfterFinished: 86400  # Auto-suppression 24h aprÃ¨s completion\n                                  # Sans Ã§a, les vieux jobs s'accumulent !\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # TEMPLATE DU POD (ce qui va Ãªtre exÃ©cutÃ©)\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  template:\n    metadata:\n      labels:\n        app: etl                  # Labels du pod (pour monitoring)\n    spec:\n      containers:\n      - name: etl                 # Nom du container\n        image: my-etl:1.0         # Image Docker\n        \n        # Ressources (TOUJOURS les dÃ©finir pour les Jobs !)\n        resources:\n          requests:               # Minimum garanti\n            memory: \"512Mi\"       # 512 Mo de RAM minimum\n            cpu: \"500m\"           # 0.5 CPU minimum\n          limits:                 # Maximum autorisÃ©\n            memory: \"1Gi\"         # 1 Go max (OOMKilled si dÃ©passÃ©)\n            cpu: \"1000m\"          # 1 CPU max (throttled si dÃ©passÃ©)\n      \n      restartPolicy: OnFailure    # Que faire si le container crashe ?\n                                  # OnFailure = relancer le pod\n                                  # Never = ne pas relancer (backoffLimit gÃ¨re les retries)\n\n\nParamÃ¨tres clÃ©s expliquÃ©s\n\n\n\n\n\n\n\n\nParamÃ¨tre\nQue fait-il ?\nValeur typique ETL\n\n\n\n\ncompletions\nCombien de succÃ¨s pour terminer le job ?\n1 (une seule exÃ©cution)\n\n\nparallelism\nCombien de pods en mÃªme temps ?\n1 Ã  N selon le use case\n\n\nbackoffLimit\nCombien de tentatives en cas dâ€™Ã©chec ?\n3 Ã  5\n\n\nactiveDeadlineSeconds\nTimeout global du job\n1800-7200 (30min-2h)\n\n\nttlSecondsAfterFinished\nQuand supprimer le job terminÃ© ?\n86400 (24h)\n\n\nrestartPolicy\nQue faire si crash ?\nOnFailure ou Never\n\n\n\n\n\n2.2 Pattern : Traitement parallÃ¨le de fichiers\nImaginons que tu dois traiter 10 fichiers avec 3 pods en parallÃ¨le :\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  JOB PARALLÃˆLE : Chaque pod traite un fichier diffÃ©rent                  â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: etl-parallel\n  namespace: data-pipeline\n\nspec:\n  completions: 10                 # 10 fichiers Ã  traiter = 10 succÃ¨s requis\n  parallelism: 3                  # 3 pods tournent en mÃªme temps\n  \n  completionMode: Indexed         # â­ IMPORTANT : Mode indexÃ©\n                                  # Chaque pod reÃ§oit un index unique (0, 1, 2, ..., 9)\n                                  # Permet de savoir quel fichier traiter !\n  \n  template:\n    spec:\n      containers:\n      - name: etl\n        image: my-etl:1.0\n        command: [\"python\", \"etl.py\"]\n        \n        env:\n        # â­ RÃ©cupÃ©rer l'index du pod (0, 1, 2, ..., 9)\n        - name: FILE_INDEX\n          valueFrom:\n            fieldRef:             # RÃ©fÃ©rence Ã  un champ du pod lui-mÃªme\n              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n        \n        # Dans ton code Python :\n        # index = os.environ['FILE_INDEX']  # \"0\", \"1\", \"2\", ...\n        # file = f\"data_{index}.csv\"        # data_0.csv, data_1.csv, ...\n        \n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"200m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n      \n      restartPolicy: OnFailure\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  FONCTIONNEMENT DU JOB PARALLÃˆLE INDEXÃ‰                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚   completions: 10     parallelism: 3                           â”‚\nâ”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚\nâ”‚                                                                 â”‚\nâ”‚   Vague 1 : Pod-0, Pod-1, Pod-2 (en parallÃ¨le)                â”‚\nâ”‚   Vague 2 : Pod-3, Pod-4, Pod-5 (quand les prÃ©cÃ©dents finissent)â”‚\nâ”‚   Vague 3 : Pod-6, Pod-7, Pod-8                                â”‚\nâ”‚   Vague 4 : Pod-9                                               â”‚\nâ”‚                                                                 â”‚\nâ”‚   Chaque pod sait quel fichier traiter grÃ¢ce Ã  FILE_INDEX !    â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.3 CronJob avancÃ© (ligne par ligne)\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  CRONJOB : Job qui se lance automatiquement selon un schedule            â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\n  namespace: data-pipeline\n\nspec:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # SCHEDULE CRON (quand lancer le job ?)\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  #\n  # Format cron : minute heure jour-du-mois mois jour-de-semaine\n  #               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ minute (0-59)\n  #               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ heure (0-23)\n  #               â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour du mois (1-31)\n  #               â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ mois (1-12)\n  #               â”‚ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ jour de semaine (0-6, 0=dimanche)\n  #               â”‚ â”‚ â”‚ â”‚ â”‚\n  #               * * * * *\n  #\n  schedule: \"0 2 * * *\"           # Tous les jours Ã  2h00 du matin\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # POLITIQUE DE CONCURRENCE\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  concurrencyPolicy: Forbid       # Que faire si le job prÃ©cÃ©dent tourne encore ?\n                                  #\n                                  # Forbid  = NE PAS lancer le nouveau (skip)\n                                  # Allow   = Lancer quand mÃªme (risque de doublons)\n                                  # Replace = Tuer l'ancien, lancer le nouveau\n                                  #\n                                  # â†’ Pour ETL : Forbid est le plus sÃ»r !\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # HISTORIQUE DES JOBS\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  successfulJobsHistoryLimit: 3   # Garder les 3 derniers jobs rÃ©ussis\n  failedJobsHistoryLimit: 2       # Garder les 2 derniers jobs Ã©chouÃ©s\n                                  # â†’ Permet de voir les logs des runs prÃ©cÃ©dentes\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # TOLÃ‰RANCE AU RETARD\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  startingDeadlineSeconds: 300    # Si le scheduler a du retard, combien de\n                                  # secondes de tolÃ©rance ? (ici 5 minutes)\n                                  # AprÃ¨s ce dÃ©lai, le job est considÃ©rÃ© \"manquÃ©\"\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # TEMPLATE DU JOB (ce qui est crÃ©Ã© Ã  chaque exÃ©cution)\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  jobTemplate:\n    spec:\n      backoffLimit: 3             # 3 retries si Ã©chec\n      activeDeadlineSeconds: 3600 # Timeout 1h\n      \n      template:\n        spec:\n          containers:\n          - name: etl\n            image: my-etl:1.0\n            \n            # Charger la config depuis un ConfigMap\n            envFrom:\n            - configMapRef:\n                name: etl-config  # Toutes les clÃ©s du ConfigMap\n                                  # deviennent des variables d'environnement\n            \n            resources:\n              requests:\n                memory: \"512Mi\"\n                cpu: \"500m\"\n              limits:\n                memory: \"1Gi\"\n                cpu: \"1000m\"\n          \n          restartPolicy: OnFailure\n\n\nExpressions cron courantes\n\n\n\n\n\n\n\n\nExpression\nSignification\nUse case\n\n\n\n\n0 2 * * *\nTous les jours Ã  2h00\nETL quotidien\n\n\n0 */6 * * *\nToutes les 6 heures\nSynchro frÃ©quente\n\n\n*/15 * * * *\nToutes les 15 minutes\nNear real-time\n\n\n0 0 * * 0\nTous les dimanches Ã  minuit\nRapport hebdo\n\n\n0 8 1 * *\nLe 1er de chaque mois Ã  8h\nRapport mensuel\n\n\n0 9-17 * * 1-5\nToutes les heures de 9h Ã  17h, lun-ven\nHeures de bureau\n\n\n\n\n\n2.4 Commandes utiles pour les Jobs\n# Lister les Jobs\nkubectl get jobs -n data-pipeline\n\n# Lister les CronJobs\nkubectl get cronjobs -n data-pipeline\n\n# Voir les dÃ©tails d'un CronJob\nkubectl describe cronjob etl-daily -n data-pipeline\n\n# â­ DÃ©clencher MANUELLEMENT un CronJob (pour tester)\nkubectl create job test-etl --from=cronjob/etl-daily -n data-pipeline\n\n# Voir les logs du job\nkubectl logs job/test-etl -n data-pipeline\n\n# Supprimer un job\nkubectl delete job test-etl -n data-pipeline\n\n\nVoir le code\n%%bash\n# Commandes utiles pour les Jobs\n\necho \"=== Lister les Jobs ===\"\nkubectl get jobs\n\necho \"\"\necho \"=== Lister les CronJobs ===\"\nkubectl get cronjobs\n\necho \"\"\necho \"=== DÃ©clencher manuellement un CronJob ===\"\necho \"kubectl create job test-etl --from=cronjob/etl-daily\"\n\necho \"\"\necho \"=== Voir les logs d'un Job ===\"\necho \"kubectl logs job/etl-job\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#statefulsets-bases-de-donnÃ©es-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#statefulsets-bases-de-donnÃ©es-sur-kubernetes",
    "title": "Kubernetes pour Workloads Data",
    "section": "3. StatefulSets : Bases de donnÃ©es sur Kubernetes",
    "text": "3. StatefulSets : Bases de donnÃ©es sur Kubernetes\nLes StatefulSets sont conÃ§us pour les applications stateful (avec Ã©tat) comme les bases de donnÃ©es.\n\n3.1 Deployment vs StatefulSet : comprendre la diffÃ©rence\n\n\n\n\n\n\n\n\nAspect\nDeployment\nStatefulSet\n\n\n\n\nNom des pods\nAlÃ©atoire (app-7d8f9...)\nStable et ordonnÃ© (app-0, app-1, app-2)\n\n\nStockage\nPVC partagÃ© ou Ã©phÃ©mÃ¨re\nPVC unique par pod (persistant)\n\n\nOrdre de dÃ©marrage\nTous en parallÃ¨le\nSÃ©quentiel : 0 â†’ 1 â†’ 2\n\n\nOrdre dâ€™arrÃªt\nTous en parallÃ¨le\nInverse : 2 â†’ 1 â†’ 0\n\n\nRÃ©seau\nService ClusterIP classique\nHeadless Service (DNS par pod)\n\n\nUsage typique\nApps stateless (API, web)\nBases de donnÃ©es, caches, queues\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  DEPLOYMENT vs STATEFULSET                                              â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  DEPLOYMENT (stateless)          STATEFULSET (stateful)                â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚                                                                         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\nâ”‚  â”‚ app-7d8f9.. â”‚                â”‚   app-0     â”‚ â† IdentitÃ© stable      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚                               â”‚\nâ”‚  â”‚ app-x2k4m.. â”‚                â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   PVC-0     â”‚ â† Volume dÃ©diÃ©         â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\nâ”‚  â”‚ app-p9n3q.. â”‚                                                       â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚\nâ”‚                                  â”‚   app-1     â”‚                        â”‚\nâ”‚  Tous partagent le mÃªme PVC     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                        â”‚\nâ”‚  (ou pas de PVC)                       â”‚                               â”‚\nâ”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                        â”‚\nâ”‚                                  â”‚   PVC-1     â”‚                        â”‚\nâ”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.2 PostgreSQL avec StatefulSet (ligne par ligne)\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  STATEFULSET : Pour applications avec Ã©tat (bases de donnÃ©es)            â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres\n  namespace: data-pipeline\n\nspec:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # CONFIGURATION STATEFULSET\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  serviceName: postgres-headless  # â­ OBLIGATOIRE : Nom du Headless Service\n                                  # Permet le DNS : postgres-0.postgres-headless.data-pipeline.svc\n  \n  replicas: 1                     # Nombre de replicas (1 pour PostgreSQL simple)\n                                  # Pour un cluster PostgreSQL : 3 (1 primary + 2 replicas)\n  \n  selector:\n    matchLabels:\n      app: postgres               # SÃ©lecteur pour identifier les pods\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # TEMPLATE DU POD\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16        # Image officielle PostgreSQL\n        \n        ports:\n        - containerPort: 5432     # Port PostgreSQL standard\n          name: postgres\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # VARIABLES D'ENVIRONNEMENT POSTGRESQL\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        env:\n        - name: POSTGRES_USER\n          value: \"de_user\"        # Utilisateur de la base\n        \n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret  # â­ Mot de passe depuis un Secret\n              key: password          # (ne jamais mettre en clair !)\n        \n        - name: POSTGRES_DB\n          value: \"de_db\"          # Nom de la base crÃ©Ã©e au dÃ©marrage\n        \n        - name: PGDATA\n          value: \"/var/lib/postgresql/data/pgdata\"  # OÃ¹ stocker les donnÃ©es\n                                  # /pgdata est un sous-dossier pour Ã©viter\n                                  # les problÃ¨mes de permissions avec le volume\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # MONTAGE DU VOLUME\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        volumeMounts:\n        - name: postgres-data     # Nom du volume (doit matcher volumeClaimTemplates)\n          mountPath: /var/lib/postgresql/data  # OÃ¹ monter dans le container\n        \n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # RESSOURCES\n        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # VOLUME CLAIM TEMPLATES : Un PVC par pod\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # C'est LA diffÃ©rence majeure avec un Deployment !\n  # Chaque pod obtient son propre PVC persistant :\n  # - postgres-0 â†’ postgres-data-postgres-0\n  # - postgres-1 â†’ postgres-data-postgres-1 (si replicas &gt; 1)\n  #\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-data         # Nom du volume (rÃ©fÃ©rencÃ© dans volumeMounts)\n    spec:\n      accessModes: [\"ReadWriteOnce\"]  # Un seul pod peut Ã©crire Ã  la fois\n      resources:\n        requests:\n          storage: 5Gi            # Taille du disque\n      # storageClassName: fast-ssd  # Optionnel : type de stockage\n\n---\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  HEADLESS SERVICE : Permet le DNS par pod                                â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# \n# Un Headless Service (clusterIP: None) ne fait PAS de load balancing.\n# Il permet d'accÃ©der directement Ã  chaque pod via DNS :\n# - postgres-0.postgres-headless.data-pipeline.svc.cluster.local\n# - postgres-1.postgres-headless.data-pipeline.svc.cluster.local\n#\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-headless         # Ce nom doit matcher spec.serviceName\n  namespace: data-pipeline\nspec:\n  clusterIP: None                 # HEADLESS = pas d'IP de cluster\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    name: postgres\n\n---\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  SERVICE NORMAL : Pour accÃ©der facilement Ã  PostgreSQL                   â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# Ce service classique permet d'accÃ©der Ã  PostgreSQL via :\n# - postgres.data-pipeline.svc.cluster.local:5432\n# - Ou simplement \"postgres\" si tu es dans le mÃªme namespace\n#\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: data-pipeline\nspec:\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  COMMENT ACCÃ‰DER Ã€ POSTGRESQL DANS LE CLUSTER                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  Depuis le mÃªme namespace :                                            â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚\nâ”‚  Host: postgres                                                         â”‚\nâ”‚  Port: 5432                                                             â”‚\nâ”‚                                                                         â”‚\nâ”‚  Depuis un autre namespace :                                           â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚\nâ”‚  Host: postgres.data-pipeline.svc.cluster.local                        â”‚\nâ”‚  Port: 5432                                                             â”‚\nâ”‚                                                                         â”‚\nâ”‚  Connection string Python (SQLAlchemy) :                               â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚\nâ”‚  postgresql://de_user:password@postgres:5432/de_db                     â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n3.3 Le Secret pour le mot de passe\n# postgres-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: postgres-secret\n  namespace: data-pipeline\ntype: Opaque\nstringData:                       # stringData = K8s encode en base64 automatiquement\n  password: \"SuperSecretPassword123\"\n\n\nâš ï¸ Recommandation production\n\nEn production, prÃ©fÃ¨re les services managÃ©s : - AWS RDS / Aurora - GCP Cloud SQL\n- Azure Database for PostgreSQL\nPourquoi ? - Haute disponibilitÃ© automatique - Backups automatiques - Patches de sÃ©curitÃ© - Scaling facile\nLes StatefulSets sont parfaits pour : - DÃ©veloppement local - Tests dâ€™intÃ©gration - Environnements oÃ¹ tu veux tout contrÃ´ler",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#helm-package-manager-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#helm-package-manager-kubernetes",
    "title": "Kubernetes pour Workloads Data",
    "section": "4. Helm : Package Manager Kubernetes",
    "text": "4. Helm : Package Manager Kubernetes\nHelm est le gestionnaire de packages pour Kubernetes â€” comme apt pour Ubuntu ou pip pour Python.\n\n4.1 Pourquoi Helm ?\n\n\n\n\n\n\n\nSans Helm\nAvec Helm\n\n\n\n\n10+ fichiers YAML Ã  gÃ©rer\n1 commande helm install\n\n\nCopier-coller entre environnements\nvalues.yaml pour personnaliser\n\n\nPas de versioning\nRollback facile\n\n\nMise Ã  jour manuelle\nhelm upgrade\n\n\n\n\n\n4.2 Concepts clÃ©s\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nChart\nPackage K8s (templates + values)\nUn package .deb ou .rpm\n\n\nRelease\nInstance dÃ©ployÃ©e dâ€™un chart\nUne installation du package\n\n\nRepository\nMagasin de charts\nUn apt repository\n\n\nValues\nConfiguration personnalisÃ©e\nUn fichier de config\n\n\n\n\n\n4.3 Installation de Helm\n# macOS\nbrew install helm\n\n# Linux\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n# VÃ©rifier\nhelm version\n\n\n4.4 Commandes essentielles\n# Ajouter un repository\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n\n# Rechercher un chart\nhelm search repo postgresql\n\n# Voir les valeurs par dÃ©faut\nhelm show values bitnami/postgresql\n\n# Installer un chart\nhelm install my-postgres bitnami/postgresql \\\n  --namespace data \\\n  --create-namespace \\\n  --set auth.postgresPassword=mypassword\n\n# Lister les releases\nhelm list -A\n\n# Mettre Ã  jour\nhelm upgrade my-postgres bitnami/postgresql --set auth.postgresPassword=newpassword\n\n# Rollback\nhelm rollback my-postgres 1\n\n# DÃ©sinstaller\nhelm uninstall my-postgres -n data\n\n\n4.5 Fichier values.yaml personnalisÃ©\n# postgres-values.yaml\nauth:\n  postgresPassword: \"de_password\"\n  database: \"de_db\"\n\nprimary:\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"512Mi\"\n      cpu: \"500m\"\n  persistence:\n    size: 5Gi\n# Installer avec le fichier values\nhelm install my-postgres bitnami/postgresql -f postgres-values.yaml\n\n\n4.6 Charts utiles pour Data Engineering\n\n\n\n\n\n\n\n\nChart\nRepository\nUsage\n\n\n\n\nbitnami/postgresql\nbitnami\nBase de donnÃ©es relationnelle\n\n\nbitnami/redis\nbitnami\nCache, broker de messages\n\n\nminio/minio\nminio\nObject storage S3-compatible\n\n\napache-airflow/airflow\napache-airflow\nOrchestration (module 25)\n\n\nbitnami/spark\nbitnami\nSpark cluster (module 19)\n\n\n\n# Ajouter les repos utiles\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo add minio https://charts.min.io/\nhelm repo add apache-airflow https://airflow.apache.org\nhelm repo update\n\n\nVoir le code\n%%bash\n# VÃ©rifier l'installation de Helm\necho \"=== Version Helm ===\"\nhelm version --short 2&gt;/dev/null || echo \"Helm non installÃ©\"\n\necho \"\"\necho \"=== Repositories configurÃ©s ===\"\nhelm repo list 2&gt;/dev/null || echo \"Aucun repo configurÃ©\"\n\necho \"\"\necho \"=== Releases dÃ©ployÃ©es ===\"\nhelm list -A 2&gt;/dev/null || echo \"Aucune release\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#storage-avancÃ©-pour-workloads-data",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#storage-avancÃ©-pour-workloads-data",
    "title": "Kubernetes pour Workloads Data",
    "section": "5. Storage avancÃ© pour Workloads Data",
    "text": "5. Storage avancÃ© pour Workloads Data\nLe stockage est critique pour les workloads data. Voici les patterns recommandÃ©s.\n\n5.1 Quel stockage pour quel usage ?\n\n\n\n\n\n\n\n\nUsage\nType de volume\nCaractÃ©ristiques\n\n\n\n\nBase de donnÃ©es\nPVC (StatefulSet)\nPersistant, rapide (SSD)\n\n\nFichiers input\nPVC ou S3/MinIO\nExternalisÃ© si possible\n\n\nFichiers output\nPVC ou S3/MinIO\nExternalisÃ© pour durabilitÃ©\n\n\nDonnÃ©es temporaires\nemptyDir\nSupprimÃ© quand le pod meurt\n\n\nCache ultra-rapide\nemptyDir (Memory)\nRAM disk, trÃ¨s rapide mais volatile\n\n\n\n\n\n5.2 emptyDir : stockage Ã©phÃ©mÃ¨re (ligne par ligne)\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  VOLUMES Ã‰PHÃ‰MÃˆRES : DonnÃ©es temporaires pendant l'exÃ©cution             â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: v1\nkind: Pod\nmetadata:\n  name: etl-with-temp-storage\nspec:\n  containers:\n  - name: etl\n    image: my-etl:1.0\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # MONTAGE DES VOLUMES DANS LE CONTAINER\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    volumeMounts:\n    - name: tmp-data              # Volume pour fichiers temporaires\n      mountPath: /tmp/processing  # Accessible dans le container ici\n    \n    - name: cache                 # Volume cache en RAM\n      mountPath: /cache           # Accessible dans le container ici\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # DÃ‰FINITION DES VOLUMES\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  volumes:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # EMPTYDIR SUR DISQUE\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # - CrÃ©Ã© quand le pod dÃ©marre\n  # - SupprimÃ© quand le pod meurt (mÃªme si le container restart)\n  # - StockÃ© sur le disque du node\n  # - Parfait pour : fichiers intermÃ©diaires, dÃ©compression, etc.\n  #\n  - name: tmp-data\n    emptyDir: {}                  # {} = valeurs par dÃ©faut (disque du node)\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # EMPTYDIR EN RAM (ULTRA-RAPIDE)\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # - StockÃ© en RAM (pas sur disque)\n  # - TRÃˆS rapide (lecture/Ã©criture)\n  # - ATTENTION : compte dans la limite mÃ©moire du pod !\n  # - Parfait pour : cache, donnÃ©es frÃ©quemment accÃ©dÃ©es\n  #\n  - name: cache\n    emptyDir:\n      medium: Memory              # â­ StockÃ© en RAM au lieu du disque\n      sizeLimit: 256Mi            # Limite de taille (compte dans limits.memory !)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  EMPTYDIR : CYCLE DE VIE                                                â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  Pod dÃ©marre                                                            â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚\nâ”‚  â”‚  emptyDir   â”‚ â† Volume crÃ©Ã© (vide)                                  â”‚\nâ”‚  â”‚   crÃ©Ã©      â”‚                                                        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  Container Ã©crit des fichiers...                                        â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  Container crash et redÃ©marre                                           â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚\nâ”‚  â”‚  DonnÃ©es    â”‚ â† Les donnÃ©es sont PRÃ‰SERVÃ‰ES                         â”‚\nâ”‚  â”‚  intactes   â”‚   (tant que le POD existe)                            â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  Pod supprimÃ©                                                           â”‚\nâ”‚       â”‚                                                                 â”‚\nâ”‚       â–¼                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                        â”‚\nâ”‚  â”‚  emptyDir   â”‚ â† Volume SUPPRIMÃ‰ dÃ©finitivement                      â”‚\nâ”‚  â”‚  supprimÃ©   â”‚                                                        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                        â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n5.3 PVC avec StorageClass (ligne par ligne)\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  PVC : Demande de stockage persistant                                    â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: etl-data-pvc\n  namespace: data-pipeline\n\nspec:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # STORAGE CLASS (quel type de stockage ?)\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # La StorageClass dÃ©finit :\n  # - Le type de disque (SSD, HDD)\n  # - Le provisioner (AWS EBS, GCP PD, Azure Disk, etc.)\n  # - Les options de rÃ©plication\n  #\n  # Pour voir les StorageClasses disponibles :\n  # kubectl get storageclasses\n  #\n  storageClassName: standard      # \"standard\" = souvent le dÃ©faut\n                                  # \"fast-ssd\", \"premium-ssd\" selon ton cloud\n                                  # Omit = utilise la StorageClass par dÃ©faut\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # MODE D'ACCÃˆS\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  accessModes:\n    - ReadWriteOnce               # RWO = 1 seul node peut monter en Ã©criture\n                                  # ReadWriteMany (RWX) = plusieurs nodes\n                                  # ReadOnlyMany (ROX) = plusieurs en lecture seule\n                                  #\n                                  # âš ï¸ RWX n'est pas supportÃ© par tous les providers !\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # TAILLE DEMANDÃ‰E\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  resources:\n    requests:\n      storage: 10Gi               # 10 Go de stockage\n# Commandes utiles pour le storage\nkubectl get storageclasses                    # Voir les classes disponibles\nkubectl get pvc -n data-pipeline              # Voir les PVC\nkubectl get pv                                # Voir les PV (volumes provisionnÃ©s)\nkubectl describe pvc etl-data-pvc -n data-pipeline  # DÃ©tails d'un PVC",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#scaling-gestion-des-ressources",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#scaling-gestion-des-ressources",
    "title": "Kubernetes pour Workloads Data",
    "section": "6. Scaling & Gestion des ressources",
    "text": "6. Scaling & Gestion des ressources\nLes workloads data sont souvent gourmands en ressources. Voici comment les gÃ©rer.\n\n6.1 Requests vs Limits : comprendre la diffÃ©rence\nresources:\n  requests:                       # MINIMUM garanti\n    memory: \"512Mi\"               # K8s rÃ©serve 512 Mo pour ce pod\n    cpu: \"500m\"                   # K8s rÃ©serve 0.5 CPU pour ce pod\n  limits:                         # MAXIMUM autorisÃ©\n    memory: \"2Gi\"                 # Si dÃ©passÃ© â†’ OOMKilled (pod tuÃ©)\n    cpu: \"2000m\"                  # Si dÃ©passÃ© â†’ Throttling (ralenti)\n\n\n\n\n\n\n\n\nType\nCe que Ã§a fait\nConsÃ©quence si dÃ©passÃ©\n\n\n\n\nrequests\nMinimum garanti par K8s\nPod reste en Pending si pas assez de ressources sur le cluster\n\n\nlimits.memory\nMaximum de RAM\nOOMKilled : le pod est tuÃ© immÃ©diatement\n\n\nlimits.cpu\nMaximum de CPU\nThrottling : le pod est ralenti (pas tuÃ©)\n\n\n\n\n\n6.2 HorizontalPodAutoscaler (ligne par ligne)\nLe HPA scale automatiquement le nombre de pods selon les mÃ©triques.\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  HPA : Scaling automatique basÃ© sur les mÃ©triques                        â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: etl-worker-hpa\n  namespace: data-pipeline\n\nspec:\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # CIBLE : Quel Deployment scaler ?\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment              # Type de ressource Ã  scaler\n    name: etl-worker              # Nom du Deployment\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # LIMITES DE SCALING\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  minReplicas: 2                  # Minimum 2 pods (mÃªme si charge faible)\n  maxReplicas: 10                 # Maximum 10 pods (limite les coÃ»ts)\n  \n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  # MÃ‰TRIQUES DE DÃ‰CISION\n  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  metrics:\n  # MÃ©trique CPU\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70    # Si CPU &gt; 70% en moyenne â†’ scale UP\n                                  # Si CPU &lt; 70% en moyenne â†’ scale DOWN\n  \n  # MÃ©trique MÃ©moire\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80    # Si RAM &gt; 80% en moyenne â†’ scale UP\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  FONCTIONNEMENT DU HPA                                                  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  CPU actuel : 30%     CPU cible : 70%     Replicas : 2                 â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚  â†’ Charge faible, on reste Ã  2 replicas (minReplicas)                  â”‚\nâ”‚                                                                         â”‚\nâ”‚  CPU actuel : 85%     CPU cible : 70%     Replicas : 2                 â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚  â†’ CPU &gt; 70% ! HPA ajoute des pods â†’ 3 replicas                       â”‚\nâ”‚                                                                         â”‚\nâ”‚  CPU actuel : 90%     CPU cible : 70%     Replicas : 3                 â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚  â†’ Toujours &gt; 70% ! HPA ajoute encore â†’ 4 replicas                    â”‚\nâ”‚                                                                         â”‚\nâ”‚  CPU actuel : 60%     CPU cible : 70%     Replicas : 4                 â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                 â”‚\nâ”‚  â†’ CPU &lt; 70%, on attend un peu (cooldown)                              â”‚\nâ”‚  â†’ Puis HPA rÃ©duit â†’ 3 replicas                                        â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n6.3 ResourceQuota par namespace (ligne par ligne)\nLimite les ressources consommables par namespace (utile pour les Ã©quipes).\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  RESOURCEQUOTA : Limites globales par namespace                          â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: data-team-quota\n  namespace: data-pipeline        # S'applique Ã  ce namespace uniquement\n\nspec:\n  hard:                           # Limites \"dures\" (ne peuvent pas Ãªtre dÃ©passÃ©es)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LIMITES CPU\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    requests.cpu: \"10\"            # Total des requests CPU dans le namespace\n                                  # = maximum 10 CPU rÃ©servÃ©s\n    limits.cpu: \"20\"              # Total des limits CPU\n                                  # = maximum 20 CPU utilisables\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LIMITES MÃ‰MOIRE\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    requests.memory: \"20Gi\"       # Total des requests mÃ©moire = 20 Go\n    limits.memory: \"40Gi\"         # Total des limits mÃ©moire = 40 Go\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # LIMITES D'OBJETS\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    pods: \"50\"                    # Maximum 50 pods dans ce namespace\n    persistentvolumeclaims: \"10\"  # Maximum 10 PVC\n    services: \"20\"                # Maximum 20 services\n# Voir les quotas et leur utilisation\nkubectl describe resourcequota data-team-quota -n data-pipeline\n\n\n6.4 Node Affinity : placer les pods sur des nodes spÃ©cifiques\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  NODE AFFINITY : ContrÃ´ler oÃ¹ les pods sont placÃ©s                       â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nspec:\n  affinity:\n    nodeAffinity:\n      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      # RÃˆGLE OBLIGATOIRE (required)\n      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      # Le pod NE SERA PAS schedulÃ© si aucun node ne match\n      #\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: node-type        # Label du node\n            operator: In          # In, NotIn, Exists, DoesNotExist, Gt, Lt\n            values:\n            - high-memory         # Nodes avec label \"node-type=high-memory\"\n            - high-cpu            # Ou \"node-type=high-cpu\"\n      \n      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      # RÃˆGLE PRÃ‰FÃ‰RÃ‰E (preferred)\n      # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n      # Le scheduler PRÃ‰FÃˆRE ces nodes, mais peut en choisir d'autres\n      #\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100               # Poids de la prÃ©fÃ©rence (1-100)\n        preference:\n          matchExpressions:\n          - key: zone\n            operator: In\n            values:\n            - eu-west-1a          # PrÃ©fÃ¨re les nodes dans eu-west-1a\n# Voir les labels des nodes\nkubectl get nodes --show-labels\n\n# Ajouter un label Ã  un node\nkubectl label nodes node-1 node-type=high-memory\n\n\n6.5 Taints & Tolerations : rÃ©server des nodes\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# TAINT : \"Marquer\" un node pour le rÃ©server\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Seuls les pods avec la Toleration correspondante peuvent y aller\n\nkubectl taint nodes node-1 workload=data:NoSchedule\n#                          â–²           â–²\n#                          â”‚           â””â”€â”€ Effet : NoSchedule, PreferNoSchedule, NoExecute\n#                          â””â”€â”€ ClÃ©=Valeur\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  TOLERATION : Permet au pod d'aller sur un node \"taintÃ©\"                 â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nspec:\n  tolerations:\n  - key: \"workload\"               # ClÃ© du taint\n    operator: \"Equal\"             # Equal (clÃ©=valeur) ou Exists (clÃ© prÃ©sente)\n    value: \"data\"                 # Valeur du taint\n    effect: \"NoSchedule\"          # Effet Ã  tolÃ©rer\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  TAINTS & TOLERATIONS : EXEMPLE                                         â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                         â”‚\nâ”‚  Node-1 (taint: workload=data:NoSchedule)                              â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚\nâ”‚     â”‚                                                                   â”‚\nâ”‚     â”œâ”€â”€ Pod ETL (avec toleration) âœ… â†’ Peut Ãªtre schedulÃ©              â”‚\nâ”‚     â”‚                                                                   â”‚\nâ”‚     â””â”€â”€ Pod Web (sans toleration) âŒ â†’ Ne peut PAS Ãªtre schedulÃ©       â”‚\nâ”‚                                                                         â”‚\nâ”‚  Node-2 (pas de taint)                                                 â”‚\nâ”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\nâ”‚     â”‚                                                                   â”‚\nâ”‚     â”œâ”€â”€ Pod ETL âœ… â†’ Peut Ãªtre schedulÃ©                                â”‚\nâ”‚     â”‚                                                                   â”‚\nâ”‚     â””â”€â”€ Pod Web âœ… â†’ Peut Ãªtre schedulÃ©                                â”‚\nâ”‚                                                                         â”‚\nâ”‚  â†’ Les nodes \"taintÃ©s\" sont RÃ‰SERVÃ‰S aux pods data !                   â”‚\nâ”‚                                                                         â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#monitoring-observabilitÃ©",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#monitoring-observabilitÃ©",
    "title": "Kubernetes pour Workloads Data",
    "section": "7. Monitoring & ObservabilitÃ©",
    "text": "7. Monitoring & ObservabilitÃ©\nSurveiller tes workloads data est essentiel pour dÃ©tecter les problÃ¨mes.\n\n7.1 MÃ©triques de base avec kubectl\n# CPU/RAM des pods (nÃ©cessite metrics-server)\nkubectl top pods\nkubectl top pods -n data-pipeline\n\n# CPU/RAM des nodes\nkubectl top nodes\n\n# Events (problÃ¨mes rÃ©cents)\nkubectl get events --sort-by='.lastTimestamp'\nkubectl get events -n data-pipeline\n\n\n7.2 Prometheus + Grafana (aperÃ§u)\nLa stack Prometheus + Grafana est le standard pour le monitoring K8s :\n\n\n\nOutil\nRÃ´le\n\n\n\n\nPrometheus\nCollecte et stocke les mÃ©triques\n\n\nGrafana\nVisualisation et dashboards\n\n\nAlertManager\nAlertes (Slack, email, PagerDuty)\n\n\n\n# Installation rapide via Helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install monitoring prometheus-community/kube-prometheus-stack \\\n  --namespace monitoring \\\n  --create-namespace\n\n\n7.3 Logs\n# Logs d'un pod\nkubectl logs &lt;pod&gt;\nkubectl logs -f &lt;pod&gt;           # Follow\nkubectl logs -p &lt;pod&gt;           # Previous (aprÃ¨s crash)\nkubectl logs --tail=100 &lt;pod&gt;   # 100 derniÃ¨res lignes\n\n# Logs d'un Job\nkubectl logs job/&lt;job-name&gt;\n\n# Logs de tous les pods d'un label\nkubectl logs -l app=etl --all-containers\n\n\n7.4 Centralisation des logs\nPour les environnements de production, centralise les logs avec :\n\n\n\nSolution\nDescription\n\n\n\n\nELK Stack\nElasticsearch + Logstash + Kibana\n\n\nLoki + Grafana\nSolution lÃ©gÃ¨re (recommandÃ©e)\n\n\nCloud native\nCloudWatch (AWS), Cloud Logging (GCP)\n\n\n\n\n\n7.5 Dashboards utiles pour Data Engineering\n\n\n\nDashboard\nMÃ©triques\n\n\n\n\nK8s Cluster Overview\nCPU, RAM, pods par node\n\n\nJob Success Rate\nJobs succeeded vs failed\n\n\nPod Resource Usage\nConsommation vs requests/limits\n\n\nPVC Usage\nEspace disque utilisÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#aperÃ§u-spark-airflow-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#aperÃ§u-spark-airflow-sur-kubernetes",
    "title": "Kubernetes pour Workloads Data",
    "section": "8. AperÃ§u : Spark & Airflow sur Kubernetes",
    "text": "8. AperÃ§u : Spark & Airflow sur Kubernetes\n\nâš ï¸ Note : Cette section est un aperÃ§u pour te donner une vision dâ€™ensemble. - Spark sera dÃ©taillÃ© dans les modules 19-22 - Airflow sera dÃ©taillÃ© dans le module 25\n\n\n8.1 Spark on Kubernetes (aperÃ§u)\nPourquoi Spark sur K8s ? - Pas besoin de cluster YARN ou Mesos dÃ©diÃ© - Ã‰lasticitÃ© native (pods crÃ©Ã©s Ã  la demande) - Parfait pour jobs ETL batch Ã©phÃ©mÃ¨res - IntÃ©gration cloud-native\nArchitecture simplifiÃ©e :\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  spark-submit   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n         â”‚\n         â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Driver Pod    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Executor Pod   â”‚ x N\nâ”‚   (coordonne)   â”‚         â”‚  (traitement)   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nExemple de commande (aperÃ§u) :\n# Tu verras Ã§a en dÃ©tail dans le module 19\nspark-submit \\\n  --master k8s://https://&lt;K8S_API&gt; \\\n  --deploy-mode cluster \\\n  --conf spark.kubernetes.container.image=spark:3.5 \\\n  --conf spark.executor.instances=3 \\\n  local:///opt/spark/jobs/etl.py\n\n\n\n8.2 Airflow on Kubernetes (aperÃ§u)\nPourquoi Airflow sur K8s ? - ScalabilitÃ© des workers - Isolation parfaite des tÃ¢ches - KubernetesExecutor : 1 tÃ¢che = 1 pod\nArchitectures possibles :\n\n\n\nExecutor\nDescription\nUsage\n\n\n\n\nLocalExecutor\nTout dans 1 pod\nDev/test\n\n\nCeleryExecutor\nWorkers via Redis\nProduction classique\n\n\nKubernetesExecutor\n1 pod par tÃ¢che\nProduction K8s native\n\n\n\nDÃ©ploiement via Helm (aperÃ§u) :\n# Tu verras Ã§a en dÃ©tail dans le module 25\nhelm repo add apache-airflow https://airflow.apache.org\nhelm install airflow apache-airflow/airflow \\\n  --namespace airflow \\\n  --set executor=KubernetesExecutor\nKubernetesPodOperator (aperÃ§u) :\n# ExÃ©cuter une tÃ¢che dans un pod K8s dÃ©diÃ©\nfrom airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator\n\netl_task = KubernetesPodOperator(\n    task_id=\"etl_task\",\n    namespace=\"data-pipeline\",\n    image=\"my-etl:1.0\",\n    cmds=[\"python\", \"etl.py\"],\n    name=\"etl-pod\",\n)\n\nğŸ’¡ AprÃ¨s avoir terminÃ© les modules Spark (19-22), reviens sur cette section pour mieux comprendre lâ€™intÃ©gration K8s !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "Kubernetes pour Workloads Data",
    "section": "9. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "9. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nCause\nSolution\n\n\n\n\nOOMKilled\nMÃ©moire insuffisante\nAugmenter limits.memory\n\n\nDeadlineExceeded\nJob trop long\nAugmenter activeDeadlineSeconds\n\n\nPending (Job)\nPas de ressources disponibles\nVÃ©rifier quotas, rÃ©duire requests\n\n\nCrashLoopBackOff\nApp plante au dÃ©marrage\nkubectl logs &lt;pod&gt;\n\n\nImagePullBackOff\nImage/registry incorrects\nVÃ©rifier image, imagePullSecrets\n\n\nPVC Pending\nPas de PV disponible\nVÃ©rifier StorageClass\n\n\nJob jamais nettoyÃ©\nPas de TTL\nAjouter ttlSecondsAfterFinished\n\n\n\n\n\nâœ… Bonnes pratiques pour workloads data\n\n\n\n\n\n\n\nPratique\nPourquoi\n\n\n\n\nToujours dÃ©finir resources\nÃ‰vite OOM et problÃ¨mes de scheduling\n\n\nTTL sur les Jobs\nNettoyage automatique\n\n\nLogs externalisÃ©s\nNe pas dÃ©pendre des logs K8s\n\n\nConfigMaps pour paramÃ¨tres\nPas de hardcoding\n\n\nSecrets pour credentials\nSÃ©curitÃ©\n\n\nLabels systÃ©matiques\nFiltrage et monitoring\n\n\nNamespace par projet\nIsolation, quotas\n\n\nHealthchecks\nK8s sait si lâ€™app est prÃªte\n\n\n\n\n\nLabels recommandÃ©s pour workloads data\nmetadata:\n  labels:\n    app: etl-pipeline\n    component: transform    # extract, transform, load\n    env: production\n    team: data-engineering\n    version: \"1.2.0\"\n    schedule: daily         # Pour les CronJobs",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#quiz-de-fin-de-module",
    "title": "Kubernetes pour Workloads Data",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quâ€™est-ce quâ€™un â€œworkload dataâ€ ?\n\nUne application web qui affiche des donnÃ©es\n\nUne charge de travail dÃ©diÃ©e au traitement, transformation ou dÃ©placement de donnÃ©es\n\nUn dashboard de visualisation\n\nUne base de donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Un workload data est une tÃ¢che liÃ©e au traitement de donnÃ©es : ETL, transformations, ingestion, etc.\n\n\n\n\nâ“ Q2. Quel paramÃ¨tre permet de nettoyer automatiquement un Job terminÃ© aprÃ¨s 24h ?\n\nbackoffLimit: 24\n\nttlSecondsAfterFinished: 86400\n\nactiveDeadlineSeconds: 86400\n\ncleanupAfter: 24h\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ttlSecondsAfterFinished: 86400 supprime automatiquement le Job 24h aprÃ¨s sa completion.\n\n\n\n\nâ“ Q3. Quelle est la diffÃ©rence principale entre Deployment et StatefulSet ?\n\nDeployment est plus rÃ©cent\n\nStatefulSet garantit une identitÃ© stable et un stockage persistant par pod\n\nDeployment ne supporte pas les volumes\n\nStatefulSet est uniquement pour les bases NoSQL\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” StatefulSet donne une identitÃ© stable (pod-0, pod-1) et un PVC par pod, idÃ©al pour les bases de donnÃ©es.\n\n\n\n\nâ“ Q4. Quâ€™est-ce que Helm ?\n\nUn outil de monitoring\n\nUn package manager pour Kubernetes\n\nUn orchestrateur de containers\n\nUn systÃ¨me de logs\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Helm est le package manager de K8s, permettant dâ€™installer des applications complexes avec une seule commande.\n\n\n\n\nâ“ Q5. Que signifie lâ€™erreur OOMKilled ?\n\nLe pod a Ã©tÃ© tuÃ© par lâ€™administrateur\n\nLâ€™image Docker est corrompue\n\nLe container a dÃ©passÃ© sa limite de mÃ©moire\n\nLe rÃ©seau est indisponible\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” OOMKilled (Out Of Memory Killed) signifie que le container a dÃ©passÃ© limits.memory et a Ã©tÃ© tuÃ© par K8s.\n\n\n\n\nâ“ Q6. Quel executor Airflow crÃ©e un pod K8s par tÃ¢che ?\n\nLocalExecutor\n\nCeleryExecutor\n\nKubernetesExecutor\n\nSequentialExecutor\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Le KubernetesExecutor lance chaque tÃ¢che Airflow dans un pod K8s dÃ©diÃ©.\n\n\n\n\nâ“ Q7. Quelle QoS class offre la meilleure protection contre lâ€™Ã©viction ?\n\nBestEffort\n\nBurstable\n\nGuaranteed\n\nProtected\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Les pods Guaranteed (requests = limits) sont les derniers Ã  Ãªtre Ã©vincÃ©s en cas de pression sur les ressources.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#mini-projet-pipeline-etl-python-sur-kubernetes",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#mini-projet-pipeline-etl-python-sur-kubernetes",
    "title": "Kubernetes pour Workloads Data",
    "section": "Mini-projet : Pipeline ETL Python sur Kubernetes",
    "text": "Mini-projet : Pipeline ETL Python sur Kubernetes\n\nObjectif\nDÃ©ployer un pipeline ETL batch complet sur Kubernetes, sans Spark (Python/pandas), avec : - MinIO : stockage des fichiers source (S3-compatible) - PostgreSQL : destination des donnÃ©es - CronJob : ETL planifiÃ© quotidiennement\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      MinIO      â”‚â”€â”€â”€â”€â–¶â”‚   CronJob ETL   â”‚â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚\nâ”‚  (S3 / input)   â”‚     â”‚    (Python)     â”‚     â”‚    (output)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n        â”‚                       â”‚                       â”‚\n        â””â”€â”€â”€â”€â”€ Helm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€ Manifests â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nStructure du projet\nk8s-etl-project/\nâ”œâ”€â”€ helm-values/\nâ”‚   â”œâ”€â”€ minio-values.yaml\nâ”‚   â””â”€â”€ postgres-values.yaml\nâ”œâ”€â”€ manifests/\nâ”‚   â”œâ”€â”€ namespace.yaml\nâ”‚   â”œâ”€â”€ etl-configmap.yaml\nâ”‚   â”œâ”€â”€ etl-secret.yaml\nâ”‚   â””â”€â”€ etl-cronjob.yaml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ”œâ”€â”€ data/\nâ”‚   â””â”€â”€ sales.csv\nâ””â”€â”€ README.md\n\n\nÃ‰tapes\n\nCrÃ©er le namespace data-pipeline\nDÃ©ployer MinIO via Helm\nDÃ©ployer PostgreSQL via Helm\nUploader les donnÃ©es dans MinIO\nBuild & push lâ€™image ETL\nDÃ©ployer le CronJob\nTester manuellement\nVÃ©rifier les donnÃ©es dans PostgreSQL\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. manifests/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: data-pipeline\n  labels:\n    team: data-engineering\n    project: etl-demo\n2. helm-values/minio-values.yaml\nrootUser: admin\nrootPassword: minio123456\npersistence:\n  size: 5Gi\nresources:\n  requests:\n    memory: \"256Mi\"\n    cpu: \"100m\"\n3. helm-values/postgres-values.yaml\nauth:\n  postgresPassword: \"postgres123\"\n  username: \"de_user\"\n  password: \"de_password\"\n  database: \"de_db\"\nprimary:\n  resources:\n    requests:\n      memory: \"256Mi\"\n      cpu: \"250m\"\n  persistence:\n    size: 2Gi\n4. manifests/etl-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: etl-config\n  namespace: data-pipeline\ndata:\n  MINIO_ENDPOINT: \"minio.data-pipeline.svc.cluster.local:9000\"\n  MINIO_BUCKET: \"raw-data\"\n  MINIO_FILE: \"sales.csv\"\n  DB_HOST: \"postgres-postgresql.data-pipeline.svc.cluster.local\"\n  DB_PORT: \"5432\"\n  DB_NAME: \"de_db\"\n  DB_USER: \"de_user\"\n5. manifests/etl-secret.yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: etl-secret\n  namespace: data-pipeline\ntype: Opaque\nstringData:\n  MINIO_ACCESS_KEY: \"admin\"\n  MINIO_SECRET_KEY: \"minio123456\"\n  DB_PASSWORD: \"de_password\"\n6. manifests/etl-cronjob.yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etl-daily\n  namespace: data-pipeline\n  labels:\n    app: etl-pipeline\n    schedule: daily\nspec:\n  schedule: \"0 2 * * *\"  # Tous les jours Ã  2h\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 3\n  failedJobsHistoryLimit: 2\n  jobTemplate:\n    spec:\n      backoffLimit: 3\n      activeDeadlineSeconds: 1800\n      ttlSecondsAfterFinished: 86400\n      template:\n        metadata:\n          labels:\n            app: etl-job\n        spec:\n          containers:\n          - name: etl\n            image: my-etl:1.0  # Remplacer par ton image\n            envFrom:\n            - configMapRef:\n                name: etl-config\n            - secretRef:\n                name: etl-secret\n            resources:\n              requests:\n                memory: \"256Mi\"\n                cpu: \"200m\"\n              limits:\n                memory: \"512Mi\"\n                cpu: \"500m\"\n          restartPolicy: OnFailure\n7. etl/requirements.txt\npandas==2.1.4\nboto3==1.34.0\npsycopg2-binary==2.9.9\nsqlalchemy==2.0.25\n8. etl/Dockerfile\nFROM python:3.11-slim\n\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY etl.py .\n\nCMD [\"python\", \"etl.py\"]\n9. etl/etl.py\nimport os\nimport pandas as pd\nimport boto3\nfrom sqlalchemy import create_engine\nfrom io import BytesIO\n\ndef main():\n    print(\"ğŸš€ DÃ©marrage ETL...\")\n    \n    # Config MinIO\n    minio_endpoint = os.environ['MINIO_ENDPOINT']\n    minio_access = os.environ['MINIO_ACCESS_KEY']\n    minio_secret = os.environ['MINIO_SECRET_KEY']\n    bucket = os.environ['MINIO_BUCKET']\n    file_key = os.environ['MINIO_FILE']\n    \n    # Config PostgreSQL\n    db_host = os.environ['DB_HOST']\n    db_port = os.environ['DB_PORT']\n    db_name = os.environ['DB_NAME']\n    db_user = os.environ['DB_USER']\n    db_pass = os.environ['DB_PASSWORD']\n    \n    # EXTRACT : Lire depuis MinIO\n    print(f\"ğŸ“¥ Lecture depuis MinIO: {bucket}/{file_key}\")\n    s3 = boto3.client(\n        's3',\n        endpoint_url=f'http://{minio_endpoint}',\n        aws_access_key_id=minio_access,\n        aws_secret_access_key=minio_secret\n    )\n    \n    response = s3.get_object(Bucket=bucket, Key=file_key)\n    df = pd.read_csv(BytesIO(response['Body'].read()))\n    print(f\"   {len(df)} lignes lues\")\n    \n    # TRANSFORM\n    print(\"ğŸ”„ Transformation...\")\n    df['total'] = df['quantity'] * df['price']\n    df['loaded_at'] = pd.Timestamp.now()\n    \n    # LOAD : Ã‰crire dans PostgreSQL\n    print(f\"ğŸ“¤ Chargement dans PostgreSQL...\")\n    engine = create_engine(\n        f'postgresql://{db_user}:{db_pass}@{db_host}:{db_port}/{db_name}'\n    )\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© avec succÃ¨s !\")\n    print(df.head())\n\nif __name__ == \"__main__\":\n    main()\n10. Commandes de dÃ©ploiement\n# 1. CrÃ©er le namespace\nkubectl apply -f manifests/namespace.yaml\n\n# 2. Installer MinIO\nhelm repo add minio https://charts.min.io/\nhelm install minio minio/minio \\\n  -n data-pipeline \\\n  -f helm-values/minio-values.yaml\n\n# 3. Installer PostgreSQL\nhelm repo add bitnami https://charts.bitnami.com/bitnami\nhelm install postgres bitnami/postgresql \\\n  -n data-pipeline \\\n  -f helm-values/postgres-values.yaml\n\n# 4. Attendre que tout soit prÃªt\nkubectl get pods -n data-pipeline -w\n\n# 5. Upload data dans MinIO (via port-forward)\nkubectl port-forward svc/minio -n data-pipeline 9000:9000 &\n# Puis utiliser mc (MinIO client) ou l'UI\n\n# 6. Build & push image ETL\ncd etl\ndocker build -t my-etl:1.0 .\n# docker push my-registry/my-etl:1.0\n\n# 7. DÃ©ployer les manifests\nkubectl apply -f manifests/etl-configmap.yaml\nkubectl apply -f manifests/etl-secret.yaml\nkubectl apply -f manifests/etl-cronjob.yaml\n\n# 8. Tester manuellement\nkubectl create job test-etl --from=cronjob/etl-daily -n data-pipeline\n\n# 9. Voir les logs\nkubectl logs -f job/test-etl -n data-pipeline\n\n# 10. VÃ©rifier dans PostgreSQL\nkubectl exec -it postgres-postgresql-0 -n data-pipeline -- \\\n  psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#ressources-pour-aller-plus-loin",
    "title": "Kubernetes pour Workloads Data",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nKubernetes Jobs â€” Documentation Jobs\nKubernetes CronJobs â€” Documentation CronJobs\nStatefulSets â€” Applications stateful\nHelm Docs â€” Documentation Helm\n\n\n\nğŸ“¦ Charts Helm utiles\n\nArtifact Hub â€” Recherche de charts Helm\nBitnami Charts â€” Charts de qualitÃ© production\n\n\n\nğŸ”§ Outils\n\nk9s â€” Terminal UI pour Kubernetes\nLens â€” IDE Kubernetes\nMinIO Client (mc) â€” CLI pour MinIO",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/16_k8s_for_data_workloads.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/16_k8s_for_data_workloads.html#prochaine-Ã©tape",
    "title": "Kubernetes pour Workloads Data",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les workloads data sur Kubernetes, passons au traitement de donnÃ©es haute performance avec Python !\nğŸ‘‰ Module suivant : 17_polars_for_data_engineering â€” Polars : le DataFrame ultra-rapide\nTu vas apprendre : - Pourquoi Polars est plus rapide que Pandas - API lazy vs eager - Optimisations automatiques - Migration depuis Pandas\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Kubernetes pour Workloads Data.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "16 Â· K8s pour Data Workloads"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  containeriser tes applications, lancer des services data en quelques secondes, et packager tes pipelines ETL de maniÃ¨re reproductible et portable â€” des compÃ©tences indispensables pour un Data Engineer moderne !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#prÃ©requis",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#prÃ©requis",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nAvoir suivi les modules 01_intro_data_engineering et 02_bash_for_data_engineers\n\n\nâœ… Requis\nConnaissances de base en Python\n\n\nâœ… Requis\nAvoir accÃ¨s Ã  un terminal (Linux, Mac, ou Windows avec WSL)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#objectifs-du-module",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#objectifs-du-module",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nExpliquer ce quâ€™est Docker et pourquoi il est essentiel en Data Engineering\nInstaller Docker sur Windows, macOS ou Linux\nLancer des services data (PostgreSQL, Kafka, Sparkâ€¦) en une commande\nÃ‰crire un Dockerfile pour packager un script ETL\nUtiliser les volumes, rÃ©seaux et docker-compose\nDÃ©bugger des containers comme un pro\nAppliquer les bonnes pratiques professionnelles",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#cest-quoi-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#cest-quoi-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "Câ€™est quoi Docker ?",
    "text": "Câ€™est quoi Docker ?\n\nDocker est un outil qui permet dâ€™exÃ©cuter des applications dans des environnements isolÃ©s, reproductibles et portables, appelÃ©s containers.\n\nAu lieu dâ€™installer une application (et toutes ses dÃ©pendances) directement sur ton systÃ¨me, tu la mets dans une â€œboÃ®teâ€ (le container) avec tout ce dont elle a besoin.\n\nDocker â‰  Machine Virtuelle\n\n\n\n\n\n\n\n\nAspect\nMachine Virtuelle (VM)\nContainer Docker\n\n\n\n\nContenu\nOS complet + kernel + drivers + apps\nApp + libs + systÃ¨me minimal\n\n\nTaille\nPlusieurs Go\nQuelques Mo Ã  centaines de Mo\n\n\nDÃ©marrage\nMinutes\nSecondes/millisecondes\n\n\nPerformance\nOverhead important\nQuasi-natif\n\n\nIsolation\nComplÃ¨te (hyperviseur)\nAu niveau processus\n\n\n\n\n\nAnalogies pour bien comprendre\n\n\n\n\n\n\n\nAnalogie\nExplication\n\n\n\n\nğŸ§³ La valise prÃªte\nUn container = une valise dÃ©jÃ  remplie. Tu la prends, tu voyages, tu es opÃ©rationnel partout.\n\n\nğŸ± Le tupperware\nTu prÃ©pares un plat, tu le mets dans une boÃ®te hermÃ©tique. Chez toi ou ailleurs : câ€™est le mÃªme plat.\n\n\nğŸ“¦ Le zip complet\nCode + librairies + config dans un package. Mais avec la garantie que lâ€™exÃ©cution est identique partout.\n\n\n\n\nâ„¹ï¸ Le savais-tu ?\nDocker a Ã©tÃ© crÃ©Ã© en 2013 par Solomon Hykes chez dotCloud (devenu Docker, Inc.).\nLe nom vient des dockers (ouvriers portuaires) qui chargent et dÃ©chargent des containers sur les bateaux â€” exactement ce que fait Docker avec les applications !\nğŸ“– Histoire de Docker sur Wikipedia",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#pourquoi-docker-est-indispensable-pour-un-data-engineer",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#pourquoi-docker-est-indispensable-pour-un-data-engineer",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "1. Pourquoi Docker est indispensable pour un Data Engineer",
    "text": "1. Pourquoi Docker est indispensable pour un Data Engineer\nEn Data Engineering, tu dois souvent :\n\nManipuler des bases de donnÃ©es (PostgreSQL, MySQL, MongoDBâ€¦)\nLancer des brokers de messages (Kafka, RabbitMQâ€¦)\nDÃ©ployer des pipelines ETL\nFaire tourner des jobs Spark ou des APIs\n\n\nâŒ Sans Docker\n\n\n\n\n\n\n\nProblÃ¨me\nConsÃ©quence\n\n\n\n\nInstallation manuelle complexe\nHeures perdues en config\n\n\nConflits de versions (Java, Python, drivers)\nâ€œÃ‡a marchait hierâ€¦â€\n\n\nEnvironnements diffÃ©rents (local â‰  prod)\nBugs en production uniquement\n\n\nOnboarding difficile\nNouveaux = 2 jours pour installer\n\n\n\n\n\nâœ… Avec Docker\n\n\n\nAvantage\nExemple concret\n\n\n\n\nPostgreSQL en 1 commande\ndocker run postgres:16\n\n\nTest Kafka + Spark sur laptop\nStack complÃ¨te en local\n\n\nETL packagÃ© et portable\nTourne identique partout\n\n\nOnboarding en 10 minutes\ndocker-compose up et câ€™est parti\n\n\n\n\nğŸ’¡ En rÃ©sumÃ© : Docker est un outil central pour crÃ©er des environnements data reproductibles et industrialisables. Fini le ğŸ˜… â€œchez moi Ã§a marcheâ€ !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#concepts-clÃ©s-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#concepts-clÃ©s-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "2. Concepts clÃ©s Docker",
    "text": "2. Concepts clÃ©s Docker\nAvant dâ€™aller plus loin, maÃ®trise ces 6 notions fondamentales :\n\n\n\n\n\n\n\n\nConcept\nDescription\nAnalogie\n\n\n\n\nImage\nModÃ¨le figÃ© (blueprint) contenant OS + dÃ©pendances + code\nUne recette de cuisine\n\n\nContainer\nInstance en cours dâ€™exÃ©cution dâ€™une image\nUn plat prÃ©parÃ© Ã  partir de la recette\n\n\nRegistry\nMagasin dâ€™images (Docker Hub, ECR, GHCR)\nUn catalogue de recettes en ligne\n\n\nVolume\nStockage persistant en dehors du container\nUn disque dur externe branchÃ©\n\n\nNetwork\nRÃ©seau virtuel entre containers\nUn rÃ©seau local privÃ©\n\n\nBuild context\nFichiers envoyÃ©s Ã  Docker lors du build\nLe dossier de travail\n\n\n\n\nImage Docker\nUne image contient : - Un systÃ¨me de base (ex: python:3.11-slim) - Des bibliothÃ¨ques (pandas, pyarrow, pysparkâ€¦) - Ton code (scripts, fichiers de config)\nOn ne modifie pas une image â€” on en reconstruit une nouvelle Ã  partir dâ€™un Dockerfile.\n\n\nContainer\nUn container est une instance vivante dâ€™une image : - Tu crÃ©es une image â†’ tu la lances â†’ tu obtiens un container - Tu peux dÃ©marrer, arrÃªter, supprimer un container sans toucher Ã  lâ€™image - Plusieurs containers peuvent tourner Ã  partir de la mÃªme image\n\n\nRegistry\nUn registry stocke et partage les images : - Docker Hub (public/privÃ©) â€” le plus connu - GitHub Container Registry (GHCR) - AWS ECR, GCP Artifact Registry, Azure ACR\n\n\nVolume\nLes donnÃ©es ne doivent pas â€œmourirâ€ avec le container : - Sans volume : container supprimÃ© = donnÃ©es perdues - Avec volume : donnÃ©es persistantes, partageables\n\n\nNetwork\nPermet aux containers de communiquer entre eux : - Ex: container etl se connecte Ã  container postgres - Isolation du rÃ©seau de la machine hÃ´te\n\n\nBuild context\nQuand tu fais docker build -t mon-image . : - Le . = tout ce que Docker envoie au daemon - Dossier de 10 Go = envoi de 10 Go - Dâ€™oÃ¹ lâ€™importance du .dockerignore !\n\n\nSchÃ©ma visuel : Architecture Docker\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                      DOCKER HOST                            â”‚\nâ”‚               (Laptop / Server / Cloud)                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚                  DOCKER ENGINE                      â”‚    â”‚\nâ”‚  â”‚                                                     â”‚    â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚\nâ”‚  â”‚   â”‚  IMAGE   â”‚  â”‚  IMAGE   â”‚  â”‚  IMAGE   â”‚          â”‚    â”‚ \nâ”‚  â”‚   â”‚ postgres â”‚  â”‚  python  â”‚  â”‚  spark   â”‚          â”‚    â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚    â”‚\nâ”‚  â”‚        â”‚             â”‚             â”‚                â”‚    â”‚\nâ”‚  â”‚        â–¼             â–¼             â–¼                â”‚    â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚    â”‚\nâ”‚  â”‚   â”‚CONTAINER â”‚  â”‚CONTAINER â”‚  â”‚CONTAINER â”‚          â”‚    â”‚\nâ”‚  â”‚   â”‚ de-postgresâ”‚ â”‚  de-etl  â”‚  â”‚ de-spark â”‚         â”‚    â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚    â”‚\nâ”‚  â”‚        â”‚                                            â”‚    â”‚\nâ”‚  â”‚        â–¼                                            â”‚    â”‚\nâ”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚\nâ”‚  â”‚   â”‚  VOLUME  â”‚       â”‚   NETWORK    â”‚               â”‚    â”‚\nâ”‚  â”‚   â”‚ pg_data  â”‚       â”‚  de-network  â”‚               â”‚    â”‚\nâ”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                                                             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#installation-de-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#installation-de-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "3. Installation de Docker",
    "text": "3. Installation de Docker\n\n\n\nSystÃ¨me\nComment installer\n\n\n\n\nğŸªŸ Windows\nDocker Desktop + WSL2\n\n\nğŸ macOS\nDocker Desktop (Intel ou Apple Silicon)\n\n\nğŸ§ Linux\nDocker Engine (apt/yum)\n\n\n\n\nğŸªŸ Windows (Docker Desktop + WSL2)\n1. Activer WSL2 :\n# Dans PowerShell en administrateur\nwsl --install\n2. TÃ©lÃ©charger Docker Desktop : - ğŸ”— https://www.docker.com/products/docker-desktop/\n3. Installer et redÃ©marrer\n4. Tester :\ndocker --version\ndocker run --rm hello-world\n\n\n\nğŸ macOS (Intel & Apple Silicon)\n1. TÃ©lÃ©charger Docker Desktop : - ğŸ”— https://www.docker.com/products/docker-desktop/ - Choisir la version Intel ou Apple Silicon (M1/M2/M3)\n2. Glisser lâ€™app dans Applications\n3. Lancer Docker Desktop (icÃ´ne ğŸ³ dans la barre)\n4. Tester :\ndocker --version\ndocker run --rm hello-world\n\n\n\nğŸ§ Linux (Ubuntu/Debian)\n# 1. Mettre Ã  jour et installer les prÃ©requis\nsudo apt-get update\nsudo apt-get install ca-certificates curl gnupg lsb-release\n\n# 2. Ajouter la clÃ© GPG officielle\nsudo mkdir -p /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\n\n# 3. Ajouter le dÃ©pÃ´t Docker\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n\n# 4. Installer Docker\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io\n\n# 5. Tester\nsudo docker run --rm hello-world\n\n# 6. (Optionnel) Utiliser Docker sans sudo\nsudo usermod -aG docker $USER\n# Puis dÃ©connexion/reconnexion\n\n\nVÃ©rifier ton installation\n\n\nVoir le code\n%%bash\n# VÃ©rifier la version de Docker\ndocker --version\n\n# VÃ©rifier que Docker fonctionne\ndocker run --rm hello-world",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#commandes-docker-essentielles",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#commandes-docker-essentielles",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "4. Commandes Docker essentielles",
    "text": "4. Commandes Docker essentielles\nVoici ton cheat sheet de base :\n\nLancer un container\ndocker run image                    # Lancer un container\ndocker run -d image                 # En arriÃ¨re-plan (detached)\ndocker run -it image bash           # Mode interactif\ndocker run --rm image               # Supprime le container Ã  la fin\ndocker run --name mon-container image  # Nommer le container\n\n\nLister\ndocker ps                           # Containers en cours\ndocker ps -a                        # Tous les containers (y compris stoppÃ©s)\ndocker images                       # Lister les images locales\n\n\nStopper / Supprimer\ndocker stop CONTAINER_ID            # ArrÃªter un container\ndocker rm CONTAINER_ID              # Supprimer un container\ndocker rmi IMAGE_ID                 # Supprimer une image\n\n\nTÃ©lÃ©charger une image\ndocker pull postgres:16             # TÃ©lÃ©charger depuis Docker Hub\n\n\nLogs\ndocker logs CONTAINER_ID            # Voir les logs\ndocker logs -f CONTAINER_ID         # Suivre les logs en temps rÃ©el\n\n\nRaccourcis utiles\n\n\n\nCommande\nDescription\n\n\n\n\ndocker ps -a\nVoir tous les containers\n\n\ndocker logs -f\nSuivre les logs en live\n\n\ndocker exec -it &lt;container&gt; bash\nEntrer dans un container\n\n\ndocker system prune\nNettoyer les ressources inutilisÃ©es\n\n\n\n\n\nVoir le code\n%%bash\n# Lister les images disponibles localement\necho \"=== Images locales ===\"\ndocker images\n\necho \"\"\necho \"=== Containers en cours ===\"\ndocker ps\n\necho \"\"\necho \"=== Tous les containers ===\"\ndocker ps -a",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-pour-lancer-des-services-data",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-pour-lancer-des-services-data",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "5. Docker pour lancer des services Data",
    "text": "5. Docker pour lancer des services Data\nDocker est extrÃªmement pratique pour tester rapidement des services utilisÃ©s en Data Engineering.\n\nPostgreSQL (exemple dÃ©taillÃ©)\ndocker run -d \\\n  --name demo-postgres \\\n  -e POSTGRES_USER=de_user \\\n  -e POSTGRES_PASSWORD=de_pass \\\n  -e POSTGRES_DB=de_db \\\n  -p 5432:5432 \\\n  postgres:16\nConnexion : - Host: localhost - Port: 5432 - User: de_user - Password: de_pass - Database: de_db\nTu peux ensuite te connecter avec DBeaver, psql, Python (psycopg2), etc.\n\n\n\nAutres services (one-liners)\n\n\n\n\n\n\n\nService\nCommande\n\n\n\n\nRedis\ndocker run -d --name demo-redis -p 6379:6379 redis:latest\n\n\nMongoDB\ndocker run -d --name demo-mongo -p 27017:27017 mongo:latest\n\n\nKafka\ndocker run -d --name demo-kafka -p 9092:9092 bitnami/kafka:latest\n\n\nSpark\ndocker run -it --name demo-spark bitnami/spark:latest pyspark\n\n\nAirflow\ndocker pull apache/airflow:latest\n\n\nJupyter\ndocker run -p 8888:8888 jupyter/scipy-notebook\n\n\n\n\nğŸ’¡ Astuce : Pour Kafka et Airflow, prÃ©fÃ¨re docker-compose car ils nÃ©cessitent plusieurs services (Zookeeper, webserver, schedulerâ€¦), on le verra un peu plus en bas.\n\n\n\nVoir le code\n%%bash\n# Lancer PostgreSQL (si Docker est installÃ©)\ndocker run -d \\\n  --name demo-postgres \\\n  -e POSTGRES_USER=de_user \\\n  -e POSTGRES_PASSWORD=de_pass \\\n  -e POSTGRES_DB=de_db \\\n  -p 5432:5432 \\\n  postgres:16\n\necho \"PostgreSQL lancÃ© sur localhost:5432\"\n\n# VÃ©rifier qu'il tourne\ndocker ps --filter name=demo-postgres\n\n\n\n\nVoir le code\n%%bash\n# Nettoyage : stopper et supprimer le container\ndocker stop demo-postgres 2&gt;/dev/null\ndocker rm demo-postgres 2&gt;/dev/null\necho \"ğŸ§¹ Container demo-postgres supprimÃ©\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#dockerfile-crÃ©er-son-image",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#dockerfile-crÃ©er-son-image",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "6. Dockerfile : crÃ©er son image",
    "text": "6. Dockerfile : crÃ©er son image\nLe Dockerfile est un fichier texte qui dÃ©crit comment construire une image.\n\nInstructions principales\n\n\n\nInstruction\nRÃ´le\nExemple\n\n\n\n\nFROM\nImage de base\nFROM python:3.11-slim\n\n\nWORKDIR\nRÃ©pertoire de travail\nWORKDIR /app\n\n\nCOPY\nCopier des fichiers\nCOPY etl.py .\n\n\nRUN\nExÃ©cuter une commande (build)\nRUN pip install pandas\n\n\nCMD\nCommande par dÃ©faut (run)\nCMD [\"python\", \"etl.py\"]\n\n\nENTRYPOINT\nPoint dâ€™entrÃ©e fixe\nENTRYPOINT [\"python\"]\n\n\nENV\nVariable dâ€™environnement\nENV PYTHONUNBUFFERED=1\n\n\nEXPOSE\nPort exposÃ© (documentation)\nEXPOSE 8000\n\n\n\n\n\nStructure de projet recommandÃ©e\netl_project/\nâ”œâ”€â”€ Dockerfile\nâ”œâ”€â”€ .dockerignore\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ etl.py\nâ”‚   â””â”€â”€ utils.py\nâ””â”€â”€ data/               # âš ï¸ Ne pas inclure dans l'image !\n    â””â”€â”€ input.csv\n\nğŸ’¡ Important : Le Dockerfile doit Ãªtre Ã  la racine du service que tu veux packager.\n\n\n\nExemple : Dockerfile pour un ETL Python\n# 1. Image de base lÃ©gÃ¨re\nFROM python:3.11-slim\n\n# 2. Variables d'environnement\nENV PYTHONUNBUFFERED=1\nENV PYTHONDONTWRITEBYTECODE=1\n\n# 3. RÃ©pertoire de travail\nWORKDIR /app\n\n# 4. Copier et installer les dÃ©pendances (cache Docker)\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# 5. Copier le code\nCOPY src/ ./src/\n\n# 6. Commande par dÃ©faut\nCMD [\"python\", \"src/etl.py\"]\n\n\nConstruire lâ€™image\ncd etl_project\ndocker build -t etl-image:1.0 .\n\n\nLancer le container\ndocker run --rm etl-image:1.0",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#le-fichier-.dockerignore",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#le-fichier-.dockerignore",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "7. Le fichier .dockerignore",
    "text": "7. Le fichier .dockerignore\nLe .dockerignore empÃªche dâ€™envoyer des fichiers inutiles dans le build context.\n\nâŒ Sans .dockerignore\n\nImages Ã©normes (datasets inclus)\nBuilds lents (envoi de Go de donnÃ©es)\nFuites de secrets (.env, clÃ©s SSH)\n\n\n\nâœ… Exemple de .dockerignore (Data Engineer)\n# DonnÃ©es\ndata/\n*.csv\n*.parquet\n*.json\n\n# Python\n__pycache__/\n*.pyc\n*.pyo\nvenv/\n.venv/\n*.egg-info/\n\n# Secrets\n.env\n*.key\n*.pem\nsecrets/\n\n# Notebooks\n*.ipynb\n.ipynb_checkpoints/\n\n# Git\n.git/\n.gitignore\n\n# IDE\n.idea/\n.vscode/\n*.swp\n\n# Logs\nlogs/\n*.log\n\n# OS\n.DS_Store\nThumbs.db\n\n# Docker\nDockerfile\ndocker-compose*.yml\n\nğŸ’¡ RÃ¨gle dâ€™or : le .dockerignore est aussi important que le Dockerfile !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#multi-stage-builds",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#multi-stage-builds",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "8. Multi-stage builds",
    "text": "8. Multi-stage builds\nLes multi-stage builds permettent de crÃ©er des images plus lÃ©gÃ¨res en sÃ©parant :\n\nStage build : compilation, installation des dÃ©pendances lourdes\nStage runtime : uniquement ce qui est nÃ©cessaire pour exÃ©cuter\n\n\nPourquoi câ€™est utile ?\n\n\n\nSans multi-stage\nAvec multi-stage\n\n\n\n\nImage de 1.5 Go\nImage de 200 Mo\n\n\nOutils de build inclus\nSeulement le runtime\n\n\nSurface dâ€™attaque large\nSÃ©curitÃ© renforcÃ©e\n\n\n\n\n\nExemple : ETL Python avec multi-stage\n# ============== STAGE 1 : BUILD ==============\nFROM python:3.11-slim AS builder\n\nWORKDIR /app\n\n# Installer les dÃ©pendances dans un dossier isolÃ©\nCOPY requirements.txt .\nRUN pip install --prefix=/install --no-cache-dir -r requirements.txt\n\n# ============== STAGE 2 : RUNTIME ==============\nFROM python:3.11-slim AS runtime\n\nWORKDIR /app\n\n# Copier seulement les dÃ©pendances installÃ©es\nCOPY --from=builder /install /usr/local\n\n# Copier le code\nCOPY src/ ./src/\n\n# Variables d'environnement\nENV PYTHONUNBUFFERED=1\n\nCMD [\"python\", \"src/etl.py\"]\nRÃ©sultat : image finale lÃ©gÃ¨re et sÃ©curisÃ©e ! ğŸ‰",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#volumes-networks",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#volumes-networks",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "9. Volumes & Networks",
    "text": "9. Volumes & Networks\n\n9.1 Volumes : persister les donnÃ©es\nLes volumes permettent de stocker des donnÃ©es en dehors des containers.\nTypes de volumes :\n\n\n\n\n\n\n\n\nType\nSyntaxe\nUsage\n\n\n\n\nBind mount\n-v /host/path:/container/path\nDonnÃ©es locales (dev)\n\n\nVolume nommÃ©\n-v myvolume:/container/path\nDonnÃ©es persistantes (prod)\n\n\n\nExemple : monter un dossier local\ndocker run -d \\\n  --name etl-with-data \\\n  -v $(pwd)/data:/app/data \\\n  etl-image:1.0\n\n$(pwd)/data â†’ dossier sur ta machine\n/app/data â†’ dossier dans le container\nTu supprimes le container â†’ les donnÃ©es restent dans ./data\n\n\n\n\n9.2 Networks : faire communiquer les containers\nPar dÃ©faut, Docker crÃ©e un rÃ©seau bridge. Tu peux crÃ©er un rÃ©seau dÃ©diÃ© :\n# CrÃ©er un rÃ©seau\ndocker network create de-network\n\n# Lancer des containers sur ce rÃ©seau\ndocker run -d --name de-postgres --network de-network postgres:16\ndocker run -d --name de-etl --network de-network etl-image:1.0\nAvantage : dans le code Python, tu te connectes Ã  de-postgres (nom du container) au lieu de localhost !\n# Dans etl.py\nconn = psycopg2.connect(\n    host=\"de-postgres\",  # Nom du container !\n    database=\"de_db\",\n    user=\"de_user\",\n    password=\"de_pass\"\n)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-compose",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#docker-compose",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "10. Docker Compose",
    "text": "10. Docker Compose\nQuand tu as plusieurs services (Postgres + ETL + APIâ€¦), tu ne veux pas tout lancer Ã  la main.\ndocker-compose.yml permet de dÃ©crire une stack complÃ¨te et de la lancer avec une seule commande.\n\nStructure projet avec docker-compose\nde-pipeline/\nâ”œâ”€â”€ docker-compose.yml      # Ã€ la racine !\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ””â”€â”€ data/\n    â””â”€â”€ input.csv\n\n\nExemple : PostgreSQL + ETL\nversion: \"3.9\"\n\nservices:\n  postgres:\n    image: postgres:16\n    container_name: de-postgres\n    environment:\n      POSTGRES_USER: de_user\n      POSTGRES_PASSWORD: de_pass\n      POSTGRES_DB: de_db\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U de_user -d de_db\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  etl:\n    build: ./etl\n    container_name: de-etl\n    depends_on:\n      postgres:\n        condition: service_healthy\n    environment:\n      DB_HOST: postgres\n      DB_USER: de_user\n      DB_PASSWORD: de_pass\n      DB_NAME: de_db\n    volumes:\n      - ./data:/app/data\n\nvolumes:\n  pg_data:\n\n\nCommandes docker-compose\n# Lancer la stack\ndocker compose up -d\n\n# Voir les logs\ndocker compose logs -f\n\n# Stopper la stack\ndocker compose down\n\n# Stopper et supprimer les volumes\ndocker compose down -v\n\n# Reconstruire les images\ndocker compose up --build",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#debug-docker",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#debug-docker",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "11. Debug Docker",
    "text": "11. Debug Docker\nEn Data Engineering, tu devras souvent dÃ©bugger un job qui tourne dans un container.\n\nVoir les logs\ndocker logs de-etl                  # Voir les logs\ndocker logs -f de-etl               # Suivre en temps rÃ©el\ndocker logs --tail 100 de-etl       # Les 100 derniÃ¨res lignes\n\n\nEntrer dans un container\ndocker exec -it de-etl bash         # Ouvrir un shell bash\ndocker exec -it de-etl sh           # Pour images Alpine\nCas dâ€™usage : - Inspecter les fichiers (ls, cat) - Tester une connexion DB (psql, ping) - VÃ©rifier les variables dâ€™environnement (env) - Lancer un script manuellement (python etl.py)\n\n\nInspecter un container\ndocker inspect de-etl               # DÃ©tails complets (JSON)\ndocker inspect --format='{{.NetworkSettings.IPAddress}}' de-etl  # IP du container\n\n\nCas Data Engineering typiques\n\n\n\n\n\n\n\nProblÃ¨me\nSolution\n\n\n\n\nETL qui plante sans message\ndocker logs de-etl\n\n\nConnexion DB refusÃ©e\ndocker exec -it de-etl bash puis ping postgres\n\n\nSpark ne voit pas Kafka\nVÃ©rifier les networks Docker\n\n\nFichier introuvable\ndocker exec -it de-etl ls /app/data\n\n\nVariable dâ€™env manquante\ndocker exec -it de-etl env",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#erreurs-frÃ©quentes-bonnes-pratiques",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "12. Erreurs frÃ©quentes & Bonnes pratiques",
    "text": "12. Erreurs frÃ©quentes & Bonnes pratiques\n\nâŒ Erreurs frÃ©quentes\n\n\n\n\n\n\n\n\nErreur\nConsÃ©quence\nSolution\n\n\n\n\nPas de .dockerignore\nImages de plusieurs Go\nCrÃ©er un .dockerignore complet\n\n\nTout en latest\nComportement non reproductible\nTags versionnÃ©s (image:1.0.0)\n\n\nDockerfile mal placÃ©\nBuild context gigantesque\nDockerfile Ã  la racine du service\n\n\nSecrets dans lâ€™image\nFuite de credentials\nVariables dâ€™env ou secrets manager\n\n\nPas de nettoyage\nDisque saturÃ©\ndocker system prune rÃ©guliÃ¨rement\n\n\nDonnÃ©es dans lâ€™image\nImage Ã©norme, non portable\nUtiliser des volumes\n\n\nVersions non fixÃ©es\nBuilds cassÃ©s aprÃ¨s mise Ã  jour\nFixer les versions dans requirements.txt\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nImages slim\npython:3.11-slim = plus lÃ©ger et rapide\n\n\nMulti-stage builds\nImages finales lÃ©gÃ¨res\n\n\nTags versionnÃ©s\netl:1.0.0, etl:prod, etl:staging\n\n\n.dockerignore\nBuilds rapides et sÃ©curisÃ©s\n\n\nHealthchecks\nSavoir si un service est prÃªt\n\n\nVolumes pour les donnÃ©es\nPersistance et portabilitÃ©\n\n\nNettoyage rÃ©gulier\ndocker system prune\n\n\n\n\n\nğŸ§¹ Commandes de nettoyage\n# Supprimer les containers arrÃªtÃ©s\ndocker container prune\n\n# Supprimer les images non utilisÃ©es\ndocker image prune\n\n# Tout nettoyer (âš ï¸ attention en prod !)\ndocker system prune -a",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#quiz-de-fin-de-module",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\nRÃ©ponds aux questions suivantes pour vÃ©rifier tes acquis.\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre une image et un container Docker ?\n\nUne image est un container en cours dâ€™exÃ©cution\n\nUn container est une instance en cours dâ€™exÃ©cution dâ€™une image\n\nCâ€™est la mÃªme chose\n\nUne image contient plusieurs containers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Une image est un modÃ¨le figÃ©, un container est son exÃ©cution vivante.\n\n\n\n\nâ“ Q2. Pourquoi le fichier .dockerignore est-il important ?\n\nPour ignorer les erreurs Docker\n\nPour rÃ©duire la taille du build context et sÃ©curiser les builds\n\nPour ignorer les logs\n\nCâ€™est optionnel et inutile\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Il Ã©vite dâ€™envoyer des fichiers inutiles (donnÃ©es, secrets) dans le build context.\n\n\n\n\nâ“ Q3. Quelle commande permet dâ€™entrer dans un container en cours dâ€™exÃ©cution ?\n\ndocker run -it container bash\n\ndocker exec -it container bash\n\ndocker enter container\n\ndocker ssh container\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” docker exec -it &lt;container&gt; bash ouvre un shell interactif.\n\n\n\n\nâ“ Q4. Ã€ quoi servent les multi-stage builds ?\n\nÃ€ lancer plusieurs containers en parallÃ¨le\n\nÃ€ crÃ©er des images plus lÃ©gÃ¨res en sÃ©parant build et runtime\n\nÃ€ versionner les images\n\nÃ€ gÃ©rer les rÃ©seaux Docker\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Multi-stage = image finale lÃ©gÃ¨re avec seulement le runtime nÃ©cessaire.\n\n\n\n\nâ“ Q5. OÃ¹ doit-on placer le fichier docker-compose.yml ?\n\nDans le dossier /etc/docker/\n\nÃ€ la racine du projet multi-services\n\nDans chaque sous-dossier de service\n\nNâ€™importe oÃ¹\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le docker-compose.yml est Ã  la racine du projet, avec les services dans des sous-dossiers.\n\n\n\n\nâ“ Q6. Quelle commande permet de voir les logs dâ€™un container en temps rÃ©el ?\n\ndocker logs container\n\ndocker logs -f container\n\ndocker watch container\n\ndocker tail container\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Lâ€™option -f (follow) suit les logs en temps rÃ©el.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#mini-projet-etl-dockerisÃ©-complet",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#mini-projet-etl-dockerisÃ©-complet",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "Mini-projet : ETL DockerisÃ© complet",
    "text": "Mini-projet : ETL DockerisÃ© complet\n\nObjectif\nCrÃ©er un pipeline ETL DockerisÃ© qui lit un CSV, transforme les donnÃ©es, et les charge dans PostgreSQL.\n\n\nContexte\nTu dois packager un job ETL Python avec Docker et le faire communiquer avec une base PostgreSQL, le tout orchestrÃ© par docker-compose.\n\n\nContraintes\n\nCrÃ©er la structure de projet suivante :\n\nde-mini-projet/\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ etl/\nâ”‚   â”œâ”€â”€ Dockerfile\nâ”‚   â”œâ”€â”€ .dockerignore\nâ”‚   â”œâ”€â”€ requirements.txt\nâ”‚   â””â”€â”€ etl.py\nâ””â”€â”€ data/\n    â””â”€â”€ sales.csv\n\nLâ€™ETL doit :\n\nLire data/sales.csv\nCalculer une colonne total = quantity * price\nInsÃ©rer les donnÃ©es dans PostgreSQL\n\nUtiliser un healthcheck pour attendre que Postgres soit prÃªt\nLes donnÃ©es doivent Ãªtre montÃ©es via un volume\n\n\n\nâœ… Solution du mini-projet\n\n\nğŸ“¥ Afficher la solution complÃ¨te\n\n1. data/sales.csv\ndate,product,quantity,price\n2024-01-01,Laptop,5,999.99\n2024-01-02,Mouse,20,29.99\n2024-01-03,Keyboard,15,79.99\n2024-01-04,Monitor,8,299.99\n2024-01-05,Laptop,3,999.99\n2. etl/requirements.txt\npandas==2.1.4\npsycopg2-binary==2.9.9\nsqlalchemy==2.0.25\n3. etl/.dockerignore\n__pycache__/\n*.pyc\n.env\n*.log\n4. etl/Dockerfile\nFROM python:3.11-slim\n\nENV PYTHONUNBUFFERED=1\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY etl.py .\n\nCMD [\"python\", \"etl.py\"]\n5. etl/etl.py\nimport os\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\ndef main():\n    # Configuration depuis variables d'environnement\n    db_host = os.environ.get('DB_HOST', 'localhost')\n    db_user = os.environ.get('DB_USER', 'de_user')\n    db_pass = os.environ.get('DB_PASSWORD', 'de_pass')\n    db_name = os.environ.get('DB_NAME', 'de_db')\n    \n    print(\"ğŸš€ DÃ©marrage de l'ETL...\")\n    \n    # Extract\n    print(\"ğŸ“¥ Lecture du fichier CSV...\")\n    df = pd.read_csv('/app/data/sales.csv')\n    print(f\"   {len(df)} lignes lues\")\n    \n    # Transform\n    print(\"ğŸ”„ Transformation des donnÃ©es...\")\n    df['total'] = df['quantity'] * df['price']\n    df['loaded_at'] = pd.Timestamp.now()\n    \n    # Load\n    print(\"ğŸ“¤ Chargement dans PostgreSQL...\")\n    engine = create_engine(f'postgresql://{db_user}:{db_pass}@{db_host}/{db_name}')\n    df.to_sql('sales', engine, if_exists='replace', index=False)\n    \n    print(\"âœ… ETL terminÃ© avec succÃ¨s !\")\n    print(df.head())\n\nif __name__ == \"__main__\":\n    main()\n6. docker-compose.yml\nversion: \"3.9\"\n\nservices:\n  postgres:\n    image: postgres:16\n    container_name: de-postgres\n    environment:\n      POSTGRES_USER: de_user\n      POSTGRES_PASSWORD: de_pass\n      POSTGRES_DB: de_db\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - pg_data:/var/lib/postgresql/data\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U de_user -d de_db\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  etl:\n    build: ./etl\n    container_name: de-etl\n    depends_on:\n      postgres:\n        condition: service_healthy\n    environment:\n      DB_HOST: postgres\n      DB_USER: de_user\n      DB_PASSWORD: de_pass\n      DB_NAME: de_db\n    volumes:\n      - ./data:/app/data\n\nvolumes:\n  pg_data:\n7. Lancer le projet :\ncd de-mini-projet\ndocker compose up --build\n8. VÃ©rifier les donnÃ©es dans Postgres :\ndocker exec -it de-postgres psql -U de_user -d de_db -c \"SELECT * FROM sales;\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#ressources-pour-aller-plus-loin",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nDocker Docs â€” Documentation officielle\nDocker Hub â€” Registry dâ€™images publiques\nDockerfile reference â€” Toutes les instructions\n\n\n\nğŸ® Pratique\n\nPlay with Docker â€” Environnement Docker gratuit en ligne\nDocker 101 Tutorial â€” Tutoriel officiel interactif\n\n\n\nğŸ“¦ Images utiles pour Data Engineering\n\npostgres â€” Base de donnÃ©es relationnelle\napache/airflow â€” Orchestrateur\nbitnami/spark â€” Traitement distribuÃ©\nbitnami/kafka â€” Streaming\njupyter/scipy-notebook â€” Notebooks",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/14_docker_for_data_engineers.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/14_docker_for_data_engineers.html#prochaine-Ã©tape",
    "title": "ğŸ³ Docker pour Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Docker, passons Ã  lâ€™orchestration de containers Ã  grande Ã©chelle !\nğŸ‘‰ Module suivant : 15_kubernetes_fundamentals â€” Orchestrer des containers avec Kubernetes\nTu vas apprendre : - Pods, Deployments, Services - ConfigMaps, Secrets - Spark et Airflow sur Kubernetes\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Docker pour Data Engineers.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ³ Containers & Cloud",
      "14 Â· Docker pour Data Engineers"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser Spark SQL en profondeur. Tu dÃ©couvriras les Window Functions, les agrÃ©gations avancÃ©es, le reshaping de donnÃ©es, et comment optimiser tes requÃªtes SQL.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#prÃ©requis",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#prÃ©requis",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 19 : PySpark Advanced\n\n\nâœ… Requis\nConnaissances SQL de base\n\n\nğŸ’¡ RecommandÃ©\nExpÃ©rience avec des requÃªtes analytiques",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#objectifs-du-module",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#objectifs-du-module",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nMaÃ®triser les Window Functions (ranking, lag/lead, frames)\nUtiliser PIVOT/UNPIVOT pour reshaper les donnÃ©es\nAppliquer GROUPING SETS, CUBE, ROLLUP pour des agrÃ©gations multidimensionnelles\nManipuler les donnÃ©es semi-structurÃ©es avec EXPLODE\nStructurer des requÃªtes complexes avec CTEs\nOptimiser les requÃªtes SQL (hints, statistiques)\nConstruire un datamart analytique complet\n\n\n\nVoir le code\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\n\n# CrÃ©er une SparkSession\nspark = SparkSession.builder \\\n    .appName(\"Spark SQL Deep Dive\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .getOrCreate()\n\nprint(f\"âœ… Spark {spark.version} initialisÃ©\")\nprint(f\"ğŸ” Spark UI : {spark.sparkContext.uiWebUrl}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#sql-dans-spark-rappels-fondamentaux",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#sql-dans-spark-rappels-fondamentaux",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "1. SQL dans Spark : Rappels & Fondamentaux",
    "text": "1. SQL dans Spark : Rappels & Fondamentaux\n\nâš¡ Rappel rapide â€” Les dÃ©tails sur Catalyst et lâ€™architecture sont dans le module 19.\n\n\n1.1 CrÃ©er et utiliser des vues\n\n\nVoir le code\n# CrÃ©er des donnÃ©es de test\nsales_data = [\n    (1, \"Electronics\", \"North\", 1200, \"2024-01-15\"),\n    (2, \"Electronics\", \"South\", 800, \"2024-01-16\"),\n    (3, \"Clothing\", \"North\", 450, \"2024-01-15\"),\n    (4, \"Clothing\", \"South\", 650, \"2024-01-17\"),\n    (5, \"Electronics\", \"North\", 950, \"2024-01-18\"),\n    (6, \"Food\", \"South\", 320, \"2024-01-15\"),\n]\n\nsales_df = spark.createDataFrame(sales_data, \n    [\"id\", \"category\", \"region\", \"amount\", \"date\"])\n\n# CrÃ©er une vue temporaire\nsales_df.createOrReplaceTempView(\"sales\")\n\n# Utiliser SQL\nresult = spark.sql(\"\"\"\n    SELECT category, SUM(amount) as total_sales\n    FROM sales\n    WHERE amount &gt; 500\n    GROUP BY category\n    ORDER BY total_sales DESC\n\"\"\")\n\nresult.show()\n\n\n\n\n1.2 DataFrame API vs SQL\n\n\n\nAspect\nDataFrame API\nSQL\n\n\n\n\nTypage\nCompile-time\nRuntime\n\n\nLisibilitÃ©\nCode Python\nFamilier aux analystes\n\n\nRÃ©utilisabilitÃ©\nFonctions Python\nCTEs, Vues\n\n\nPerformance\nIdentique\nIdentique\n\n\nDebug\nStack trace Python\nErreurs SQL\n\n\nIDE Support\nAutocomplÃ©tion\nVariable\n\n\n\n\nğŸ’¡ RÃ¨gle : Utilise ce qui est le plus lisible pour ton Ã©quipe. Les deux sont optimisÃ©s par Catalyst.\n\n\n\nVoir le code\n# Ã‰quivalence DataFrame API vs SQL\n\n# === DataFrame API ===\nresult_df = sales_df \\\n    .filter(col(\"amount\") &gt; 500) \\\n    .groupBy(\"category\") \\\n    .agg(sum(\"amount\").alias(\"total_sales\")) \\\n    .orderBy(desc(\"total_sales\"))\n\n# === SQL ===\nresult_sql = spark.sql(\"\"\"\n    SELECT category, SUM(amount) as total_sales\n    FROM sales\n    WHERE amount &gt; 500\n    GROUP BY category\n    ORDER BY total_sales DESC\n\"\"\")\n\nprint(\"DataFrame API:\")\nresult_df.show()\n\nprint(\"SQL:\")\nresult_sql.show()\n\n# Les plans sont identiques !\nprint(\"\\nâœ… Les deux approches gÃ©nÃ¨rent le mÃªme plan d'exÃ©cution\")\n\n\n\n\n1.3 Lire un plan dâ€™exÃ©cution SQL\n\n\nVoir le code\n# CrÃ©er des donnÃ©es pour dÃ©montrer les joins\nproducts_data = [\n    (1, \"Laptop\", \"Electronics\"),\n    (2, \"T-Shirt\", \"Clothing\"),\n    (3, \"Apple\", \"Food\"),\n]\nproducts_df = spark.createDataFrame(products_data, [\"product_id\", \"name\", \"category\"])\nproducts_df.createOrReplaceTempView(\"products\")\n\n# Voir le plan d'exÃ©cution\nspark.sql(\"\"\"\n    EXPLAIN FORMATTED\n    SELECT s.*, p.name\n    FROM sales s\n    JOIN products p ON s.category = p.category\n    WHERE s.amount &gt; 500\n\"\"\").show(truncate=False)\n\n\nCe quâ€™il faut repÃ©rer dans le plan :\n\n\n\nÃ‰lÃ©ment\nSignification\nBon/Mauvais\n\n\n\n\nBroadcastHashJoin\nPetite table broadcastÃ©e\nâœ… Bon\n\n\nSortMergeJoin\nShuffle des deux tables\nâš ï¸ CoÃ»teux\n\n\nPushedFilters\nFiltre appliquÃ© Ã  la source\nâœ… Bon\n\n\nExchange\nShuffle (redistribution)\nâš ï¸ Ã€ surveiller",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#window-functions-le-cÅ“ur-analytique",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#window-functions-le-cÅ“ur-analytique",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "2. Window Functions â€” Le cÅ“ur analytique",
    "text": "2. Window Functions â€” Le cÅ“ur analytique\n\nğŸ”¥ Section la plus importante du module. Les Window Functions sont essentielles pour lâ€™analytics.\n\n\n2.1 Syntaxe et concepts\nFUNCTION() OVER (\n  PARTITION BY column1, column2   -- Grouper les donnÃ©es\n  ORDER BY column3                -- Ordonner dans chaque groupe\n  ROWS BETWEEN start AND end      -- DÃ©finir la fenÃªtre de calcul\n)\nğŸ–¼ï¸ Comment fonctionne une Window Function :\n\nDonnÃ©es originales          PARTITION BY category    ORDER BY date       Calcul sur la fenÃªtre\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ cat â”‚ date â”‚ amtâ”‚         â”‚ Electronics     â”‚     â”‚ 01-15 â”‚ 1200   â”‚  â”‚ ROW_NUMBER = 1  â”‚\nâ”‚ Elecâ”‚ 01-15â”‚1200â”‚   â†’     â”‚ â”œâ”€ 01-15 â”‚ 1200â”‚  â†’  â”‚ 01-16 â”‚  800   â”‚  â”‚ ROW_NUMBER = 2  â”‚\nâ”‚ Elecâ”‚ 01-16â”‚ 800â”‚         â”‚ â”œâ”€ 01-16 â”‚  800â”‚     â”‚ 01-18 â”‚  950   â”‚  â”‚ ROW_NUMBER = 3  â”‚\nâ”‚ Elecâ”‚ 01-18â”‚ 950â”‚         â”‚ â””â”€ 01-18 â”‚  950â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nâ”‚ Clthâ”‚ 01-15â”‚ 450â”‚         â”‚                 â”‚\nâ”‚ Clthâ”‚ 01-17â”‚ 650â”‚         â”‚ Clothing        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ â”œâ”€ 01-15 â”‚  450â”‚\n                            â”‚ â””â”€ 01-17 â”‚  650â”‚\n                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\n# CrÃ©er des donnÃ©es plus riches pour les window functions\norders_data = [\n    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n    (6, 103, \"2024-01-10\", 400.0, \"Premium\"),\n    (7, 103, \"2024-01-25\", 400.0, \"Premium\"),  # Ex-aequo intentionnel\n    (8, 103, \"2024-02-10\", 350.0, \"Premium\"),\n    (9, 104, \"2024-01-03\", 100.0, \"Standard\"),\n    (10, 104, \"2024-02-15\", 120.0, \"Standard\"),\n]\n\norders_df = spark.createDataFrame(orders_data,\n    [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\norders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n\norders_df.createOrReplaceTempView(\"orders\")\norders_df.show()\n\n\n\n\n2.2 Fonctions de Ranking\n\n\n\nFonction\nDescription\nGÃ¨re les ex-aequo\n\n\n\n\nROW_NUMBER()\nNumÃ©ro unique sÃ©quentiel\nNon (arbitraire)\n\n\nRANK()\nRang avec gaps aprÃ¨s ex-aequo\nOui (1,1,3)\n\n\nDENSE_RANK()\nRang sans gaps\nOui (1,1,2)\n\n\nNTILE(n)\nDivise en n groupes Ã©gaux\n-\n\n\nPERCENT_RANK()\nRang en percentile (0-1)\nOui\n\n\nCUME_DIST()\nDistribution cumulative\nOui\n\n\n\n\n\nVoir le code\n# DÃ©monstration des fonctions de ranking\nranking_result = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as row_num,\n        RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rank,\n        DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as dense_rank,\n        NTILE(2) OVER (PARTITION BY customer_id ORDER BY amount DESC) as ntile_2\n    FROM orders\n    WHERE customer_id = 103\n    ORDER BY customer_id, amount DESC\n\"\"\")\n\nprint(\"ğŸ“Š Comparaison ROW_NUMBER vs RANK vs DENSE_RANK (customer 103 avec ex-aequo):\")\nranking_result.show()\n\n\n\n\nVoir le code\n# Cas d'usage : Top 2 commandes par client\ntop_orders = spark.sql(\"\"\"\n    SELECT * FROM (\n        SELECT \n            customer_id,\n            order_date,\n            amount,\n            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY amount DESC) as rn\n        FROM orders\n    )\n    WHERE rn &lt;= 2\n    ORDER BY customer_id, rn\n\"\"\")\n\nprint(\"ğŸ† Top 2 commandes par client:\")\ntop_orders.show()\n\n\n\n\n2.3 Fonctions de dÃ©calage (LAG/LEAD)\n\n\n\nFonction\nDescription\n\n\n\n\nLAG(col, n, default)\nValeur n lignes AVANT\n\n\nLEAD(col, n, default)\nValeur n lignes APRÃˆS\n\n\nFIRST_VALUE(col)\nPremiÃ¨re valeur de la fenÃªtre\n\n\nLAST_VALUE(col)\nDerniÃ¨re valeur de la fenÃªtre\n\n\nNTH_VALUE(col, n)\nN-iÃ¨me valeur de la fenÃªtre\n\n\n\n\n\nVoir le code\n# Cas d'usage : DÃ©lai entre commandes\norder_gaps = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n        LEAD(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_order_date,\n        DATEDIFF(\n            order_date, \n            LAG(order_date, 1) OVER (PARTITION BY customer_id ORDER BY order_date)\n        ) as days_since_last_order\n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“… Analyse du dÃ©lai entre commandes:\")\norder_gaps.show()\n\n\n\n\nVoir le code\n# FIRST_VALUE et LAST_VALUE\nfirst_last = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        FIRST_VALUE(order_date) OVER (\n            PARTITION BY customer_id ORDER BY order_date\n        ) as first_order_date,\n        FIRST_VALUE(amount) OVER (\n            PARTITION BY customer_id ORDER BY order_date\n        ) as first_order_amount\n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“Š FIRST_VALUE - Date et montant de la premiÃ¨re commande:\")\nfirst_last.show()\n\n\n\n\n2.4 AgrÃ©gations dans une fenÃªtre\n\n\nVoir le code\n# Running total, Running average, % du total\nwindow_agg = spark.sql(\"\"\"\n    SELECT \n        customer_id,\n        order_date,\n        amount,\n        \n        -- Somme cumulative\n        SUM(amount) OVER (\n            PARTITION BY customer_id \n            ORDER BY order_date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as running_total,\n        \n        -- Total du client (pour calculer %)\n        SUM(amount) OVER (PARTITION BY customer_id) as customer_total,\n        \n        -- % de chaque commande\n        ROUND(amount / SUM(amount) OVER (PARTITION BY customer_id) * 100, 1) as pct_of_customer_total\n        \n    FROM orders\n    ORDER BY customer_id, order_date\n\"\"\")\n\nprint(\"ğŸ“Š AgrÃ©gations fenÃªtrÃ©es:\")\nwindow_agg.show()\n\n\n\n\n2.5 Window Frames (PiÃ¨ges Spark)\nROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW  -- Du dÃ©but jusqu'Ã  maintenant\nROWS BETWEEN 6 PRECEDING AND CURRENT ROW          -- 7 derniÃ¨res lignes (rolling)\nROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING  -- De maintenant jusqu'Ã  la fin\nROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING          -- 3 lignes (prÃ©cÃ©dente, courante, suivante)\n\n\n\n\n\n\n\n\nClause\nBasÃ©e sur\nComportement\n\n\n\n\nROWS\nPosition physique\nPrÃ©visible, recommandÃ©\n\n\nRANGE\nValeur logique\nâš ï¸ Peut inclure plus de lignes si doublons\n\n\n\n\nğŸ’¡ En cas de doute, utilise ROWS (plus prÃ©visible)\n\n\n\nVoir le code\n# Rolling average (moyenne mobile)\n# CrÃ©er plus de donnÃ©es pour dÃ©montrer\ndaily_sales = spark.createDataFrame([\n    (\"2024-01-01\", 100), (\"2024-01-02\", 150), (\"2024-01-03\", 120),\n    (\"2024-01-04\", 180), (\"2024-01-05\", 90), (\"2024-01-06\", 200),\n    (\"2024-01-07\", 170), (\"2024-01-08\", 140), (\"2024-01-09\", 160),\n    (\"2024-01-10\", 190),\n], [\"date\", \"sales\"])\ndaily_sales = daily_sales.withColumn(\"date\", to_date(col(\"date\")))\ndaily_sales.createOrReplaceTempView(\"daily_sales\")\n\n# Moyenne mobile sur 3 jours\nrolling = spark.sql(\"\"\"\n    SELECT \n        date,\n        sales,\n        ROUND(AVG(sales) OVER (\n            ORDER BY date\n            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n        ), 1) as rolling_avg_3d,\n        SUM(sales) OVER (\n            ORDER BY date\n            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n        ) as cumulative_sum\n    FROM daily_sales\n\"\"\")\n\nprint(\"ğŸ“ˆ Moyenne mobile 3 jours et somme cumulative:\")\nrolling.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#pivot-unpivot-reshaping-des-donnÃ©es",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#pivot-unpivot-reshaping-des-donnÃ©es",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "3. PIVOT & UNPIVOT â€” Reshaping des donnÃ©es",
    "text": "3. PIVOT & UNPIVOT â€” Reshaping des donnÃ©es\n\n3.1 PIVOT : lignes â†’ colonnes\nTransforme les valeurs dâ€™une colonne en colonnes distinctes.\n\n\nVoir le code\n# DonnÃ©es mensuelles\nmonthly_data = spark.createDataFrame([\n    (\"Alice\", \"Jan\", 1000), (\"Alice\", \"Feb\", 1200), (\"Alice\", \"Mar\", 1100),\n    (\"Bob\", \"Jan\", 800), (\"Bob\", \"Feb\", 900), (\"Bob\", \"Mar\", 950),\n    (\"Charlie\", \"Jan\", 1500), (\"Charlie\", \"Feb\", 1400), (\"Charlie\", \"Mar\", 1600),\n], [\"salesperson\", \"month\", \"revenue\"])\n\nmonthly_data.createOrReplaceTempView(\"monthly_sales\")\n\nprint(\"DonnÃ©es originales (format long):\")\nmonthly_data.show()\n\n# PIVOT : transformer les mois en colonnes\npivoted = spark.sql(\"\"\"\n    SELECT * FROM monthly_sales\n    PIVOT (\n        SUM(revenue)\n        FOR month IN ('Jan', 'Feb', 'Mar')\n    )\n\"\"\")\n\nprint(\"AprÃ¨s PIVOT (format large):\")\npivoted.show()\n\n\n\n\nVoir le code\n# PIVOT avec DataFrame API\npivoted_df = monthly_data.groupBy(\"salesperson\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\"]).sum(\"revenue\")\n\nprint(\"PIVOT avec DataFrame API:\")\npivoted_df.show()\n\n\n\n\n3.2 UNPIVOT : colonnes â†’ lignes\nLâ€™opÃ©ration inverse de PIVOT.\n\n\nVoir le code\n# CrÃ©er une vue du DataFrame pivotÃ©\npivoted_df.createOrReplaceTempView(\"pivoted_sales\")\n\n# UNPIVOT (Spark 3.4+)\n# Pour les versions antÃ©rieures, utiliser stack()\nunpivoted = spark.sql(\"\"\"\n    SELECT \n        salesperson,\n        stack(3, \n            'Jan', Jan, \n            'Feb', Feb, \n            'Mar', Mar\n        ) as (month, revenue)\n    FROM pivoted_sales\n\"\"\")\n\nprint(\"AprÃ¨s UNPIVOT (retour au format long):\")\nunpivoted.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#agrÃ©gations-avancÃ©es-grouping-sets-cube-rollup",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#agrÃ©gations-avancÃ©es-grouping-sets-cube-rollup",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "4. AgrÃ©gations AvancÃ©es â€” GROUPING SETS, CUBE, ROLLUP",
    "text": "4. AgrÃ©gations AvancÃ©es â€” GROUPING SETS, CUBE, ROLLUP\nCes fonctions permettent de calculer plusieurs niveaux dâ€™agrÃ©gation en une seule requÃªte.\n\n4.1 GROUPING SETS : agrÃ©gations multiples\n\n\nVoir le code\n# GROUPING SETS : plusieurs agrÃ©gations en une requÃªte\ngrouping_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales,\n        COUNT(*) as num_transactions\n    FROM sales\n    GROUP BY GROUPING SETS (\n        (category, region),  -- Par catÃ©gorie ET rÃ©gion\n        (category),          -- Par catÃ©gorie seulement\n        (region),            -- Par rÃ©gion seulement\n        ()                   -- Total global\n    )\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š GROUPING SETS - AgrÃ©gations multiples:\")\ngrouping_result.show()\n\n\n\n\n4.2 ROLLUP : hiÃ©rarchie dâ€™agrÃ©gations\nROLLUP(a, b, c) = GROUPING SETS ((a,b,c), (a,b), (a), ())\n\n\nVoir le code\n# ROLLUP : hiÃ©rarchie (catÃ©gorie â†’ rÃ©gion â†’ total)\nrollup_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales\n    FROM sales\n    GROUP BY ROLLUP (category, region)\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š ROLLUP - HiÃ©rarchie d'agrÃ©gations:\")\nrollup_result.show()\n\n\n\n\n4.3 CUBE : toutes les combinaisons\nCUBE(a, b) = GROUPING SETS ((a,b), (a), (b), ())\n\n\nVoir le code\n# CUBE : toutes les combinaisons possibles\ncube_result = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales\n    FROM sales\n    GROUP BY CUBE (category, region)\n    ORDER BY category NULLS LAST, region NULLS LAST\n\"\"\")\n\nprint(\"ğŸ“Š CUBE - Toutes les combinaisons:\")\ncube_result.show()\n\n\n\n\nVoir le code\n# GROUPING() : identifier les sous-totaux\ngrouping_with_flags = spark.sql(\"\"\"\n    SELECT \n        category,\n        region,\n        SUM(amount) as total_sales,\n        GROUPING(category) as is_category_subtotal,\n        GROUPING(region) as is_region_subtotal,\n        CASE \n            WHEN GROUPING(category) = 1 AND GROUPING(region) = 1 THEN 'GRAND TOTAL'\n            WHEN GROUPING(category) = 1 THEN 'Region Subtotal'\n            WHEN GROUPING(region) = 1 THEN 'Category Subtotal'\n            ELSE 'Detail'\n        END as row_type\n    FROM sales\n    GROUP BY CUBE (category, region)\n    ORDER BY GROUPING(category), GROUPING(region), category, region\n\"\"\")\n\nprint(\"ğŸ“Š GROUPING() pour identifier les sous-totaux:\")\ngrouping_with_flags.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#donnÃ©es-semi-structurÃ©es-explode-json",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#donnÃ©es-semi-structurÃ©es-explode-json",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "5. DonnÃ©es Semi-StructurÃ©es â€” EXPLODE & JSON",
    "text": "5. DonnÃ©es Semi-StructurÃ©es â€” EXPLODE & JSON\n\n5.1 EXPLODE : Ã©clater arrays et maps\n\n\nVoir le code\n# CrÃ©er des donnÃ©es avec arrays\ncustomers_with_tags = spark.createDataFrame([\n    (1, \"Alice\", [\"premium\", \"loyal\", \"newsletter\"]),\n    (2, \"Bob\", [\"new\", \"newsletter\"]),\n    (3, \"Charlie\", [\"premium\", \"vip\"]),\n], [\"id\", \"name\", \"tags\"])\n\ncustomers_with_tags.createOrReplaceTempView(\"customers_tags\")\n\nprint(\"DonnÃ©es avec arrays:\")\ncustomers_with_tags.show(truncate=False)\n\n# EXPLODE : une ligne par tag\nexploded = spark.sql(\"\"\"\n    SELECT id, name, tag\n    FROM customers_tags\n    LATERAL VIEW EXPLODE(tags) t AS tag\n\"\"\")\n\nprint(\"AprÃ¨s EXPLODE:\")\nexploded.show()\n\n\n\n\nVoir le code\n# POSEXPLODE : avec position\nposexploded = spark.sql(\"\"\"\n    SELECT id, name, pos, tag\n    FROM customers_tags\n    LATERAL VIEW POSEXPLODE(tags) t AS pos, tag\n\"\"\")\n\nprint(\"POSEXPLODE (avec index):\")\nposexploded.show()\n\n\n\n\nVoir le code\n# EXPLODE avec Map\ncustomers_with_attrs = spark.createDataFrame([\n    (1, \"Alice\", {\"city\": \"Paris\", \"country\": \"France\"}),\n    (2, \"Bob\", {\"city\": \"London\", \"country\": \"UK\", \"postal\": \"SW1\"}),\n], [\"id\", \"name\", \"attributes\"])\n\ncustomers_with_attrs.createOrReplaceTempView(\"customers_attrs\")\n\nprint(\"DonnÃ©es avec Map:\")\ncustomers_with_attrs.show(truncate=False)\n\n# EXPLODE sur Map â†’ (key, value)\nexploded_map = spark.sql(\"\"\"\n    SELECT id, name, key, value\n    FROM customers_attrs\n    LATERAL VIEW EXPLODE(attributes) t AS key, value\n\"\"\")\n\nprint(\"EXPLODE sur Map:\")\nexploded_map.show()\n\n\n\n\n5.2 AccÃ¨s JSON\n\n\nVoir le code\n# DonnÃ©es JSON\nevents = spark.createDataFrame([\n    (1, '{\"user\": \"alice\", \"action\": \"click\", \"details\": {\"page\": \"home\", \"duration\": 5}}'),\n    (2, '{\"user\": \"bob\", \"action\": \"purchase\", \"details\": {\"page\": \"cart\", \"amount\": 99.99}}'),\n], [\"id\", \"json_data\"])\n\nevents.createOrReplaceTempView(\"events\")\n\n# Extraire des champs JSON\njson_extracted = spark.sql(\"\"\"\n    SELECT \n        id,\n        get_json_object(json_data, '$.user') as user,\n        get_json_object(json_data, '$.action') as action,\n        get_json_object(json_data, '$.details.page') as page\n    FROM events\n\"\"\")\n\nprint(\"Extraction JSON avec get_json_object:\")\njson_extracted.show()\n\n\n\n\nVoir le code\n# Parser JSON complet avec schema\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\ndetails_schema = StructType([\n    StructField(\"page\", StringType()),\n    StructField(\"duration\", DoubleType()),\n    StructField(\"amount\", DoubleType())\n])\n\nevent_schema = StructType([\n    StructField(\"user\", StringType()),\n    StructField(\"action\", StringType()),\n    StructField(\"details\", details_schema)\n])\n\nparsed = events.withColumn(\"parsed\", from_json(col(\"json_data\"), event_schema))\nparsed.select(\"id\", \"parsed.user\", \"parsed.action\", \"parsed.details.page\").show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#ctes-subqueries-structurer-ses-requÃªtes",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#ctes-subqueries-structurer-ses-requÃªtes",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "6. CTEs & Subqueries â€” Structurer ses requÃªtes",
    "text": "6. CTEs & Subqueries â€” Structurer ses requÃªtes\n\n6.1 Common Table Expressions (CTEs)\n\n\nVoir le code\n# CTE : structurer une requÃªte complexe\ncte_result = spark.sql(\"\"\"\n    WITH \n    -- Ã‰tape 1 : AgrÃ©gation par client\n    customer_stats AS (\n        SELECT \n            customer_id,\n            COUNT(*) as num_orders,\n            SUM(amount) as total_spent,\n            AVG(amount) as avg_order\n        FROM orders\n        GROUP BY customer_id\n    ),\n    \n    -- Ã‰tape 2 : Moyenne globale\n    global_avg AS (\n        SELECT AVG(total_spent) as avg_total_spent\n        FROM customer_stats\n    )\n    \n    -- RequÃªte finale : clients au-dessus de la moyenne\n    SELECT \n        c.*,\n        g.avg_total_spent,\n        CASE WHEN c.total_spent &gt; g.avg_total_spent THEN 'Above Average' ELSE 'Below Average' END as status\n    FROM customer_stats c\n    CROSS JOIN global_avg g\n    ORDER BY total_spent DESC\n\"\"\")\n\nprint(\"ğŸ“Š Analyse client avec CTEs:\")\ncte_result.show()\n\n\n\n\n6.2 CTEs vs Subqueries\n\n\n\n\n\n\n\n\nAspect\nCTE\nSubquery\n\n\n\n\nLisibilitÃ©\nâœ… Excellent\nâš ï¸ Peut Ãªtre confus\n\n\nRÃ©utilisabilitÃ©\nâœ… Peut Ãªtre rÃ©fÃ©rencÃ© plusieurs fois\nâŒ RÃ©pÃ©tition nÃ©cessaire\n\n\nPerformance\nâš ï¸ Pas toujours optimisÃ©\nâš ï¸ Variable\n\n\n\n\nâš ï¸ Mythe : Les CTEs ne sont PAS toujours plus rapides. Catalyst les traite comme des subqueries inline.\n\n\n\nVoir le code\n# âŒ Subquery corrÃ©lÃ©e (potentiellement lent)\n# SELECT * FROM orders o\n# WHERE amount &gt; (SELECT AVG(amount) FROM orders WHERE customer_id = o.customer_id)\n\n# âœ… Window function (plus efficace)\nefficient_query = spark.sql(\"\"\"\n    SELECT * FROM (\n        SELECT \n            *,\n            AVG(amount) OVER (PARTITION BY customer_id) as avg_customer_amount\n        FROM orders\n    )\n    WHERE amount &gt; avg_customer_amount\n\"\"\")\n\nprint(\"âœ… Commandes au-dessus de la moyenne du client (window function):\")\nefficient_query.show()",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#optimisation-sql-dans-spark",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#optimisation-sql-dans-spark",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "7. Optimisation SQL dans Spark",
    "text": "7. Optimisation SQL dans Spark\n\n7.1 Join Hints\n\n\nVoir le code\n# CrÃ©er des tables pour dÃ©montrer les hints\nspark.sql(\"DROP TABLE IF EXISTS dim_segment\")\nsegments = spark.createDataFrame([\n    (\"Premium\", 1.2), (\"Standard\", 1.0)\n], [\"segment\", \"multiplier\"])\nsegments.createOrReplaceTempView(\"dim_segment\")\n\n# BROADCAST hint\nbroadcast_join = spark.sql(\"\"\"\n    SELECT /*+ BROADCAST(dim_segment) */ \n        o.*,\n        s.multiplier,\n        o.amount * s.multiplier as adjusted_amount\n    FROM orders o\n    JOIN dim_segment s ON o.segment = s.segment\n\"\"\")\n\nprint(\"Plan avec BROADCAST hint:\")\nbroadcast_join.explain()\nprint(\"\\nRÃ©sultat:\")\nbroadcast_join.show(5)\n\n\nHints disponibles :\n\n\n\n\n\n\n\n\nHint\nUsage\nQuand lâ€™utiliser\n\n\n\n\n/*+ BROADCAST(table) */\nForce broadcast\nPetite table (&lt; 100 MB)\n\n\n/*+ MERGE(t1, t2) */\nForce sort-merge join\nGrandes tables triÃ©es\n\n\n/*+ SHUFFLE_HASH(t1) */\nForce shuffle hash\nTables moyennes\n\n\n/*+ COALESCE(n) */\nRÃ©duit les partitions\nAvant Ã©criture\n\n\n\n\n\nVoir le code\n# Collecter les statistiques (amÃ©liore l'optimiseur)\n# Note : fonctionne sur des tables persistÃ©es, pas des vues temporaires\n\nprint(\"ğŸ’¡ Pour collecter des statistiques sur une table persistÃ©e :\")\nprint(\"\"\"\n-- Statistiques de base\nANALYZE TABLE my_table COMPUTE STATISTICS\n\n-- Statistiques sur colonnes spÃ©cifiques\nANALYZE TABLE my_table COMPUTE STATISTICS FOR COLUMNS col1, col2\n\n-- Voir les statistiques\nDESCRIBE EXTENDED my_table\n\"\"\")\n\n\n\n\n7.2 Anti-patterns SQL\n\n\n\n\n\n\n\n\nAnti-pattern\nProblÃ¨me\nSolution\n\n\n\n\nSELECT *\nLit toutes les colonnes\nSELECT col1, col2\n\n\nORDER BY sans LIMIT\nTri global coÃ»teux\nAjouter LIMIT\n\n\nUDF dans WHERE\nPas de pushdown\nExpression native\n\n\nCOUNT(DISTINCT) haute cardinalitÃ©\nTrÃ¨s lent\nAPPROX_COUNT_DISTINCT\n\n\nNOT IN avec NULL\nRÃ©sultats inattendus\nNOT EXISTS ou LEFT JOIN\n\n\n\n\n\nVoir le code\n# Approximation vs COUNT DISTINCT\ncomparison = spark.sql(\"\"\"\n    SELECT \n        COUNT(DISTINCT customer_id) as exact_count,\n        APPROX_COUNT_DISTINCT(customer_id) as approx_count\n    FROM orders\n\"\"\")\n\nprint(\"COUNT DISTINCT vs APPROX_COUNT_DISTINCT:\")\ncomparison.show()\nprint(\"ğŸ’¡ APPROX_COUNT_DISTINCT est ~10x plus rapide sur de grandes tables\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#mini-projet-datamart-customer-analytics",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#mini-projet-datamart-customer-analytics",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "Mini-Projet : Datamart Customer Analytics",
    "text": "Mini-Projet : Datamart Customer Analytics\n\nObjectif\nConstruire un datamart analytique client complet en utilisant toutes les techniques apprises.\n\n\nMÃ©triques Ã  calculer\n\nDate premiÃ¨re commande (FIRST_VALUE)\nNombre de commandes (COUNT OVER)\nRevenu total client (SUM OVER)\nDÃ©lai moyen entre commandes (LAG + AVG)\nRang du client par segment (RANK)\n% du revenu du segment (SUM OVER partition)\n\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Orders    â”‚     â”‚  Segments   â”‚\nâ”‚   (fact)    â”‚     â”‚   (dim)     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       â”‚                   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  CTE: enriched    â”‚\n       â”‚  - joins          â”‚\n       â”‚  - window funcs   â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚  CTE: customer    â”‚\n       â”‚      stats        â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                 â”‚\n       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n       â”‚    Datamart       â”‚\n       â”‚   [Parquet]       â”‚\n       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nVoir le code\nimport os\nimport shutil\n\n# CrÃ©er des donnÃ©es plus riches\norders_extended = spark.createDataFrame([\n    # Customer 101 - Premium, 3 commandes\n    (1, 101, \"2024-01-01\", 150.0, \"Premium\"),\n    (2, 101, \"2024-01-15\", 200.0, \"Premium\"),\n    (3, 101, \"2024-02-01\", 180.0, \"Premium\"),\n    # Customer 102 - Standard, 4 commandes\n    (4, 102, \"2024-01-05\", 300.0, \"Standard\"),\n    (5, 102, \"2024-01-20\", 250.0, \"Standard\"),\n    (6, 102, \"2024-02-10\", 275.0, \"Standard\"),\n    (7, 102, \"2024-03-01\", 320.0, \"Standard\"),\n    # Customer 103 - Premium, 2 commandes\n    (8, 103, \"2024-01-10\", 400.0, \"Premium\"),\n    (9, 103, \"2024-02-15\", 450.0, \"Premium\"),\n    # Customer 104 - Standard, 3 commandes\n    (10, 104, \"2024-01-03\", 100.0, \"Standard\"),\n    (11, 104, \"2024-01-25\", 120.0, \"Standard\"),\n    (12, 104, \"2024-02-20\", 90.0, \"Standard\"),\n    # Customer 105 - Premium, 1 commande\n    (13, 105, \"2024-02-01\", 500.0, \"Premium\"),\n], [\"order_id\", \"customer_id\", \"order_date\", \"amount\", \"segment\"])\n\norders_extended = orders_extended.withColumn(\"order_date\", to_date(col(\"order_date\")))\norders_extended.createOrReplaceTempView(\"orders_ext\")\n\nprint(\"ğŸ“¦ DonnÃ©es source:\")\norders_extended.show()\n\n\n\n\nVoir le code\n# Construction du Datamart avec CTEs et Window Functions\ndatamart = spark.sql(\"\"\"\n    WITH \n    -- Ã‰tape 1 : Enrichir les commandes avec mÃ©triques temporelles\n    orders_enriched AS (\n        SELECT \n            customer_id,\n            segment,\n            order_date,\n            amount,\n            \n            -- NumÃ©ro de commande du client\n            ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_number,\n            \n            -- Date de la commande prÃ©cÃ©dente\n            LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_order_date,\n            \n            -- DÃ©lai depuis la derniÃ¨re commande\n            DATEDIFF(\n                order_date,\n                LAG(order_date) OVER (PARTITION BY customer_id ORDER BY order_date)\n            ) as days_since_last\n        FROM orders_ext\n    ),\n    \n    -- Ã‰tape 2 : AgrÃ©ger par client\n    customer_stats AS (\n        SELECT \n            customer_id,\n            segment,\n            \n            -- PremiÃ¨re commande\n            MIN(order_date) as first_order_date,\n            \n            -- DerniÃ¨re commande\n            MAX(order_date) as last_order_date,\n            \n            -- Nombre de commandes\n            COUNT(*) as total_orders,\n            \n            -- Revenus\n            SUM(amount) as total_revenue,\n            AVG(amount) as avg_order_value,\n            \n            -- DÃ©lai moyen entre commandes (exclut NULL de la premiÃ¨re commande)\n            AVG(days_since_last) as avg_days_between_orders\n        FROM orders_enriched\n        GROUP BY customer_id, segment\n    ),\n    \n    -- Ã‰tape 3 : Ajouter des mÃ©triques de segment\n    customer_with_segment_stats AS (\n        SELECT \n            *,\n            \n            -- Total du segment\n            SUM(total_revenue) OVER (PARTITION BY segment) as segment_total_revenue,\n            \n            -- % du revenu du segment\n            ROUND(total_revenue / SUM(total_revenue) OVER (PARTITION BY segment) * 100, 2) as pct_of_segment,\n            \n            -- Rang dans le segment\n            RANK() OVER (PARTITION BY segment ORDER BY total_revenue DESC) as rank_in_segment,\n            \n            -- Nombre de clients dans le segment\n            COUNT(*) OVER (PARTITION BY segment) as customers_in_segment\n        FROM customer_stats\n    )\n    \n    -- RÃ©sultat final\n    SELECT \n        customer_id,\n        segment,\n        first_order_date,\n        last_order_date,\n        total_orders,\n        ROUND(total_revenue, 2) as total_revenue,\n        ROUND(avg_order_value, 2) as avg_order_value,\n        ROUND(avg_days_between_orders, 1) as avg_days_between_orders,\n        rank_in_segment,\n        pct_of_segment,\n        customers_in_segment\n    FROM customer_with_segment_stats\n    ORDER BY segment, rank_in_segment\n\"\"\")\n\nprint(\"ğŸ“Š DATAMART CUSTOMER ANALYTICS:\")\ndatamart.show(truncate=False)\n\n\n\n\nVoir le code\n# Exporter le datamart\noutput_path = \"/tmp/customer_datamart\"\nif os.path.exists(output_path):\n    shutil.rmtree(output_path)\n\ndatamart.write.partitionBy(\"segment\").parquet(output_path)\n\nprint(f\"âœ… Datamart exportÃ© : {output_path}\")\nprint(f\"ğŸ“ PartitionnÃ© par segment\")\n\n# VÃ©rifier la structure\nfor root, dirs, files in os.walk(output_path):\n    level = root.replace(output_path, '').count(os.sep)\n    indent = ' ' * 2 * level\n    print(f\"{indent}{os.path.basename(root)}/\")\n\n\n\n\nVoir le code\n# RÃ©sumÃ© du mini-projet\nprint(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘           ğŸ“Š RÃ‰SUMÃ‰ DU DATAMART                              â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                              â•‘\nâ•‘  Techniques utilisÃ©es :                                      â•‘\nâ•‘  âœ… CTEs pour structurer la transformation                   â•‘\nâ•‘  âœ… ROW_NUMBER() pour numÃ©roter les commandes                â•‘\nâ•‘  âœ… LAG() pour calculer le dÃ©lai entre commandes             â•‘\nâ•‘  âœ… RANK() pour classer les clients par segment              â•‘\nâ•‘  âœ… SUM() OVER pour calculer les totaux de segment           â•‘\nâ•‘  âœ… Partitionnement Parquet par segment                      â•‘\nâ•‘                                                              â•‘\nâ•‘  MÃ©triques calculÃ©es :                                       â•‘\nâ•‘  â€¢ Date premiÃ¨re/derniÃ¨re commande                           â•‘\nâ•‘  â€¢ Nombre total de commandes                                 â•‘\nâ•‘  â€¢ Revenu total et moyen                                     â•‘\nâ•‘  â€¢ DÃ©lai moyen entre commandes                               â•‘\nâ•‘  â€¢ Rang dans le segment                                      â•‘\nâ•‘  â€¢ % du revenu du segment                                    â•‘\nâ•‘                                                              â•‘\nâ•‘  ğŸ’¡ Ce type de transformation sera automatisÃ©                â•‘\nâ•‘     avec dbt dans le module 26                               â•‘\nâ•‘                                                              â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\"\"\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#quiz-de-fin-de-module",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre ROW_NUMBER() et RANK() ?\n\nROW_NUMBER() est plus rapide\n\nRANK() gÃ¨re les ex-aequo avec des gaps, ROW_NUMBER() donne des numÃ©ros uniques\n\nROW_NUMBER() nÃ©cessite ORDER BY, pas RANK()\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” RANK() donne le mÃªme rang aux ex-aequo (1,1,3), ROW_NUMBER() donne toujours des numÃ©ros uniques (1,2,3).\n\n\n\n\nâ“ Q2. Quelle est la diffÃ©rence entre ROWS et RANGE dans une window frame ?\n\nROWS est plus rapide\n\nRANGE supporte plus de fonctions\n\nROWS se base sur la position physique, RANGE sur la valeur logique\n\nRANGE ne fonctionne quâ€™avec des dates\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” ROWS compte les lignes physiquement, RANGE peut inclure plusieurs lignes si elles ont la mÃªme valeur.\n\n\n\n\nâ“ Q3. Que fait la fonction GROUPING() ?\n\nGroupe les donnÃ©es\n\nIdentifie si une colonne est agrÃ©gÃ©e (sous-total)\n\nCompte le nombre de groupes\n\nTrie les groupes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” GROUPING() retourne 1 si la colonne est agrÃ©gÃ©e (NULL dans un sous-total), 0 sinon.\n\n\n\n\nâ“ Q4. Comment forcer un broadcast join en SQL Spark ?\n\nFORCE BROADCAST\n\n/*+ BROADCAST(table) */\n\nBROADCAST JOIN\n\nSET spark.broadcast = true\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” On utilise le hint SQL /*+ BROADCAST(table) */ aprÃ¨s SELECT.\n\n\n\n\nâ“ Q5. PIVOT transformeâ€¦ ?\n\nColonnes en lignes\n\nLignes en colonnes\n\nJSON en colonnes\n\nArrays en lignes\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” PIVOT transforme les valeurs distinctes dâ€™une colonne en colonnes sÃ©parÃ©es.\n\n\n\n\nâ“ Q6. EXPLODE est utilisÃ© pourâ€¦ ?\n\nCompresser les donnÃ©es\n\nSupprimer les doublons\n\nTransformer un array en plusieurs lignes\n\nJoindre des tables\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” EXPLODE Ã©clate un array (ou map) en crÃ©ant une ligne pour chaque Ã©lÃ©ment.\n\n\n\n\nâ“ Q7. Les CTEs sont-ils toujours plus performants que les subqueries ?\n\nOui, toujours\n\nNon, Catalyst les traite de maniÃ¨re similaire\n\nOui, car ils sont matÃ©rialisÃ©s\n\nNon, ils sont toujours plus lents\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Câ€™est un mythe ! Catalyst inline les CTEs comme des subqueries. Lâ€™avantage est la lisibilitÃ©.\n\n\n\n\nâ“ Q8. Quel hint utiliser pour Ã©viter un shuffle lors dâ€™un join avec une petite table ?\n\n/*+ MERGE */\n\n/*+ SHUFFLE_HASH */\n\n/*+ BROADCAST */\n\n/*+ COALESCE */\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” BROADCAST envoie la petite table Ã  tous les executors, Ã©vitant le shuffle.\n\n\n\n\nâ“ Q9. UNBOUNDED PRECEDING signifieâ€¦ ?\n\nLa ligne prÃ©cÃ©dente\n\nToutes les lignes depuis le dÃ©but de la partition\n\nLa premiÃ¨re ligne de la table\n\nAucune limite de mÃ©moire\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” UNBOUNDED PRECEDING inclut toutes les lignes depuis le dÃ©but de la partition jusquâ€™Ã  la position actuelle.\n\n\n\n\nâ“ Q10. Comment collecter des statistiques sur une table pour amÃ©liorer lâ€™optimiseur ?\n\nCOMPUTE STATS table\n\nANALYZE TABLE table COMPUTE STATISTICS\n\nCOLLECT STATISTICS table\n\nDESCRIBE STATISTICS table\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ANALYZE TABLE table COMPUTE STATISTICS collecte les statistiques pour lâ€™optimiseur Catalyst.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#ressources-pour-aller-plus-loin",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nSpark SQL Guide\nWindow Functions\nSQL Syntax\n\n\n\nğŸ“– Articles & Tutoriels\n\nDatabricks - Window Functions\nAdvanced SQL Optimization",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/20_spark_sql_deep_dive.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/20_spark_sql_deep_dive.html#prochaine-Ã©tape",
    "title": "Spark SQL Deep Dive & Optimisation",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises Spark SQL, passons Ã  spark sur K8S !\nğŸ‘‰ Module suivant : 21_spark_on_kubernetes - Spark & Kubernetes\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\n\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nWindow Functions\nROW_NUMBER, RANK, LAG/LEAD, FIRST_VALUE, frames\n\n\nPIVOT/UNPIVOT\nReshaping des donnÃ©es\n\n\nGROUPING SETS\nCUBE, ROLLUP, agrÃ©gations multidimensionnelles\n\n\nEXPLODE\nDonnÃ©es semi-structurÃ©es, JSON\n\n\nCTEs\nStructurer les requÃªtes complexes\n\n\nOptimisation\nHints, statistiques, anti-patterns\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Spark SQL Deep Dive.\n\n\nVoir le code\n# Nettoyage\nspark.stop()\nprint(\"âœ… SparkSession arrÃªtÃ©e\")\n\n# Nettoyage des fichiers temporaires (optionnel)\n# import shutil\n# if os.path.exists(\"/tmp/customer_datamart\"):\n#     shutil.rmtree(\"/tmp/customer_datamart\")\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "âš¡ Spark AvancÃ©",
      "20 Â· Spark SQL Deep Dive"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html",
    "href": "notebooks/intermediate/18_high_performance_python.html",
    "title": "High Performance Python",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas apprendre Ã  accÃ©lÃ©rer considÃ©rablement tes pipelines Python. Tu dÃ©couvriras comment contourner les limitations du GIL, parallÃ©liser tes traitements, et gÃ©rer des fichiers plus grands que ta RAM !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#prÃ©requis",
    "href": "notebooks/intermediate/18_high_performance_python.html#prÃ©requis",
    "title": "High Performance Python",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\nNiveau\nCompÃ©tence\n\n\n\n\nâœ… Requis\nMaÃ®triser Python (fonctions, classes)\n\n\nâœ… Requis\nAvoir suivi le module 17_polars_for_data_engineering\n\n\nğŸ’¡ RecommandÃ©\nConnaÃ®tre Pandas",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#objectifs-du-module",
    "href": "notebooks/intermediate/18_high_performance_python.html#objectifs-du-module",
    "title": "High Performance Python",
    "section": "ğŸ¯ Objectifs du module",
    "text": "ğŸ¯ Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre le GIL et ses implications sur la performance\nProfiler ton code pour identifier les vrais goulots dâ€™Ã©tranglement\nUtiliser concurrent.futures pour parallÃ©liser simplement\nMaÃ®triser asyncio pour lâ€™I/O massivement parallÃ¨le\nExploiter Dask pour traiter des fichiers plus grands que la RAM\nChoisir le bon outil selon ton problÃ¨me",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#le-gil-comprendre-la-limitation-fondamentale",
    "href": "notebooks/intermediate/18_high_performance_python.html#le-gil-comprendre-la-limitation-fondamentale",
    "title": "High Performance Python",
    "section": "1. Le GIL : Comprendre la limitation fondamentale",
    "text": "1. Le GIL : Comprendre la limitation fondamentale\n\nCette section est essentielle pour comprendre pourquoi certaines techniques fonctionnent et dâ€™autres non.\n\n\n1.1 Quâ€™est-ce que le GIL ?\nLe Global Interpreter Lock (GIL) est un verrou interne Ã  Python qui empÃªche deux threads dâ€™exÃ©cuter du code Python en mÃªme temps.\n\nPourquoi le GIL existe ?\nPython gÃ¨re la mÃ©moire automatiquement (garbage collector). Sans le GIL, deux threads pourraient modifier le mÃªme objet simultanÃ©ment â†’ corruption de mÃ©moire. Le GIL est une solution simple mais qui limite la performance.\n\n\nVisualisation du GIL\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                          AVEC LE GIL (threading)                         â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                          â•‘\nâ•‘  Thread 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘             â•‘\nâ•‘  Thread 2: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             â•‘\nâ•‘            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps    â•‘\nâ•‘                                                                          â•‘\nâ•‘  â†’ Un seul thread s'exÃ©cute Ã  la fois !                                 â•‘\nâ•‘  â†’ Les threads ALTERNENT, ils ne sont pas vraiment parallÃ¨les           â•‘\nâ•‘  â†’ Pour du calcul CPU : PAS de gain de performance !                    â•‘\nâ•‘                                                                          â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                       SANS GIL (multiprocessing)                         â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                          â•‘\nâ•‘  Process 1: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             â•‘\nâ•‘  Process 2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             â•‘\nâ•‘             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ temps   â•‘\nâ•‘                                                                          â•‘\nâ•‘  â†’ VRAIE exÃ©cution parallÃ¨le sur plusieurs CPUs !                       â•‘\nâ•‘  â†’ Chaque process a son propre interprÃ©teur Python                      â•‘\nâ•‘  â†’ Pour du calcul CPU : gain de performance proportionnel aux CPUs      â•‘\nâ•‘                                                                          â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\n\n1.2 CPU-bound vs I/O-bound : La distinction CRUCIALE\nAvant de choisir une technique, tu DOIS savoir si ton problÃ¨me est CPU-bound ou I/O-bound :\n\n\n\n\n\n\n\n\n\n\nType\nCâ€™est quoi ?\nExemples\nLe GIL bloque ?\nSolution\n\n\n\n\nCPU-bound\nLe CPU travaille en continu\nCalculs mathÃ©matiques, transformations de donnÃ©es, parsing, compression\nâœ… OUI\nmultiprocessing, ProcessPoolExecutor\n\n\nI/O-bound\nLe CPU attend des donnÃ©es externes\nRequÃªtes API, lecture fichiers, requÃªtes base de donnÃ©es\nâŒ NON\nthreading, asyncio\n\n\n\n\nPourquoi le GIL ne bloque PAS lâ€™I/O ?\nQuand Python attend une rÃ©ponse (rÃ©seau, disque, etc.), il relÃ¢che le GIL automatiquement :\nThread 1:  [code Python]â”€â”€â–¶[attend rÃ©seau]â”€â”€â–¶[code Python]\n                 GIL â†“           â†“ GIL libre     â†“ GIL\n                 \nThread 2:  [attend GIL]â”€â”€â–¶[code Python]â”€â”€â–¶[attend GIL]\n                              GIL â†“\n\nâ†’ Pendant que Thread 1 attend le rÃ©seau, Thread 2 peut s'exÃ©cuter !\n\n\n\n1.3 DÃ©monstration : GIL en action\n\nâ„¹ï¸ Le savais-tu ?\nLe GIL a Ã©tÃ© introduit dans Python pour simplifier la gestion de la mÃ©moire. Il rend Python thread-safe par dÃ©faut, mais au prix de la performance multi-thread.\nDes projets comme nogil (Python 3.13+) et subinterpreters travaillent Ã  supprimer ou contourner cette limitation.\nEn attendant, les Data Engineers utilisent multiprocessing pour contourner le GIL !\n\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  DÃ‰MONSTRATION DU GIL : Threading vs Multiprocessing                     â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport time\nimport threading\nfrom multiprocessing import Process\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION CPU-INTENSIVE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Cette fonction fait des calculs lourds (boucle + opÃ©rations mathÃ©matiques)\n# C'est du \"CPU-bound\" car le CPU travaille en continu sans attendre\n\ndef cpu_intensive(n):\n    \"\"\"\n    TÃ¢che CPU-bound : calcul intensif.\n    \n    Args:\n        n: Nombre d'itÃ©rations (plus c'est grand, plus c'est long)\n    \n    Returns:\n        La somme des carrÃ©s de 0 Ã  n-1\n    \"\"\"\n    total = 0\n    for i in range(n):\n        total += i ** 2  # OpÃ©ration CPU : Ã©lÃ©vation au carrÃ©\n    return total\n\n# ParamÃ¨tre : 5 millions d'itÃ©rations par appel\nN = 5_000_000\n\nprint(\"=\" * 60)\nprint(\"DÃ‰MONSTRATION : Impact du GIL sur les tÃ¢ches CPU-bound\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# TEST 1 : EXÃ‰CUTION SÃ‰QUENTIELLE (baseline)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# On exÃ©cute la fonction 2 fois, l'une aprÃ¨s l'autre\n# C'est notre rÃ©fÃ©rence pour mesurer le gain des autres mÃ©thodes\n\nprint(\"\\n Test 1 : SÃ©quentiel (rÃ©fÃ©rence)\")\nstart = time.time()\n\ncpu_intensive(N)  # Premier appel\ncpu_intensive(N)  # DeuxiÃ¨me appel (attend que le premier finisse)\n\nseq_time = time.time() - start\nprint(f\"   Temps : {seq_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# TEST 2 : THREADING (bloquÃ© par le GIL)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# On crÃ©e 2 threads qui exÃ©cutent cpu_intensive en \"parallÃ¨le\"\n# MAIS : le GIL empÃªche l'exÃ©cution simultanÃ©e !\n# RÃ©sultat attendu : temps SIMILAIRE au sÃ©quentiel (pas de gain)\n\nprint(\"\\n Test 2 : Threading (2 threads)\")\nstart = time.time()\n\n# CrÃ©er les threads\nt1 = threading.Thread(target=cpu_intensive, args=(N,))  # Thread 1\nt2 = threading.Thread(target=cpu_intensive, args=(N,))  # Thread 2\n\n# DÃ©marrer les threads\nt1.start()  # Lance Thread 1\nt2.start()  # Lance Thread 2 (mais GIL bloque l'exÃ©cution simultanÃ©e !)\n\n# Attendre que les threads finissent\nt1.join()  # Attend Thread 1\nt2.join()  # Attend Thread 2\n\nthread_time = time.time() - start\nprint(f\"   Temps : {thread_time:.2f}s\")\nprint(f\"   Pas plus rapide ! Le GIL empÃªche la parallÃ©lisation.\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# TEST 3 : MULTIPROCESSING (contourne le GIL)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# On crÃ©e 2 PROCESSUS sÃ©parÃ©s (pas des threads)\n# Chaque processus a son propre interprÃ©teur Python = son propre GIL\n# RÃ©sultat attendu : temps DIVISÃ‰ PAR 2 (vraie parallÃ©lisation)\n\nprint(\"\\n Test 3 : Multiprocessing (2 processus)\")\nstart = time.time()\n\n# CrÃ©er les processus\np1 = Process(target=cpu_intensive, args=(N,))  # Processus 1\np2 = Process(target=cpu_intensive, args=(N,))  # Processus 2\n\n# DÃ©marrer les processus\np1.start()  # Lance Processus 1 sur CPU 1\np2.start()  # Lance Processus 2 sur CPU 2 (VRAIMENT en parallÃ¨le !)\n\n# Attendre que les processus finissent\np1.join()  # Attend Processus 1\np2.join()  # Attend Processus 2\n\nproc_time = time.time() - start\nprint(f\"   Temps : {proc_time:.2f}s\")\nprint(f\"    ~2x plus rapide ! Le multiprocessing contourne le GIL.\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# RÃ‰SUMÃ‰\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RÃ‰SUMÃ‰\")\nprint(\"=\" * 60)\nprint(f\"   SÃ©quentiel      : {seq_time:.2f}s (rÃ©fÃ©rence)\")\nprint(f\"   Threading       : {thread_time:.2f}s (speedup : {seq_time/thread_time:.1f}x)\")\nprint(f\"   Multiprocessing : {proc_time:.2f}s (speedup : {seq_time/proc_time:.1f}x)\")\nprint()\nprint(\"CONCLUSION :\")\nprint(\"   Pour du CPU-bound â†’ utilise multiprocessing (ou ProcessPoolExecutor)\")\nprint(\"   Pour de l\\'I/O-bound â†’ threading ou asyncio fonctionnent bien\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#identifier-les-goulots-profiling",
    "href": "notebooks/intermediate/18_high_performance_python.html#identifier-les-goulots-profiling",
    "title": "High Performance Python",
    "section": "2. Identifier les goulots : Profiling",
    "text": "2. Identifier les goulots : Profiling\n\nâš ï¸ â€œPremature optimization is the root of all evilâ€ â€” Donald Knuth\nAvant dâ€™optimiser, il faut mesurer pour identifier le vrai problÃ¨me.\n\n\n2.1 Outils de profiling\n\n\n\nOutil\nUsage\nComment lâ€™utiliser\n\n\n\n\n%%time\nTemps dâ€™une cellule\nJupyter magic\n\n\n%%timeit\nTemps moyen (plusieurs runs)\nJupyter magic\n\n\ncProfile\nProfiling par fonction\npython -m cProfile script.py\n\n\nline_profiler\nProfiling ligne par ligne\n@profile decorator\n\n\nmemory_profiler\nUsage RAM\n@profile + mprof run\n\n\n\n\n\nVoir le code\n# %%time - mesure le temps d'exÃ©cution d'une cellule\nimport time\n\ndef slow_function():\n    total = 0\n    for i in range(1_000_000):\n        total += i\n    return total\n\n%time result = slow_function()\n\n\n\n\nVoir le code\n# %%timeit - moyenne sur plusieurs exÃ©cutions\ndef fast_function():\n    return sum(range(1_000_000))\n\n%timeit fast_function()\n\n\n\n\nVoir le code\nimport cProfile\nimport pstats\nfrom io import StringIO\n\ndef main_pipeline():\n    \"\"\"Pipeline simulÃ© avec plusieurs Ã©tapes\"\"\"\n    data = list(range(100_000))\n    \n    # Ã‰tape 1 : transformation\n    transformed = [x ** 2 for x in data]\n    \n    # Ã‰tape 2 : filtrage\n    filtered = [x for x in transformed if x % 2 == 0]\n    \n    # Ã‰tape 3 : agrÃ©gation\n    result = sum(filtered)\n    \n    return result\n\n# Profiler le code\nprofiler = cProfile.Profile()\nprofiler.enable()\n\nresult = main_pipeline()\n\nprofiler.disable()\n\n# Afficher les rÃ©sultats\nstream = StringIO()\nstats = pstats.Stats(profiler, stream=stream).sort_stats('cumulative')\nstats.print_stats(10)\nprint(stream.getvalue())",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#stratÃ©gies-de-performance-vue-densemble",
    "href": "notebooks/intermediate/18_high_performance_python.html#stratÃ©gies-de-performance-vue-densemble",
    "title": "High Performance Python",
    "section": "3. StratÃ©gies de performance : Vue dâ€™ensemble",
    "text": "3. StratÃ©gies de performance : Vue dâ€™ensemble\n\n\n\n\n\n\n\n\nBesoin\nSolution\nQuand lâ€™utiliser\n\n\n\n\nMulti-CPU (CPU-bound)\nProcessPoolExecutor\nETL lourd, calculs\n\n\nI/O parallÃ¨le (simple)\nThreadPoolExecutor\n&lt; 20 requÃªtes/fichiers\n\n\nI/O parallÃ¨le (massif)\nasyncio\n100+ requÃªtes API\n\n\nGros fichiers (&gt; RAM)\nPolars streaming, Dask\n10-100+ Go\n\n\nParallÃ©lisation simple\njoblib\nBoucles, ML\n\n\n\n\nArbre de dÃ©cision\nTon problÃ¨me est...\nâ”‚\nâ”œâ”€â–¶ CPU-bound (calculs, transformations) ?\nâ”‚   â”œâ”€â–¶ Simple/boucle â†’ joblib\nâ”‚   â””â”€â–¶ Complexe/chunks â†’ ProcessPoolExecutor\nâ”‚\nâ”œâ”€â–¶ I/O-bound (API, fichiers, DB) ?\nâ”‚   â”œâ”€â–¶ &lt; 20 requÃªtes â†’ ThreadPoolExecutor\nâ”‚   â””â”€â–¶ 100+ requÃªtes â†’ asyncio\nâ”‚\nâ””â”€â–¶ Gros fichiers (&gt; RAM) ?\n    â”œâ”€â–¶ Single file, Polars-like â†’ Polars streaming\n    â”œâ”€â–¶ Multi-files, Pandas-like â†’ Dask\n    â””â”€â–¶ Cluster distribuÃ© â†’ Spark (module 19)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#concurrent.futures-lapi-moderne-et-simple",
    "href": "notebooks/intermediate/18_high_performance_python.html#concurrent.futures-lapi-moderne-et-simple",
    "title": "High Performance Python",
    "section": "4. concurrent.futures â€” Lâ€™API moderne et simple",
    "text": "4. concurrent.futures â€” Lâ€™API moderne et simple\n\nRecommandÃ© : Plus simple que multiprocessing et threading bruts Interface unifiÃ©e : MÃªme API pour threads et processes\n\n\n4.1 ThreadPoolExecutor : Pour les tÃ¢ches I/O-bound\nLe ThreadPoolExecutor crÃ©e un pool de threads rÃ©utilisables. Câ€™est idÃ©al pour : - TÃ©lÃ©charger plusieurs fichiers - Faire plusieurs requÃªtes API - Lire/Ã©crire plusieurs fichiers\n\nComment Ã§a fonctionne ?\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                      ThreadPoolExecutor (5 workers)                       â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                          â•‘\nâ•‘   TÃ¢ches Ã  faire : [T1, T2, T3, T4, T5, T6, T7, T8, T9, T10]            â•‘\nâ•‘                                                                          â•‘\nâ•‘   Pool de threads :                                                      â•‘\nâ•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â•‘\nâ•‘   â”‚Thread 1 â”‚ â”‚Thread 2 â”‚ â”‚Thread 3 â”‚ â”‚Thread 4 â”‚ â”‚Thread 5 â”‚          â•‘\nâ•‘   â”‚   T1    â”‚ â”‚   T2    â”‚ â”‚   T3    â”‚ â”‚   T4    â”‚ â”‚   T5    â”‚          â•‘\nâ•‘   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          â•‘\nâ•‘        â”‚           â”‚           â”‚           â”‚           â”‚                 â•‘\nâ•‘        â”‚ T1 fini   â”‚           â”‚           â”‚           â”‚                 â•‘\nâ•‘        â–¼           â”‚           â”‚           â”‚           â”‚                 â•‘\nâ•‘   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚           â”‚           â”‚           â”‚                 â•‘\nâ•‘   â”‚Thread 1 â”‚      â”‚           â”‚           â”‚           â”‚                 â•‘\nâ•‘   â”‚   T6    â”‚ â—€â”€â”€â”€â”€â”´â”€â”€ Les threads prennent la tÃ¢che suivante           â•‘\nâ•‘   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          dÃ¨s qu'ils ont fini                              â•‘\nâ•‘                                                                          â•‘\nâ•‘   â†’ Max 5 tÃ¢ches en parallÃ¨le, les autres attendent                     â•‘\nâ•‘                                                                          â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  ThreadPoolExecutor : ParallÃ©liser des tÃ¢ches I/O-bound                  â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION QUI SIMULE UNE TÃ‚CHE I/O (requÃªte API, lecture fichier, etc.)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef simulate_io_task(task_id):\n    \"\"\"\n    Simule une tÃ¢che I/O-bound (ex: requÃªte API).\n    \n    Dans la vraie vie, Ã§a pourrait Ãªtre :\n    - requests.get(\"https://api.example.com/data\")\n    - open(\"fichier.csv\").read()\n    - cursor.execute(\"SELECT * FROM table\")\n    \n    Args:\n        task_id: Identifiant de la tÃ¢che (pour le logging)\n    \n    Returns:\n        Message de confirmation\n    \"\"\"\n    print(f\"    TÃ¢che {task_id} : dÃ©but\")\n    time.sleep(0.5)  # Simule 500ms de latence rÃ©seau\n    print(f\"    TÃ¢che {task_id} : terminÃ©e\")\n    return f\"RÃ©sultat de la tÃ¢che {task_id}\"\n\n# Liste des tÃ¢ches Ã  effectuer (10 tÃ¢ches)\ntasks = list(range(10))\n\nprint(\"=\" * 60)\nprint(\"ğŸ”¬ ThreadPoolExecutor : ParallÃ©liser des tÃ¢ches I/O\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 1 : SÃ‰QUENTIEL (pour comparer)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 10 tÃ¢ches Ã— 0.5s = 5s au total\n\nprint(\"\\n MÃ©thode 1 : SÃ©quentiel\")\nprint(\"-\" * 40)\nstart = time.time()\n\nresults_seq = [simulate_io_task(t) for t in tasks]\n\nseq_time = time.time() - start\nprint(f\"\\n Temps sÃ©quentiel : {seq_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 2 : THREADPOOLEXECUTOR\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Avec 5 workers : 10 tÃ¢ches / 5 workers = 2 vagues = ~1s\n\nprint(\"\\n MÃ©thode 2 : ThreadPoolExecutor (5 workers)\")\nprint(\"-\" * 40)\nstart = time.time()\n\n# CrÃ©er un pool de 5 threads\n# with ... as : le pool se ferme automatiquement Ã  la fin\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    #\n    # executor.map() applique la fonction Ã  chaque Ã©lÃ©ment de la liste\n    # - Distribue les tÃ¢ches aux threads disponibles\n    # - Retourne les rÃ©sultats DANS L'ORDRE de la liste originale\n    # - Bloque jusqu'Ã  ce que toutes les tÃ¢ches soient terminÃ©es\n    #\n    results_parallel = list(executor.map(simulate_io_task, tasks))\n\nparallel_time = time.time() - start\nprint(f\"\\n Temps parallÃ¨le : {parallel_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# COMPARAISON\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARAISON\")\nprint(\"=\" * 60)\nprint(f\"   SÃ©quentiel : {seq_time:.2f}s\")\nprint(f\"   ParallÃ¨le  : {parallel_time:.2f}s\")\nprint(f\"   Speedup    : {seq_time/parallel_time:.1f}x plus rapide !\")\nprint()\nprint(\"Avec 5 workers pour 10 tÃ¢ches :\")\nprint(\"   â†’ 2 vagues de 5 tÃ¢ches\")\nprint(\"   â†’ 2 Ã— 0.5s = ~1s au lieu de 10 Ã— 0.5s = 5s\")\n\n\n\n\n\n\n4.2 ProcessPoolExecutor (CPU-bound)\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  ProcessPoolExecutor : ParallÃ©liser des tÃ¢ches CPU-bound                 â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nfrom concurrent.futures import ProcessPoolExecutor\nimport time\nimport os\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION CPU-INTENSIVE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef cpu_task(n):\n    \"\"\"\n    TÃ¢che CPU-bound : calcul intensif.\n    \n    Cette fonction fait travailler le CPU en continu.\n    Exemples rÃ©els : parsing, compression, transformations de donnÃ©es,\n    calculs mathÃ©matiques, feature engineering.\n    \n    Args:\n        n: Nombre d'itÃ©rations\n    \n    Returns:\n        Somme des carrÃ©s\n    \"\"\"\n    return sum(i ** 2 for i in range(n))\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# DONNÃ‰ES Ã€ TRAITER\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# On simule 8 \"chunks\" de donnÃ©es Ã  traiter\n# Chaque chunk nÃ©cessite 1 million d'itÃ©rations\n\ndata_chunks = [1_000_000] * 8  # 8 chunks de 1M itÃ©rations chacun\n\nprint(\"=\" * 60)\nprint(\" ProcessPoolExecutor : ParallÃ©liser des tÃ¢ches CPU-bound\")\nprint(\"=\" * 60)\nprint(f\" CPUs disponibles : {os.cpu_count()}\")\nprint(f\" Chunks Ã  traiter : {len(data_chunks)}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 1 : SÃ‰QUENTIEL\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n MÃ©thode 1 : SÃ©quentiel\")\nprint(\"-\" * 40)\nstart = time.time()\n\nresults_seq = [cpu_task(chunk) for chunk in data_chunks]\n\nseq_time = time.time() - start\nprint(f\" Temps sÃ©quentiel : {seq_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 2 : PROCESSPOOL\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Chaque worker est un PROCESSUS sÃ©parÃ© avec son propre GIL\n# â†’ Vraie parallÃ©lisation sur plusieurs CPUs !\n\nprint(\"\\n MÃ©thode 2 : ProcessPoolExecutor (4 workers)\")\nprint(\"-\" * 40)\nstart = time.time()\n\n# CrÃ©er un pool de 4 processus\n# Note : max_workers = nombre de CPUs est gÃ©nÃ©ralement optimal\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    #\n    # executor.map() distribue les chunks aux processus\n    # - Processus 1 traite chunk[0], chunk[4]\n    # - Processus 2 traite chunk[1], chunk[5]\n    # - etc.\n    #\n    results_parallel = list(executor.map(cpu_task, data_chunks))\n\nproc_time = time.time() - start\nprint(f\" Temps parallÃ¨le : {proc_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# COMPARAISON\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARAISON\")\nprint(\"=\" * 60)\nprint(f\"   SÃ©quentiel : {seq_time:.2f}s\")\nprint(f\"   ParallÃ¨le  : {proc_time:.2f}s\")\nprint(f\"   Speedup    : {seq_time/proc_time:.1f}x plus rapide !\")\nprint()\nprint(\"Explication :\")\nprint(\"   â†’ 8 chunks avec 4 workers = 2 vagues\")\nprint(\"   â†’ Chaque processus utilise 100% d'un CPU\")\nprint(\"   â†’ Speedup thÃ©orique max = min(workers, chunks) = 4x\")\nprint()\nprint(\"âš ï¸ IMPORTANT :\")\nprint(\"   â†’ ProcessPoolExecutor pour CPU-bound (contourne le GIL)\")\nprint(\"   â†’ ThreadPoolExecutor pour I/O-bound (GIL pas bloquant)\")\n\n\n\n\n\n4.3 Gestion avancÃ©e : submit() et as_completed()\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  submit() et as_completed() : ContrÃ´le avancÃ© des tÃ¢ches                 â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# executor.map() est simple mais limitÃ© :\n# - Attend que TOUTES les tÃ¢ches finissent\n# - Retourne les rÃ©sultats dans l'ordre original\n#\n# submit() + as_completed() permet :\n# - Traiter les rÃ©sultats dÃ¨s qu'ils arrivent\n# - GÃ©rer les erreurs individuellement\n# - Ajouter des timeouts par tÃ¢che\n\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport random\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION AVEC TEMPS D'EXÃ‰CUTION VARIABLE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef fetch_data(task_id):\n    \"\"\"\n    Simule une requÃªte avec temps variable.\n    Certaines tÃ¢ches sont rapides, d'autres lentes.\n    \"\"\"\n    delay = random.uniform(0.1, 1.0)  # Entre 0.1s et 1s\n    time.sleep(delay)\n    \n    # Simuler une erreur alÃ©atoire (10% de chance)\n    if random.random() &lt; 0.1:\n        raise Exception(f\"Erreur sur la tÃ¢che {task_id}\")\n    \n    return {\"task_id\": task_id, \"delay\": round(delay, 2)}\n\nprint(\"=\" * 60)\nprint(\" submit() et as_completed() : ContrÃ´le avancÃ©\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# UTILISATION DE submit() + as_completed()\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Ã‰TAPE 1 : Soumettre les tÃ¢ches avec submit()\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # submit() retourne un objet Future (promesse de rÃ©sultat futur)\n    # On crÃ©e un dictionnaire {future: task_id} pour identifier les tÃ¢ches\n    \n    futures = {}\n    for i in range(5):\n        future = executor.submit(fetch_data, i)  # Soumet la tÃ¢che\n        futures[future] = i  # Associe le future Ã  l'ID de la tÃ¢che\n    \n    print(f\"\\n {len(futures)} tÃ¢ches soumises\")\n    print(\"-\" * 40)\n    \n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # Ã‰TAPE 2 : Traiter les rÃ©sultats avec as_completed()\n    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    # as_completed() itÃ¨re sur les futures dans l'ORDRE DE COMPLÃ‰TION\n    # (pas l'ordre de soumission !)\n    # â†’ La premiÃ¨re tÃ¢che terminÃ©e est traitÃ©e en premier\n    \n    for future in as_completed(futures):\n        task_id = futures[future]  # RÃ©cupÃ©rer l'ID de la tÃ¢che\n        \n        try:\n            # future.result() rÃ©cupÃ¨re le rÃ©sultat\n            # timeout=5 : lÃ¨ve TimeoutError si &gt; 5 secondes\n            result = future.result(timeout=5)\n            print(f\" TÃ¢che {task_id} terminÃ©e en {result['delay']}s\")\n            \n        except Exception as e:\n            # Gestion individuelle des erreurs\n            print(f\"âŒ TÃ¢che {task_id} a Ã©chouÃ© : {e}\")\n\nprint(\"\\n AVANTAGES de submit() + as_completed() :\")\nprint(\"   â†’ Traiter les rÃ©sultats dÃ¨s qu'ils arrivent\")\nprint(\"   â†’ GÃ©rer les erreurs individuellement\")\nprint(\"   â†’ Ajouter des timeouts par tÃ¢che\")\nprint(\"   â†’ Plus de contrÃ´le que executor.map()\")\n\n\n\n\n\n4.4 Quand utiliser quoi ?\n\n\n\n\n\n\n\n\n\nExecutor\nGIL contournÃ© ?\nUsage\nExemple\n\n\n\n\nThreadPoolExecutor\nâŒ Non\nI/O : API, fichiers\nTÃ©lÃ©charger 50 fichiers\n\n\nProcessPoolExecutor\nâœ… Oui\nCPU : calculs, ETL\nTransformer 8 chunks de donnÃ©es",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#multiprocessing-contrÃ´le-avancÃ©",
    "href": "notebooks/intermediate/18_high_performance_python.html#multiprocessing-contrÃ´le-avancÃ©",
    "title": "High Performance Python",
    "section": "ğŸ”§ 5. multiprocessing â€” ContrÃ´le avancÃ©",
    "text": "ğŸ”§ 5. multiprocessing â€” ContrÃ´le avancÃ©\nPour les cas oÃ¹ concurrent.futures ne suffit pas.\n\n5.1 Pool avec map\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  multiprocessing.Pool : Traitement parallÃ¨le avec partitioning           â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# Pool est utile quand tu veux :\n# - Partitionner des donnÃ©es en chunks\n# - Traiter chaque chunk en parallÃ¨le\n# - Combiner les rÃ©sultats\n\nfrom multiprocessing import Pool, cpu_count\nimport numpy as np\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION DE TRAITEMENT D'UN CHUNK\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef process_chunk(chunk):\n    \"\"\"\n    Traite un chunk de donnÃ©es.\n    \n    Dans la vraie vie :\n    - Transformation de donnÃ©es\n    - Feature engineering\n    - Calculs statistiques\n    \n    Args:\n        chunk: numpy array (portion des donnÃ©es)\n    \n    Returns:\n        RÃ©sultat du traitement (ici : somme des carrÃ©s)\n    \"\"\"\n    return np.sum(chunk ** 2)\n\nprint(\"=\" * 60)\nprint(\" multiprocessing.Pool : Partitionner et parallÃ©liser\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 1 : CrÃ©er les donnÃ©es\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nbig_array = np.random.rand(1_000_000)  # 1 million de valeurs\nprint(f\"\\nğŸ“Š DonnÃ©es : {len(big_array):,} valeurs\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 2 : Partitionner en chunks\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# On divise les donnÃ©es en autant de chunks que de CPUs\n# Chaque CPU traitera un chunk\n\nn_workers = cpu_count()\nchunks = np.array_split(big_array, n_workers)\n\nprint(f\" CPUs disponibles : {n_workers}\")\nprint(f\" Chunks crÃ©Ã©s : {len(chunks)}\")\nprint(f\"   Taille de chaque chunk : ~{len(chunks[0]):,} valeurs\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 3 : Traitement parallÃ¨le avec Pool\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n Traitement parallÃ¨le...\")\n\n# Pool() crÃ©e un pool de processus rÃ©utilisables\n# with ... as : le pool se ferme automatiquement Ã  la fin\nwith Pool(processes=n_workers) as pool:\n    #\n    # pool.map() applique process_chunk Ã  chaque chunk\n    # - Les chunks sont distribuÃ©s aux processus disponibles\n    # - Chaque processus traite son chunk indÃ©pendamment\n    # - Les rÃ©sultats sont retournÃ©s dans l'ordre des chunks\n    #\n    results = pool.map(process_chunk, chunks)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 4 : Combiner les rÃ©sultats\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Chaque processus a calculÃ© la somme des carrÃ©s de son chunk\n# On additionne tous les rÃ©sultats partiels\n\ntotal = sum(results)\nprint(f\"\\n RÃ©sultat total : {total:.2f}\")\n\nprint(\"\\n PATTERN CLASSIQUE : Map-Reduce\")\nprint(\"   1. SPLIT : Diviser les donnÃ©es en chunks\")\nprint(\"   2. MAP : Traiter chaque chunk en parallÃ¨le\")\nprint(\"   3. REDUCE : Combiner les rÃ©sultats (ici: sum)\")\n\n\n\n\n\n5.2 starmap pour plusieurs arguments\n\n\nVoir le code\nfrom multiprocessing import Pool\n\ndef process_with_params(data, multiplier, offset):\n    \"\"\"Fonction avec plusieurs paramÃ¨tres\"\"\"\n    return sum(data) * multiplier + offset\n\n# PrÃ©parer les arguments\nargs_list = [\n    ([1, 2, 3], 2, 10),\n    ([4, 5, 6], 3, 20),\n    ([7, 8, 9], 4, 30),\n]\n\nwith Pool(3) as pool:\n    results = pool.starmap(process_with_params, args_list)\n\nprint(\"RÃ©sultats:\", results)\n\n\n\n\n5.3 Limites et prÃ©cautions\n\n\n\nâš ï¸ Limite\nExplication\n\n\n\n\nOverhead\nCrÃ©er des process prend du temps (~100ms)\n\n\nSÃ©rialisation\nLes donnÃ©es sont copiÃ©es (pickle)\n\n\nif __name__ == \"__main__\"\nObligatoire sur Windows\n\n\nMÃ©moire\nChaque process a sa propre mÃ©moire\n\n\n\n# âš ï¸ Toujours protÃ©ger avec if __name__ == \"__main__\"\nif __name__ == \"__main__\":\n    with Pool(4) as pool:\n        results = pool.map(my_func, data)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#asyncio-io-massivement-parallÃ¨le",
    "href": "notebooks/intermediate/18_high_performance_python.html#asyncio-io-massivement-parallÃ¨le",
    "title": "High Performance Python",
    "section": "6. asyncio â€” I/O massivement parallÃ¨le",
    "text": "6. asyncio â€” I/O massivement parallÃ¨le\n\nâœ… IdÃ©al pour : 100+ requÃªtes API, ingestion massive, crawling web âŒ Pas pour : Calculs CPU-intensive\n\n\n6.1 Comprendre async/await\nasyncio utilise un modÃ¨le single-thread non-bloquant. Au lieu de crÃ©er plusieurs threads, un seul thread gÃ¨re plusieurs tÃ¢ches en switchant entre elles quand lâ€™une attend.\n\nComment Ã§a fonctionne ?\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                         asyncio : Single thread                          â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                          â•‘\nâ•‘  UN SEUL THREAD gÃ¨re plusieurs tÃ¢ches :                                 â•‘\nâ•‘                                                                          â•‘\nâ•‘  TÃ¢che 1: [code]â”€â”€â–¶[await: attend API]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶[code]â”€â”€â–¶ done    â•‘\nâ•‘                           â†“                              â†‘               â•‘\nâ•‘                    Le thread switch                      â”‚               â•‘\nâ•‘                           â†“                              â”‚               â•‘\nâ•‘  TÃ¢che 2: [attend]â”€â”€â–¶[code]â”€â”€â–¶[await: attend DB]â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â–¶ done      â•‘\nâ•‘                                      â†“                  â”‚               â•‘\nâ•‘                               Le thread switch          â”‚               â•‘\nâ•‘                                      â†“                  â”‚               â•‘\nâ•‘  TÃ¢che 3: [attend]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶[code]â”€â”€â–¶[await]â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–¶ done      â•‘\nâ•‘                                                                          â•‘\nâ•‘  â†’ Le thread ne reste jamais \"bloquÃ©\" Ã  attendre                        â•‘\nâ•‘  â†’ DÃ¨s qu'une tÃ¢che attend, il passe Ã  une autre                        â•‘\nâ•‘  â†’ TrÃ¨s efficace pour l'I/O : un seul thread peut gÃ©rer 1000+ requÃªtes  â•‘\nâ•‘                                                                          â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\n\n6.2 Les mots-clÃ©s async/await\n\n\n\n\n\n\n\n\nMot-clÃ©\nCe que Ã§a fait\nQuand lâ€™utiliser\n\n\n\n\nasync def\nDÃ©clare une fonction coroutine\nFonctions qui font de lâ€™I/O asynchrone\n\n\nawait\nAttend le rÃ©sultat dâ€™une coroutine\nQuand tu appelles une fonction async\n\n\nasyncio.gather()\nLance plusieurs coroutines en parallÃ¨le\nPour parallÃ©liser des tÃ¢ches\n\n\nasyncio.run()\nPoint dâ€™entrÃ©e pour exÃ©cuter du code async\nDans un script (pas dans Jupyter)\n\n\n\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  asyncio : Comprendre async/await                                        â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport asyncio\nimport time\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION ASYNCHRONE (coroutine)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# \"async def\" crÃ©e une COROUTINE, pas une fonction normale\n# Une coroutine peut Ãªtre \"mise en pause\" avec await\n\nasync def fetch_data(task_id):\n    \"\"\"\n    Simule une requÃªte API asynchrone.\n    \n    Dans la vraie vie, Ã§a serait :\n    - async with aiohttp.ClientSession() as session:\n    -     async with session.get(url) as response:\n    -         return await response.json()\n    \n    Args:\n        task_id: Identifiant de la tÃ¢che\n    \n    Returns:\n        RÃ©sultat simulÃ©\n    \"\"\"\n    print(f\"   TÃ¢che {task_id} : dÃ©but\")\n    \n    # await asyncio.sleep() = attente NON-BLOQUANTE\n    # Pendant que cette tÃ¢che attend, d'autres tÃ¢ches peuvent s'exÃ©cuter !\n    # C'est LA diffÃ©rence avec time.sleep() qui bloque tout\n    await asyncio.sleep(1)  # Simule 1s de latence rÃ©seau\n    \n    print(f\"   TÃ¢che {task_id} : terminÃ©e\")\n    return f\"result_{task_id}\"\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION PRINCIPALE ASYNCHRONE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nasync def main():\n    \"\"\"\n    Fonction principale qui orchestre les tÃ¢ches.\n    \n    asyncio.gather() lance plusieurs coroutines EN PARALLÃˆLE\n    et attend que TOUTES soient terminÃ©es.\n    \"\"\"\n    print(\"=\" * 60)\n    print(\"asyncio : Lancer 5 tÃ¢ches en parallÃ¨le\")\n    print(\"=\" * 60)\n    \n    # CrÃ©er une liste de coroutines (pas encore exÃ©cutÃ©es !)\n    # Note : fetch_data(i) retourne un objet coroutine, pas le rÃ©sultat\n    tasks = [fetch_data(i) for i in range(5)]\n    \n    print(f\"\\n {len(tasks)} tÃ¢ches crÃ©Ã©es\")\n    print(\"=\" * 60)\n    \n    # asyncio.gather() lance TOUTES les tÃ¢ches en parallÃ¨le\n    # - Les 5 tÃ¢ches dÃ©marrent en mÃªme temps\n    # - Chaque tÃ¢che attend 1s (await asyncio.sleep(1))\n    # - MAIS elles attendent en parallÃ¨le !\n    # - Temps total : ~1s au lieu de 5s\n    results = await asyncio.gather(*tasks)\n    \n    # *tasks \"dÃ©balle\" la liste : gather(task[0], task[1], task[2], ...)\n    \n    return results\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# EXÃ‰CUTION\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Dans Jupyter : await main() fonctionne directement\n# Dans un script Python normal : asyncio.run(main())\n\nstart = time.time()\nresults = await main()  # Jupyter permet d'utiliser await directement\ntotal_time = time.time() - start\n\nprint(\"=\" * 60)\nprint(\"RÃ‰SULTAT\")\nprint(\"=\" * 60)\nprint(f\" Temps total : {total_time:.2f}s\")\nprint(f\" RÃ©sultats : {results}\")\nprint()\nprint(\" EXPLICATION :\")\nprint(\"   â†’ 5 tÃ¢ches de 1s chacune\")\nprint(\"   â†’ En sÃ©quentiel : 5 Ã— 1s = 5s\")\nprint(\"   â†’ En parallÃ¨le (asyncio) : ~1s (toutes en mÃªme temps)\")\nprint()\nprint(\"âš ï¸ IMPORTANT :\")\nprint(\"   â†’ await asyncio.sleep() â‰  time.sleep()\")\nprint(\"   â†’ asyncio.sleep() libÃ¨re le thread pour d'autres tÃ¢ches\")\nprint(\"   â†’ time.sleep() bloque TOUT le programme\")\n\n\n\n\n\n6.2 Exemple rÃ©el avec aiohttp\n\n\nVoir le code\n# Installation : pip install aiohttp\nimport asyncio\n\n# Simulons aiohttp pour l'exemple (sans vraies requÃªtes)\nasync def fetch_url(session, url):\n    \"\"\"Simule une requÃªte HTTP\"\"\"\n    await asyncio.sleep(0.1)  # Simule latence\n    return {\"url\": url, \"status\": 200}\n\nasync def fetch_all_urls(urls):\n    \"\"\"Fetch toutes les URLs en parallÃ¨le\"\"\"\n    session = None  # En vrai : async with aiohttp.ClientSession() as session:\n    \n    tasks = [fetch_url(session, url) for url in urls]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return results\n\n# Simuler 20 URLs\nurls = [f\"https://api.example.com/data/{i}\" for i in range(20)]\n\nstart = time.time()\nresults = await fetch_all_urls(urls)\nprint(f\" 20 requÃªtes en {time.time() - start:.2f}s\")\nprint(f\" SuccÃ¨s : {len([r for r in results if isinstance(r, dict)])}\")\n\n\n\n\n6.3 Semaphore : limiter les connexions simultanÃ©es\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  Semaphore : Limiter le nombre de connexions simultanÃ©es                 â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# PROBLÃˆME : Tu veux faire 100 requÃªtes API, mais l'API limite Ã  5\n#            connexions simultanÃ©es (rate limiting).\n#\n# SOLUTION : Semaphore = un compteur qui limite les accÃ¨s concurrents\n\nimport asyncio\nimport time\n\nprint(\"=\" * 60)\nprint(\"ğŸ”¬ Semaphore : Limiter la concurrence\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# CRÃ‰ER UN SEMAPHORE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Semaphore(5) = maximum 5 tÃ¢ches peuvent s'exÃ©cuter en mÃªme temps\n# Les autres attendent qu'une place se libÃ¨re\n\nMAX_CONCURRENT = 5\nsemaphore = asyncio.Semaphore(MAX_CONCURRENT)\n\nprint(f\"\\nğŸš¦ Semaphore crÃ©Ã© : max {MAX_CONCURRENT} tÃ¢ches simultanÃ©es\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION AVEC SEMAPHORE\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nasync def fetch_limited(task_id):\n    \"\"\"\n    Fait une requÃªte en respectant la limite de concurrence.\n    \n    async with semaphore :\n    - Tente d'acquÃ©rir le semaphore (dÃ©crÃ©mente le compteur)\n    - Si compteur = 0, ATTEND qu'une tÃ¢che se termine\n    - Ã€ la sortie du with, libÃ¨re le semaphore (incrÃ©mente le compteur)\n    \"\"\"\n    # async with semaphore : attend si dÃ©jÃ  5 tÃ¢ches en cours\n    async with semaphore:\n        print(f\"   TÃ¢che {task_id:2d} dÃ©marre (slot acquis)\")\n        await asyncio.sleep(0.5)  # Simule la requÃªte\n        print(f\"   TÃ¢che {task_id:2d} terminÃ©e (slot libÃ©rÃ©)\")\n        return task_id\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# EXÃ‰CUTION : 15 tÃ¢ches avec max 5 simultanÃ©es\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nasync def main():\n    print(\"\\n\" + \"-\" * 40)\n    print(f\" Lancement de 15 tÃ¢ches (max {MAX_CONCURRENT} simultanÃ©es)\")\n    print(\"-\" * 40 + \"\\n\")\n    \n    # CrÃ©er 15 tÃ¢ches\n    tasks = [fetch_limited(i) for i in range(15)]\n    \n    # Lancer toutes les tÃ¢ches\n    # MAIS le semaphore limite Ã  5 simultanÃ©es !\n    results = await asyncio.gather(*tasks)\n    \n    return results\n\nstart = time.time()\nresults = await main()\ntotal_time = time.time() - start\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"RÃ‰SULTAT\")\nprint(\"=\" * 60)\nprint(f\"Temps total : {total_time:.2f}s\")\nprint(f\"TÃ¢ches complÃ©tÃ©es : {len(results)}\")\n\nprint(\"\\n EXPLICATION :\")\nprint(f\"   â†’ 15 tÃ¢ches de 0.5s chacune\")\nprint(f\"   â†’ Max {MAX_CONCURRENT} simultanÃ©es = 3 vagues\")\nprint(f\"   â†’ 3 vagues Ã— 0.5s = ~1.5s (au lieu de 7.5s sÃ©quentiel)\")\nprint(\"\\n USE CASES :\")\nprint(\"   â†’ Rate limiting API (ex: max 10 req/s)\")\nprint(\"   â†’ Limiter les connexions DB\")\nprint(\"   â†’ Ã‰viter de surcharger un service\")\n\n\n\n\n\n6.4 Quand NE PAS utiliser asyncio\n\n\n\nSituation\nasyncio efficace ?\nAlternative\n\n\n\n\n100+ appels API\nâœ… Oui\n-\n\n\nLecture S3/DB massives\nâœ… Oui\n-\n\n\nCalculs CPU\nâŒ Non\nProcessPoolExecutor\n\n\n5 requÃªtes simples\nâŒ Overkill\nThreadPoolExecutor\n\n\nCode synchrone existant\nâŒ Refactoring lourd\nThreadPoolExecutor",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#dask-traiter-des-fichiers-plus-grands-que-la-ram",
    "href": "notebooks/intermediate/18_high_performance_python.html#dask-traiter-des-fichiers-plus-grands-que-la-ram",
    "title": "High Performance Python",
    "section": "7. Dask â€” Traiter des fichiers plus grands que la RAM",
    "text": "7. Dask â€” Traiter des fichiers plus grands que la RAM\n\nLe plus utile quand tes donnÃ©es ne tiennent pas en mÃ©moire.\n\n\n7.1 Pourquoi Dask ?\n\n\n\n\n\n\n\n\nProblÃ¨me\nSolution classique\nSolution Dask\n\n\n\n\nFichier de 50 Go\nMemoryError !\nâœ… TraitÃ© par chunks\n\n\n1000 fichiers CSV\nBoucle lente\nâœ… ParallÃ©lisÃ© automatiquement\n\n\nBesoin dâ€™apprendre une nouvelle API\nğŸ˜«\nâœ… API quasi-identique Ã  Pandas\n\n\n\n\n\n7.2 Comment Dask fonctionne ?\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘                    Dask : Lazy Evaluation + Partitions                   â•‘\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\nâ•‘                                                                          â•‘\nâ•‘  Ã‰TAPE 1 : Lecture (LAZY - pas de chargement en mÃ©moire !)              â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â•‘\nâ•‘                                                                          â•‘\nâ•‘  ddf = dd.read_csv(\"data/*.csv\")                                        â•‘\nâ•‘                                                                          â•‘\nâ•‘  â†’ Dask SCANNE les fichiers (schÃ©ma, taille)                            â•‘\nâ•‘  â†’ Mais NE CHARGE PAS les donnÃ©es !                                     â•‘\nâ•‘  â†’ CrÃ©e un \"plan d'exÃ©cution\"                                           â•‘\nâ•‘                                                                          â•‘\nâ•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â•‘\nâ•‘  â”‚ Partition 1 â”‚ â”‚ Partition 2 â”‚ â”‚ Partition 3 â”‚ â”‚ Partition 4 â”‚       â•‘\nâ•‘  â”‚ (fichier 1) â”‚ â”‚ (fichier 2) â”‚ â”‚ (fichier 3) â”‚ â”‚ (fichier 4) â”‚       â•‘\nâ•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â•‘\nâ•‘                                                                          â•‘\nâ•‘  Ã‰TAPE 2 : OpÃ©rations (LAZY - construit le plan)                        â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â•‘\nâ•‘                                                                          â•‘\nâ•‘  result = ddf.groupby(\"col\").sum()   # Pas encore exÃ©cutÃ© !             â•‘\nâ•‘                                                                          â•‘\nâ•‘  Ã‰TAPE 3 : compute() (EXÃ‰CUTION - traitement rÃ©el)                      â•‘\nâ•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â•‘\nâ•‘                                                                          â•‘\nâ•‘  result.compute()                                                        â•‘\nâ•‘                                                                          â•‘\nâ•‘  â†’ Dask charge partition 1, la traite, la dÃ©charge                      â•‘\nâ•‘  â†’ Puis partition 2, etc.                                               â•‘\nâ•‘  â†’ Seule 1 partition en mÃ©moire Ã  la fois !                             â•‘\nâ•‘  â†’ Utilise tous les CPUs pour traiter les partitions en parallÃ¨le       â•‘\nâ•‘                                                                          â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  Dask DataFrame : Traiter des fichiers plus grands que la RAM            â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Installation : pip install \"dask[complete]\"\nimport dask.dataframe as dd\nimport pandas as pd\nimport os\nimport time\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 0 : CrÃ©er des fichiers de test\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"=\" * 60)\nprint(\"CrÃ©ation des fichiers de test\")\nprint(\"=\" * 60)\n\nos.makedirs(\"data/dask_demo\", exist_ok=True)\n\nfor i in range(5):\n    df = pd.DataFrame({\n        \"id\": range(i * 10000, (i + 1) * 10000),\n        \"category\": [f\"cat_{j % 5}\" for j in range(10000)],\n        \"amount\": [float(j % 1000) for j in range(10000)]\n    })\n    df.to_csv(f\"data/dask_demo/file_{i}.csv\", index=False)\n\nprint(\"5 fichiers CSV crÃ©Ã©s (50,000 lignes au total)\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 1 : Lecture LAZY avec Dask\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Ã‰tape 1 : Lecture avec Dask (LAZY)\")\nprint(\"=\" * 60)\n\n# dd.read_csv() avec pattern glob : lit TOUS les fichiers correspondants\n# IMPORTANT : les donnÃ©es ne sont PAS chargÃ©es en mÃ©moire !\nddf = dd.read_csv(\"data/dask_demo/*.csv\")\n\nprint(f\"\\nType de l'objet : {type(ddf)}\")\nprint(f\"Nombre de partitions : {ddf.npartitions}\")\nprint(f\"   â†’ Chaque fichier = 1 partition\")\nprint(f\"   â†’ Les partitions seront traitÃ©es indÃ©pendamment\")\n\nprint(\"\\n SchÃ©ma des donnÃ©es (sans les charger) :\")\nprint(ddf)\n\nprint(\"\\nâš ï¸ IMPORTANT : Ã€ ce stade, les donnÃ©es ne sont PAS en mÃ©moire !\")\nprint(\"   Dask a juste lu les en-tÃªtes et crÃ©Ã© un plan d'exÃ©cution.\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 2 : Pipeline de transformations (LAZY)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Ã‰tape 2 : Pipeline de transformations (LAZY)\")\nprint(\"=\" * 60)\n\n# Toutes ces opÃ©rations sont LAZY : rien n'est calculÃ© !\n# Dask construit un graphe d'exÃ©cution (DAG) optimisÃ©\n\nresult = (\n    ddf\n    # Filtrer : garder seulement les montants &gt; 100\n    .query(\"amount &gt; 100\")\n    \n    # CrÃ©er une nouvelle colonne\n    .assign(amount_doubled=ddf.amount * 2)\n    \n    # Grouper par catÃ©gorie et calculer la somme\n    .groupby(\"category\")\n    .amount.sum()\n)\n\nprint(\"Pipeline dÃ©fini :\")\nprint(\"   1. Filtrer amount &gt; 100\")\nprint(\"   2. CrÃ©er colonne amount_doubled\")\nprint(\"   3. GroupBy category + sum\")\nprint(\"\\n Toujours LAZY ! Rien n'est calculÃ©.\")\nprint(f\"Type du rÃ©sultat : {type(result)}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Ã‰TAPE 3 : ExÃ©cution avec compute()\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Ã‰tape 3 : ExÃ©cution avec compute()\")\nprint(\"=\" * 60)\n\nstart = time.time()\n\n# compute() dÃ©clenche l'exÃ©cution RÃ‰ELLE du pipeline\n# - Dask optimise le plan d'exÃ©cution\n# - Traite les partitions en parallÃ¨le (utilise tous les CPUs)\n# - Retourne un objet Pandas (DataFrame ou Series)\nfinal_result = result.compute()\n\nprint(f\"\\n Temps d'exÃ©cution : {time.time() - start:.2f}s\")\nprint(f\"Type du rÃ©sultat final : {type(final_result)}\")\nprint(\"\\n RÃ©sultat :\")\nprint(final_result)\n\nprint(\"\\n CE QU'IL FAUT RETENIR :\")\nprint(\"   â†’ dd.read_csv() : lecture lazy (pas de chargement)\")\nprint(\"   â†’ OpÃ©rations (filter, groupby...) : construisent le plan\")\nprint(\"   â†’ compute() : exÃ©cute le plan et retourne un Pandas\")\nprint(\"   â†’ Une seule partition en mÃ©moire Ã  la fois !\")\n\n\n\n\n\nVoir le code\nimport dask.dataframe as dd\nimport time\n\n# Lire TOUS les fichiers avec glob pattern (lazy !)\nddf = dd.read_csv(\"data/dask_demo/*.csv\")\n\nprint(\"Type:\", type(ddf))\nprint(f\"Partitions: {ddf.npartitions}\")\nprint(\"\\n Les donnÃ©es ne sont PAS encore chargÃ©es !\")\nprint(ddf)  # Affiche le schÃ©ma, pas les donnÃ©es\n\n\n\n\nVoir le code\n# Pipeline Dask (lazy)\nresult = (\n    ddf\n    .filter(ddf.amount &gt; 100)  # Filtrage\n    .assign(amount_doubled=ddf.amount * 2)  # Transformation\n    .groupby(\"category\")  # AgrÃ©gation\n    .amount.sum()\n)\n\nprint(\"Pipeline dÃ©fini (lazy) :\")\nprint(result)\n\n# ExÃ©cuter avec compute()\nstart = time.time()\nfinal_result = result.compute()\nprint(f\"\\n ExÃ©cution : {time.time() - start:.2f}s\")\nprint(\"\\n RÃ©sultat :\")\nprint(final_result)\n\n\n\n\n7.2 Dask vs Polars vs Spark\n\n\n\nAspect\nPolars\nDask\nSpark\n\n\n\n\nSingle machine\nâ­â­â­\nâ­â­\nâ­\n\n\nCluster\nâŒ\nâ­â­\nâ­â­â­\n\n\nAPI\nPropre\nPandas-like\nPropre\n\n\nSetup\nSimple\nSimple\nComplexe\n\n\nVitesse (single node)\nâ­â­â­\nâ­â­\nâ­\n\n\nÃ‰cosystÃ¨me\nNouveau\nMature\nTrÃ¨s mature\n\n\n\n\n\n7.3 Quand utiliser Dask ?\n\nâœ… Fichiers &gt; RAM mais &lt; 100 Go\nâœ… Tu connais dÃ©jÃ  Pandas\nâœ… Pas besoin dâ€™un cluster Spark\nâœ… Traitement multi-fichiers\nâŒ Si single file &lt; 10 Go â†’ utilise Polars\nâŒ Si &gt; 100 Go ou cluster â†’ utilise Spark",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#joblib-parallÃ©lisation-simple",
    "href": "notebooks/intermediate/18_high_performance_python.html#joblib-parallÃ©lisation-simple",
    "title": "High Performance Python",
    "section": "8. joblib â€” ParallÃ©lisation simple",
    "text": "8. joblib â€” ParallÃ©lisation simple\n\nUltra-simple â€” parfait pour parallÃ©liser une boucle rapidement TrÃ¨s utilisÃ© en Data Science (sklearn lâ€™utilise en interne)\n\n\n\nVoir le code\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  joblib : ParallÃ©lisation ultra-simple                                   â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n#\n# joblib est LA solution la plus simple pour parallÃ©liser une boucle\n# TrÃ¨s utilisÃ© en Data Science (scikit-learn l'utilise en interne)\n#\n# Installation : pip install joblib\n\nfrom joblib import Parallel, delayed\nimport time\n\nprint(\"=\" * 60)\nprint(\"joblib : ParallÃ©liser une boucle en 1 ligne\")\nprint(\"=\" * 60)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# FONCTION Ã€ PARALLÃ‰LISER\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef expensive_computation(x):\n    \"\"\"\n    Calcul coÃ»teux Ã  parallÃ©liser.\n    \n    Dans la vraie vie :\n    - EntraÃ®nement d'un modÃ¨le\n    - Transformation de fichier\n    - Calcul de features\n    \"\"\"\n    time.sleep(0.1)  # Simule un calcul de 100ms\n    return x ** 2\n\n# DonnÃ©es Ã  traiter\ndata = list(range(20))\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 1 : SÃ‰QUENTIEL (classique)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n MÃ©thode 1 : List comprehension (sÃ©quentiel)\")\nprint(\"-\" * 40)\n\nstart = time.time()\nresults_seq = [expensive_computation(x) for x in data]\nseq_time = time.time() - start\n\nprint(f\"Temps : {seq_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# MÃ‰THODE 2 : JOBLIB (parallÃ¨le)\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\nMÃ©thode 2 : joblib.Parallel (parallÃ¨le)\")\nprint(\"-\" * 40)\n\nstart = time.time()\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# â•‘  Parallel(n_jobs=-1)(delayed(func)(args) for args in data)               â•‘\n# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n# â•‘                                                                          â•‘\n# â•‘  DÃ©composition :                                                         â•‘\n# â•‘                                                                          â•‘\n# â•‘  Parallel(n_jobs=-1)     â†’ CrÃ©e un pool de workers                      â•‘\n# â•‘                            n_jobs=-1 = utiliser TOUS les CPUs           â•‘\n# â•‘                            n_jobs=4 = utiliser 4 CPUs                   â•‘\n# â•‘                                                                          â•‘\n# â•‘  delayed(func)           â†’ Wrapper qui \"retarde\" l'exÃ©cution            â•‘\n# â•‘                            La fonction n'est pas appelÃ©e immÃ©diatement   â•‘\n# â•‘                                                                          â•‘\n# â•‘  delayed(func)(args)     â†’ PrÃ©pare l'appel func(args)                   â•‘\n# â•‘                            Retourne un \"callable\" diffÃ©rÃ©                â•‘\n# â•‘                                                                          â•‘\n# â•‘  for x in data           â†’ GÃ©nÃ¨re tous les appels diffÃ©rÃ©s              â•‘\n# â•‘                                                                          â•‘\n# â•‘  Le tout entre ()        â†’ Parallel distribue et exÃ©cute les appels     â•‘\n# â•‘                                                                          â•‘\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nresults_parallel = Parallel(n_jobs=-1)(\n    delayed(expensive_computation)(x) for x in data\n)\n\nparallel_time = time.time() - start\nprint(f\"Temps : {parallel_time:.2f}s\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# COMPARAISON\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"\\n\" + \"=\" * 60)\nprint(\"COMPARAISON\")\nprint(\"=\" * 60)\nprint(f\"   SÃ©quentiel : {seq_time:.2f}s\")\nprint(f\"   ParallÃ¨le  : {parallel_time:.2f}s\")\nprint(f\"   Speedup    : {seq_time/parallel_time:.1f}x plus rapide !\")\n\nprint(\"\\nğŸ’¡ SYNTAXE JOBLIB :\")\nprint(\"   Parallel(n_jobs=-1)(delayed(func)(arg) for arg in data)\")\nprint(\"         â”‚                   â”‚      â”‚         â”‚\")\nprint(\"         â”‚                   â”‚      â”‚         â””â”€ DonnÃ©es Ã  traiter\")\nprint(\"         â”‚                   â”‚      â””â”€ Argument de la fonction\")\nprint(\"         â”‚                   â””â”€ Fonction Ã  parallÃ©liser\")\nprint(\"         â””â”€ -1 = tous les CPUs\")\n\nprint(\"\\nğŸ¯ QUAND UTILISER JOBLIB :\")\nprint(\"   ParallÃ©liser une boucle simple\")\nprint(\"   Data Science / ML (cross-validation, grid search)\")\nprint(\"   Quand tu veux quelque chose qui marche vite\")\nprint(\"   Pour du I/O massif (prÃ©fÃ¨re asyncio)\")\n\n\n\n\n\nVoir le code\nfrom joblib import Parallel, delayed\n\ndef io_task(x):\n    time.sleep(0.1)\n    return x\n\n# Options utiles\nresults = Parallel(\n    n_jobs=4,              # Nombre de workers\n    backend=\"threading\",   # \"threading\" pour I/O, \"loky\" (dÃ©faut) pour CPU\n    verbose=1              # Affiche la progression\n)(delayed(io_task)(x) for x in range(10))\n\nprint(f\"\\n RÃ©sultats : {results}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#choisir-la-bonne-technologie",
    "href": "notebooks/intermediate/18_high_performance_python.html#choisir-la-bonne-technologie",
    "title": "High Performance Python",
    "section": "ğŸŒ³ 9. Choisir la bonne technologie",
    "text": "ğŸŒ³ 9. Choisir la bonne technologie\n\nRÃ©capitulatif\n\n\n\nOutil\nType\nGIL contournÃ©\nComplexitÃ©\nUse case\n\n\n\n\nThreadPoolExecutor\nI/O\nâŒ Non\nâ­\n&lt; 20 requÃªtes/fichiers\n\n\nProcessPoolExecutor\nCPU\nâœ… Oui\nâ­â­\nETL, calculs\n\n\nasyncio\nI/O\nâŒ Non\nâ­â­â­\n100+ requÃªtes API\n\n\njoblib\nCPU/I/O\nâœ… (loky)\nâ­\nBoucles simples, ML\n\n\nDask\nBig Data\nâœ… Oui\nâ­â­\nFichiers &gt; RAM\n\n\n\n\n\nğŸ–¼ï¸ Arbre de dÃ©cision visuel\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚  Quel problÃ¨me? â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                             â”‚\n         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n         â”‚                   â”‚                   â”‚\n         â–¼                   â–¼                   â–¼\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚CPU-boundâ”‚        â”‚I/O-boundâ”‚        â”‚Fichiers &gt;RAMâ”‚\n    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n         â”‚                  â”‚                    â”‚\n    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n Simple?  Complex?  &lt;20 req?  100+ req?  &lt;100Go?  &gt;100Go?\n    â”‚         â”‚        â”‚         â”‚         â”‚         â”‚\n    â–¼         â–¼        â–¼         â–¼         â–¼         â–¼\n joblib   Process   Thread    asyncio    Dask     Spark\n          Pool      Pool                        (mod 19)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "href": "notebooks/intermediate/18_high_performance_python.html#bonnes-pratiques-erreurs-frÃ©quentes",
    "title": "High Performance Python",
    "section": "10. Bonnes pratiques & Erreurs frÃ©quentes",
    "text": "10. Bonnes pratiques & Erreurs frÃ©quentes\n\nâŒ Erreurs frÃ©quentes\n\n\n\nErreur\nPourquoi câ€™est faux\nSolution\n\n\n\n\nThreading pour CPU\nGIL bloque\nProcessPoolExecutor\n\n\nAsync pour calculs\nPas de gain\nProcessPoolExecutor\n\n\nPandas sur 50 Go\nCrash RAM\nDask ou Polars streaming\n\n\n100 workers pour 10 tÃ¢ches\nOverhead inutile\nAdapter au workload\n\n\nPas de profiling\nOptimise au hasard\nToujours profiler dâ€™abord\n\n\nOublier if __name__\nCrash sur Windows\nToujours protÃ©ger\n\n\n\n\n\nâœ… Bonnes pratiques\n\n\n\nPratique\nPourquoi\n\n\n\n\nProfiler dâ€™abord\nIdentifier le vrai goulot\n\n\nÃ‰crire en Parquet\nI/O plus rapide\n\n\nPartitionner intelligemment\nÃ‰vite surcharge mÃ©moire\n\n\nTester avec peu de workers\nPuis augmenter progressivement\n\n\nif __name__ == \"__main__\"\nObligatoire pour multiprocessing\n\n\nUtiliser n_jobs=-1\nUtilise tous les CPUs disponibles",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/18_high_performance_python.html#quiz-de-fin-de-module",
    "title": "High Performance Python",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\n\n\nâ“ Q1. Quâ€™est-ce que le GIL empÃªche en Python ?\n\nLâ€™exÃ©cution de code Python\n\nLâ€™exÃ©cution simultanÃ©e de plusieurs threads Python\n\nLâ€™utilisation de la mÃ©moire\n\nLa lecture de fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le GIL (Global Interpreter Lock) empÃªche lâ€™exÃ©cution simultanÃ©e de plusieurs threads Python, les forÃ§ant Ã  sâ€™exÃ©cuter en alternance.\n\n\n\n\nâ“ Q2. Pour un traitement CPU-intensive, quel outil utiliser ?\n\nThreadPoolExecutor\n\nProcessPoolExecutor\n\nasyncio\n\nTous sont Ã©quivalents\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” ProcessPoolExecutor contourne le GIL en utilisant des processus sÃ©parÃ©s, permettant une vraie parallÃ©lisation CPU.\n\n\n\n\nâ“ Q3. Quand utiliser asyncio ?\n\nPour des calculs mathÃ©matiques complexes\n\nPour tÃ©lÃ©charger 100+ fichiers depuis une API\n\nPour trier un gros tableau\n\nPour compresser des fichiers\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” asyncio est idÃ©al pour lâ€™I/O massivement parallÃ¨le (requÃªtes API, tÃ©lÃ©chargements). Les autres sont CPU-bound.\n\n\n\n\nâ“ Q4. Que fait ddf.compute() dans Dask ?\n\nDÃ©finit le pipeline\n\nAffiche le schÃ©ma\n\nDÃ©clenche lâ€™exÃ©cution et retourne un DataFrame Pandas\n\nSauvegarde les donnÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” .compute() dÃ©clenche lâ€™exÃ©cution du pipeline lazy et retourne le rÃ©sultat sous forme de DataFrame Pandas.\n\n\n\n\nâ“ Q5. Que signifie n_jobs=-1 dans joblib ?\n\nDÃ©sactive le parallÃ©lisme\n\nUtilise un seul CPU\n\nUtilise tous les CPUs disponibles\n\nErreur de configuration\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” n_jobs=-1 indique Ã  joblib dâ€™utiliser tous les CPUs disponibles sur la machine.\n\n\n\n\nâ“ Q6. Pourquoi ThreadPoolExecutor ne accÃ©lÃ¨re pas les calculs CPU ?\n\nParce quâ€™il est mal implÃ©mentÃ©\n\nParce que le GIL force lâ€™exÃ©cution sÃ©quentielle des threads Python\n\nParce quâ€™il utilise trop de mÃ©moire\n\nParce quâ€™il est obsolÃ¨te\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le GIL empÃªche les threads Python de sâ€™exÃ©cuter en parallÃ¨le. Pour du CPU-bound, il faut utiliser des processus.\n\n\n\n\nâ“ Q7. Pour traiter un fichier de 50 Go avec une API Pandas-like, quel outil choisir ?\n\nPandas\n\nPolars\n\nDask\n\nasyncio\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Dask permet de traiter des fichiers plus grands que la RAM avec une API similaire Ã  Pandas.\n\n\n\n\nâ“ Q8. Quelle est la premiÃ¨re Ã©tape avant dâ€™optimiser du code ?\n\nAjouter du multiprocessing\n\nRÃ©Ã©crire en Rust\n\nProfiler pour identifier le goulot dâ€™Ã©tranglement\n\nUtiliser asyncio\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” â€œPremature optimization is the root of all evilâ€. Il faut dâ€™abord mesurer pour savoir oÃ¹ optimiser.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#mini-projet-pipeline-haute-performance",
    "href": "notebooks/intermediate/18_high_performance_python.html#mini-projet-pipeline-haute-performance",
    "title": "High Performance Python",
    "section": "Mini-projet : Pipeline haute performance",
    "text": "Mini-projet : Pipeline haute performance\n\nObjectif\nCombiner plusieurs techniques pour crÃ©er un pipeline performant : - ProcessPoolExecutor pour transformation CPU-intensive - Dask pour agrÃ©gation - Export Parquet\n\n\nArchitecture\ndata/raw/*.csv\n      â”‚\n      â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ ProcessPoolExecutor â”‚  Transformation parallÃ¨le (CPU)\nâ”‚  - Nettoyage        â”‚\nâ”‚  - Feature eng.     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\ndata/intermediate/*.csv\n           â”‚\n           â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Dask DataFrame    â”‚  AgrÃ©gation (multi-fichiers)\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n           â”‚\n           â–¼\ndata/processed/result.parquet\n\n\nVoir le code\n# Setup : crÃ©er les donnÃ©es de test\nimport pandas as pd\nimport numpy as np\nimport os\n\nos.makedirs(\"data/raw\", exist_ok=True)\nos.makedirs(\"data/intermediate\", exist_ok=True)\nos.makedirs(\"data/processed\", exist_ok=True)\n\n# CrÃ©er 10 fichiers CSV (simule des logs)\nnp.random.seed(42)\ncategories = [\"web\", \"api\", \"db\", \"cache\", \"auth\"]\nstatuses = [\"success\", \"error\", \"timeout\"]\n\nfor i in range(10):\n    n_rows = 10000\n    df = pd.DataFrame({\n        \"timestamp\": pd.date_range(\"2024-01-01\", periods=n_rows, freq=\"s\"),\n        \"category\": np.random.choice(categories, n_rows),\n        \"status\": np.random.choice(statuses, n_rows, p=[0.8, 0.15, 0.05]),\n        \"response_time_ms\": np.random.exponential(100, n_rows),\n        \"bytes_sent\": np.random.randint(100, 10000, n_rows)\n    })\n    df.to_csv(f\"data/raw/logs_{i:02d}.csv\", index=False)\n\nprint(\"10 fichiers CSV crÃ©Ã©s (100,000 lignes au total)\")\n\n\n\n\nVoir le code\nfrom concurrent.futures import ProcessPoolExecutor\nimport pandas as pd\nimport numpy as np\nimport glob\nimport time\n\ndef transform_file(filepath):\n    \"\"\"\n    Transformation CPU-intensive d'un fichier.\n    - Nettoyage\n    - Feature engineering\n    - Export intermÃ©diaire\n    \"\"\"\n    # Lire\n    df = pd.read_csv(filepath)\n    \n    # Nettoyage : filtrer les timeouts extrÃªmes\n    df = df[df[\"response_time_ms\"] &lt; 10000]\n    \n    # Feature engineering (CPU-intensive)\n    df[\"response_time_log\"] = np.log1p(df[\"response_time_ms\"])\n    df[\"is_error\"] = (df[\"status\"] != \"success\").astype(int)\n    df[\"throughput\"] = df[\"bytes_sent\"] / (df[\"response_time_ms\"] + 1)\n    \n    # Extraction date\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"day_of_week\"] = df[\"timestamp\"].dt.dayofweek\n    \n    # Export intermÃ©diaire\n    output_path = filepath.replace(\"raw\", \"intermediate\")\n    df.to_csv(output_path, index=False)\n    \n    return output_path\n\n# Liste des fichiers\ninput_files = sorted(glob.glob(\"data/raw/*.csv\"))\nprint(f\"ğŸ“ {len(input_files)} fichiers Ã  traiter\")\n\n# ============ TRANSFORMATION PARALLÃˆLE ============\nstart = time.time()\n\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    output_files = list(executor.map(transform_file, input_files))\n\nprint(f\"\\nâ±ï¸ Transformation : {time.time() - start:.2f}s\")\nprint(f\"{len(output_files)} fichiers transformÃ©s\")\n\n\n\n\nVoir le code\nimport dask.dataframe as dd\nimport time\n\n# ============ AGRÃ‰GATION AVEC DASK ============\nstart = time.time()\n\n# Lire tous les fichiers intermÃ©diaires (lazy)\nddf = dd.read_csv(\"data/intermediate/*.csv\")\n\n# Pipeline d'agrÃ©gation\nresult = (\n    ddf\n    .groupby([\"category\", \"status\", \"hour\"])\n    .agg({\n        \"response_time_ms\": [\"mean\", \"max\", \"count\"],\n        \"bytes_sent\": \"sum\",\n        \"is_error\": \"sum\",\n        \"throughput\": \"mean\"\n    })\n    .compute()  # ExÃ©cution\n)\n\n# Aplatir les colonnes multi-index\nresult.columns = ['_'.join(col).strip() for col in result.columns.values]\nresult = result.reset_index()\n\nprint(f\"â±ï¸ AgrÃ©gation Dask : {time.time() - start:.2f}s\")\nprint(f\"\\nğŸ“Š RÃ©sultat : {len(result)} lignes\")\nprint(result.head(10))\n\n\n\n\nVoir le code\n# ============ EXPORT PARQUET ============\nresult.to_parquet(\"data/processed/aggregated_logs.parquet\", index=False)\nprint(\"âœ… RÃ©sultat exportÃ© : data/processed/aggregated_logs.parquet\")\n\n# VÃ©rification\nimport os\nsize_bytes = os.path.getsize(\"data/processed/aggregated_logs.parquet\")\nprint(f\"ğŸ“¦ Taille : {size_bytes / 1024:.1f} KB\")\n\n\n\n\nVoir le code\n# ============ RÃ‰SUMÃ‰ DU PIPELINE ============\nprint(\"\\n\" + \"=\"*50)\nprint(\"RÃ‰SUMÃ‰ DU PIPELINE HAUTE PERFORMANCE\")\nprint(\"=\"*50)\nprint(f\"\"\"  \n1ï¸âƒ£ Input : 10 fichiers CSV (100K lignes)\n2ï¸âƒ£ Transformation : ProcessPoolExecutor (4 workers)\n   - Nettoyage\n   - Feature engineering\n3ï¸âƒ£ AgrÃ©gation : Dask DataFrame\n   - GroupBy multi-colonnes\n   - Aggregations multiples\n4ï¸âƒ£ Output : Parquet ({size_bytes / 1024:.1f} KB)\n\"\"\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/18_high_performance_python.html#ressources-pour-aller-plus-loin",
    "title": "High Performance Python",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nconcurrent.futures â€” Doc Python\nasyncio â€” Doc Python\nDask Documentation â€” Guide complet\njoblib â€” ParallÃ©lisation simple\n\n\n\nğŸ“– Articles & Tutoriels\n\nReal Python - Async IO â€” Tutoriel complet\nSpeed Up Your Python Code â€” Guide concurrence\n\n\n\nğŸ”§ Outils de profiling\n\npy-spy â€” Sampling profiler\nScalene â€” CPU + mÃ©moire + GPU profiler",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/18_high_performance_python.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/18_high_performance_python.html#prochaine-Ã©tape",
    "title": "High Performance Python",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises les techniques de performance en Python, passons au traitement distribuÃ© Ã  grande Ã©chelle avec Spark !\nğŸ‘‰ Module suivant : 19_pyspark_advanced â€” PySpark AvancÃ©\nTu vas apprendre : - Architecture Spark (Driver, Executors) - RDDs et DataFrames Spark - Optimisations (partitioning, caching) - Spark sur Kubernetes\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nOutil\nType\nQuand lâ€™utiliser\n\n\n\n\nThreadPoolExecutor\nI/O\n&lt; 20 requÃªtes/fichiers\n\n\nProcessPoolExecutor\nCPU\nCalculs, transformations\n\n\nasyncio\nI/O massif\n100+ requÃªtes API\n\n\njoblib\nSimple\nParallÃ©liser une boucle\n\n\nDask\nBig Data\nFichiers &gt; RAM, API Pandas\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module High Performance Python.\n\n\nVoir le code\n# Nettoyage des fichiers temporaires (optionnel)\nimport shutil\nimport os\n\n# DÃ©commenter pour nettoyer\n# for folder in [\"data/raw\", \"data/intermediate\", \"data/processed\", \"data/dask_demo\"]:\n#     if os.path.exists(folder):\n#         shutil.rmtree(folder)\n# print(\"ğŸ§¹ Fichiers temporaires supprimÃ©s\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸš€ High Performance Python",
      "18 Â· High Performance Python"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "",
    "text": "Bienvenue dans ce module oÃ¹ tu vas maÃ®triser le Cloud Computing et lâ€™Object Storage, les fondations de tout Data Lake moderne.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#prÃ©requis",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#prÃ©requis",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "PrÃ©requis",
    "text": "PrÃ©requis\n\n\n\n\n\n\n\n\nNiveau\nModule\nCompÃ©tence\n\n\n\n\nâœ… Requis\nModule 14\nDocker Fundamentals\n\n\nâœ… Requis\nModule 19\nPySpark Advanced\n\n\nâœ… Requis\nModule 21\nSpark on Kubernetes\n\n\nğŸ’¡ RecommandÃ©\nModule 08\nBig Data Introduction (Medallion Architecture)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#objectifs-du-module",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#objectifs-du-module",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "Objectifs du module",
    "text": "Objectifs du module\nÃ€ la fin de ce module, tu seras capable de :\n\nComprendre les modÃ¨les Cloud (IaaS, PaaS, SaaS)\nConnaÃ®tre les services Data Engineering sur AWS, GCP, Azure\nMaÃ®triser les concepts de lâ€™Object Storage (buckets, keys, prefixes)\nLire/Ã©crire sur S3, Azure Blob, GCS avec Python et Spark\nDÃ©ployer MinIO pour pratiquer localement\nOptimiser les performances (formats, partitionnement, small files)\nGÃ©rer la sÃ©curitÃ© et les coÃ»ts",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-pourquoi-le-cloud-pour-le-data-engineering",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-pourquoi-le-cloud-pour-le-data-engineering",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "1. Introduction â€” Pourquoi le Cloud pour le Data Engineering ?",
    "text": "1. Introduction â€” Pourquoi le Cloud pour le Data Engineering ?\n\n1.1 Lâ€™Ã©volution du Data Engineering\n2000s                    2010s                    2020s+\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  On-Premise â”‚         â”‚   Hadoop    â”‚         â”‚    Cloud    â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”‚\nâ”‚  â”‚ RDBMS â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚ HDFS  â”‚  â”‚  â”€â”€â”€â”€â–¶  â”‚  â”‚  S3   â”‚  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\nâ”‚  CoÃ»teux    â”‚         â”‚  Complexe   â”‚         â”‚  Scalable   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n1.2 Avantages du Cloud pour le Data Engineering\n\n\n\nAvantage\nDescription\n\n\n\n\nScalabilitÃ©\nStockage et compute illimitÃ©s Ã  la demande\n\n\nCoÃ»t\nPay-as-you-go, pas dâ€™investissement initial\n\n\nManaged Services\nMoins dâ€™ops, plus de focus sur les donnÃ©es\n\n\nSÃ©paration compute/storage\nScaler indÃ©pendamment\n\n\nDurabilitÃ©\n99.999999999% (11 nines) pour S3\n\n\nGlobal\nDonnÃ©es accessibles partout",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-au-cloud-computing",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#introduction-au-cloud-computing",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "2. Introduction au Cloud Computing",
    "text": "2. Introduction au Cloud Computing\n\n2.1 Câ€™est quoi le Cloud ?\nLe Cloud = des serveurs, du stockage et des services accessibles via Internet, gÃ©rÃ©s par un provider.\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                        LE CLOUD                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\nâ”‚  â”‚ Compute â”‚  â”‚ Storage â”‚  â”‚ Network â”‚  â”‚  ....   â”‚       â”‚\nâ”‚  â”‚  (VMs)  â”‚  â”‚ (Disks) â”‚  â”‚  (VPC)  â”‚  â”‚         â”‚       â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\nâ”‚                                                             â”‚\nâ”‚  Tu ne gÃ¨res pas le hardware, juste les services            â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                              â–²\n                              â”‚ Internet\n                              â”‚\n                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                    â”‚   Ton application  â”‚\n                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n2.2 Les 3 modÃ¨les de service\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    RESPONSABILITÃ‰                              â”‚\nâ”‚                                                                â”‚\nâ”‚   On-Premise      IaaS           PaaS           SaaS          â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\nâ”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚Applicationâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â—€â”€â”€ Provider â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚  Data    â”‚  â”‚  Data    â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Runtime  â”‚  â”‚ Runtime  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚   OS     â”‚  â”‚   OS     â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Virtual  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Servers  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Storage  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â”‚ Network  â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚      â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\nâ”‚                                                                â”‚\nâ”‚  â–ˆ = GÃ©rÃ© par le Cloud Provider                               â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\n\nModÃ¨le\nDescription\nExemples\nTu gÃ¨res\n\n\n\n\nIaaS\nInfrastructure as a Service\nEC2, VMs, VPC\nOS, Runtime, App\n\n\nPaaS\nPlatform as a Service\nRDS, Cloud SQL, EKS\nApp, Data\n\n\nSaaS\nSoftware as a Service\nSnowflake, Databricks\nRien (juste utiliser)\n\n\n\n\n\n2.3 Les 3 grands Cloud Providers\n\n\n\n\n\n\n\n\n\nProvider\nPart de marchÃ©\nForces\nFaiblesse\n\n\n\n\nAWS\n~32%\nLeader, plus de services, maturitÃ©\nComplexitÃ©, coÃ»ts\n\n\nAzure\n~23%\nIntÃ©gration Microsoft, entreprises\nUX parfois confuse\n\n\nGCP\n~10%\nBigQuery, ML/AI, Kubernetes\nMoins de services\n\n\n\n\n\n2.4 RÃ©gions & Zones de disponibilitÃ©\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                     AWS Global                              â”‚\nâ”‚                                                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚\nâ”‚  â”‚  Region:        â”‚    â”‚  Region:        â”‚                â”‚\nâ”‚  â”‚  eu-west-1      â”‚    â”‚  us-east-1      â”‚                â”‚\nâ”‚  â”‚  (Ireland)      â”‚    â”‚  (N. Virginia)  â”‚                â”‚\nâ”‚  â”‚                 â”‚    â”‚                 â”‚                â”‚\nâ”‚  â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚    â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚                â”‚\nâ”‚  â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚    â”‚ â”‚AZ1â”‚ â”‚AZ2â”‚ â”‚AZ3â”‚â”‚                â”‚\nâ”‚  â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚    â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜â”‚                â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\nâ”‚                                                             â”‚\nâ”‚  AZ = Availability Zone = Data Center isolÃ©                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRegion : Zone gÃ©ographique (Paris, Dublin, N. Virginia)\nAvailability Zone : Data center isolÃ© dans une rÃ©gion\nLatence : Choisir la rÃ©gion proche des utilisateurs\nCompliance : GDPR â†’ donnÃ©es en Europe\n\n\n\nExercice 1 : Identifier IaaS / PaaS / SaaS\nClasse ces services dans la bonne catÃ©gorie :\n\n\n\nService\nIaaS / PaaS / SaaS ?\n\n\n\n\nAmazon EC2\n?\n\n\nGoogle BigQuery\n?\n\n\nSnowflake\n?\n\n\nAzure Kubernetes Service\n?\n\n\nAmazon S3\n?\n\n\nDatabricks\n?\n\n\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\n\n\n\n\n\n\n\nService\nCatÃ©gorie\nExplication\n\n\n\n\nAmazon EC2\nIaaS\nTu gÃ¨res lâ€™OS et tout ce quâ€™il y a dessus\n\n\nGoogle BigQuery\nPaaS/SaaS\nServerless, tu gÃ¨res juste les requÃªtes\n\n\nSnowflake\nSaaS\nEntiÃ¨rement managÃ©\n\n\nAzure Kubernetes Service\nPaaS\nK8s managÃ©, tu gÃ¨res les workloads\n\n\nAmazon S3\nPaaS\nStockage managÃ©, tu gÃ¨res les donnÃ©es\n\n\nDatabricks\nPaaS/SaaS\nSpark managÃ©",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#services-cloud-pour-le-data-engineering",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#services-cloud-pour-le-data-engineering",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "3. Services Cloud pour le Data Engineering",
    "text": "3. Services Cloud pour le Data Engineering\n\n3.1 Tableau comparatif complet\n\n\n\n\n\n\n\n\n\nCatÃ©gorie\nAWS\nGCP\nAzure\n\n\n\n\nObject Storage\nS3\nGCS\nBlob Storage\n\n\nData Warehouse\nRedshift\nBigQuery\nSynapse Analytics\n\n\nETL Serverless\nGlue\nDataflow\nData Factory\n\n\nQuery Engine\nAthena\nBigQuery\nSynapse Serverless\n\n\nOrchestration\nMWAA (Airflow)\nComposer (Airflow)\nData Factory\n\n\nStreaming\nKinesis\nPub/Sub + Dataflow\nEvent Hubs\n\n\nCatalog\nGlue Catalog\nData Catalog\nPurview\n\n\nKubernetes\nEKS\nGKE\nAKS\n\n\nServerless Compute\nLambda\nCloud Functions\nFunctions\n\n\nNoSQL\nDynamoDB\nFirestore/Bigtable\nCosmosDB\n\n\nMessage Queue\nSQS/SNS\nPub/Sub\nService Bus\n\n\n\n\n\n3.2 Focus sur les services Data Engineering\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                  ARCHITECTURE DATA CLOUD                        â”‚\nâ”‚                                                                 â”‚\nâ”‚  Sources              Ingestion           Storage               â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚ API â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Kinesis â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   S3    â”‚           â”‚\nâ”‚  â”‚ DB  â”‚             â”‚ Pub/Sub â”‚         â”‚  GCS    â”‚           â”‚\nâ”‚  â”‚ Filesâ”‚             â”‚ EventHubâ”‚         â”‚  Blob   â”‚           â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\nâ”‚                                               â”‚                 â”‚\nâ”‚                                               â–¼                 â”‚\nâ”‚  Processing                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Glue   â”‚           â”‚\nâ”‚  â”‚                                       â”‚Dataflow â”‚           â”‚\nâ”‚  â”‚                                       â”‚  ADF    â”‚           â”‚\nâ”‚  â”‚                                       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚\nâ”‚  â”‚                                            â”‚                 â”‚\nâ”‚  â”‚                                            â–¼                 â”‚\nâ”‚  â”‚  Serving                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚Redshift â”‚           â”‚\nâ”‚  â””â”€â”€â”‚ Athena  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚BigQuery â”‚           â”‚\nâ”‚     â”‚ Synapse â”‚                          â”‚ Synapse â”‚           â”‚\nâ”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#storage-models-block-vs-file-vs-object",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#storage-models-block-vs-file-vs-object",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "4. Storage Models : Block vs File vs Object",
    "text": "4. Storage Models : Block vs File vs Object\n\n4.1 Comparaison dÃ©taillÃ©e\n\n\n\nCritÃ¨re\nBlock Storage\nFile Storage\nObject Storage\n\n\n\n\nStructure\nBlocs bruts\nHiÃ©rarchie fichiers\nClÃ©-valeur plat\n\n\nAccÃ¨s\nBas niveau (disque)\nPOSIX (NFS, SMB)\nAPI HTTP (REST)\n\n\nMetadata\nMinimales\nBasiques\nRiches, custom\n\n\nScalabilitÃ©\nLimitÃ©e (TB)\nLimitÃ©e (TB)\nIllimitÃ©e (PB+)\n\n\nPerformance\nTrÃ¨s haute\nMoyenne\nVariable\n\n\nCoÃ»t\nÃ‰levÃ©\nMoyen\nFaible\n\n\nUse case\nBases de donnÃ©es\nPartage fichiers\nData Lakes\n\n\nExemples\nEBS, Azure Disk\nEFS, Azure Files\nS3, GCS, Blob\n\n\n\n\n\n4.2 Pourquoi Object Storage pour les Data Lakes ?\nâœ… ScalabilitÃ© illimitÃ©e      âœ… CoÃ»t faible\nâœ… DurabilitÃ© 11 nines        âœ… API HTTP standard\nâœ… SÃ©paration compute/storage âœ… Metadata riches\nâœ… Formats natifs (Parquet)   âœ… Multi-cloud possible\n\n\n4.3 SÃ©paration Compute / Storage\nAVANT (Hadoop/HDFS)                    APRÃˆS (Cloud/Object Storage)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚      Node 1         â”‚               â”‚    Compute (Spark)   â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”‚               â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚\nâ”‚  â”‚Computeâ”‚ â”‚Data â”‚  â”‚               â”‚    â”‚  Executor â”‚     â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â”‚               â”‚    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â”‚\nâ”‚      CouplÃ©s !      â”‚               â”‚          â”‚           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                                 â”‚ Network\n                                                 â–¼\n                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n                                      â”‚   Storage (S3)      â”‚\n                                      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚\n                                      â”‚    â”‚  Data   â”‚      â”‚\n                                      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚\n                                      â”‚   Scale sÃ©parÃ©ment  â”‚\n                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nExercice 2 : Choisir le bon type de storage\nQuel type de storage pour chaque use case ?\n\n\n\nUse case\nBlock / File / Object ?\n\n\n\n\nBase de donnÃ©es PostgreSQL\n?\n\n\nData Lake avec fichiers Parquet\n?\n\n\nPartage de documents entre Ã©quipes\n?\n\n\nLogs dâ€™application (PB de donnÃ©es)\n?\n\n\nVM avec OS Windows\n?\n\n\n\n\n\nğŸ’¡ Voir les rÃ©ponses\n\n\n\n\nUse case\nType\nRaison\n\n\n\n\nPostgreSQL\nBlock\nIOPS Ã©levÃ©s, accÃ¨s bas niveau\n\n\nData Lake Parquet\nObject\nScalable, coÃ»t faible\n\n\nPartage documents\nFile\nAccÃ¨s POSIX, permissions\n\n\nLogs application\nObject\nVolume Ã©norme, coÃ»t faible\n\n\nVM Windows\nBlock\nDisque systÃ¨me",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#object-storage-concepts-fondamentaux",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#object-storage-concepts-fondamentaux",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "5. Object Storage â€” Concepts fondamentaux",
    "text": "5. Object Storage â€” Concepts fondamentaux\n\n5.1 Buckets, Keys, Prefixes\ns3://my-bucket/bronze/2024/01/transactions.parquet\n     â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n       Bucket           Prefix            Object Key\n\nâš ï¸ IMPORTANT : Les prÃ©fixes NE SONT PAS des dossiers !\n   C'est juste une convention de nommage (clÃ©-valeur plat)\n\n\n\n\n\n\n\n\nConcept\nDescription\nExemple\n\n\n\n\nBucket\nConteneur racine, nom unique global\nmy-company-datalake\n\n\nKey\nIdentifiant unique de lâ€™objet\nbronze/2024/01/data.parquet\n\n\nPrefix\nâ€œFaux dossierâ€, filtre de listing\nbronze/2024/\n\n\nObject\nLe fichier + ses metadata\nParquet, CSV, JSONâ€¦\n\n\n\n\n\n5.2 Metadata & Tags\nChaque objet peut avoir des metadata custom :\n\n\nVoir le code\n# Exemple de metadata sur un objet S3\nmetadata_example = {\n    # Metadata systÃ¨me (automatiques)\n    \"Content-Type\": \"application/octet-stream\",\n    \"Content-Length\": 1048576,\n    \"Last-Modified\": \"2024-01-15T10:30:00Z\",\n    \"ETag\": \"d41d8cd98f00b204e9800998ecf8427e\",\n    \n    # Metadata custom (x-amz-meta-*)\n    \"x-amz-meta-source\": \"kafka-topic-orders\",\n    \"x-amz-meta-pipeline\": \"etl-daily\",\n    \"x-amz-meta-schema-version\": \"2.1\",\n}\n\n# Tags (pour billing, governance)\ntags = {\n    \"Environment\": \"production\",\n    \"Team\": \"data-engineering\",\n    \"CostCenter\": \"DE-001\",\n}\n\nprint(\"Metadata et Tags permettent de :\")\nprint(\"- Tracer l'origine des donnÃ©es\")\nprint(\"- Filtrer pour la gouvernance\")\nprint(\"- Allouer les coÃ»ts par Ã©quipe\")\n\n\n\n\n5.3 Versioning & Lifecycle Policies\nVersioning : Garder plusieurs versions dâ€™un mÃªme objet\ns3://bucket/data.csv\n   â”‚\n   â”œâ”€â”€ Version 1 (2024-01-01) â† Ancienne\n   â”œâ”€â”€ Version 2 (2024-01-15) â† Ancienne\n   â””â”€â”€ Version 3 (2024-02-01) â† Current\nLifecycle Policies : Automatiser la gestion du stockage\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    30 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   90 jours    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚   Standard  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ Infrequent  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚   Glacier   â”‚\nâ”‚   $0.023/GB â”‚                â”‚   $0.0125/GBâ”‚               â”‚  $0.004/GB  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n     Hot                           Cool                          Archive\n\n\n5.4 Classes de stockage\n\n\n\nClasse\nUsage\nLatence\nCoÃ»t stockage\n\n\n\n\nStandard\nAccÃ¨s frÃ©quent\nms\n$0.023/GB\n\n\nIA (Infrequent Access)\nAccÃ¨s rare\nms\n$0.0125/GB\n\n\nGlacier\nArchivage\nminutes-heures\n$0.004/GB\n\n\nGlacier Deep Archive\nCompliance\nheures\n$0.00099/GB\n\n\n\n\n\n5.5 Protocoles & Drivers\n\nğŸ”¥ Section essentielle â€” Source de confusion frÃ©quente !\n\n\n\n\n\n\n\n\n\n\n\nProtocole\nCloud\nUsage\nDriver/SDK\nExemple\n\n\n\n\ns3://\nAWS\nCLI, Python\nboto3, aws cli\ns3://bucket/key\n\n\ns3a://\nAWS\nSpark/Hadoop\nhadoop-aws\ns3a://bucket/key\n\n\nabfs://\nAzure\nSpark (legacy)\nhadoop-azure\nabfs://container@account.dfs.core.windows.net/\n\n\nabfss://\nAzure\nSpark (TLS)\nhadoop-azure\nabfss://container@account.dfs.core.windows.net/\n\n\ngs://\nGCP\nSpark, CLI\ngcs-connector\ngs://bucket/key\n\n\nhttps://\nTous\nDirect HTTP\nrequests\nSigned URLs\n\n\n\nâš ï¸ ATTENTION :\n\ns3://  â‰   s3a://\n\n- s3://  â†’ AWS CLI, boto3 (haut niveau)\n- s3a:// â†’ Hadoop/Spark (bas niveau, optimisÃ© Big Data)\n\nDans Spark, TOUJOURS utiliser s3a://, abfss://, ou gs://\n\n\n5.6 Metadata Catalogs â€” Transition vers le module 23\nLâ€™Object Storage stocke des fichiers bruts. Mais pour faire du SQL, on a besoin de : - SchÃ©ma des tables (colonnes, types) - Localisation des partitions - Statistiques pour lâ€™optimiseur\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    METADATA CATALOG                             â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\nâ”‚  â”‚  Table: sales                                           â”‚   â”‚\nâ”‚  â”‚  Location: s3://bucket/silver/sales/                    â”‚   â”‚\nâ”‚  â”‚  Schema: id INT, amount DOUBLE, date DATE               â”‚   â”‚\nâ”‚  â”‚  Partitions: date=2024-01-01, date=2024-01-02, ...      â”‚   â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n                                â”‚\n                                â–¼\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    OBJECT STORAGE (S3)                          â”‚\nâ”‚  s3://bucket/silver/sales/date=2024-01-01/part-00000.parquet   â”‚\nâ”‚  s3://bucket/silver/sales/date=2024-01-02/part-00000.parquet   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\nSolutions Catalog :\n\n\n\nSolution\nType\nUtilisÃ© par\n\n\n\n\nHive Metastore\nOpen-source\nSpark, Presto, Trino\n\n\nAWS Glue Catalog\nManaged\nAthena, Glue, EMR\n\n\nGCP Data Catalog\nManaged\nBigQuery, Dataproc\n\n\nAzure Purview\nManaged\nSynapse, Databricks\n\n\n\n\nğŸ’¡ Preview Module 23 : Delta Lake et Iceberg intÃ¨grent leur propre Transaction Log directement dans lâ€™Object Storage. Plus besoin de catalogue externe !\n\n\n\n5.7 Layout Data Lake â€” Rappel (rÃ©f module 08)\ns3://my-datalake/\nâ”‚\nâ”œâ”€â”€ bronze/                    â† Raw, immutable, source of truth\nâ”‚   â”œâ”€â”€ orders/\nâ”‚   â”‚   â””â”€â”€ 2024/01/01/\nâ”‚   â”‚       â””â”€â”€ orders_raw.json\nâ”‚   â””â”€â”€ customers/\nâ”‚       â””â”€â”€ customers_full.csv\nâ”‚\nâ”œâ”€â”€ silver/                    â† Cleaned, validated, deduplicated\nâ”‚   â”œâ”€â”€ orders/\nâ”‚   â”‚   â””â”€â”€ date=2024-01-01/\nâ”‚   â”‚       â””â”€â”€ part-00000.parquet\nâ”‚   â””â”€â”€ customers/\nâ”‚       â””â”€â”€ part-00000.parquet\nâ”‚\nâ””â”€â”€ gold/                      â† Aggregated, business-ready\n    â”œâ”€â”€ daily_sales/\n    â”‚   â””â”€â”€ part-00000.parquet\n    â””â”€â”€ customer_360/\n        â””â”€â”€ part-00000.parquet\n\n\nExercice 3 : Calculer le coÃ»t de stockage\nScÃ©nario : Tu as un Data Lake avec :\n\nBronze : 500 GB (accÃ¨s rare)\nSilver : 200 GB (accÃ¨s frÃ©quent)\nGold : 50 GB (accÃ¨s trÃ¨s frÃ©quent)\n\nCalcule le coÃ»t mensuel optimal sur S3.\n\n\nğŸ’¡ Voir la solution\n\n\n\n\nLayer\nTaille\nClasse\nPrix/GB\nCoÃ»t\n\n\n\n\nBronze\n500 GB\nS3 IA\n$0.0125\n$6.25\n\n\nSilver\n200 GB\nStandard\n$0.023\n$4.60\n\n\nGold\n50 GB\nStandard\n$0.023\n$1.15\n\n\nTotal\n750 GB\n\n\n$12.00/mois\n\n\n\nSans optimisation (tout en Standard) : 750 Ã— $0.023 = $17.25/mois\nÃ‰conomie : 30% !",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#aws-s3-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#aws-s3-deep-dive",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "6. AWS S3 â€” Deep Dive",
    "text": "6. AWS S3 â€” Deep Dive\n\n6.1 Concepts & Classes de stockage\n\n\n\nClasse\nDurabilitÃ©\nDisponibilitÃ©\nMin storage\nRetrieval\n\n\n\n\nStandard\n11 nines\n99.99%\n-\nImmÃ©diat\n\n\nIntelligent-Tiering\n11 nines\n99.9%\n-\nImmÃ©diat\n\n\nStandard-IA\n11 nines\n99.9%\n30 jours\nImmÃ©diat\n\n\nGlacier Instant\n11 nines\n99.9%\n90 jours\nms\n\n\nGlacier Flexible\n11 nines\n99.99%\n90 jours\n1-12h\n\n\nGlacier Deep Archive\n11 nines\n99.99%\n180 jours\n12-48h\n\n\n\n\n\n6.2 OpÃ©rations CLI\n\n\nVoir le code\ns3_cli_commands = \"\"\"\n# Lister les buckets\naws s3 ls\n\n# Lister le contenu d'un bucket\naws s3 ls s3://my-bucket/bronze/\n\n# Copier un fichier local vers S3\naws s3 cp data.csv s3://my-bucket/bronze/data.csv\n\n# Copier un fichier S3 vers local\naws s3 cp s3://my-bucket/bronze/data.csv ./data.csv\n\n# Synchroniser un dossier\naws s3 sync ./local-folder/ s3://my-bucket/bronze/\n\n# Supprimer un fichier\naws s3 rm s3://my-bucket/bronze/old-data.csv\n\n# Supprimer rÃ©cursivement\naws s3 rm s3://my-bucket/temp/ --recursive\n\"\"\"\nprint(s3_cli_commands)\n\n\n\n\n6.3 Python avec boto3\n\n\nVoir le code\n# Installation : pip install boto3\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n# CrÃ©er un client S3\ns3 = boto3.client('s3')\n\n# --- Upload ---\ndef upload_file(file_path, bucket, key):\n    \"\"\"Upload un fichier vers S3.\"\"\"\n    s3.upload_file(file_path, bucket, key)\n    print(f\"âœ… Uploaded {file_path} to s3://{bucket}/{key}\")\n\n# --- Download ---\ndef download_file(bucket, key, file_path):\n    \"\"\"Download un fichier depuis S3.\"\"\"\n    s3.download_file(bucket, key, file_path)\n    print(f\"âœ… Downloaded s3://{bucket}/{key} to {file_path}\")\n\n# --- List objects ---\ndef list_objects(bucket, prefix=\"\"):\n    \"\"\"Liste les objets dans un bucket.\"\"\"\n    response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n    if 'Contents' in response:\n        for obj in response['Contents']:\n            print(f\"  {obj['Key']} ({obj['Size']} bytes)\")\n    return response.get('Contents', [])\n\n# --- Presigned URL ---\ndef generate_presigned_url(bucket, key, expiration=3600):\n    \"\"\"GÃ©nÃ¨re une URL temporaire pour accÃ¨s direct.\"\"\"\n    url = s3.generate_presigned_url(\n        'get_object',\n        Params={'Bucket': bucket, 'Key': key},\n        ExpiresIn=expiration\n    )\n    return url\n\n# Exemple d'utilisation (commentÃ© car pas de credentials)\n# upload_file('data.csv', 'my-bucket', 'bronze/data.csv')\n# list_objects('my-bucket', 'bronze/')\nprint(\"ğŸ“ Fonctions boto3 dÃ©finies (upload, download, list, presigned URL)\")\n\n\n\n\n6.4 S3 avec Spark (s3a://)\n\n\nVoir le code\n# Configuration Spark pour S3\nspark_s3_config = \"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"S3 Access\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n    .getOrCreate()\n\n# Lire depuis S3\ndf = spark.read.parquet(\"s3a://my-bucket/silver/sales/\")\n\n# Ã‰crire vers S3\ndf.write.mode(\"overwrite\").parquet(\"s3a://my-bucket/gold/aggregates/\")\n\"\"\"\n\n# Optimisations S3A\ns3a_optimizations = \"\"\"\n# Performance optimizations\nspark.hadoop.fs.s3a.connection.maximum=200\nspark.hadoop.fs.s3a.threads.max=64\nspark.hadoop.fs.s3a.fast.upload=true\nspark.hadoop.fs.s3a.fast.upload.buffer=bytebuffer\nspark.hadoop.fs.s3a.multipart.size=104857600  # 100MB\n\"\"\"\nprint(spark_s3_config)\nprint(\"\\n--- Optimisations S3A ---\")\nprint(s3a_optimizations)\n\n\n\n\n6.5 Authentification & IAM\n\n\n\n\n\n\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nAccess Keys\nâš ï¸ Faible\nDev local\nâŒ Non\n\n\nInstance Profile\nâœ… Haute\nEC2\nâœ… Oui\n\n\nIRSA (IAM Roles for Service Accounts)\nâœ… Haute\nEKS/K8s\nâœ… Oui\n\n\nAssumeRole\nâœ… Haute\nCross-account\nâœ… Oui\n\n\n\nğŸ” Best Practice : JAMAIS de credentials dans le code !\n\nUtiliser :\n- Variables d'environnement\n- Instance Profiles (EC2)\n- IRSA (Kubernetes) â† Module 21\n- AWS Secrets Manager\n\n\nExercice 4 : Upload/Download avec boto3\nÃ‰cris un script Python qui :\n\nCrÃ©e un fichier CSV local avec 3 lignes\nLâ€™upload vers S3 (ou MinIO)\nListe les fichiers du bucket\nTÃ©lÃ©charge le fichier sous un autre nom\n\n\n\nğŸ’¡ Voir la solution\n\nimport boto3\nimport csv\n\n# 1. CrÃ©er un fichier CSV\nwith open('test_data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['id', 'name', 'amount'])\n    writer.writerow([1, 'Alice', 100])\n    writer.writerow([2, 'Bob', 200])\n    writer.writerow([3, 'Charlie', 150])\n\n# 2. Upload\ns3 = boto3.client('s3')\ns3.upload_file('test_data.csv', 'my-bucket', 'bronze/test_data.csv')\n\n# 3. List\nresponse = s3.list_objects_v2(Bucket='my-bucket', Prefix='bronze/')\nfor obj in response.get('Contents', []):\n    print(obj['Key'])\n\n# 4. Download\ns3.download_file('my-bucket', 'bronze/test_data.csv', 'downloaded_data.csv')",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#azure-blob-storage-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#azure-blob-storage-deep-dive",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "7. Azure Blob Storage â€” Deep Dive",
    "text": "7. Azure Blob Storage â€” Deep Dive\n\n7.1 Concepts\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                   AZURE STORAGE ACCOUNT                     â”‚\nâ”‚                   (mystorageaccount)                        â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\nâ”‚  â”‚   Container:    â”‚  â”‚   Container:    â”‚                  â”‚\nâ”‚  â”‚   bronze        â”‚  â”‚   silver        â”‚                  â”‚\nâ”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                  â”‚\nâ”‚  â”‚  â”‚ data.csv  â”‚  â”‚  â”‚  â”‚ data.parq â”‚  â”‚                  â”‚\nâ”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                  â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\nConcept Azure\nÃ‰quivalent S3\n\n\n\n\nStorage Account\n(pas dâ€™Ã©quivalent, niveau compte)\n\n\nContainer\nBucket\n\n\nBlob\nObject\n\n\nADLS Gen2\nS3 + Glue Catalog intÃ©grÃ©\n\n\n\n\n\n7.2 Access Tiers\n\n\n\nTier\nUsage\nCoÃ»t stockage\nCoÃ»t accÃ¨s\n\n\n\n\nHot\nFrÃ©quent\nÃ‰levÃ©\nFaible\n\n\nCool\nRare (30+ jours)\nMoyen\nMoyen\n\n\nArchive\nArchivage (180+ jours)\nTrÃ¨s faible\nÃ‰levÃ©\n\n\n\n\n\nVoir le code\n# Installation : pip install azure-storage-blob\n\nazure_blob_example = \"\"\"\nfrom azure.storage.blob import BlobServiceClient, BlobClient\n\n# Connection string (dev only !)\nconnection_string = \"DefaultEndpointsProtocol=https;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net\"\n\n# CrÃ©er le client\nblob_service = BlobServiceClient.from_connection_string(connection_string)\n\n# Upload\nblob_client = blob_service.get_blob_client(container=\"bronze\", blob=\"data.csv\")\nwith open(\"data.csv\", \"rb\") as f:\n    blob_client.upload_blob(f, overwrite=True)\n\n# Download\nwith open(\"downloaded.csv\", \"wb\") as f:\n    f.write(blob_client.download_blob().readall())\n\n# List blobs\ncontainer_client = blob_service.get_container_client(\"bronze\")\nfor blob in container_client.list_blobs():\n    print(blob.name)\n\"\"\"\nprint(azure_blob_example)\n\n\n\n\n7.3 Azure Blob avec Spark (abfss://)\n\n\nVoir le code\nspark_azure_config = \"\"\"\n# Configuration Spark pour Azure Blob (ADLS Gen2)\nstorage_account = \"mystorageaccount\"\ncontainer = \"bronze\"\n\n# MÃ©thode 1 : Access Key (dev only)\nspark.conf.set(\n    f\"fs.azure.account.key.{storage_account}.dfs.core.windows.net\",\n    \"YOUR_ACCESS_KEY\"\n)\n\n# MÃ©thode 2 : Service Principal (production)\nspark.conf.set(f\"fs.azure.account.auth.type.{storage_account}.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account}.dfs.core.windows.net\",\n               \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account}.dfs.core.windows.net\", \"CLIENT_ID\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account}.dfs.core.windows.net\", \"CLIENT_SECRET\")\nspark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account}.dfs.core.windows.net\",\n               \"https://login.microsoftonline.com/TENANT_ID/oauth2/token\")\n\n# Lire\ndf = spark.read.parquet(f\"abfss://{container}@{storage_account}.dfs.core.windows.net/data/\")\n\"\"\"\nprint(spark_azure_config)\n\n\n\n\n7.4 Authentification Azure\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nAccess Keys\nâš ï¸ Faible\nDev\nâŒ Non\n\n\nSAS Token\nâš ï¸ Moyenne\nTemporaire, externe\nâš ï¸ LimitÃ©\n\n\nService Principal\nâœ… Haute\nApps, CI/CD\nâœ… Oui\n\n\nManaged Identity\nâœ… TrÃ¨s haute\nVMs, AKS\nâœ… Oui\n\n\n\n\n\nExercice 5 : GÃ©nÃ©rer un SAS Token\nUn SAS Token permet de donner un accÃ¨s temporaire Ã  un blob.\n\n\nğŸ’¡ Voir la solution\n\nfrom azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\nfrom datetime import datetime, timedelta\n\naccount_name = \"mystorageaccount\"\naccount_key = \"YOUR_KEY\"\ncontainer = \"bronze\"\nblob_name = \"data.csv\"\n\n# GÃ©nÃ©rer SAS Token (valide 1 heure)\nsas_token = generate_blob_sas(\n    account_name=account_name,\n    container_name=container,\n    blob_name=blob_name,\n    account_key=account_key,\n    permission=BlobSasPermissions(read=True),\n    expiry=datetime.utcnow() + timedelta(hours=1)\n)\n\n# URL complÃ¨te\nurl = f\"https://{account_name}.blob.core.windows.net/{container}/{blob_name}?{sas_token}\"\nprint(url)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#google-cloud-storage-deep-dive",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#google-cloud-storage-deep-dive",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "8. Google Cloud Storage â€” Deep Dive",
    "text": "8. Google Cloud Storage â€” Deep Dive\n\n8.1 Concepts\n\n\n\nClasse\nUsage\nSLA\nCoÃ»t\n\n\n\n\nStandard\nFrÃ©quent\n99.99%\n$0.020/GB\n\n\nNearline\n1x/mois\n99.9%\n$0.010/GB\n\n\nColdline\n1x/trimestre\n99.9%\n$0.004/GB\n\n\nArchive\n1x/an\n99.9%\n$0.0012/GB\n\n\n\n\n\nVoir le code\ngcs_cli_commands = \"\"\"\n# gsutil - CLI pour GCS\n\n# Lister les buckets\ngsutil ls\n\n# Lister le contenu d'un bucket\ngsutil ls gs://my-bucket/bronze/\n\n# Copier\ngsutil cp data.csv gs://my-bucket/bronze/\ngsutil cp gs://my-bucket/bronze/data.csv ./\n\n# Synchroniser (comme rsync)\ngsutil rsync -r ./local/ gs://my-bucket/bronze/\n\n# Copie parallÃ¨le (gros fichiers)\ngsutil -m cp -r ./data/ gs://my-bucket/bronze/\n\"\"\"\nprint(gcs_cli_commands)\n\n\n\n\nVoir le code\n# Installation : pip install google-cloud-storage\n\ngcs_python_example = \"\"\"\nfrom google.cloud import storage\n\n# CrÃ©er le client (utilise GOOGLE_APPLICATION_CREDENTIALS)\nclient = storage.Client()\n\n# AccÃ©der au bucket\nbucket = client.bucket(\"my-bucket\")\n\n# Upload\nblob = bucket.blob(\"bronze/data.csv\")\nblob.upload_from_filename(\"data.csv\")\n\n# Download\nblob.download_to_filename(\"downloaded.csv\")\n\n# List blobs\nblobs = bucket.list_blobs(prefix=\"bronze/\")\nfor blob in blobs:\n    print(blob.name)\n\n# Signed URL (temporaire)\nurl = blob.generate_signed_url(expiration=3600)  # 1 heure\n\"\"\"\nprint(gcs_python_example)\n\n\n\n\n8.3 GCS avec Spark (gs://)\n\n\nVoir le code\nspark_gcs_config = \"\"\"\n# Configuration Spark pour GCS\n\n# Option 1 : Service Account Key (dev)\nspark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\nspark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/path/to/keyfile.json\")\n\n# Option 2 : Application Default Credentials (GKE, Cloud Functions)\n# Pas de config nÃ©cessaire si ADC est configurÃ©\n\n# Lire depuis GCS\ndf = spark.read.parquet(\"gs://my-bucket/silver/data/\")\n\n# Ã‰crire vers GCS\ndf.write.mode(\"overwrite\").parquet(\"gs://my-bucket/gold/aggregates/\")\n\"\"\"\nprint(spark_gcs_config)\n\n\n\n\n8.4 Authentification GCP\n\n\n\n\n\n\n\n\n\nMÃ©thode\nSÃ©curitÃ©\nUsage\nProduction ?\n\n\n\n\nService Account Key\nâš ï¸ Moyenne\nDev, CI/CD\nâš ï¸ Avec prÃ©caution\n\n\nWorkload Identity\nâœ… Haute\nGKE\nâœ… Oui\n\n\nADC (Application Default Credentials)\nâœ… Haute\nCloud Functions, Cloud Run\nâœ… Oui\n\n\n\n\n\nExercice 6 : Lister et tÃ©lÃ©charger depuis GCS\nÃ‰cris un script qui liste tous les fichiers .parquet dans un bucket et tÃ©lÃ©charge le premier.\n\n\nğŸ’¡ Voir la solution\n\nfrom google.cloud import storage\n\nclient = storage.Client()\nbucket = client.bucket(\"my-bucket\")\n\n# Lister les fichiers .parquet\nparquet_files = []\nfor blob in bucket.list_blobs(prefix=\"silver/\"):\n    if blob.name.endswith('.parquet'):\n        parquet_files.append(blob)\n        print(f\"Found: {blob.name}\")\n\n# TÃ©lÃ©charger le premier\nif parquet_files:\n    first_file = parquet_files[0]\n    first_file.download_to_filename(\"downloaded.parquet\")\n    print(f\"Downloaded: {first_file.name}\")",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#minio-object-storage-local",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#minio-object-storage-local",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "9. MinIO â€” Object Storage Local",
    "text": "9. MinIO â€” Object Storage Local\n\n9.1 Pourquoi MinIO ?\n\n\n\nAvantage\nDescription\n\n\n\n\n100% S3 compatible\nMÃªme API, mÃªme code boto3\n\n\nGratuit\nOpen-source, pas de compte cloud\n\n\nLocal\nParfait pour dev/test\n\n\nLÃ©ger\nDocker, un seul binaire\n\n\nProduction-ready\nUtilisÃ© aussi en production (on-premise)\n\n\n\nğŸ’¡ Le code Ã©crit pour MinIO fonctionne sur S3 sans modification !\n   Il suffit de changer l'endpoint.\n\n\n9.2 Installation avec Docker\n\n\nVoir le code\n%%writefile /tmp/minio/docker-compose.yaml\nversion: '3.8'\n\nservices:\n  minio:\n    image: minio/minio:latest\n    container_name: minio\n    ports:\n      - \"9000:9000\"   # API S3\n      - \"9001:9001\"   # Console Web\n    environment:\n      MINIO_ROOT_USER: minioadmin\n      MINIO_ROOT_PASSWORD: minioadmin\n    command: server /data --console-address \":9001\"\n    volumes:\n      - minio-data:/data\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:9000/minio/health/live\"]\n      interval: 30s\n      timeout: 10s\n      retries: 3\n\nvolumes:\n  minio-data:\n\n\n\n\nVoir le code\nminio_commands = \"\"\"\n# DÃ©marrer MinIO\ndocker-compose up -d\n\n# AccÃ©der Ã  la console web\n# http://localhost:9001\n# Login: minioadmin / minioadmin\n\n# Installer mc (MinIO Client)\n# Linux\nwget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\nsudo mv mc /usr/local/bin/\n\n# Mac\nbrew install minio/stable/mc\n\n# Configurer mc\nmc alias set myminio http://localhost:9000 minioadmin minioadmin\n\n# CrÃ©er des buckets\nmc mb myminio/bronze\nmc mb myminio/silver\nmc mb myminio/gold\n\n# Lister\nmc ls myminio/\n\n# Upload\nmc cp data.csv myminio/bronze/\n\"\"\"\nprint(minio_commands)\n\n\n\n\n9.4 Python avec MinIO (boto3)\n\n\nVoir le code\n# Le mÃªme code boto3 fonctionne avec MinIO !\n# Il suffit de spÃ©cifier endpoint_url\n\nminio_boto3_example = \"\"\"\nimport boto3\nfrom botocore.client import Config\n\n# Configuration pour MinIO\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://localhost:9000',  # â† La seule diffÃ©rence !\n    aws_access_key_id='minioadmin',\n    aws_secret_access_key='minioadmin',\n    config=Config(signature_version='s3v4')\n)\n\n# CrÃ©er un bucket\ns3.create_bucket(Bucket='bronze')\n\n# Upload (identique Ã  S3)\ns3.upload_file('data.csv', 'bronze', 'raw/data.csv')\n\n# List (identique Ã  S3)\nresponse = s3.list_objects_v2(Bucket='bronze')\nfor obj in response.get('Contents', []):\n    print(obj['Key'])\n\n# Download (identique Ã  S3)\ns3.download_file('bronze', 'raw/data.csv', 'downloaded.csv')\n\"\"\"\nprint(minio_boto3_example)\nprint(\"\\nğŸ’¡ Ce code fonctionne sur S3 en enlevant juste endpoint_url !\")\n\n\n\n\n9.5 Spark avec MinIO\n\n\nVoir le code\nspark_minio_config = \"\"\"\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName(\"MinIO Access\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\n# Lire depuis MinIO (mÃªme syntaxe que S3 !)\ndf = spark.read.csv(\"s3a://bronze/raw/data.csv\", header=True)\n\n# Ã‰crire vers MinIO\ndf.write.mode(\"overwrite\").parquet(\"s3a://silver/clean/data/\")\n\"\"\"\nprint(spark_minio_config)\n\n\n\n\nExercice 7 : DÃ©ployer MinIO et crÃ©er un bucket\n\nLance MinIO avec Docker\nAccÃ¨de Ã  la console (http://localhost:9001)\nCrÃ©e les buckets bronze, silver, gold\nUpload un fichier via la console\n\n# Solution rapide\ndocker run -d --name minio \\\n  -p 9000:9000 -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#performance-optimisation",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#performance-optimisation",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "10. Performance & Optimisation",
    "text": "10. Performance & Optimisation\n\n10.1 Le problÃ¨me des petits fichiers\nâŒ MAUVAIS : 10,000 fichiers Ã— 1 MB = 10 GB\n   - 10,000 requÃªtes API (LIST + GET)\n   - Overhead Ã©norme pour Spark\n   - Temps de lecture : minutes\n\nâœ… BON : 100 fichiers Ã— 100 MB = 10 GB\n   - 100 requÃªtes API\n   - ParallÃ©lisme optimal\n   - Temps de lecture : secondes\nTaille idÃ©ale : 100 MB - 1 GB par fichier\nSolutions : - df.coalesce(n) ou df.repartition(n) avant Ã©criture - Compaction pÃ©riodique - Delta Lake / Iceberg (Module 23) = compaction automatique\n\n\nVoir le code\n# Ã‰viter les petits fichiers avec Spark\nsmall_files_solution = \"\"\"\n# Lecture de beaucoup de petits fichiers\ndf = spark.read.parquet(\"s3a://bronze/data/\")  # 10,000 fichiers\n\n# âŒ Ã‰criture directe = mÃªme nombre de fichiers\n# df.write.parquet(\"s3a://silver/data/\")\n\n# âœ… Repartitionner avant d'Ã©crire\ndf.repartition(100) \\  # 100 fichiers de ~100 MB\n  .write.mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/data/\")\n\n# âœ… Ou coalesce (moins de shuffle)\ndf.coalesce(100) \\\n  .write.mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/data/\")\n\"\"\"\nprint(small_files_solution)\n\n\n\n\n10.2 Partitionnement\ns3://bucket/silver/sales/\nâ”œâ”€â”€ year=2024/\nâ”‚   â”œâ”€â”€ month=01/\nâ”‚   â”‚   â”œâ”€â”€ day=01/\nâ”‚   â”‚   â”‚   â””â”€â”€ part-00000.parquet\nâ”‚   â”‚   â””â”€â”€ day=02/\nâ”‚   â”‚       â””â”€â”€ part-00000.parquet\nâ”‚   â””â”€â”€ month=02/\nâ”‚       â””â”€â”€ ...\nâ””â”€â”€ year=2023/\n    â””â”€â”€ ...\nAvantages : - Partition pruning (Spark ne lit que les partitions nÃ©cessaires) - RequÃªtes plus rapides\nâš ï¸ Attention au over-partitioning :\n\nTrop de partitions = trop de petits fichiers\nRÃ¨gle : max 10,000 partitions\n\n\n\nVoir le code\n# Partitionnement avec Spark\npartitioning_example = \"\"\"\n# Ã‰criture partitionnÃ©e\ndf.write \\\n  .partitionBy(\"year\", \"month\") \\\n  .mode(\"overwrite\") \\\n  .parquet(\"s3a://silver/sales/\")\n\n# Lecture avec partition pruning\ndf = spark.read.parquet(\"s3a://silver/sales/\")\n\n# Cette requÃªte ne lit QUE year=2024/month=01\ndf.filter(\"year = 2024 AND month = 1\").show()\n\"\"\"\nprint(partitioning_example)\n\n\n\n\n10.3 Formats de fichiers\n\n\n\nFormat\nType\nCompression\nLecture colonnes\nUse case\n\n\n\n\nCSV\nRow\nNon\nâŒ\nÃ‰change, debug\n\n\nJSON\nRow\nNon\nâŒ\nAPIs, logs\n\n\nAvro\nRow\nOui\nâŒ\nStreaming, Kafka\n\n\nParquet\nColumnar\nOui\nâœ…\nAnalytics, Data Lake\n\n\nORC\nColumnar\nOui\nâœ…\nHive, analytics\n\n\n\nğŸ’¡ Pour le Data Engineering : PARQUET est le standard\n   - Compression excellente (snappy, zstd)\n   - Lecture par colonnes\n   - Predicate pushdown\n   - Schema intÃ©grÃ©\n\n\nExercice 8 : Comparer Parquet vs CSV\n\n\nVoir le code\n# Exercice : Comparer la taille et le temps de lecture\n\nformat_comparison = \"\"\"\nimport time\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"Format Comparison\").getOrCreate()\n\n# CrÃ©er un DataFrame de test (1M lignes)\ndf = spark.range(1000000).toDF(\"id\") \\\n    .withColumn(\"name\", lit(\"test_name\")) \\\n    .withColumn(\"amount\", rand() * 1000)\n\n# Ã‰crire en CSV\nstart = time.time()\ndf.write.mode(\"overwrite\").csv(\"s3a://test/csv/\")\ncsv_write_time = time.time() - start\n\n# Ã‰crire en Parquet\nstart = time.time()\ndf.write.mode(\"overwrite\").parquet(\"s3a://test/parquet/\")\nparquet_write_time = time.time() - start\n\n# Lire CSV\nstart = time.time()\ndf_csv = spark.read.csv(\"s3a://test/csv/\").count()\ncsv_read_time = time.time() - start\n\n# Lire Parquet\nstart = time.time()\ndf_parquet = spark.read.parquet(\"s3a://test/parquet/\").count()\nparquet_read_time = time.time() - start\n\nprint(f\"CSV Write: {csv_write_time:.2f}s, Read: {csv_read_time:.2f}s\")\nprint(f\"Parquet Write: {parquet_write_time:.2f}s, Read: {parquet_read_time:.2f}s\")\n\"\"\"\nprint(format_comparison)\nprint(\"\\nğŸ’¡ RÃ©sultat attendu : Parquet 5-10x plus rapide et 5-10x plus petit\")\n\n\n\n\nExercice 9 : Impact de la taille des fichiers\nObjectif : Mesurer lâ€™impact du nombre de fichiers sur les performances.\n# ScÃ©nario A : 1000 petits fichiers\ndf.repartition(1000).write.parquet(\"s3a://test/small-files/\")\n\n# ScÃ©nario B : 10 gros fichiers\ndf.repartition(10).write.parquet(\"s3a://test/large-files/\")\n\n# Mesurer le temps de lecture pour chaque\nRÃ©sultat attendu : ScÃ©nario B sera beaucoup plus rapide.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#sÃ©curitÃ©-gouvernance",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#sÃ©curitÃ©-gouvernance",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "11. SÃ©curitÃ© & Gouvernance",
    "text": "11. SÃ©curitÃ© & Gouvernance\n\n11.1 Encryption\n\n\n\nType\nDescription\nQui gÃ¨re la clÃ© ?\n\n\n\n\nSSE-S3\nEncryption cÃ´tÃ© serveur, clÃ© AWS\nAWS\n\n\nSSE-KMS\nEncryption avec AWS KMS\nAWS (tu choisis la clÃ©)\n\n\nSSE-C\nEncryption avec clÃ© client\nToi\n\n\nClient-side\nEncryption avant upload\nToi\n\n\n\nğŸ’¡ Best Practice : SSE-KMS pour la plupart des cas\n   - Rotation automatique des clÃ©s\n   - Audit dans CloudTrail\n   - ContrÃ´le d'accÃ¨s fin\n\n\n11.2 Access Control\n\n\nVoir le code\n# Exemple de bucket policy S3\nbucket_policy_example = \"\"\"\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"AllowDataTeamRead\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::123456789:role/DataEngineerRole\"\n            },\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::my-datalake\",\n                \"arn:aws:s3:::my-datalake/*\"\n            ]\n        },\n        {\n            \"Sid\": \"DenyPublicAccess\",\n            \"Effect\": \"Deny\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:*\",\n            \"Resource\": \"arn:aws:s3:::my-datalake/*\",\n            \"Condition\": {\n                \"Bool\": {\n                    \"aws:SecureTransport\": \"false\"\n                }\n            }\n        }\n    ]\n}\n\"\"\"\nprint(bucket_policy_example)\n\n\n\n\n11.3 Audit & Compliance\n\n\n\nService\nCloud\nCe quâ€™il trace\n\n\n\n\nS3 Access Logs\nAWS\nQui accÃ¨de Ã  quoi\n\n\nCloudTrail\nAWS\nToutes les API calls\n\n\nActivity Logs\nAzure\nOpÃ©rations sur Blob\n\n\nAudit Logs\nGCP\nAccÃ¨s aux ressources",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#coÃ»ts-optimisation-financiÃ¨re",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#coÃ»ts-optimisation-financiÃ¨re",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "12. CoÃ»ts & Optimisation financiÃ¨re",
    "text": "12. CoÃ»ts & Optimisation financiÃ¨re\n\n12.1 Composantes du coÃ»t\n\n\n\nComposante\nDescription\nExemple S3\n\n\n\n\nStockage\nGB/mois\n$0.023/GB (Standard)\n\n\nPUT/POST\nÃ‰critures\n$0.005 / 1000 requÃªtes\n\n\nGET\nLectures\n$0.0004 / 1000 requÃªtes\n\n\nLIST\nListing\n$0.005 / 1000 requÃªtes\n\n\nEgress\nSortie du cloud\n$0.09/GB (vers Internet)\n\n\n\n\n\n12.2 CoÃ»ts cachÃ©s\nâš ï¸ Attention aux coÃ»ts cachÃ©s :\n\n1. LISTING frÃ©quent\n   - Spark fait un LIST avant chaque lecture\n   - 1M de fichiers = 1000 LIST calls = $5\n\n2. EGRESS\n   - DonnÃ©es sortant du cloud = coÃ»teux\n   - Cross-region = $0.02/GB\n   - Vers Internet = $0.09/GB\n\n3. Small files\n   - Plus de requÃªtes API\n   - Plus de listing\n\n\n12.3 Lifecycle Policies\n\n\nVoir le code\n# Exemple de lifecycle policy S3\nlifecycle_policy = \"\"\"\n{\n    \"Rules\": [\n        {\n            \"ID\": \"ArchiveBronzeData\",\n            \"Status\": \"Enabled\",\n            \"Filter\": {\n                \"Prefix\": \"bronze/\"\n            },\n            \"Transitions\": [\n                {\n                    \"Days\": 30,\n                    \"StorageClass\": \"STANDARD_IA\"\n                },\n                {\n                    \"Days\": 90,\n                    \"StorageClass\": \"GLACIER\"\n                }\n            ],\n            \"Expiration\": {\n                \"Days\": 365\n            }\n        }\n    ]\n}\n\"\"\"\nprint(lifecycle_policy)\n\n\n\n\nExercice 10 : Estimer le coÃ»t mensuel dâ€™un Data Lake\nScÃ©nario :\n\nBronze : 1 TB (accÃ¨s rare)\nSilver : 500 GB (accÃ¨s frÃ©quent)\nGold : 100 GB (accÃ¨s trÃ¨s frÃ©quent)\n100,000 requÃªtes GET/jour\n10,000 requÃªtes PUT/jour\n50 GB egress/mois\n\nCalcule le coÃ»t mensuel sur AWS S3.\n\n\nğŸ’¡ Voir la solution\n\n\n\n\nÃ‰lÃ©ment\nCalcul\nCoÃ»t\n\n\n\n\nBronze (S3 IA)\n1000 GB Ã— $0.0125\n$12.50\n\n\nSilver (Standard)\n500 GB Ã— $0.023\n$11.50\n\n\nGold (Standard)\n100 GB Ã— $0.023\n$2.30\n\n\nGET requests\n100K Ã— 30 / 1000 Ã— $0.0004\n$1.20\n\n\nPUT requests\n10K Ã— 30 / 1000 Ã— $0.005\n$1.50\n\n\nEgress\n50 GB Ã— $0.09\n$4.50\n\n\nTotal\n\n~$33.50/mois",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#mini-projet-data-lake-avec-minio",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#mini-projet-data-lake-avec-minio",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "13. Mini-Projet : Data Lake avec MinIO",
    "text": "13. Mini-Projet : Data Lake avec MinIO\n\nObjectif\nConstruire un Data Lake local complet avec MinIO.\n\n\nArchitecture\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                       MinIO (Docker)                            â”‚\nâ”‚                                                                 â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚\nâ”‚  â”‚   bronze/   â”‚    â”‚   silver/   â”‚    â”‚    gold/    â”‚        â”‚\nâ”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\nâ”‚  â”‚  raw.csv    â”‚â”€â”€â”€â–¶â”‚ clean.parq  â”‚â”€â”€â”€â–¶â”‚  agg.parq   â”‚        â”‚\nâ”‚  â”‚             â”‚    â”‚             â”‚    â”‚             â”‚        â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚\nâ”‚        â–²                  â–²                  â–²                  â”‚\nâ”‚        â”‚                  â”‚                  â”‚                  â”‚\nâ”‚     Upload            Transform          Aggregate              â”‚\nâ”‚    (boto3)           (PySpark)         (Spark SQL)             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nÃ‰tapes du projet\n\n\nVoir le code\n# Ã‰tape 1 : DÃ©marrer MinIO\nprint(\"\"\"\n# Terminal 1 : DÃ©marrer MinIO\ndocker run -d --name minio \\\n  -p 9000:9000 -p 9001:9001 \\\n  -e MINIO_ROOT_USER=minioadmin \\\n  -e MINIO_ROOT_PASSWORD=minioadmin \\\n  minio/minio server /data --console-address \":9001\"\n\n# VÃ©rifier que MinIO tourne\ncurl http://localhost:9000/minio/health/live\n\"\"\")\n\n\n\n\nVoir le code\n# Ã‰tape 2 : CrÃ©er les buckets et uploader les donnÃ©es\n\nstep2_code = \"\"\"\nimport boto3\nfrom botocore.client import Config\nimport csv\n\n# Connexion Ã  MinIO\ns3 = boto3.client(\n    's3',\n    endpoint_url='http://localhost:9000',\n    aws_access_key_id='minioadmin',\n    aws_secret_access_key='minioadmin',\n    config=Config(signature_version='s3v4')\n)\n\n# CrÃ©er les buckets\nfor bucket in ['bronze', 'silver', 'gold']:\n    try:\n        s3.create_bucket(Bucket=bucket)\n        print(f\"âœ… Bucket '{bucket}' crÃ©Ã©\")\n    except Exception as e:\n        print(f\"âš ï¸ Bucket '{bucket}' existe dÃ©jÃ \")\n\n# CrÃ©er des donnÃ©es de test\nwith open('sales_data.csv', 'w', newline='') as f:\n    writer = csv.writer(f)\n    writer.writerow(['date', 'product', 'category', 'amount', 'quantity'])\n    writer.writerow(['2024-01-01', 'Laptop', 'Electronics', 1200, 1])\n    writer.writerow(['2024-01-01', 'Mouse', 'Electronics', 25, 3])\n    writer.writerow(['2024-01-02', 'Desk', 'Furniture', 350, 1])\n    writer.writerow(['2024-01-02', 'Chair', 'Furniture', 150, 2])\n    writer.writerow(['2024-01-03', 'Laptop', 'Electronics', 1200, 2])\n    writer.writerow(['2024-01-03', 'Monitor', 'Electronics', 400, 1])\n\n# Upload vers bronze\ns3.upload_file('sales_data.csv', 'bronze', 'raw/sales_data.csv')\nprint(\"âœ… DonnÃ©es uploadÃ©es vers bronze/raw/sales_data.csv\")\n\"\"\"\nprint(step2_code)\n\n\n\n\nVoir le code\n# Ã‰tape 3 : Transformer avec PySpark (Bronze â†’ Silver)\n\nstep3_code = \"\"\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, to_date, upper\n\n# CrÃ©er SparkSession avec config MinIO\nspark = SparkSession.builder \\\n    .appName(\"Bronze to Silver\") \\\n    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n    .getOrCreate()\n\n# Lire depuis Bronze\ndf_bronze = spark.read.csv(\n    \"s3a://bronze/raw/sales_data.csv\",\n    header=True,\n    inferSchema=True\n)\n\nprint(\"ğŸ“¥ DonnÃ©es Bronze:\")\ndf_bronze.show()\n\n# Transformer\ndf_silver = df_bronze \\\n    .withColumn(\"date\", to_date(col(\"date\"))) \\\n    .withColumn(\"category\", upper(col(\"category\"))) \\\n    .withColumn(\"total\", col(\"amount\") * col(\"quantity\")) \\\n    .dropDuplicates()\n\nprint(\"ğŸ”„ DonnÃ©es transformÃ©es:\")\ndf_silver.show()\n\n# Ã‰crire vers Silver (Parquet partitionnÃ©)\ndf_silver.write \\\n    .mode(\"overwrite\") \\\n    .partitionBy(\"category\") \\\n    .parquet(\"s3a://silver/sales/\")\n\nprint(\"âœ… DonnÃ©es Ã©crites vers silver/sales/\")\n\"\"\"\nprint(step3_code)\n\n\n\n\nVoir le code\n# Ã‰tape 4 : AgrÃ©ger avec Spark SQL (Silver â†’ Gold)\n\nstep4_code = \"\"\"\n# Lire depuis Silver\ndf_silver = spark.read.parquet(\"s3a://silver/sales/\")\n\n# CrÃ©er une vue temporaire\ndf_silver.createOrReplaceTempView(\"sales\")\n\n# AgrÃ©gation avec Spark SQL\ndf_gold = spark.sql(\\\"\\\"\\\"\n    SELECT \n        category,\n        COUNT(*) as num_transactions,\n        SUM(quantity) as total_quantity,\n        SUM(total) as total_revenue,\n        AVG(total) as avg_transaction\n    FROM sales\n    GROUP BY category\n    ORDER BY total_revenue DESC\n\\\"\\\"\\\")\n\nprint(\"ğŸ“Š AgrÃ©gations Gold:\")\ndf_gold.show()\n\n# Ã‰crire vers Gold\ndf_gold.coalesce(1) \\\n    .write \\\n    .mode(\"overwrite\") \\\n    .parquet(\"s3a://gold/category_summary/\")\n\nprint(\"âœ… DonnÃ©es Ã©crites vers gold/category_summary/\")\n\"\"\"\nprint(step4_code)\n\n\n\n\nVoir le code\n# Ã‰tape 5 : VÃ©rifier les rÃ©sultats\n\nstep5_code = \"\"\"\n# Lister les fichiers crÃ©Ã©s\nimport subprocess\n\n# Avec mc CLI\nprint(\"ğŸ“ Contenu de bronze/:\")\n!mc ls myminio/bronze/ --recursive\n\nprint(\"\\nğŸ“ Contenu de silver/:\")\n!mc ls myminio/silver/ --recursive\n\nprint(\"\\nğŸ“ Contenu de gold/:\")\n!mc ls myminio/gold/ --recursive\n\n# Ou avec boto3\nfor bucket in ['bronze', 'silver', 'gold']:\n    print(f\"\\nğŸ“ {bucket}/\")\n    response = s3.list_objects_v2(Bucket=bucket)\n    for obj in response.get('Contents', []):\n        print(f\"   {obj['Key']} ({obj['Size']} bytes)\")\n\"\"\"\nprint(step5_code)",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#quiz-de-fin-de-module",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#quiz-de-fin-de-module",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "Quiz de fin de module",
    "text": "Quiz de fin de module\n\n\nâ“ Q1. Quelle est la diffÃ©rence entre un prefix et un dossier dans S3 ?\n\nAucune diffÃ©rence\n\nUn prefix est un vrai dossier crÃ©Ã© par S3\n\nUn prefix est juste une convention de nommage, S3 est un key-value store plat\n\nUn dossier peut contenir des sous-dossiers, pas un prefix\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” S3 est un key-value store plat. Les â€œ/â€ dans les clÃ©s sont juste des caractÃ¨res comme les autres.\n\n\n\n\nâ“ Q2. Quel protocole utiliser pour lire S3 avec Spark ?\n\ns3://\n\ns3a://\n\nhttps://\n\nhdfs://\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” s3a:// est le protocole Hadoop optimisÃ© pour Spark. s3:// est pour AWS CLI/boto3.\n\n\n\n\nâ“ Q3. Pourquoi les petits fichiers sont-ils un problÃ¨me ?\n\nIls prennent plus de place\n\nIls gÃ©nÃ¨rent trop de requÃªtes API et dâ€™overhead\n\nIls ne sont pas supportÃ©s par Parquet\n\nIls ne peuvent pas Ãªtre partitionnÃ©s\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Chaque fichier = une requÃªte API. 10,000 fichiers = 10,000 GET requests = lent et coÃ»teux.\n\n\n\n\nâ“ Q4. Quelle est la diffÃ©rence entre SAS Token et Managed Identity sur Azure ?\n\nSAS Token est plus sÃ©curisÃ©\n\nManaged Identity est temporaire, SAS est permanent\n\nSAS Token est temporaire et partageable, Managed Identity est liÃ©e Ã  une ressource Azure\n\nAucune diffÃ©rence\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” SAS Token = URL temporaire partageable. Managed Identity = identitÃ© attachÃ©e Ã  une VM/AKS, plus sÃ©curisÃ©.\n\n\n\n\nâ“ Q5. Pourquoi MinIO est-il compatible avec S3 ?\n\nCâ€™est un produit AWS\n\nIl implÃ©mente la mÃªme API REST que S3\n\nIl utilise les mÃªmes serveurs\n\nIl copie les donnÃ©es depuis S3\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” MinIO implÃ©mente lâ€™API S3 (REST). Le mÃªme code boto3 fonctionne avec les deux.\n\n\n\n\nâ“ Q6. Quelle classe de stockage pour des donnÃ©es rarement lues ?\n\nS3 Standard\n\nS3 Intelligent-Tiering\n\nS3 Glacier\n\nS3 One Zone-IA\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Glacier pour lâ€™archivage (donnÃ©es rarement lues). Intelligent-Tiering si le pattern dâ€™accÃ¨s est imprÃ©visible.\n\n\n\n\nâ“ Q7. Quel est lâ€™avantage du partitionnement dans un Data Lake ?\n\nLes fichiers sont plus petits\n\nSpark peut ignorer les partitions non pertinentes (partition pruning)\n\nLe stockage coÃ»te moins cher\n\nLes donnÃ©es sont automatiquement compressÃ©es\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Partition pruning = Spark lit uniquement les partitions qui matchent le filtre.\n\n\n\n\nâ“ Q8. Comment authentifier Spark on K8s vers S3 en production ?\n\nAccess Keys dans le code\n\nVariables dâ€™environnement\n\nIAM Roles for Service Accounts (IRSA)\n\nFichier de config local\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” IRSA permet dâ€™associer un IAM Role Ã  un ServiceAccount K8s. Pas de credentials dans le code.\n\n\n\n\nâ“ Q9. Quâ€™est-ce quâ€™un Metadata Catalog (Glue, Hive Metastore) ?\n\nUn systÃ¨me de stockage\n\nUn registre des schÃ©mas et partitions des tables\n\nUn outil de requÃªtage SQL\n\nUn systÃ¨me de cache\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : b â€” Le catalog stocke les mÃ©tadonnÃ©es : schÃ©ma, localisation, partitions, stats. Lâ€™Object Storage ne stocke que les fichiers bruts.\n\n\n\n\nâ“ Q10. Quel format de fichier est recommandÃ© pour un Data Lake analytique ?\n\nCSV\n\nJSON\n\nParquet\n\nXML\n\n\n\nğŸ’¡ Voir la rÃ©ponse\n\nâœ… RÃ©ponse : c â€” Parquet est columnar, compressÃ©, avec schema intÃ©grÃ©. IdÃ©al pour lâ€™analytics.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#ressources-pour-aller-plus-loin",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#ressources-pour-aller-plus-loin",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "ğŸ“š Ressources pour aller plus loin",
    "text": "ğŸ“š Ressources pour aller plus loin\n\nğŸŒ Documentation officielle\n\nAWS S3 Documentation\nAzure Blob Storage\nGoogle Cloud Storage\nMinIO Documentation\n\n\n\nğŸ“– Articles & Tutoriels\n\nSpark + S3 Best Practices\nData Lake Architecture",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  },
  {
    "objectID": "notebooks/intermediate/22_cloud_and_object_storage.html#prochaine-Ã©tape",
    "href": "notebooks/intermediate/22_cloud_and_object_storage.html#prochaine-Ã©tape",
    "title": "Cloud & Object Storage for Data Engineers",
    "section": "â¡ï¸ Prochaine Ã©tape",
    "text": "â¡ï¸ Prochaine Ã©tape\nMaintenant que tu maÃ®trises lâ€™Object Storage, passons aux Table Formats pour transformer ton Data Lake en Lakehouse !\nğŸ‘‰ Module suivant : 23_table_formats_delta_iceberg â€” Delta Lake & Apache Iceberg\nTu vas apprendre :\n\nDelta Lake : ACID, Time Travel, Schema Evolution\nApache Iceberg : Table format open-source\nTransaction Log : Comment Ã§a remplace le Metastore\nOptimisations : Compaction, Z-Ordering, Vacuum\n\n\n\nğŸ“ RÃ©capitulatif de ce module\n\n\n\nConcept\nCe que tu as appris\n\n\n\n\nCloud Computing\nIaaS, PaaS, SaaS, rÃ©gions\n\n\nServices Cloud DE\nS3, Glue, BigQuery, Synapseâ€¦\n\n\nStorage Models\nBlock vs File vs Object\n\n\nObject Storage\nBuckets, keys, prefixes, protocols\n\n\nAWS S3\nboto3, s3a://, IAM\n\n\nAzure Blob\nSDK, abfss://, SAS, Managed Identity\n\n\nGCS\ngsutil, gs://, Workload Identity\n\n\nMinIO\nObject Storage local S3-compatible\n\n\nPerformance\nSmall files, partitioning, formats\n\n\nSÃ©curitÃ©\nEncryption, bucket policies\n\n\nCoÃ»ts\nClasses de stockage, lifecycle\n\n\n\n\nğŸ‰ FÃ©licitations ! Tu as terminÃ© le module Cloud & Object Storage.",
    "crumbs": [
      "ğŸ  Accueil",
      "ğŸŸ© Niveau 2 : IntermÃ©diaire",
      "ğŸ  Lakehouse & Streaming & Cloud",
      "22 Â· Cloud Object Storage"
    ]
  }
]